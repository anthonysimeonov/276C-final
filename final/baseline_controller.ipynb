{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline PPO Controller\n",
    "Train a PPO baseline policy.\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:  \n",
    "<img src=\"./notebookImages/invertedpendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"./notebookImages/halfcheetah.png\" width=\"300\">\n",
    "\n",
    "#### Ant environment:\n",
    "<img src=\"./notebookImages/ant.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from ppo import *\n",
    "import datetime\n",
    "\n",
    "import gym\n",
    "import my_envs\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From original PPO code, is used to setup multiprocessing environments\n",
    "def make_env(env_name):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Constants and flags</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE = False #Flag to save videos\n",
    "logging_interval = 10 #Freqeuncy of video saving\n",
    "COMPENSATION = False #Flag to implement compensation in test function\n",
    "EARLY_STOPPING = True #Flag to stop when reward reaches threshold\n",
    "num_envs = 4 #Number of environments to train across in multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "\n",
    "# env_name = 'InvertedPendulum-v2'\n",
    "# env_key = \"v10\"   #Unique identifier for custom envs (case sensitive)\n",
    "\n",
    "env_name = 'HalfCheetah-v2'\n",
    "env_key = 'v12'\n",
    "\n",
    "\"\"\"\"\n",
    "['InvertedPendulum-v2',\n",
    " 'InvertedPendulumModified-base-v10',\n",
    " 'InvertedPendulumModified-mass-v10',\n",
    " 'InvertedPendulumModified-inertia-v10',\n",
    " 'InvertedPendulumModified-friction-v10',\n",
    " 'InvertedPendulumModified-tilt-v10',\n",
    " 'InvertedPendulumModified-motor-v10',\n",
    " 'InvertedPendulumModified-multi-v10']\n",
    " \"\"\"\n",
    "\n",
    "env_ids = [spec.id for spec in envs.registry.all()]\n",
    "test_env_names = ['HalfCheetah-v2'] + [x for x in env_ids if str(env_key) in x] #Returns a list of environment names matching identifier\n",
    "training_env_index = test_env_names.index(env_name)\n",
    "\n",
    "#Training envs (all the same)\n",
    "envs = [make_env(env_name) for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "#Plotting Results and figures, save weights\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "time_stamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "results_dir = os.path.join(script_dir, 'baseline_plots/' + env_name + time_stamp + '/')\n",
    "baseline_dir = os.path.join(script_dir, 'baseline_weights/' + env_name + time_stamp + '/')\n",
    "\n",
    "\n",
    "print(script_dir)\n",
    "print(time_stamp)\n",
    "print(results_dir)\n",
    "print(baseline_dir)\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.mkdir(results_dir)\n",
    "    \n",
    "if not os.path.isdir(baseline_dir):\n",
    "    os.mkdir(baseline_dir)\n",
    "    \n",
    "#Testing on original and new envs\n",
    "tests = testing_envs(test_env_names, VISUALIZE, COMPENSATION, results_dir, training_env_index, logging_interval = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training baseline PPO controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_inputs  = envs.observation_space.shape[0]\n",
    "num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 64\n",
    "lr               = 3e-4\n",
    "num_steps        = 2048\n",
    "mini_batch_size  = 64\n",
    "ppo_epochs       = 10\n",
    "threshold_reward = 1600\n",
    "\n",
    "#PPO object\n",
    "ppo_baseline = PPO(num_inputs, \n",
    "                   num_outputs, \n",
    "                   hidden_size=hidden_size, \n",
    "                   num_steps=num_steps, \n",
    "                   threshold_reward=threshold_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 1000000\n",
    "test_avg_rewards = []\n",
    "test_stds = []\n",
    "test_itrs = 10\n",
    "save_interval = 5 #How often to save weights and figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False #From original ppo code, flag used to end loop when stopping condition hit\n",
    "ppo_updates = 0 #ppo update counter\n",
    "\n",
    "#Which plotting figues to plot\n",
    "indvplots=0; rewplots=1; stdplots=1\n",
    "which_plts = [indvplots,rewplots,stdplots]\n",
    "\n",
    "while ppo_baseline.frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    #collect data\n",
    "    log_probs, values, states, actions, rewards, masks, next_value = ppo_baseline.collect_data(envs)\n",
    "    \n",
    "    #compute gae\n",
    "    returns = ppo_baseline.compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    #update policy\n",
    "    ppo_baseline.ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, values)\n",
    "    \n",
    "    #plot\n",
    "    avg_rew = []\n",
    "    std = []\n",
    "    \n",
    "    #Environment testing and data logging\n",
    "    #***************************************************************************************\n",
    "    for env in tests.envs:\n",
    "        env_rewards = ([tests.test_env(env, ppo_baseline.model) for _ in range(test_itrs)])\n",
    "        avg_rew.append(np.mean(env_rewards))\n",
    "        std.append(np.std(env_rewards))\n",
    "\n",
    "    test_avg_rewards.append(avg_rew)\n",
    "    test_stds.append(std)\n",
    "\n",
    "    if avg_rew[training_env_index] > threshold_reward and EARLY_STOPPING: #avg_rew[0] is testing on the non edited environment\n",
    "        tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 1)\n",
    "        early_stop = True\n",
    "    else:\n",
    "        if ppo_updates and ppo_updates % save_interval == 0:\n",
    "            ppo_baseline.save_weights(baseline_dir + env_name + '_weights' + str(ppo_updates/save_interval))\n",
    "            tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 1, str(ppo_updates/save_interval))\n",
    "        else:\n",
    "            tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 0)\n",
    "            \n",
    "    ppo_updates = ppo_updates + 1 #Loop counter\n",
    "    #***************************************************************************************\n",
    "\n",
    "ppo_baseline.save_weights(baseline_dir + env_name + '_endweights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# All above code in a loop over environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# coding: utf-8\n",
    "\n",
    "# # Baseline PPO Controller\n",
    "# Train a PPO baseline policy.\n",
    "# ### Environments:\n",
    "# \n",
    "# #### InvertedPendulum-v2 environment:  \n",
    "# <img src=\"./notebookImages/invertedpendulum.png\" width=\"300\">\n",
    "# \n",
    "# #### Halfcheetah-v2 environment:\n",
    "# <img src=\"./notebookImages/halfcheetah.png\" width=\"300\">\n",
    "# \n",
    "# #### Ant environment:\n",
    "# <img src=\"./notebookImages/ant.png\" width=\"300\">\n",
    "\n",
    "# <h2>Imports</h2>\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import math\n",
    "import random\n",
    "import os\n",
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from ppo import *\n",
    "import datetime\n",
    "\n",
    "import gym\n",
    "import my_envs\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "\n",
    "\n",
    "# <h2>Use CUDA</h2>\n",
    "\n",
    "# In[2]:\n",
    "\n",
    "\n",
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "\n",
    "# <h2>Create Environments</h2>\n",
    "\n",
    "# In[3]:\n",
    "\n",
    "\n",
    "#From original PPO code, is used to setup multiprocessing environments\n",
    "def make_env(env_name):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "    return _thunk\n",
    "\n",
    "\n",
    "# <h2>Constants and flags</h2>\n",
    "\n",
    "# In[4]:\n",
    "\n",
    "\n",
    "VISUALIZE = False #Flag to save videos\n",
    "logging_interval = 10 #Freqeuncy of video saving\n",
    "COMPENSATION = False #Flag to implement compensation in test function\n",
    "EARLY_STOPPING = True #Flag to stop when reward reaches threshold\n",
    "num_envs = 4 #Number of environments to train across in multiprocessing\n",
    "\n",
    "\n",
    "# In[10]:\n",
    "\n",
    "\n",
    "from gym import envs\n",
    "\n",
    "#env_name = 'InvertedPendulumModified-multi-v10'\n",
    "#env_name = \"Pendulum-v0\"\n",
    "#env_name = \"HalfCheetah-v2\"\n",
    "#env_name = 'FetchReach-v1'\n",
    "# env_name = 'InvertedPendulum-v2'\n",
    "# env_key = \"v10\" #Unique identifier for custom envs (case sensitive)\n",
    "\n",
    "env_name = 'HalfCheetah-v2'\n",
    "env_key = \"v12\"\n",
    "\n",
    "\"\"\"\"\n",
    "['InvertedPendulum-v2',\n",
    " 'InvertedPendulumModified-base-v10',\n",
    " 'InvertedPendulumModified-mass-v10',\n",
    " 'InvertedPendulumModified-inertia-v10',\n",
    " 'InvertedPendulumModified-friction-v10',\n",
    " 'InvertedPendulumModified-tilt-v10',\n",
    " 'InvertedPendulumModified-motor-v10',\n",
    " 'InvertedPendulumModified-multi-v10']\n",
    " \"\"\"\n",
    "\n",
    "env_ids = [spec.id for spec in envs.registry.all()]\n",
    "test_env_names = ['HalfCheetah-v2'] + [x for x in env_ids if str(env_key) in x] #Returns a list of environment names matching identifier\n",
    "\n",
    "for env_name in test_env_names:\n",
    "\n",
    "    training_env_index = test_env_names.index(env_name)\n",
    "\n",
    "    #Training envs (all the same)\n",
    "    envs = [make_env(env_name) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(envs)\n",
    "\n",
    "    #Plotting Results and figures, save weights\n",
    "\n",
    "    script_dir = os.getcwd()\n",
    "    time_stamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "    results_dir = os.path.join(script_dir, 'baseline_plots/' + env_name + time_stamp + '/')\n",
    "    baseline_dir = os.path.join(script_dir, 'baseline_weights/' + env_name + time_stamp + '/')\n",
    "    \n",
    "\n",
    "    print(script_dir)\n",
    "    print(time_stamp)\n",
    "    print(results_dir)\n",
    "    print(baseline_dir)\n",
    "\n",
    "    \n",
    "\n",
    "    if not os.path.isdir(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "        \n",
    "    if not os.path.isdir(baseline_dir):\n",
    "        os.mkdir(baseline_dir)\n",
    "        \n",
    "    #Testing on original and new envs\n",
    "    tests = testing_envs(test_env_names, VISUALIZE, COMPENSATION, results_dir, training_env_index, logging_interval = 10)\n",
    "\n",
    "\n",
    "    # ## Training baseline PPO controller\n",
    "\n",
    "    # In[6]:\n",
    "\n",
    "\n",
    "    num_inputs  = envs.observation_space.shape[0]\n",
    "    num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "    #Hyper params:\n",
    "    hidden_size      = 64\n",
    "    lr               = 3e-4\n",
    "    num_steps        = 2048\n",
    "    mini_batch_size  = 64\n",
    "    ppo_epochs       = 10\n",
    "    threshold_reward = 1500\n",
    "\n",
    "    #PPO object\n",
    "    ppo_baseline = PPO(num_inputs, \n",
    "                       num_outputs, \n",
    "                       hidden_size=hidden_size, \n",
    "                       num_steps=num_steps,\n",
    "                       threshold_reward=threshold_reward\n",
    "                      )\n",
    "\n",
    "\n",
    "    # In[7]:\n",
    "\n",
    "\n",
    "    max_frames = 1000000\n",
    "    test_avg_rewards = []\n",
    "    test_stds = []\n",
    "    test_itrs = 20\n",
    "    save_interval = 5 #How often to save weights and figures\n",
    "\n",
    "\n",
    "    # In[8]:\n",
    "\n",
    "\n",
    "    state = envs.reset()\n",
    "    early_stop = False #From original ppo code, flag used to end loop when stopping condition hit\n",
    "    ppo_updates = 0 #ppo update counter\n",
    "\n",
    "    #Which plotting figues to plot\n",
    "    indvplots=0; rewplots=1; stdplots=1\n",
    "    which_plts = [indvplots,rewplots,stdplots]\n",
    "\n",
    "    while ppo_baseline.frame_idx < max_frames and not early_stop:\n",
    "\n",
    "        #collect data\n",
    "        log_probs, values, states, actions, rewards, masks, next_value = ppo_baseline.collect_data(envs)\n",
    "        \n",
    "        #compute gae\n",
    "        returns = ppo_baseline.compute_gae(next_value, rewards, masks, values)\n",
    "        \n",
    "        #update policy\n",
    "        ppo_baseline.ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, values)\n",
    "        \n",
    "        #plot\n",
    "        avg_rew = []\n",
    "        std = []\n",
    "        \n",
    "        #Environment testing and data logging\n",
    "        #***************************************************************************************\n",
    "        for env in tests.envs:\n",
    "            env_rewards = ([tests.test_env(env, ppo_baseline.model) for _ in range(test_itrs)])\n",
    "            avg_rew.append(np.mean(env_rewards))\n",
    "            std.append(np.std(env_rewards))\n",
    "\n",
    "        test_avg_rewards.append(avg_rew)\n",
    "        test_stds.append(std)\n",
    "\n",
    "        if avg_rew[training_env_index] > threshold_reward and EARLY_STOPPING: #avg_rew[0] is testing on the non edited environment\n",
    "            tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 1)\n",
    "            early_stop = True\n",
    "        else:\n",
    "            if ppo_updates and ppo_updates % save_interval == 0:\n",
    "                ppo_baseline.save_weights(baseline_dir + env_name + '_weights' + str(ppo_updates/save_interval))\n",
    "                tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 1, str(ppo_updates/save_interval))\n",
    "            else:\n",
    "                tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 0)\n",
    "                \n",
    "        ppo_updates = ppo_updates + 1 #Loop counter\n",
    "        #***************************************************************************************\n",
    "\n",
    "    ppo_baseline.save_weights(baseline_dir + env_name + '_endweights')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
