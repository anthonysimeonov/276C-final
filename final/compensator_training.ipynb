{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  PPO Compensator Controller\n",
    "Train a PPO compensaor policy using a baseline controller.\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:  \n",
    "<img src=\"./notebookImages/invertedpendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"./notebookImages/halfcheetah.png\" width=\"300\">\n",
    "\n",
    "#### Ant environment:\n",
    "<img src=\"./notebookImages/ant.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from ppo import *\n",
    "import datetime\n",
    "\n",
    "import gym\n",
    "import my_envs\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From original PPO code, is used to setup multiprocessing environments\n",
    "def make_env(env_name):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VISUALIZE = False #Flag to save videos\n",
    "logging_interval = 10 #Freqeuncy of video saving\n",
    "COMPENSATION = True #Flag to implement compensation in test function\n",
    "ACTION_APPEND = True #Flag to indicate to train with appended action or not\n",
    "EARLY_STOPPING = True #Flag to stop when reward reaches threshold\n",
    "num_envs = 4 #Number of environments to train across in multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "\n",
    "#env_name = 'InvertedPendulumModified-multi-v10'\n",
    "#env_name = \"Pendulum-v0\"\n",
    "#env_name = \"HalfCheetah-v2\"\n",
    "#env_name = 'FetchReach-v1'\n",
    "\n",
    "# env_name = 'InvertedPendulumModified-multi-v10'\n",
    "# env_key = \"v10\" #Unique identifier for custom envs (case sensitive)\n",
    "\n",
    "env_name = 'HalfCheetahModified-multi-v12'\n",
    "env_key = \"v12\" #Unique identifier for custom envs (case sensitive)\n",
    "\n",
    "policy_env = 'HalfCheetahBase'\n",
    "\n",
    "\"\"\"\"\n",
    "['InvertedPendulum-v2',\n",
    " 'InvertedPendulumModified-base-v10',\n",
    " 'InvertedPendulumModified-mass-v10',\n",
    " 'InvertedPendulumModified-inertia-v10',\n",
    " 'InvertedPendulumModified-friction-v10',\n",
    " 'InvertedPendulumModified-tilt-v10',\n",
    " 'InvertedPendulumModified-motor-v10',\n",
    " 'InvertedPendulumModified-multi-v10']\n",
    " \"\"\"\n",
    "\n",
    "env_ids = [spec.id for spec in envs.registry.all()]\n",
    "test_env_names = ['HalfCheetah-v2'] + [x for x in env_ids if str(env_key) in x] #Returns a list of environment names matching identifier\n",
    "\n",
    "print(test_env_names)\n",
    "\n",
    "for env_name in test_env_names:#[3:-1]:\n",
    "    training_env_index = test_env_names.index(env_name)\n",
    "\n",
    "    #Training envs (all the same)\n",
    "    envs = [make_env(env_name) for i in range(num_envs)]\n",
    "    envs = SubprocVecEnv(envs)\n",
    "\n",
    "    #Plotting Results and figures, save weights\n",
    "    script_dir = os.getcwd()\n",
    "    time_stamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "    results_dir = os.path.join(script_dir, 'compensator_plots/' + env_name + time_stamp + '/')\n",
    "    baseline_dir = os.path.join(script_dir, 'compensator_weights/' + env_name + time_stamp + '/')\n",
    "\n",
    "    if not os.path.isdir(results_dir):\n",
    "        os.mkdir(results_dir)\n",
    "\n",
    "    if not os.path.isdir(baseline_dir):\n",
    "        os.mkdir(baseline_dir)\n",
    "\n",
    "    #Testing on original and new envs\n",
    "    tests = testing_envs(test_env_names, VISUALIZE, COMPENSATION, results_dir, training_env_index, logging_interval = 10)\n",
    "\n",
    "    num_inputs  = envs.observation_space.shape[0]\n",
    "    num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "    #Hyper params:\n",
    "    hidden_size      = 64\n",
    "    lr               = 3e-4\n",
    "    num_steps        = 2048\n",
    "    mini_batch_size  = 64\n",
    "    ppo_epochs       = 10\n",
    "    threshold_reward = 1550\n",
    "\n",
    "    #Getting right path to baselibe controller weights\n",
    "    dirs = os.listdir(os.path.join(os.getcwd(), 'baseline_weights/'))\n",
    "    sub_dir = None\n",
    "    for names in dirs:\n",
    "        if policy_env in names:\n",
    "            sub_dir = names\n",
    "\n",
    "    weights = os.listdir(os.path.join(os.getcwd(), 'baseline_weights/' + sub_dir))\n",
    "    for weight in weights:\n",
    "        if 'endweights' in weight:\n",
    "            baseline_weights = weight\n",
    "\n",
    "    ppo_baseline = PPO(num_inputs, num_outputs, num_steps=num_steps)\n",
    "    print(\"Loading weights from {}\".format(os.getcwd() + '/baseline_weights/' + sub_dir + '/' + baseline_weights))\n",
    "    ppo_baseline.load_weights(os.getcwd() + '/baseline_weights/' + sub_dir + '/' + baseline_weights)\n",
    "    compensator_test = compensator(num_inputs, num_outputs, ppo_baseline, action_appended = ACTION_APPEND)\n",
    "    \n",
    "    max_frames = 2000000\n",
    "    test_avg_rewards = []\n",
    "    test_stds = []\n",
    "    test_itrs = 20\n",
    "    save_interval = 5 #How often to save weights and figures\n",
    "    \n",
    "    state = envs.reset()\n",
    "    early_stop = False\n",
    "    ppo_updates = 0\n",
    "    save_interval = 5\n",
    "    #Plotting Flags\n",
    "    indvplots=0\n",
    "    rewplots=1\n",
    "    stdplots=1\n",
    "    which_plts = [indvplots,rewplots,stdplots]\n",
    "\n",
    "    test_avg_rewards = []\n",
    "    test_stds = []\n",
    "\n",
    "    while compensator_test.ppo_compensator.frame_idx < max_frames and not early_stop:\n",
    "\n",
    "        #collect data\n",
    "        log_probs, values, states, actions, rewards, masks, next_value = compensator_test.collect_data(envs)\n",
    "\n",
    "        #compute gae\n",
    "        returns = compensator_test.ppo_compensator.compute_gae(next_value, rewards, masks, values)\n",
    "\n",
    "        #update policy\n",
    "        compensator_test.ppo_compensator.ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, values)\n",
    "\n",
    "        #plot\n",
    "        avg_rew = []\n",
    "        std = []\n",
    "\n",
    "        #Environment testing and data logging\n",
    "        #***************************************************************************************\n",
    "        for env in tests.envs:\n",
    "            env_rewards = ([tests.test_env(env, compensator_test) for _ in range(test_itrs)])\n",
    "            avg_rew.append(np.mean(env_rewards))\n",
    "            std.append(np.std(env_rewards))\n",
    "\n",
    "        test_avg_rewards.append(avg_rew)\n",
    "        test_stds.append(std)\n",
    "\n",
    "        if avg_rew[training_env_index] > threshold_reward and EARLY_STOPPING: #avg_rew[0] is testing on the non edited environment\n",
    "            tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 1)\n",
    "            early_stop = True\n",
    "        else:\n",
    "            if ppo_updates and ppo_updates % save_interval == 0:\n",
    "                ppo_baseline.save_weights(baseline_dir + env_name + '_weights' + str(ppo_updates/save_interval))\n",
    "                tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 1, str(ppo_updates/save_interval))\n",
    "            else:\n",
    "                tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 0)\n",
    "\n",
    "        ppo_updates = ppo_updates + 1 #Loop counter\n",
    "        #***************************************************************************************\n",
    "\n",
    "    compensator_test.ppo_compensator.save_weights(baseline_dir + env_name + '_compensatorAction_endweights')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gym import envs\n",
    "\n",
    "# #env_name = 'InvertedPendulumModified-multi-v10'\n",
    "# #env_name = \"Pendulum-v0\"\n",
    "# #env_name = \"HalfCheetah-v2\"\n",
    "# #env_name = 'FetchReach-v1'\n",
    "# env_name = 'InvertedPendulumModified-multi-v10'\n",
    "# env_key = \"v10\" #Unique identifier for custom envs (case sensitive)\n",
    "\n",
    "# \"\"\"\"\n",
    "# ['InvertedPendulum-v2',\n",
    "#  'InvertedPendulumModified-base-v10',\n",
    "#  'InvertedPendulumModified-mass-v10',\n",
    "#  'InvertedPendulumModified-inertia-v10',\n",
    "#  'InvertedPendulumModified-friction-v10',\n",
    "#  'InvertedPendulumModified-tilt-v10',\n",
    "#  'InvertedPendulumModified-motor-v10',\n",
    "#  'InvertedPendulumModified-multi-v10']\n",
    "#  \"\"\"\n",
    "\n",
    "# env_ids = [spec.id for spec in envs.registry.all()]\n",
    "# test_env_names = ['InvertedPendulum-v2'] + [x for x in env_ids if str(env_key) in x] #Returns a list of environment names matching identifier\n",
    "# training_env_index = test_env_names.index(env_name)\n",
    "\n",
    "# #Training envs (all the same)\n",
    "# envs = [make_env(env_name) for i in range(num_envs)]\n",
    "# envs = SubprocVecEnv(envs)\n",
    "\n",
    "# #Plotting Results and figures, save weights\n",
    "# script_dir = os.getcwd()\n",
    "# time_stamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "# results_dir = os.path.join(script_dir, 'compensator_plots/' + env_name + time_stamp + '/')\n",
    "# baseline_dir = os.path.join(script_dir, 'compensator_weights/' + env_name + time_stamp + '/')\n",
    "\n",
    "# if not os.path.isdir(results_dir):\n",
    "#     os.mkdir(results_dir)\n",
    "    \n",
    "# if not os.path.isdir(baseline_dir):\n",
    "#     os.mkdir(baseline_dir)\n",
    "\n",
    "# #Testing on original and new envs\n",
    "# tests = testing_envs(test_env_names, VISUALIZE, COMPENSATION, results_dir, training_env_index, logging_interval = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_inputs  = envs.observation_space.shape[0]\n",
    "# num_outputs = envs.action_space.shape[0]\n",
    "\n",
    "# #Hyper params:\n",
    "# hidden_size      = 64\n",
    "# lr               = 3e-4\n",
    "# num_steps        = 2048\n",
    "# mini_batch_size  = 64\n",
    "# ppo_epochs       = 10\n",
    "# threshold_reward = 900\n",
    "\n",
    "# #Getting right path to baselibe controller weights\n",
    "# dirs = os.listdir(os.path.join(os.getcwd(), 'baseline_weights/'))\n",
    "# sub_dir = None\n",
    "# for names in dirs:\n",
    "#     if env_name in names:\n",
    "#         sub_dir = names\n",
    "    \n",
    "# weights = os.listdir(os.path.join(os.getcwd(), 'baseline_weights/' + sub_dir))\n",
    "# for weight in weights:\n",
    "#     if 'endweights' in weight:\n",
    "#         baseline_weights = weight\n",
    "\n",
    "# ppo_baseline = PPO(num_inputs, num_outputs)\n",
    "# ppo_baseline.load_weights(os.getcwd() + '/baseline_weights/' + sub_dir + '/' + baseline_weights)\n",
    "# compensator_test = compensator(num_inputs, num_outputs, ppo_baseline, action_appended = ACTION_APPEND)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weights_dir = os.path.join(os.getcwd(), 'baseline_weights/')\n",
    "# dirs = os.listdir(os.path.join(os.getcwd(), 'baseline_weights/'))\n",
    "# sub_dir = None\n",
    "# for names in dirs:\n",
    "#     if env_name in names:\n",
    "#         sub_dir = names\n",
    "    \n",
    "# weights = os.listdir(os.path.join(os.getcwd(), 'baseline_weights/' + sub_dir))\n",
    "# for weight in weights:\n",
    "#     if 'endweights' in weight:\n",
    "#         baseline_weights = weight\n",
    "# baseline_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# max_frames = 2000000\n",
    "# test_avg_rewards = []\n",
    "# test_stds = []\n",
    "# test_itrs = 20\n",
    "# save_interval = 5 #How often to save weights and figures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train compensator with action appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# state = envs.reset()\n",
    "# early_stop = False\n",
    "# ppo_updates = 0\n",
    "# save_interval = 5\n",
    "# #Plotting Flags\n",
    "# indvplots=0\n",
    "# rewplots=1\n",
    "# stdplots=1\n",
    "# which_plts = [indvplots,rewplots,stdplots]\n",
    "\n",
    "# test_avg_rewards = []\n",
    "# test_stds = []\n",
    "\n",
    "# while compensator_test.ppo_compensator.frame_idx < max_frames and not early_stop:\n",
    "\n",
    "#     #collect data\n",
    "#     log_probs, values, states, actions, rewards, masks, next_value = compensator_test.collect_data(envs)\n",
    "    \n",
    "#     #compute gae\n",
    "#     returns = compensator_test.ppo_compensator.compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "#     #update policy\n",
    "#     compensator_test.ppo_compensator.ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, values)\n",
    "    \n",
    "#     #plot\n",
    "#     avg_rew = []\n",
    "#     std = []\n",
    "    \n",
    "#     #Environment testing and data logging\n",
    "#     #***************************************************************************************\n",
    "#     for env in tests.envs:\n",
    "#         env_rewards = ([tests.test_env(env, compensator_test) for _ in range(test_itrs)])\n",
    "#         avg_rew.append(np.mean(env_rewards))\n",
    "#         std.append(np.std(env_rewards))\n",
    "\n",
    "#     test_avg_rewards.append(avg_rew)\n",
    "#     test_stds.append(std)\n",
    "\n",
    "#     if avg_rew[training_env_index] > threshold_reward and EARLY_STOPPING: #avg_rew[0] is testing on the non edited environment\n",
    "#         tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, 1)\n",
    "#         early_stop = True\n",
    "#     else:\n",
    "#         if ppo_updates and ppo_updates % save_interval == 0:\n",
    "#             ppo_baseline.save_weights(baseline_dir + env_name + '_weights' + str(ppo_updates/save_interval))\n",
    "#             tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, 1, str(ppo_updates/save_interval))\n",
    "#         else:\n",
    "#             tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, 0)\n",
    "            \n",
    "#     ppo_updates = ppo_updates + 1 #Loop counter\n",
    "#     #***************************************************************************************\n",
    "\n",
    "# ppo_baseline.save_weights(baseline_dir + env_name + '_compensatorAction_endweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
