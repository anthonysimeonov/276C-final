{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from ppo import *\n",
    "from ensembles import *\n",
    "import datetime\n",
    "import fnmatch\n",
    "\n",
    "import gym\n",
    "import my_envs\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load baseline policies\n",
    "Load trained baseline policies on individually modified environments for testing on multi-modified environment\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:  \n",
    "<img src=\"./notebookImages/invertedpendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"./notebookImages/halfcheetah.png\" width=\"300\">\n",
    "\n",
    "#### Ant environment:\n",
    "<img src=\"./notebookImages/ant.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments and Import Baseline Policies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "\n",
    "VISUALIZE = False\n",
    "COMPENSATION = False\n",
    "EARLY_STOPPING = True\n",
    "ACTION_APPEND = False\n",
    "logging_interval = 10\n",
    "num_envs = 4\n",
    "env_key = \"multi-v10\" #Unique identifier for custom envs (case sensitive)\n",
    "\n",
    "env_name = 'InvertedPendulumModified-multi-v10'\n",
    "\n",
    "env_ids = [spec.id for spec in envs.registry.all()]\n",
    "test_env_names = [x for x in env_ids if str(env_key) in x] #Returns a list of environment names matching identifier\n",
    "test_env_idx = 0\n",
    "\n",
    "template_env_name = 'InvertedPendulum-v2'\n",
    "template_env = gym.make(template_env_name)\n",
    "\n",
    "#Training envs (all the same)\n",
    "envs = [make_env(env_name) for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "#Plotting Results and figures, save weights\n",
    "script_dir = os.getcwd()\n",
    "time_stamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "results_dir = os.path.join(script_dir, 'weighted_compensator_results/' + env_name + time_stamp + '/')\n",
    "comp_dir = os.path.join(script_dir, 'weighted_compensator_weights/' + env_name + time_stamp + '/')\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.isdir(comp_dir):\n",
    "    os.mkdir(comp_dir)\n",
    "    \n",
    "tests = ensemble_testing_envs(test_env_names, VISUALIZE, COMPENSATION, results_dir, test_env_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training baseline PPO controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "base_inputs  = template_env.observation_space.shape[0]\n",
    "base_outputs = template_env.action_space.shape[0]\n",
    "\n",
    "tags = ['friction', 'inertia', 'mass', 'motor', 'tilt']\n",
    "base_name = 'InvertedPendulumModified-'\n",
    "\n",
    "#set is_comp flag in initialization to use compensators\n",
    "comp_ensemble = ensemble(base_env_name=base_name, \n",
    "                              import_tags=tags, \n",
    "                              num_inputs=base_inputs, \n",
    "                              num_outputs=base_outputs,\n",
    "                              is_comp = True,\n",
    "                              debug=False)\n",
    "\n",
    "\n",
    "comp_ensemble.import_policies(baseline_file = './baseline_weights/InvertedPendulumModified-multi-v102018-06-10T13:55:10/InvertedPendulumModified-multi-v10_endweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ACTION_APPEND:\n",
    "    num_inputs = template_env.observation_space.shape[0] + template_env.action_space.shape[0]\n",
    "else:\n",
    "    num_inputs = template_env.observation_space.shape[0]\n",
    "num_outputs = len(comp_ensemble.compensator_policy_list)\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 64\n",
    "lr               = 3e-4\n",
    "num_steps        = 2048\n",
    "mini_batch_size  = 64\n",
    "ppo_epochs       = 10\n",
    "threshold_reward = 900\n",
    "\n",
    "compensator_weighting = PPO_Ensemble(num_inputs, num_outputs, comp_ensemble, hidden_size=hidden_size, num_steps=num_steps)\n",
    "print(compensator_weighting.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 1000000\n",
    "frame_idx  = 0\n",
    "test_avg_rewards = []\n",
    "test_stds = []\n",
    "test_itrs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "ppo_updates = 0\n",
    "save_interval = 5\n",
    "#Plotting Flags\n",
    "indvplots=0\n",
    "rewplots=1\n",
    "stdplots=1\n",
    "which_plts = [indvplots,rewplots,stdplots]\n",
    "\n",
    "while compensator_weighting.frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    #collect data\n",
    "    log_probs, values, states, actions, rewards, masks, next_value = compensator_weighting.collect_data(envs)\n",
    "    \n",
    "    #compute gae\n",
    "    returns = compensator_weighting.compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    #update policy\n",
    "    compensator_weighting.ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, values)\n",
    "    \n",
    "    #plot\n",
    "    avg_rew = []\n",
    "    std = []\n",
    "    \n",
    "    #Environment testing and data logging\n",
    "    #***************************************************************************************\n",
    "    for env in tests.envs:\n",
    "        env_rewards = ([tests.test_env(env, comp_ensemble, compensator_weighting, action_append=ACTION_APPEND) for _ in range(test_itrs)])\n",
    "        avg_rew.append(np.mean(env_rewards))\n",
    "        std.append(np.std(env_rewards))\n",
    "\n",
    "    test_avg_rewards.append(avg_rew)\n",
    "    test_stds.append(std)\n",
    "\n",
    "    if avg_rew[test_env_idx] > threshold_reward and EARLY_STOPPING: #avg_rew[0] is testing on the non edited environment\n",
    "        tests.plot(compensator_weighting.frame_idx, test_avg_rewards, test_stds, which_plts, 1)\n",
    "        early_stop = True\n",
    "    else:\n",
    "        if ppo_updates and ppo_updates % save_interval == 0:\n",
    "            compensator_weighting.save_weights(comp_dir + env_name + '_weights' + str(ppo_updates/save_interval))\n",
    "            tests.plot(compensator_weighting.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 1, str(ppo_updates/save_interval))\n",
    "        else:\n",
    "            tests.plot(compensator_weighting.frame_idx, test_avg_rewards, test_stds, which_plts, num_steps, 0)\n",
    "            \n",
    "    ppo_updates = ppo_updates + 1 #Loop counter\n",
    "    #***************************************************************************************\n",
    "\n",
    "compensator_weighting.save_weights(comp_dir + env_name + '_endweights')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
