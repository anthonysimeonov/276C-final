{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Imports</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import random\n",
    "import os\n",
    "from common.multiprocessing_env import SubprocVecEnv\n",
    "from ppo import *\n",
    "import datetime\n",
    "import fnmatch\n",
    "\n",
    "import gym\n",
    "import my_envs\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "\n",
    "from IPython.display import clear_output, display\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "\n",
    "np.random.seed(0)\n",
    "torch.manual_seed(2)\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load baseline policies\n",
    "Load trained baseline policies on individually modified environments for testing on multi-modified environment\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum-v2 environment:  \n",
    "<img src=\"./notebookImages/invertedpendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah-v2 environment:\n",
    "<img src=\"./notebookImages/halfcheetah.png\" width=\"300\">\n",
    "\n",
    "#### Ant environment:\n",
    "<img src=\"./notebookImages/ant.png\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Use CUDA</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "#use_cuda = False\n",
    "device   = torch.device(\"cuda\" if use_cuda else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_name):\n",
    "    def _thunk():\n",
    "        env = gym.make(env_name)\n",
    "        return env\n",
    "    return _thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "class baseline_ensemble():\n",
    "    def __init__(self, base_env_name, import_tags, num_inputs, num_outputs, debug = False):\n",
    "        self.base_env_name = base_env_name\n",
    "        self.import_tags = import_tags\n",
    "        self.import_folders = {} \n",
    "        self.policies = {}\n",
    "        self.policy_list = []\n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.debug = debug\n",
    "        \n",
    "    def import_policies(self):\n",
    "        folder_names = os.listdir('./baseline_weights')\n",
    "        for mod in self.import_tags:\n",
    "            self.import_folders[mod] = fnmatch.filter(folder_names, self.base_env_name + mod + '*')\n",
    "        \n",
    "        for key, val in self.import_folders.items():\n",
    "            folder = os.listdir('./baseline_weights/' + str(val[0]))\n",
    "            \n",
    "            if self.debug:\n",
    "                print(\"weight folder:     \", folder)\n",
    "            \n",
    "            weight_file = fnmatch.filter(folder, '*endweights')\n",
    "            weight_file = weight_file[0]\n",
    "            \n",
    "            if self.debug:\n",
    "                print(\"Weight file:     \", weight_file)\n",
    "            \n",
    "            self.policies[key] = PPO(self.num_inputs, self.num_outputs)\n",
    "            full_weight_file = './baseline_weights/' + str(val[0]) + '/' + weight_file\n",
    "            self.policies[key].load_weights(full_weight_file)\n",
    "            \n",
    "        if self.debug:\n",
    "            print(\"Imported Policies:\\n\")\n",
    "            for mod, policy in self.policies.items():\n",
    "                print(\"Modification:    \", mod)\n",
    "                print(policy.model)\n",
    "                \n",
    "        self.policy_list = self.policies.values()\n",
    "        \n",
    "    def uniform_action(self, state):\n",
    "        \"\"\" Outputs a uniform average of actions from variable number of policies\"\"\"\n",
    "        num_policies = len(self.policy_list)\n",
    "        actions = np.array([policy.model.sample_action(state) for policy in self.policy_list])\n",
    "\n",
    "        action = action.sum()/num_policies\n",
    "        return action\n",
    "\n",
    "    def weighted_action(self, state, policy_weights):\n",
    "        \"\"\" Outputs a weighted sum of actions defined by weight vector and variable number of policies\"\"\"\n",
    "        if policy_weights.shape[1] != len(self.policy_list):\n",
    "            print(\"Weight vector length must match number of policies\\n\")\n",
    "            print(\"weights:    \", policy_weights)\n",
    "            print(\"length \", policy_weights.shape[1])\n",
    "            print(\"policy list:    \", self.policy_list)\n",
    "            print(\"length: \", len(self.policy_list))\n",
    "            return \n",
    "        \n",
    "        if self.debug:\n",
    "            print(\"weights:    \", policy_weights)\n",
    "            print(\"length \", policy_weights.shape[1])\n",
    "            print(\"policy list:    \", self.policy_list)\n",
    "            print(\"length: \", len(self.policy_list))\n",
    "\n",
    "        for i, policy in enumerate(self.policy_list):\n",
    "            if i == 0:\n",
    "                actions = policy.model.sample_action(state)\n",
    "            else:\n",
    "                actions = torch.cat((actions, policy.model.sample_action(state)), 2)\n",
    "            \n",
    "        actions = actions.squeeze(0)\n",
    "        weights = policy_weights\n",
    "\n",
    "        if self.debug:\n",
    "            print(\"multi-policy multi-env action tensor: \")\n",
    "            print(actions, actions.size())\n",
    "            print(\"ensemble weights:\")\n",
    "            print(weights, weights.size())\n",
    "        \n",
    "        action = torch.sum((actions * weights), dim=1).unsqueeze(1)\n",
    "        if self.debug:\n",
    "            print(\"action:\")\n",
    "            print(action, action.size())\n",
    "        return action\n",
    "    \n",
    "    \n",
    "class PPO_Ensemble(PPO):\n",
    "    def __init__(self, num_inputs, num_outputs, ensemble, hidden_size=64, lr = 3e-4, num_steps = 2048,\n",
    "                 mini_batch_size = 64, ppo_epochs = 10, threshold_reward = 950):\n",
    "        super().__init__(num_inputs, num_outputs, hidden_size=64, lr = 3e-4, num_steps = 2048,\n",
    "                 mini_batch_size = 64, ppo_epochs = 10, threshold_reward = 950)\n",
    "        self.ensemble = ensemble\n",
    "        \n",
    "    def collect_data(self, envs):\n",
    "        if self.state is None:\n",
    "            state = envs.reset()\n",
    "            \n",
    "        #----------------------------------\n",
    "        #collect data\n",
    "        #----------------------------------\n",
    "        log_probs = []\n",
    "        values    = []\n",
    "        states    = []\n",
    "        actions   = []\n",
    "        rewards   = []\n",
    "        masks     = []\n",
    "        entropy = 0\n",
    "        counter = 0\n",
    "\n",
    "        for _ in range(self.num_steps):\n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = self.model(state)\n",
    "\n",
    "            weights = dist.sample()\n",
    "            action = self.ensemble.weighted_action(state.cpu().numpy(), weights)\n",
    "            next_state, reward, done, _ = envs.step(action.cpu().numpy())\n",
    "#             next_state, reward, done, _ = envs.step(action)\n",
    "            \n",
    "            log_prob = dist.log_prob(weights)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "            masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(weights)\n",
    "\n",
    "            state = next_state\n",
    "            self.frame_idx += 1\n",
    "\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        _, next_value = self.model(next_state)\n",
    "\n",
    "        return log_probs, values, states, actions, rewards, masks, next_value\n",
    "        \n",
    "class ensemble_testing_envs(testing_envs):\n",
    "    def __init__(self, env_names, VISUALIZE, COMPENSATION, results_dir, train_env_index, logging_interval = 10):\n",
    "        super().__init__(env_names, VISUALIZE, COMPENSATION, results_dir, train_env_index, logging_interval = 10)\n",
    "        \n",
    "    def test_env(self, env, ensemble_net, weight_net, comp_model = None):\n",
    "        \n",
    "        def test_action(state):\n",
    "            dist, value = weight_net.model(state)\n",
    "            weights = dist.sample()\n",
    "            action = ensemble_net.weighted_action(state, weights)\n",
    "            return action\n",
    "        \n",
    "        state = env.reset()\n",
    "        if self.vis: \n",
    "            env.render()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        while not done:\n",
    "            \n",
    "            sample = test_action(state)\n",
    "            \n",
    "            #state = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
    "            #dist, _ = control_model(state)\n",
    "            #sample = dist.sample().cpu().numpy()[0]\n",
    "                            \n",
    "            next_state, reward, done, _ = env.step(sample.cpu().numpy())\n",
    "            state = next_state\n",
    "            if self.vis:\n",
    "                env.render()\n",
    "            total_reward += reward\n",
    "        return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Create Environments and Import Baseline Policies</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "VISUALIZE = False\n",
    "COMPENSATION = False\n",
    "EARLY_STOPPING = True\n",
    "logging_interval = 10\n",
    "num_envs = 4\n",
    "env_key = \"multi-v10\" #Unique identifier for custom envs (case sensitive)\n",
    "\n",
    "env_name = 'InvertedPendulumModified-multi-v10'\n",
    "\n",
    "env_ids = [spec.id for spec in envs.registry.all()]\n",
    "test_env_names = [x for x in env_ids if str(env_key) in x] #Returns a list of environment names matching identifier\n",
    "test_env_idx = 0\n",
    "\n",
    "template_env_name = 'InvertedPendulum-v2'\n",
    "template_env = gym.make(template_env_name)\n",
    "\n",
    "#Training envs (all the same)\n",
    "envs = [make_env(env_name) for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "#Plotting Results and figures, save weights\n",
    "script_dir = os.getcwd()\n",
    "time_stamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "results_dir = os.path.join(script_dir, 'weighted_baseline_results/' + env_name + time_stamp + '/')\n",
    "baseline_dir = os.path.join(script_dir, 'weighted_baseline_weights/' + env_name + time_stamp + '/')\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.isdir(baseline_dir):\n",
    "    os.mkdir(baseline_dir)\n",
    "    \n",
    "tests = ensemble_testing_envs(test_env_names, VISUALIZE, COMPENSATION, results_dir, test_env_idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training baseline PPO controller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weight folder:      ['InvertedPendulumModified-friction-v10_weights2.0', 'InvertedPendulumModified-friction-v10_weights1.0', 'InvertedPendulumModified-friction-v10_endweights']\n",
      "Weight file:      InvertedPendulumModified-friction-v10_endweights\n",
      "weight folder:      ['InvertedPendulumModified-inertia-v10_endweights', 'InvertedPendulumModified-inertia-v10_weights1.0']\n",
      "Weight file:      InvertedPendulumModified-inertia-v10_endweights\n",
      "weight folder:      ['InvertedPendulumModified-mass-v10_weights1.0', 'InvertedPendulumModified-mass-v10_endweights']\n",
      "Weight file:      InvertedPendulumModified-mass-v10_endweights\n",
      "weight folder:      ['InvertedPendulumModified-tilt-v10_weights1.0', 'InvertedPendulumModified-tilt-v10_endweights']\n",
      "Weight file:      InvertedPendulumModified-tilt-v10_endweights\n",
      "weight folder:      ['InvertedPendulumModified-motor-v10_endweights', 'InvertedPendulumModified-motor-v10_weights1.0']\n",
      "Weight file:      InvertedPendulumModified-motor-v10_endweights\n",
      "Imported Policies:\n",
      "\n",
      "Modification:     friction\n",
      "ActorCritic(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Modification:     inertia\n",
      "ActorCritic(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Modification:     mass\n",
      "ActorCritic(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Modification:     tilt\n",
      "ActorCritic(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "Modification:     motor\n",
      "ActorCritic(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "base_inputs  = template_env.observation_space.shape[0]\n",
    "base_outputs = template_env.action_space.shape[0]\n",
    "\n",
    "tags = ['friction', 'inertia', 'mass', 'motor', 'tilt']\n",
    "base_name = 'InvertedPendulumModified-'\n",
    "\n",
    "ensemble = baseline_ensemble(base_env_name=base_name, \n",
    "                              import_tags=tags, \n",
    "                              num_inputs=base_inputs, \n",
    "                              num_outputs=base_outputs, \n",
    "                              debug=True)\n",
    "\n",
    "\n",
    "ensemble.import_policies()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ActorCritic(\n",
      "  (critic): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      "  (actor): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=64, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=64, out_features=5, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "num_inputs = template_env.observation_space.shape[0]\n",
    "num_outputs = len(ensemble.policy_list)\n",
    "\n",
    "#Hyper params:\n",
    "hidden_size      = 64\n",
    "lr               = 3e-4\n",
    "num_steps        = 2048\n",
    "mini_batch_size  = 64\n",
    "ppo_epochs       = 10\n",
    "threshold_reward = 900\n",
    "\n",
    "baseline_weighting = PPO_Ensemble(num_inputs, num_outputs, ensemble)\n",
    "print(baseline_weighting.model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_frames = 30000\n",
    "frame_idx  = 0\n",
    "test_avg_rewards = []\n",
    "test_stds = []\n",
    "test_itrs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-1.2173,  0.2580,  0.9028,  0.6382,  0.8889],\n",
      "        [-0.7573,  0.2549,  0.9654, -0.6247,  0.4870],\n",
      "        [ 1.0721,  0.6340,  2.9013, -1.6542, -0.2887],\n",
      "        [-1.4916, -0.2780, -0.5732, -1.4354,  0.9966]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2162,  0.0203,  0.2531, -0.2331, -0.1414],\n",
      "        [ 0.2998, -0.4314, -0.1003, -0.3046,  0.3065],\n",
      "        [ 0.8150,  0.0152, -0.0506,  0.2816, -0.2706],\n",
      "        [ 0.2995, -0.0831, -0.2483, -0.2816,  0.8998]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2173,  0.2580,  0.9028,  0.6382,  0.8889],\n",
      "        [-0.7573,  0.2549,  0.9654, -0.6247,  0.4870],\n",
      "        [ 1.0721,  0.6340,  2.9013, -1.6542, -0.2887],\n",
      "        [-1.4916, -0.2780, -0.5732, -1.4354,  0.9966]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3039],\n",
      "        [-0.0943],\n",
      "        [ 0.3489],\n",
      "        [ 1.0197]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4822, -1.0091,  1.2809, -0.9892, -0.8256],\n",
      "        [-1.4873,  1.8110,  0.1767, -0.8793,  0.2095],\n",
      "        [ 2.1229, -0.0182, -0.9421,  0.4121, -0.1068],\n",
      "        [-1.9030, -0.2815,  1.4148, -2.0749, -0.2064]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2473, -0.1257,  0.2002,  0.4858,  0.1886],\n",
      "        [ 0.3064,  0.6250, -0.2028,  0.2357,  0.0832],\n",
      "        [ 0.5594, -0.4821, -0.2347,  0.3506,  0.1805],\n",
      "        [-0.0851, -0.2305, -0.5537, -0.3747, -1.1243]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4822, -1.0091,  1.2809, -0.9892, -0.8256],\n",
      "        [-1.4873,  1.8110,  0.1767, -0.8793,  0.2095],\n",
      "        [ 2.1229, -0.0182, -0.9421,  0.4121, -0.1068],\n",
      "        [-1.9030, -0.2815,  1.4148, -2.0749, -0.2064]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1338],\n",
      "        [ 0.4504],\n",
      "        [ 1.5427],\n",
      "        [ 0.4530]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2444, -2.4633, -0.2706, -0.2581,  0.9889],\n",
      "        [ 0.8005,  0.0411, -0.1914, -0.3270,  1.3708],\n",
      "        [-0.5206, -1.0001, -1.1396,  0.4411, -0.8900],\n",
      "        [-0.8208,  1.2672,  0.2660, -0.4151,  0.7333]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3165, -0.0775, -0.0289, -0.1788,  0.1142],\n",
      "        [ 0.0641,  0.7343,  0.1115,  0.1282,  0.0881],\n",
      "        [-0.3010, -0.1303, -0.7319, -0.6269, -1.5113],\n",
      "        [-0.6214, -0.1397, -0.0830, -0.0591, -0.9031]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2444, -2.4633, -0.2706, -0.2581,  0.9889],\n",
      "        [ 0.8005,  0.0411, -0.1914, -0.3270,  1.3708],\n",
      "        [-0.5206, -1.0001, -1.1396,  0.4411, -0.8900],\n",
      "        [-0.8208,  1.2672,  0.2660, -0.4151,  0.7333]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4352],\n",
      "        [ 0.1390],\n",
      "        [ 2.1897],\n",
      "        [-0.3267]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1433, -0.6631, -0.1861, -1.2311,  1.2605],\n",
      "        [-0.0833,  0.6006, -0.4751,  0.4016,  0.7026],\n",
      "        [ 0.5713,  0.0679, -0.4388, -2.0625,  0.4475],\n",
      "        [ 0.7772,  0.4107, -0.0889, -0.2243,  1.3477]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3319, -0.4242, -0.1628,  0.6114,  0.3056],\n",
      "        [ 0.1034, -0.5600, -0.3155,  0.0106, -0.3167],\n",
      "        [-1.1394, -1.1484, -1.3517, -1.9769, -2.8264],\n",
      "        [-0.1774, -0.5738,  0.2781, -0.0577, -0.0341]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1433, -0.6631, -0.1861, -1.2311,  1.2605],\n",
      "        [-0.0833,  0.6006, -0.4751,  0.4016,  0.7026],\n",
      "        [ 0.5713,  0.0679, -0.4388, -2.0625,  0.4475],\n",
      "        [ 0.7772,  0.4107, -0.0889, -0.2243,  1.3477]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1034],\n",
      "        [-0.4133],\n",
      "        [ 2.6765],\n",
      "        [-0.4313]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0027,  0.3617,  1.4324,  0.6689, -1.9896],\n",
      "        [-0.5522, -0.6073,  0.3486,  1.6035, -0.2354],\n",
      "        [ 1.2220, -0.1964,  0.8426,  0.2272,  0.0705],\n",
      "        [-0.7821,  1.0600, -1.5386,  1.0194, -0.8267]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4764,  0.3327,  0.3873,  0.0031,  0.4766],\n",
      "        [ 0.1473, -0.1577,  0.5018,  0.4207, -0.0341],\n",
      "        [-1.2462, -2.1495, -2.2402, -2.6085, -4.5955],\n",
      "        [ 0.4770, -0.5114, -0.1068, -0.6065,  0.5411]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0027,  0.3617,  1.4324,  0.6689, -1.9896],\n",
      "        [-0.5522, -0.6073,  0.3486,  1.6035, -0.2354],\n",
      "        [ 1.2220, -0.1964,  0.8426,  0.2272,  0.0705],\n",
      "        [-0.7821,  1.0600, -1.5386,  1.0194, -0.8267]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2698],\n",
      "        [ 0.8719],\n",
      "        [-3.9049],\n",
      "        [-1.8163]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0463,  0.9012,  1.5433,  0.2991,  0.3796],\n",
      "        [-0.3009, -1.0330, -0.2097, -0.4163, -0.0036],\n",
      "        [-0.7666,  0.1436,  0.4164, -1.5476,  1.5041],\n",
      "        [ 1.6976, -0.8543, -0.4689,  0.2216, -0.0852]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0882,  0.5923,  0.3840, -0.4893, -0.0995],\n",
      "        [-0.2465,  0.2144,  0.4954, -0.1353, -0.8902],\n",
      "        [-0.0957, -0.0699, -0.1392,  0.3030, -0.6089],\n",
      "        [ 0.7629,  0.8279,  0.7587,  0.4812,  0.5466]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0463,  0.9012,  1.5433,  0.2991,  0.3796],\n",
      "        [-0.3009, -1.0330, -0.2097, -0.4163, -0.0036],\n",
      "        [-0.7666,  0.1436,  0.4164, -1.5476,  1.5041],\n",
      "        [ 1.6976, -0.8543, -0.4689,  0.2216, -0.0852]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9463],\n",
      "        [-0.1916],\n",
      "        [-1.3793],\n",
      "        [ 0.2921]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3197, -0.7898,  1.4648,  1.0347, -0.9337],\n",
      "        [ 0.1727,  0.2154, -0.5693,  1.5642, -0.1504],\n",
      "        [ 1.2783,  0.1839, -0.5051,  0.4140,  0.5176],\n",
      "        [ 1.7916, -0.0969, -0.1768,  1.6292, -0.8306]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0331, -0.3526, -0.4015, -0.0935, -0.9024],\n",
      "        [ 0.2926, -0.2578,  1.2454,  0.2907,  0.1502],\n",
      "        [ 0.1097,  0.1552,  0.5115,  0.3160,  1.0200],\n",
      "        [ 0.5058,  0.2571,  0.6664,  0.3027,  0.3316]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3197, -0.7898,  1.4648,  1.0347, -0.9337],\n",
      "        [ 0.1727,  0.2154, -0.5693,  1.5642, -0.1504],\n",
      "        [ 1.2783,  0.1839, -0.5051,  0.4140,  0.5176],\n",
      "        [ 1.7916, -0.0969, -0.1768,  1.6292, -0.8306]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4257],\n",
      "        [-0.2820],\n",
      "        [ 0.5692],\n",
      "        [ 0.9810]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2886,  0.7620, -0.4122, -0.8954, -0.3075],\n",
      "        [ 1.2314,  0.8925,  1.4372,  0.7323,  1.2493],\n",
      "        [-0.0952,  1.0455, -0.3045,  0.1811, -0.2665],\n",
      "        [ 1.0271,  1.1794,  0.1731,  0.0199, -0.0328]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3026, -0.1070, -0.1148,  0.0263,  0.2748],\n",
      "        [-0.0930, -0.4056, -0.3312,  0.1219, -0.2594],\n",
      "        [ 0.6503,  0.2476,  0.3925, -0.0279,  0.1894],\n",
      "        [ 0.0525, -0.3166, -0.4122, -0.1477, -0.0970]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2886,  0.7620, -0.4122, -0.8954, -0.3075],\n",
      "        [ 1.2314,  0.8925,  1.4372,  0.7323,  1.2493],\n",
      "        [-0.0952,  1.0455, -0.3045,  0.1811, -0.2665],\n",
      "        [ 1.0271,  1.1794,  0.1731,  0.0199, -0.0328]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2296],\n",
      "        [-1.1872],\n",
      "        [ 0.0219],\n",
      "        [-0.3906]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0995,  1.1695, -1.9040,  2.9206, -0.0658],\n",
      "        [ 0.8764,  0.6112,  0.2097, -0.2480, -0.7551],\n",
      "        [ 1.4194,  1.1918,  0.7009, -0.1150,  0.1312],\n",
      "        [-0.1696,  1.0471, -0.6110, -0.8421,  1.3127]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1556,  0.0464,  0.3070,  0.1982,  0.1046],\n",
      "        [ 0.9782,  0.8433,  0.0184,  0.2107,  0.7965],\n",
      "        [ 0.3686,  0.4170, -1.0549,  0.2676, -0.2362],\n",
      "        [ 0.3048, -0.4666, -0.3051,  0.4141,  0.5893]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0995,  1.1695, -1.9040,  2.9206, -0.0658],\n",
      "        [ 0.8764,  0.6112,  0.2097, -0.2480, -0.7551],\n",
      "        [ 1.4194,  1.1918,  0.7009, -0.1150,  0.1312],\n",
      "        [-0.1696,  1.0471, -0.6110, -0.8421,  1.3127]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0263],\n",
      "        [ 0.7229],\n",
      "        [ 0.2191],\n",
      "        [ 0.0710]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7961, -1.7038,  0.6229, -2.0783,  0.2549],\n",
      "        [ 1.5920,  0.6328,  0.5509,  1.8987,  0.8733],\n",
      "        [ 0.1939, -0.9487, -1.6569,  0.1777,  0.8917],\n",
      "        [-0.2553,  0.6300, -0.5049, -0.7748, -0.2559]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8245,  0.3415,  0.8401,  0.3324,  0.0379],\n",
      "        [-0.2099,  0.2081,  0.6320,  0.0689,  0.1022],\n",
      "        [ 0.3665, -0.0434, -0.1207,  0.5025,  0.7006],\n",
      "        [ 0.6567,  0.0221,  0.3887, -0.1919,  0.3177]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7961, -1.7038,  0.6229, -2.0783,  0.2549],\n",
      "        [ 1.5920,  0.6328,  0.5509,  1.8987,  0.8733],\n",
      "        [ 0.1939, -0.9487, -1.6569,  0.1777,  0.8917],\n",
      "        [-0.2553,  0.6300, -0.5049, -0.7748, -0.2559]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3961],\n",
      "        [ 0.3658],\n",
      "        [ 1.0262],\n",
      "        [-0.2827]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8296, -0.3332,  0.3235,  0.3472,  0.3382],\n",
      "        [-0.8322,  0.6809, -0.4536,  0.1833,  0.2440],\n",
      "        [ 1.9539,  1.1727,  1.1136, -0.4445,  0.7880],\n",
      "        [ 0.0432,  0.9133, -0.0579,  0.7904, -0.5118]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1403,  0.4964,  0.1650,  0.6988,  1.1262],\n",
      "        [ 0.5766,  0.4996,  0.0275,  0.1549, -0.1787],\n",
      "        [-0.7334, -0.1588,  0.4835,  0.1015, -0.3392],\n",
      "        [ 0.5476,  0.0265,  0.6348, -0.2836,  0.0435]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8296, -0.3332,  0.3235,  0.3472,  0.3382],\n",
      "        [-0.8322,  0.6809, -0.4536,  0.1833,  0.2440],\n",
      "        [ 1.9539,  1.1727,  1.1136, -0.4445,  0.7880],\n",
      "        [ 0.0432,  0.9133, -0.0579,  0.7904, -0.5118]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6278],\n",
      "        [-0.1674],\n",
      "        [-1.3931],\n",
      "        [-0.2353]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1927,  0.1386,  1.2418,  1.1186,  1.6279],\n",
      "        [ 0.8292, -1.7731, -0.6289,  0.0520,  0.8496],\n",
      "        [-1.7686,  0.6075, -0.8020, -0.5019,  1.0489],\n",
      "        [-2.1095, -0.6306,  1.5141,  2.2089, -1.3485]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4480,  0.6339, -0.4300,  0.6468,  0.7971],\n",
      "        [ 0.1679, -0.3141,  0.7305,  1.0007,  0.4526],\n",
      "        [ 0.9743,  0.4770,  0.7726,  0.6198,  0.5332],\n",
      "        [ 0.0519, -0.5080,  0.0523, -0.2123, -0.1557]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1927,  0.1386,  1.2418,  1.1186,  1.6279],\n",
      "        [ 0.8292, -1.7731, -0.6289,  0.0520,  0.8496],\n",
      "        [-1.7686,  0.6075, -0.8020, -0.5019,  1.0489],\n",
      "        [-2.1095, -0.6306,  1.5141,  2.2089, -1.3485]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6613],\n",
      "        [ 0.6732],\n",
      "        [-1.8048],\n",
      "        [ 0.0312]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6496, -0.3711, -0.3404,  0.3772,  0.4069],\n",
      "        [-1.3337, -1.1306,  1.0550,  0.1676, -0.0686],\n",
      "        [ 1.2846, -1.5626,  1.5079,  1.0724,  0.4192],\n",
      "        [ 1.1997, -0.5043, -0.1481, -0.2804,  2.7836]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5357, -0.6666, -0.3013, -0.8991, -1.8252],\n",
      "        [ 0.0335,  0.5700,  0.8245,  0.4114,  0.2254],\n",
      "        [ 0.4528,  1.1701,  0.3478,  0.7211,  1.7278],\n",
      "        [-0.1631,  0.3111,  0.3949,  0.2717, -0.6672]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6496, -0.3711, -0.3404,  0.3772,  0.4069],\n",
      "        [-1.3337, -1.1306,  1.0550,  0.1676, -0.0686],\n",
      "        [ 1.2846, -1.5626,  1.5079,  1.0724,  0.4192],\n",
      "        [ 1.1997, -0.5043, -0.1481, -0.2804,  2.7836]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3840],\n",
      "        [ 0.2342],\n",
      "        [ 0.7752],\n",
      "        [-2.3445]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9576, -0.1933,  1.9396,  2.8801,  2.5024],\n",
      "        [ 0.3134,  0.4956,  0.8829, -0.7914,  0.9365],\n",
      "        [ 0.2912,  0.2611,  0.2752, -0.3277, -2.7626],\n",
      "        [ 0.0633,  1.7353,  1.1585, -0.9332,  0.5135]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1745,  0.4802,  0.9949, -0.2011, -0.3341],\n",
      "        [-0.2008, -0.2386,  0.2130,  0.6116,  0.2052],\n",
      "        [ 0.3326,  0.8984,  0.7458,  0.5235,  0.6610],\n",
      "        [ 1.1679,  1.6638,  1.9026,  0.7394,  1.4545]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9576, -0.1933,  1.9396,  2.8801,  2.5024],\n",
      "        [ 0.3134,  0.4956,  0.8829, -0.7914,  0.9365],\n",
      "        [ 0.2912,  0.2611,  0.2752, -0.3277, -2.7626],\n",
      "        [ 0.0633,  1.7353,  1.1585, -0.9332,  0.5135]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2544],\n",
      "        [-0.2849],\n",
      "        [-1.4611],\n",
      "        [ 5.2222]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7840, -0.2711, -1.9901, -0.5979, -0.0658],\n",
      "        [ 0.9215, -1.2610, -0.7800, -1.2024,  1.2192],\n",
      "        [-0.3855,  0.4474,  0.6686,  0.8653,  1.3541],\n",
      "        [ 0.1725, -0.2486, -0.4484, -0.7624, -2.8458]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3220,  0.7495,  0.5377, -0.0345, -0.0522],\n",
      "        [-0.4022, -0.3137,  0.3683,  0.1492,  0.0374],\n",
      "        [ 0.7069,  1.0918,  1.7141,  0.6496,  1.5857],\n",
      "        [-1.2155, -0.8550, -1.6817, -1.1658, -2.6798]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7840, -0.2711, -1.9901, -0.5979, -0.0658],\n",
      "        [ 0.9215, -1.2610, -0.7800, -1.2024,  1.2192],\n",
      "        [-0.3855,  0.4474,  0.6686,  0.8653,  1.3541],\n",
      "        [ 0.1725, -0.2486, -0.4484, -0.7624, -2.8458]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5017],\n",
      "        [-0.3961],\n",
      "        [ 4.0712],\n",
      "        [ 9.2719]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.1064, -0.5697, -0.2646,  1.8224, -0.0976],\n",
      "        [ 0.9810,  0.5014, -1.1419, -0.3647, -1.0530],\n",
      "        [-0.0333,  1.4512,  0.1777,  0.4021,  0.2361],\n",
      "        [ 1.5386,  0.9723,  0.0753,  0.9462, -0.2646]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9454,  0.5644,  0.1958,  0.2965,  1.0754],\n",
      "        [-0.0456,  0.1096, -0.0810,  0.5586, -0.4436],\n",
      "        [-0.9279, -0.9148, -0.5430, -1.1221, -1.3741],\n",
      "        [ 0.6916, -0.1590,  0.2924,  0.1339,  0.2647]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.1064, -0.5697, -0.2646,  1.8224, -0.0976],\n",
      "        [ 0.9810,  0.5014, -1.1419, -0.3647, -1.0530],\n",
      "        [-0.0333,  1.4512,  0.1777,  0.4021,  0.2361],\n",
      "        [ 1.5386,  0.9723,  0.0753,  0.9462, -0.2646]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0533],\n",
      "        [ 0.3661],\n",
      "        [-2.1687],\n",
      "        [ 0.9882]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9093, -0.8636, -1.4552,  0.2534,  1.0697],\n",
      "        [ 0.5671, -1.3271, -0.6241, -0.9744, -0.1111],\n",
      "        [ 0.9587, -0.4833,  0.3442,  0.1233,  0.4926],\n",
      "        [ 0.3138,  0.4042,  1.1017,  1.4123, -0.3235]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6366, -0.6883, -0.3594,  0.1871, -0.8244],\n",
      "        [ 0.2496,  0.7510, -0.0493, -0.0140, -0.0040],\n",
      "        [ 0.4951,  0.2076, -0.1838, -0.1651,  0.9870],\n",
      "        [-0.3392, -0.2470,  0.2394,  0.3146, -1.8565]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9093, -0.8636, -1.4552,  0.2534,  1.0697],\n",
      "        [ 0.5671, -1.3271, -0.6241, -0.9744, -0.1111],\n",
      "        [ 0.9587, -0.4833,  0.3442,  0.1233,  0.4926],\n",
      "        [ 0.3138,  0.4042,  1.1017,  1.4123, -0.3235]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2960],\n",
      "        [-0.8103],\n",
      "        [ 0.7769],\n",
      "        [ 1.1025]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2466,  0.8494, -1.6398,  0.5988, -0.8798],\n",
      "        [-0.0492,  0.3291,  1.3508, -0.3720,  1.0765],\n",
      "        [-0.3941, -2.1343,  0.2235,  0.9813, -0.3443],\n",
      "        [-0.8418, -0.2058,  0.5493, -0.0709,  0.0451]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1953, -0.4127, -0.0846,  0.0823,  0.0875],\n",
      "        [ 0.5858,  0.5653,  0.3526, -0.1478,  0.7150],\n",
      "        [-0.0894,  1.0875,  0.7347,  0.7141,  1.0509],\n",
      "        [-0.6331, -1.0010, -0.4455, -0.9394, -0.7675]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2466,  0.8494, -1.6398,  0.5988, -0.8798],\n",
      "        [-0.0492,  0.3291,  1.3508, -0.3720,  1.0765],\n",
      "        [-0.3941, -2.1343,  0.2235,  0.9813, -0.3443],\n",
      "        [-0.8418, -0.2058,  0.5493, -0.0709,  0.0451]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2877],\n",
      "        [ 1.4581],\n",
      "        [-1.7825],\n",
      "        [ 0.5263]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2485,  1.3927,  0.3932,  0.7604,  1.1776],\n",
      "        [-0.6897, -0.7573, -1.0801, -0.5017, -0.3638],\n",
      "        [-1.7010,  1.8546, -1.7158,  1.1634, -0.2391],\n",
      "        [-1.0365, -0.6595,  0.8337, -1.0695, -0.0097]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1711, -0.6144,  0.3079, -0.1847,  0.5388],\n",
      "        [-0.0335, -0.6795, -1.0874, -0.5528, -0.6125],\n",
      "        [ 0.8368,  1.0702,  1.2195,  1.1384,  0.8727],\n",
      "        [-0.7808, -0.4023, -0.3591,  0.1256, -1.5537]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2485,  1.3927,  0.3932,  0.7604,  1.1776],\n",
      "        [-0.6897, -0.7573, -1.0801, -0.5017, -0.3638],\n",
      "        [-1.7010,  1.8546, -1.7158,  1.1634, -0.2391],\n",
      "        [-1.0365, -0.6595,  0.8337, -1.0695, -0.0097]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1982],\n",
      "        [ 2.2124],\n",
      "        [-0.4153],\n",
      "        [ 0.6560]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0149, -0.9646,  1.4103,  0.6679,  0.9987],\n",
      "        [ 0.0779, -0.1822, -1.2962, -0.8060, -0.6380],\n",
      "        [-1.3202,  0.0036,  1.1888,  1.1345,  1.4072],\n",
      "        [ 0.5119,  0.6601, -1.1616,  1.2520,  0.8217]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2740, -0.1882,  0.0154, -0.3463, -0.1387],\n",
      "        [-0.6302, -0.9872, -0.9971, -1.1654, -1.8222],\n",
      "        [ 0.3158,  1.3078,  1.0785,  1.0945,  1.1549],\n",
      "        [-0.9534, -0.1639, -1.1236, -0.9222, -1.3418]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0149, -0.9646,  1.4103,  0.6679,  0.9987],\n",
      "        [ 0.0779, -0.1822, -1.2962, -0.8060, -0.6380],\n",
      "        [-1.3202,  0.0036,  1.1888,  1.1345,  1.4072],\n",
      "        [ 0.5119,  0.6601, -1.1616,  1.2520,  0.8217]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1706],\n",
      "        [ 3.5251],\n",
      "        [ 3.7368],\n",
      "        [-1.5482]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4302,  0.8338,  1.2514, -1.7386,  0.5679],\n",
      "        [ 0.8224, -0.3214, -0.1686, -0.2270, -0.2597],\n",
      "        [-1.8973,  1.1992,  0.8877, -0.2293, -1.0323],\n",
      "        [-0.6408,  0.0337, -0.4565, -1.3675, -0.1722]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2360, -0.1574,  0.3691, -0.4757,  0.0729],\n",
      "        [-1.3575, -1.6240, -1.8898, -2.4741, -4.1577],\n",
      "        [-0.6835, -0.1176, -0.3239, -0.7071, -1.0376],\n",
      "        [ 0.0439,  0.9143, -0.2767, -0.0079,  0.3905]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4302,  0.8338,  1.2514, -1.7386,  0.5679],\n",
      "        [ 0.8224, -0.3214, -0.1686, -0.2270, -0.2597],\n",
      "        [-1.8973,  1.1992,  0.8877, -0.2293, -1.0323],\n",
      "        [-0.6408,  0.0337, -0.4565, -1.3675, -0.1722]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3007],\n",
      "        [ 1.3652],\n",
      "        [ 2.1015],\n",
      "        [ 0.0726]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9706, -1.1486,  0.7756,  0.7550, -1.4566],\n",
      "        [ 0.0276,  1.8714, -0.0055,  0.6827,  0.9797],\n",
      "        [-0.5359, -0.8580, -0.6231,  1.9244,  0.0684],\n",
      "        [-0.3019,  1.2953,  1.8990, -0.5485,  0.5719]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1207, -1.2379, -0.5045, -0.4520, -1.4190],\n",
      "        [ 0.4049, -0.3188, -0.1248,  0.0314,  0.0786],\n",
      "        [-0.6569, -1.2074, -1.1093, -1.2067, -2.6940],\n",
      "        [-0.3671, -0.1704, -0.4625,  0.2569,  0.2665]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9706, -1.1486,  0.7756,  0.7550, -1.4566],\n",
      "        [ 0.0276,  1.8714, -0.0055,  0.6827,  0.9797],\n",
      "        [-0.5359, -0.8580, -0.6231,  1.9244,  0.0684],\n",
      "        [-0.3019,  1.2953,  1.8990, -0.5485,  0.5719]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.9939],\n",
      "        [-0.4863],\n",
      "        [-0.4272],\n",
      "        [-0.9766]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4991,  0.7632,  0.8268,  0.9221, -0.4338],\n",
      "        [-1.4674, -0.8041,  2.2926,  0.1361,  1.0775],\n",
      "        [ 1.3253, -1.2590, -1.1376, -0.1669, -0.9085],\n",
      "        [-1.1954, -1.4442,  0.6147,  2.2165, -0.0478]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0329, -1.1509, -1.8542, -1.4748, -3.4101],\n",
      "        [ 0.2279,  0.0103, -0.8067,  0.4626, -0.1876],\n",
      "        [-0.9605, -1.0919, -0.7103, -0.7023, -1.5647],\n",
      "        [-0.0069, -0.1593, -0.8487, -0.0692,  0.7106]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4991,  0.7632,  0.8268,  0.9221, -0.4338],\n",
      "        [-1.4674, -0.8041,  2.2926,  0.1361,  1.0775],\n",
      "        [ 1.3253, -1.2590, -1.1376, -0.1669, -0.9085],\n",
      "        [-1.1954, -1.4442,  0.6147,  2.2165, -0.0478]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7765],\n",
      "        [-2.3314],\n",
      "        [ 2.4486],\n",
      "        [-0.4708]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2010,  0.5008,  0.0275,  1.0610,  1.0985],\n",
      "        [ 0.4404,  1.5732,  0.6889,  1.5454, -0.0112],\n",
      "        [-1.8241,  0.2324,  0.0572, -0.2926,  0.7698],\n",
      "        [-1.1176,  0.6175, -1.6024,  0.2165,  0.6329]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5918, -0.7864, -0.9239, -1.8822, -1.3032],\n",
      "        [ 1.5413,  0.6476,  1.4375,  0.7497,  1.6742],\n",
      "        [-0.8065, -1.8201, -1.5779, -1.2686, -3.6243],\n",
      "        [-0.3274, -0.3481, -0.6773, -0.7110,  0.0407]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2010,  0.5008,  0.0275,  1.0610,  1.0985],\n",
      "        [ 0.4404,  1.5732,  0.6889,  1.5454, -0.0112],\n",
      "        [-1.8241,  0.2324,  0.0572, -0.2926,  0.7698],\n",
      "        [-1.1176,  0.6175, -1.6024,  0.2165,  0.6329]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.1370],\n",
      "        [ 3.8279],\n",
      "        [-1.4609],\n",
      "        [ 1.1082]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0619, -0.6430, -0.4422, -0.5976,  0.4232],\n",
      "        [-0.9985, -0.1901, -0.8645,  0.4474, -1.3331],\n",
      "        [ 0.6869, -1.8362,  0.5221,  0.1488, -0.4805],\n",
      "        [ 1.6632, -0.0114, -1.5662,  0.4345,  1.4274]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4711,  0.9636,  0.5521, -0.2859,  1.3554],\n",
      "        [-0.9376, -0.8047, -0.5328, -0.7767, -1.8229],\n",
      "        [ 0.2670, -0.0932,  0.2601,  0.3874,  0.2973],\n",
      "        [-0.4783, -1.0219, -0.3444, -1.1400, -1.1856]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0619, -0.6430, -0.4422, -0.5976,  0.4232],\n",
      "        [-0.9985, -0.1901, -0.8645,  0.4474, -1.3331],\n",
      "        [ 0.6869, -1.8362,  0.5221,  0.1488, -0.4805],\n",
      "        [ 1.6632, -0.0114, -1.5662,  0.4345,  1.4274]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3810],\n",
      "        [ 3.6325],\n",
      "        [ 0.4051],\n",
      "        [-2.4321]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6279, -0.9683, -0.6601, -0.0033,  0.9705],\n",
      "        [ 0.6398,  2.1941, -1.0129,  0.2722,  1.0389],\n",
      "        [-1.0916,  1.2744,  1.4879,  1.5768,  0.3715],\n",
      "        [ 0.3835,  1.5175,  0.8438, -0.3824,  0.6787]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4216, -0.2064,  0.0595, -0.5493, -0.6992],\n",
      "        [-1.6543, -1.9956, -1.5852, -2.1351, -3.8184],\n",
      "        [ 0.0048, -0.3718,  0.0628, -0.2111,  0.2981],\n",
      "        [ 0.7633,  0.1680, -0.0737,  0.3112,  0.6517]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6279, -0.9683, -0.6601, -0.0033,  0.9705],\n",
      "        [ 0.6398,  2.1941, -1.0129,  0.2722,  1.0389],\n",
      "        [-1.0916,  1.2744,  1.4879,  1.5768,  0.3715],\n",
      "        [ 0.3835,  1.5175,  0.8438, -0.3824,  0.6787]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7809],\n",
      "        [-8.3792],\n",
      "        [-0.6078],\n",
      "        [ 0.8088]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0726,  0.1876, -1.4102,  1.0699,  0.5061],\n",
      "        [-2.4518,  0.0330,  1.0728, -1.1353, -0.7401],\n",
      "        [-0.3793,  1.7998, -0.9862,  1.6046,  0.3718],\n",
      "        [-0.3040, -0.4408,  0.0552, -0.7841, -2.5232]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2046, -1.7360, -0.8163, -0.9155, -1.0073],\n",
      "        [ 0.9030,  1.5661,  1.6169,  0.7581,  1.9671],\n",
      "        [ 0.4711,  0.5423,  0.2245,  0.2685, -0.1840],\n",
      "        [ 0.1464, -0.7589, -0.4573, -0.8162, -0.4243]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0726,  0.1876, -1.4102,  1.0699,  0.5061],\n",
      "        [-2.4518,  0.0330,  1.0728, -1.1353, -0.7401],\n",
      "        [-0.3793,  1.7998, -0.9862,  1.6046,  0.3718],\n",
      "        [-0.3040, -0.4408,  0.0552, -0.7841, -2.5232]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6490],\n",
      "        [-2.7443],\n",
      "        [ 0.9384],\n",
      "        [ 1.9753]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9614,  1.8810,  0.4250,  1.2693,  0.0454],\n",
      "        [-0.4709,  0.1458,  1.8014,  1.1019,  0.6973],\n",
      "        [-0.4467,  1.2673, -0.3195,  0.8745, -0.6670],\n",
      "        [ 1.5046,  1.1219,  0.8807,  0.9359,  0.0149]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3519, -0.8200, -0.9721, -1.7388, -0.8539],\n",
      "        [ 1.6912,  2.0550,  2.0081,  2.0511,  3.4321],\n",
      "        [-0.0336, -0.5291, -0.5872, -0.5155, -0.5867],\n",
      "        [-0.4847, -1.4735, -0.3091, -0.9227, -2.1706]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9614,  1.8810,  0.4250,  1.2693,  0.0454],\n",
      "        [-0.4709,  0.1458,  1.8014,  1.1019,  0.6973],\n",
      "        [-0.4467,  1.2673, -0.3195,  0.8745, -0.6670],\n",
      "        [ 1.5046,  1.1219,  0.8807,  0.9359,  0.0149]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.8632],\n",
      "        [ 7.7739],\n",
      "        [-0.5274],\n",
      "        [-3.5508]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2488,  1.2193,  0.2564,  0.1766,  0.3631],\n",
      "        [-0.8819, -0.2749,  0.7332,  0.1520, -1.2132],\n",
      "        [-0.4466, -0.5608,  0.7762, -1.3174,  0.5907],\n",
      "        [ 0.1361,  1.7839, -0.3240,  0.0878, -0.2496]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2744,  0.8417,  0.7856,  0.6845,  1.9006],\n",
      "        [-0.6469, -0.9654, -0.2546, -0.4941, -1.7963],\n",
      "        [ 0.4079, -0.1806, -0.2099, -0.4679, -0.0965],\n",
      "        [ 0.6433,  1.3110, -0.0798,  1.0906,  1.0460]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2488,  1.2193,  0.2564,  0.1766,  0.3631],\n",
      "        [-0.8819, -0.2749,  0.7332,  0.1520, -1.2132],\n",
      "        [-0.4466, -0.5608,  0.7762, -1.3174,  0.5907],\n",
      "        [ 0.1361,  1.7839, -0.3240,  0.0878, -0.2496]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7217],\n",
      "        [ 2.7536],\n",
      "        [ 0.3157],\n",
      "        [ 2.2867]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6521, -0.1642, -0.3629,  0.2596,  0.1702],\n",
      "        [-1.3452,  1.6797,  0.8356, -0.5070,  1.2759],\n",
      "        [ 0.2750, -0.1477,  0.1771, -0.4529, -0.3030],\n",
      "        [ 2.5317, -0.2333,  0.7314, -1.0709,  0.1516]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7607,  0.3733, -0.1413, -0.4584,  0.3557],\n",
      "        [-1.1847, -2.4892, -1.9982, -1.8801, -3.1176],\n",
      "        [-0.5162,  0.0598, -0.3522, -0.1377,  0.0909],\n",
      "        [-0.8444, -0.6120, -0.2570, -0.8020, -0.8141]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6521, -0.1642, -0.3629,  0.2596,  0.1702],\n",
      "        [-1.3452,  1.6797,  0.8356, -0.5070,  1.2759],\n",
      "        [ 0.2750, -0.1477,  0.1771, -0.4529, -0.3030],\n",
      "        [ 2.5317, -0.2333,  0.7314, -1.0709,  0.1516]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4276],\n",
      "        [-7.2818],\n",
      "        [-0.1783],\n",
      "        [-1.4476]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4397, -2.0619,  1.4408,  1.4499,  1.0572],\n",
      "        [-0.2621,  0.7562, -0.6987,  0.5860,  0.2363],\n",
      "        [-0.2170, -0.9832, -0.2421, -1.2914, -0.3723],\n",
      "        [ 0.4285,  0.2795, -0.4704, -0.6229, -0.1625]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.6776, -0.9828, -0.1315, -0.6120, -1.0111],\n",
      "        [ 1.3171,  1.3885,  0.9026,  1.5000,  2.1481],\n",
      "        [-0.3150, -0.0237,  0.1351, -0.0352,  0.1618],\n",
      "        [-0.1235, -0.3093, -0.3067, -0.5711, -0.2495]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4397, -2.0619,  1.4408,  1.4499,  1.0572],\n",
      "        [-0.2621,  0.7562, -0.6987,  0.5860,  0.2363],\n",
      "        [-0.2170, -0.9832, -0.2421, -1.2914, -0.3723],\n",
      "        [ 0.4285,  0.2795, -0.4704, -0.6229, -0.1625]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8565],\n",
      "        [ 1.4610],\n",
      "        [ 0.0442],\n",
      "        [ 0.4012]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7894, -0.3382, -0.7056, -0.2517,  1.0833],\n",
      "        [ 0.5161, -0.5014, -0.6112,  0.1247,  1.0122],\n",
      "        [ 1.6241,  0.1424,  0.4045,  1.3883,  0.3622],\n",
      "        [-1.6051, -1.6134, -0.6455, -0.6939, -1.2262]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2600, -0.7366, -0.7315, -0.7339, -0.8405],\n",
      "        [ 0.6865,  0.3534,  1.4060,  0.3812,  1.1892],\n",
      "        [ 0.7116,  0.2225,  0.2759,  0.0468, -0.4051],\n",
      "        [ 0.4993, -0.8981,  0.0230, -1.0488, -1.6334]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7894, -0.3382, -0.7056, -0.2517,  1.0833],\n",
      "        [ 0.5161, -0.5014, -0.6112,  0.1247,  1.0122],\n",
      "        [ 1.6241,  0.1424,  0.4045,  1.3883,  0.3622],\n",
      "        [-1.6051, -1.6134, -0.6455, -0.6939, -1.2262]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2448],\n",
      "        [ 0.5690],\n",
      "        [ 1.2172],\n",
      "        [ 3.3633]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2489,  1.3851,  0.2338,  0.3973, -0.0532],\n",
      "        [ 0.6592,  0.4554,  1.5241,  0.5453,  0.4536],\n",
      "        [ 1.1983,  1.3568,  0.1212, -1.3229, -2.0732],\n",
      "        [-1.2485, -1.0788, -0.0297,  1.7080, -0.7334]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4548, -1.1715, -0.8436, -1.1125, -0.9951],\n",
      "        [ 0.1042,  1.0471,  0.0239, -0.3907, -0.1180],\n",
      "        [-0.7443, -0.6280, -0.5265, -0.8045, -1.2769],\n",
      "        [-1.2994, -1.6303, -1.2184, -2.1111, -3.2330]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2489,  1.3851,  0.2338,  0.3973, -0.0532],\n",
      "        [ 0.6592,  0.4554,  1.5241,  0.5453,  0.4536],\n",
      "        [ 1.1983,  1.3568,  0.1212, -1.3229, -2.0732],\n",
      "        [-1.2485, -1.0788, -0.0297,  1.7080, -0.7334]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.7769],\n",
      "        [ 0.3154],\n",
      "        [ 1.9039],\n",
      "        [ 2.1825]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8953,  1.1635,  1.0241,  0.2342, -1.1660],\n",
      "        [-0.4249, -0.4530,  1.4581,  0.4522,  0.1272],\n",
      "        [ 0.0152, -0.3369,  0.8527, -0.6027,  0.8641],\n",
      "        [ 0.8585,  0.6480, -0.9744, -1.6105, -0.8820]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3653,  0.6557,  0.0978,  0.8227,  1.9438],\n",
      "        [ 0.1530,  0.0818, -0.4531, -0.3316,  0.4079],\n",
      "        [-0.3599, -1.3700, -0.8765, -1.0143, -2.0353],\n",
      "        [-0.2142, -0.1581,  0.6792,  0.7394, -0.1341]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8953,  1.1635,  1.0241,  0.2342, -1.1660],\n",
      "        [-0.4249, -0.4530,  1.4581,  0.4522,  0.1272],\n",
      "        [ 0.0152, -0.3369,  0.8527, -0.6027,  0.8641],\n",
      "        [ 0.8585,  0.6480, -0.9744, -1.6105, -0.8820]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4330],\n",
      "        [-0.8607],\n",
      "        [-1.4388],\n",
      "        [-2.0206]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3276, -1.1391,  1.4187,  0.8181,  0.7330],\n",
      "        [ 0.1452,  1.5597,  2.1671,  0.9147,  0.1912],\n",
      "        [-2.5011,  0.8613,  1.7956,  0.2465,  0.0292],\n",
      "        [-0.2948,  0.6368, -0.9417,  0.0438,  1.5228]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3124,  1.6441,  1.4569,  1.2729,  1.9062],\n",
      "        [ 0.8877,  0.4392,  1.3900,  0.2583,  1.3007],\n",
      "        [ 0.0524, -1.2614, -0.2281, -0.6317, -1.5279],\n",
      "        [ 0.1093,  0.9959,  1.6494,  0.9651,  1.7559]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3276, -1.1391,  1.4187,  0.8181,  0.7330],\n",
      "        [ 0.1452,  1.5597,  2.1671,  0.9147,  0.1912],\n",
      "        [-2.5011,  0.8613,  1.7956,  0.2465,  0.0292],\n",
      "        [-0.2948,  0.6368, -0.9417,  0.0438,  1.5228]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2026],\n",
      "        [ 4.3112],\n",
      "        [-1.8276],\n",
      "        [ 1.7649]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7866,  0.4555,  0.1751,  0.8693, -0.8907],\n",
      "        [-0.4200,  0.3385,  1.5663, -1.5025,  2.1052],\n",
      "        [-0.2222, -1.2917, -0.8409,  0.3252, -1.5594],\n",
      "        [-0.9795,  0.3933, -0.3996, -0.8912,  0.1195]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7437,  0.3282,  1.1563,  0.7767,  1.1126],\n",
      "        [-0.8766, -1.4899, -1.5498, -2.2808, -2.9654],\n",
      "        [ 0.5349,  0.5409, -0.2186,  0.0584,  0.7669],\n",
      "        [ 0.4899, -0.1935,  0.2207,  0.2620,  0.2398]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7866,  0.4555,  0.1751,  0.8693, -0.8907],\n",
      "        [-0.4200,  0.3385,  1.5663, -1.5025,  2.1052],\n",
      "        [-0.2222, -1.2917, -0.8409,  0.3252, -1.5594],\n",
      "        [-0.9795,  0.3933, -0.3996, -0.8912,  0.1195]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5489],\n",
      "        [-5.3800],\n",
      "        [-1.8106],\n",
      "        [-0.8490]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9381,  0.8505, -0.4382,  0.2784, -0.3561],\n",
      "        [-0.0926, -1.1202, -0.6555, -0.4405,  0.1664],\n",
      "        [-0.9858,  1.1110,  0.2160,  0.0039,  1.4315],\n",
      "        [-1.8165,  0.7089, -0.3998,  1.8230,  0.7762]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0085,  0.7040,  1.4907,  1.4014,  0.5112],\n",
      "        [ 1.0219,  0.6434,  0.8330,  1.3142,  1.2426],\n",
      "        [ 1.0479,  1.3645,  1.0522,  0.9161,  1.0904],\n",
      "        [ 0.7053,  0.6934,  0.6281,  0.1735,  0.4272]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9381,  0.8505, -0.4382,  0.2784, -0.3561],\n",
      "        [-0.0926, -1.1202, -0.6555, -0.4405,  0.1664],\n",
      "        [-0.9858,  1.1110,  0.2160,  0.0039,  1.4315],\n",
      "        [-1.8165,  0.7089, -0.3998,  1.8230,  0.7762]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7924],\n",
      "        [-1.7335],\n",
      "        [ 2.2748],\n",
      "        [-0.3928]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8982,  2.7721,  0.5746,  0.1398,  0.4008],\n",
      "        [ 0.5669,  0.6708, -0.1475, -0.3838, -1.6559],\n",
      "        [-0.1859,  0.9566, -0.7779, -0.3551,  0.0751],\n",
      "        [ 1.0421,  0.1744, -0.6862, -0.3154, -0.0780]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6161,  0.3011,  0.6675,  0.2485,  0.1405],\n",
      "        [ 1.0047,  1.1550,  1.5734,  1.8143,  1.6227],\n",
      "        [ 0.3385,  0.2272, -0.4240, -0.5053, -1.1102],\n",
      "        [ 0.8200, -0.4013,  0.0352,  0.5898,  0.0778]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8982,  2.7721,  0.5746,  0.1398,  0.4008],\n",
      "        [ 0.5669,  0.6708, -0.1475, -0.3838, -1.6559],\n",
      "        [-0.1859,  0.9566, -0.7779, -0.3551,  0.0751],\n",
      "        [ 1.0421,  0.1744, -0.6862, -0.3154, -0.0780]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7558],\n",
      "        [-2.2711],\n",
      "        [ 0.5803],\n",
      "        [ 0.5683]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4388, -1.4481,  1.8083,  1.4504, -0.1911],\n",
      "        [-0.1106,  0.5595,  0.9553,  0.9547, -0.1402],\n",
      "        [ 1.2898, -0.2992,  0.2366,  0.0654, -0.2247],\n",
      "        [-1.9996, -0.4722,  0.4094,  0.0634, -1.0948]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0917,  0.1355,  0.4594,  0.1388, -0.3824],\n",
      "        [ 1.2421,  3.0716,  2.1824,  1.5183,  3.1345],\n",
      "        [-0.2257, -0.7567,  0.6685, -0.3200, -0.5587],\n",
      "        [ 0.1502,  0.8320,  0.0407, -0.1466,  0.0521]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4388, -1.4481,  1.8083,  1.4504, -0.1911],\n",
      "        [-0.1106,  0.5595,  0.9553,  0.9547, -0.1402],\n",
      "        [ 1.2898, -0.2992,  0.2366,  0.0654, -0.2247],\n",
      "        [-1.9996, -0.4722,  0.4094,  0.0634, -1.0948]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9491],\n",
      "        [ 4.6763],\n",
      "        [ 0.1981],\n",
      "        [-0.7428]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8647, -1.2048,  1.2261, -0.3835, -2.2930],\n",
      "        [ 0.1667, -0.1984, -0.2441, -0.0588, -0.7331],\n",
      "        [-0.1900, -1.1431,  0.1996,  0.7886,  0.0640],\n",
      "        [-1.3571,  1.3814, -0.0184, -1.6501,  0.9877]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7751,  0.0516, -0.0373, -0.3549, -0.5665],\n",
      "        [ 0.2833, -0.3550,  0.3724, -0.5904,  0.3204],\n",
      "        [-0.4776, -0.7476, -0.2779,  0.2442, -0.1420],\n",
      "        [ 0.1068,  1.0947,  0.8187, -0.2654,  0.5937]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8647, -1.2048,  1.2261, -0.3835, -2.2930],\n",
      "        [ 0.1667, -0.1984, -0.2441, -0.0588, -0.7331],\n",
      "        [-0.1900, -1.1431,  0.1996,  0.7886,  0.0640],\n",
      "        [-1.3571,  1.3814, -0.0184, -1.6501,  0.9877]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.7724],\n",
      "        [-0.1735],\n",
      "        [ 1.0734],\n",
      "        [ 2.3764]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0348, -1.3704, -0.2865,  0.0498, -1.9918],\n",
      "        [-0.0312,  0.2770, -0.3119,  1.7805,  2.1635],\n",
      "        [ 0.4230,  2.3131,  0.2877,  0.9878, -0.2566],\n",
      "        [ 0.0075, -0.6123,  2.0994, -1.0782,  0.0577]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4118, -0.2159, -1.3247, -1.2422, -2.9710],\n",
      "        [ 0.4801,  0.2563, -0.0396, -0.1573,  0.0859],\n",
      "        [-0.7484, -0.5434, -0.1119,  0.1224, -1.2446],\n",
      "        [-1.0470, -0.8037, -0.8823, -0.5612, -1.8489]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0348, -1.3704, -0.2865,  0.0498, -1.9918],\n",
      "        [-0.0312,  0.2770, -0.3119,  1.7805,  2.1635],\n",
      "        [ 0.4230,  2.3131,  0.2877,  0.9878, -0.2566],\n",
      "        [ 0.0075, -0.6123,  2.0994, -1.0782,  0.0577]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 6.5455],\n",
      "        [-0.0259],\n",
      "        [-1.1655],\n",
      "        [-0.8696]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2745, -0.2731, -1.3068,  1.2650, -0.1462],\n",
      "        [-2.3416,  0.2527, -1.2024,  0.6812,  1.0025],\n",
      "        [ 0.8436,  0.5838,  1.6932,  0.1519, -0.0231],\n",
      "        [ 0.0478,  0.0400, -2.8543, -1.0412, -0.1484]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1138, -0.0369, -0.6613, -0.1876, -0.3314],\n",
      "        [ 0.0998, -0.3355,  0.0365,  0.1476, -0.3454],\n",
      "        [-0.1156, -0.8852, -0.4187, -0.0086,  0.1395],\n",
      "        [-0.0814,  0.0207, -0.5151,  0.3377, -0.4081]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2745, -0.2731, -1.3068,  1.2650, -0.1462],\n",
      "        [-2.3416,  0.2527, -1.2024,  0.6812,  1.0025],\n",
      "        [ 0.8436,  0.5838,  1.6932,  0.1519, -0.0231],\n",
      "        [ 0.0478,  0.0400, -2.8543, -1.0412, -0.1484]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6542],\n",
      "        [-0.6082],\n",
      "        [-1.3278],\n",
      "        [ 1.1762]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5421,  0.6181,  0.3444,  0.7043, -1.7331],\n",
      "        [-0.6587, -0.1296,  0.3482,  0.1377,  0.9535],\n",
      "        [-0.9718,  2.5497,  0.8544, -0.7081, -0.5417],\n",
      "        [-2.4281,  0.0353,  1.0862,  0.6265, -1.7007]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2869, -0.0428, -0.5336, -0.0724, -0.4300],\n",
      "        [ 0.1790, -0.4088,  0.2006, -0.4134,  0.0127],\n",
      "        [ 0.6388,  0.8177,  0.2495,  0.0539,  1.1609],\n",
      "        [-0.7714, -0.0902, -0.5042, -0.5919, -0.4640]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5421,  0.6181,  0.3444,  0.7043, -1.7331],\n",
      "        [-0.6587, -0.1296,  0.3482,  0.1377,  0.9535],\n",
      "        [-0.9718,  2.5497,  0.8544, -0.7081, -0.5417],\n",
      "        [-2.4281,  0.0353,  1.0862,  0.6265, -1.7007]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6395],\n",
      "        [-0.0399],\n",
      "        [ 1.0102],\n",
      "        [ 1.7404]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0957,  2.0197, -0.3108,  0.4047,  1.2493],\n",
      "        [ 0.6198,  0.2583, -1.0544, -0.4277, -0.2972],\n",
      "        [ 0.3577, -1.2084,  1.1030, -0.0158, -0.8298],\n",
      "        [ 0.2000, -2.6224,  0.8257,  0.7131,  1.1029]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3656,  0.3886, -0.2727, -0.0509,  0.3926],\n",
      "        [ 0.2354, -0.8458, -0.7591, -0.2657, -0.2280],\n",
      "        [-0.0197, -0.1547, -0.1606,  0.2009, -0.6544],\n",
      "        [-0.5762, -1.3245, -0.6989, -1.5375, -1.4583]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0957,  2.0197, -0.3108,  0.4047,  1.2493],\n",
      "        [ 0.6198,  0.2583, -1.0544, -0.4277, -0.2972],\n",
      "        [ 0.3577, -1.2084,  1.1030, -0.0158, -0.8298],\n",
      "        [ 0.2000, -2.6224,  0.8257,  0.7131,  1.1029]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7402],\n",
      "        [ 0.9093],\n",
      "        [ 0.5426],\n",
      "        [ 0.0764]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0827, -0.3055, -0.3884,  0.2835,  0.0267],\n",
      "        [ 1.1706, -0.1224, -1.8891,  0.4232,  1.1709],\n",
      "        [-0.2460,  1.1717,  1.1209, -0.2313,  1.2551],\n",
      "        [ 0.6721,  1.6448,  1.4555,  1.1344, -0.5226]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7863, -0.2772, -0.9122, -0.9752, -2.3008],\n",
      "        [-0.0298, -0.3428, -0.7101,  0.0203, -1.0788],\n",
      "        [-0.1478, -0.0937,  0.0354, -0.2378, -0.0737],\n",
      "        [-0.4224, -1.1474, -1.5524, -0.5921, -1.8632]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0827, -0.3055, -0.3884,  0.2835,  0.0267],\n",
      "        [ 1.1706, -0.1224, -1.8891,  0.4232,  1.1709],\n",
      "        [-0.2460,  1.1717,  1.1209, -0.2313,  1.2551],\n",
      "        [ 0.6721,  1.6448,  1.4555,  1.1344, -0.5226]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7503],\n",
      "        [ 0.0940],\n",
      "        [-0.0712],\n",
      "        [-4.1288]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0582,  1.0593, -0.1368,  0.3160, -0.6467],\n",
      "        [ 0.7333, -0.1484,  0.7339, -1.0991, -0.8971],\n",
      "        [-1.6352,  1.8570,  2.0395,  0.7905, -0.7005],\n",
      "        [ 1.2790, -0.1596,  1.6271, -1.2037, -0.2214]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2817, -0.1111,  0.6127, -0.1813, -0.8727],\n",
      "        [ 0.0812,  0.2604,  0.2393, -0.2382,  0.4587],\n",
      "        [ 0.2124,  0.2616,  0.1479, -0.3375, -0.7525],\n",
      "        [ 0.8905,  0.3148,  1.2404,  0.3883,  0.9626]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0582,  1.0593, -0.1368,  0.3160, -0.6467],\n",
      "        [ 0.7333, -0.1484,  0.7339, -1.0991, -0.8971],\n",
      "        [-1.6352,  1.8570,  2.0395,  0.7905, -0.7005],\n",
      "        [ 1.2790, -0.1596,  1.6271, -1.2037, -0.2214]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3220],\n",
      "        [ 0.0469],\n",
      "        [ 0.7005],\n",
      "        [ 2.4264]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0749,  0.6418,  0.8856,  0.2441,  1.1091],\n",
      "        [ 0.0821, -0.3212,  1.7709, -0.6896,  1.0548],\n",
      "        [-0.5792, -0.5553,  0.6143,  1.9446, -0.3911],\n",
      "        [-2.2151,  0.5905, -0.0689,  0.1958,  2.4847]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2351, -0.1453, -0.2507, -0.3802,  0.3478],\n",
      "        [-0.0041,  0.0571,  0.3374,  0.1157, -0.1891],\n",
      "        [-0.3127, -0.3433, -0.4609, -0.6102, -0.2137],\n",
      "        [-0.6777,  0.2835, -0.3878, -0.7716, -0.5963]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0749,  0.6418,  0.8856,  0.2441,  1.1091],\n",
      "        [ 0.0821, -0.3212,  1.7709, -0.6896,  1.0548],\n",
      "        [-0.5792, -0.5553,  0.6143,  1.9446, -0.3911],\n",
      "        [-2.2151,  0.5905, -0.0689,  0.1958,  2.4847]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0399],\n",
      "        [ 0.2994],\n",
      "        [-1.0144],\n",
      "        [ 0.0625]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5050,  0.1955,  1.1979,  0.1042,  0.1411],\n",
      "        [-0.4716,  0.0297,  0.5164, -0.8720, -0.6667],\n",
      "        [ 0.5737, -0.2616, -0.3619,  1.0215,  1.0710],\n",
      "        [-0.5859, -2.4292,  1.5572, -1.7586,  0.3621]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2096, -0.3284,  0.2663,  0.1568,  0.5479],\n",
      "        [ 0.5751,  0.2332,  0.1660,  0.5116,  0.3140],\n",
      "        [ 0.1111, -0.1246,  0.2425,  0.2418, -0.6020],\n",
      "        [-0.1590, -0.4878, -0.0378, -0.7352, -0.1077]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5050,  0.1955,  1.1979,  0.1042,  0.1411],\n",
      "        [-0.4716,  0.0297,  0.5164, -0.8720, -0.6667],\n",
      "        [ 0.5737, -0.2616, -0.3619,  1.0215,  1.0710],\n",
      "        [-0.5859, -2.4292,  1.5572, -1.7586,  0.3621]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2425],\n",
      "        [-0.8340],\n",
      "        [-0.3891],\n",
      "        [ 2.4732]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4890, -1.7774, -0.5250, -0.3931,  1.2756],\n",
      "        [-1.5228,  0.7374,  0.0535,  0.4157, -0.2383],\n",
      "        [-0.0243,  0.0124,  0.2023,  1.1572, -0.9452],\n",
      "        [ 0.2029,  0.0714,  1.8899, -0.6516,  1.5709]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6925,  0.4496, -0.5053,  0.0072,  0.1202],\n",
      "        [ 0.3790,  0.4692,  0.2726,  0.1426,  0.5631],\n",
      "        [ 0.0154, -0.3717, -0.0937, -0.1658,  0.4679],\n",
      "        [-0.3983, -0.7504, -1.2456, -0.9876, -2.4171]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4890, -1.7774, -0.5250, -0.3931,  1.2756],\n",
      "        [-1.5228,  0.7374,  0.0535,  0.4157, -0.2383],\n",
      "        [-0.0243,  0.0124,  0.2023,  1.1572, -0.9452],\n",
      "        [ 0.2029,  0.0714,  1.8899, -0.6516,  1.5709]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7219],\n",
      "        [-0.2915],\n",
      "        [-0.6580],\n",
      "        [-5.6419]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5565,  1.6256,  0.8129,  1.4703,  2.2825],\n",
      "        [-1.7574,  0.1507,  1.0675, -0.0336,  0.7002],\n",
      "        [ 0.4760, -0.9267,  1.0444, -0.3861, -1.0616],\n",
      "        [-0.7948,  1.4070, -1.0843,  0.1944,  2.2082]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1041,  0.2893,  0.4080,  0.0499, -0.0941],\n",
      "        [ 0.6788, -0.3759, -0.4515, -0.9112, -0.4604],\n",
      "        [ 0.1453, -0.5962, -0.6318, -0.0989, -0.8446],\n",
      "        [ 0.3480,  1.5879,  0.9610,  1.4192,  2.0111]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5565,  1.6256,  0.8129,  1.4703,  2.2825],\n",
      "        [-1.7574,  0.1507,  1.0675, -0.0336,  0.7002],\n",
      "        [ 0.4760, -0.9267,  1.0444, -0.3861, -1.0616],\n",
      "        [-0.7948,  1.4070, -1.0843,  0.1944,  2.2082]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6025],\n",
      "        [-2.0233],\n",
      "        [ 0.8967],\n",
      "        [ 5.6325]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2177, -0.9669,  0.8270,  0.2720,  0.3077],\n",
      "        [ 1.0350,  1.8776, -0.4444, -1.6637, -0.5590],\n",
      "        [ 0.7441,  1.0424, -0.1776, -2.0291,  1.3704],\n",
      "        [ 0.3160, -0.3787,  0.6895, -0.5390, -0.0875]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4133,  0.0853,  0.4898,  0.0623,  0.4309],\n",
      "        [ 1.0169,  0.9914,  0.1747,  1.1999,  2.1168],\n",
      "        [-0.2586, -0.8192, -0.1225, -0.6782, -0.5566],\n",
      "        [-1.2155, -1.3400, -0.1591, -2.2066, -3.2122]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2177, -0.9669,  0.8270,  0.2720,  0.3077],\n",
      "        [ 1.0350,  1.8776, -0.4444, -1.6637, -0.5590],\n",
      "        [ 0.7441,  1.0424, -0.1776, -2.0291,  1.3704],\n",
      "        [ 0.3160, -0.3787,  0.6895, -0.5390, -0.0875]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5620],\n",
      "        [-0.3433],\n",
      "        [-0.4112],\n",
      "        [ 1.4839]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1511, -1.3441,  0.7110, -1.2793,  1.3730],\n",
      "        [ 1.1595, -0.4167,  0.1422, -0.1177, -0.3138],\n",
      "        [-2.2441,  1.1587,  0.6205, -2.1232, -0.2468],\n",
      "        [ 1.5064, -0.7404, -0.0636,  2.2581,  0.3360]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1631,  0.5527, -1.0637, -0.4676, -0.0137],\n",
      "        [ 0.4867,  1.1844,  1.2674,  0.9456,  0.7205],\n",
      "        [ 0.0855, -0.0976, -0.1386, -1.0861, -0.8856],\n",
      "        [ 0.1064,  0.7125,  0.1228,  0.3275, -0.1128]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1511, -1.3441,  0.7110, -1.2793,  1.3730],\n",
      "        [ 1.1595, -0.4167,  0.1422, -0.1177, -0.3138],\n",
      "        [-2.2441,  1.1587,  0.6205, -2.1232, -0.2468],\n",
      "        [ 1.5064, -0.7404, -0.0636,  2.2581,  0.3360]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1074],\n",
      "        [-0.0863],\n",
      "        [ 2.1336],\n",
      "        [ 0.3265]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2578,  0.0978,  0.1398, -0.6003,  1.2083],\n",
      "        [ 1.2169, -0.2316,  0.1944,  0.2119,  0.1895],\n",
      "        [-0.7103,  0.7868,  0.1034,  1.0190, -0.4766],\n",
      "        [ 1.1562, -0.2301,  0.2610,  1.7283,  0.3853]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2684,  0.7362,  0.8355,  0.6091,  0.6631],\n",
      "        [ 0.9101,  0.8680,  1.9502,  0.7264,  0.7501],\n",
      "        [-0.6435, -1.2401, -0.7381, -1.0562, -1.6942],\n",
      "        [ 0.1154, -0.7879,  0.1110,  0.6555,  0.6437]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2578,  0.0978,  0.1398, -0.6003,  1.2083],\n",
      "        [ 1.2169, -0.2316,  0.1944,  0.2119,  0.1895],\n",
      "        [-0.7103,  0.7868,  0.1034,  1.0190, -0.4766],\n",
      "        [ 1.1562, -0.2301,  0.2610,  1.7283,  0.3853]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5553],\n",
      "        [ 1.5816],\n",
      "        [-0.8638],\n",
      "        [ 1.7245]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0325,  1.2485, -1.5990, -0.1964,  1.5149],\n",
      "        [ 1.5392, -0.7522,  0.6923, -1.8876, -0.5556],\n",
      "        [-0.9940,  0.1648, -0.9411, -0.5484,  0.7520],\n",
      "        [ 1.3979,  0.7351,  0.5624,  0.5861,  2.0279]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2950,  0.9768,  0.4800,  0.4367,  0.2989],\n",
      "        [ 0.3527, -0.1963, -0.1936, -0.2009, -0.6789],\n",
      "        [-0.2312, -0.0792, -0.3832, -0.4968, -0.7756],\n",
      "        [-0.7874, -0.7324, -0.1952, -1.1482, -1.4207]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0325,  1.2485, -1.5990, -0.1964,  1.5149],\n",
      "        [ 1.5392, -0.7522,  0.6923, -1.8876, -0.5556],\n",
      "        [-0.9940,  0.1648, -0.9411, -0.5484,  0.7520],\n",
      "        [ 1.3979,  0.7351,  0.5624,  0.5861,  2.0279]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5144],\n",
      "        [ 1.3129],\n",
      "        [ 0.2666],\n",
      "        [-5.3027]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0205,  1.6513,  0.7589,  0.3359,  1.2508],\n",
      "        [-0.5414,  0.4381,  0.5051, -0.7619, -0.5506],\n",
      "        [ 1.3783, -0.0412,  0.1333, -0.0043,  0.3672],\n",
      "        [-0.3018,  0.4574, -0.7103,  0.7805,  1.6531]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3961,  0.0288,  0.0432, -0.1938, -0.0700],\n",
      "        [-0.4531, -0.4112, -0.1349, -0.2522, -0.7971],\n",
      "        [-0.6347, -0.7161,  0.5386, -0.3953, -0.5237],\n",
      "        [ 1.1576,  2.0774,  1.4902,  2.0289,  2.2705]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0205,  1.6513,  0.7589,  0.3359,  1.2508],\n",
      "        [-0.5414,  0.4381,  0.5051, -0.7619, -0.5506],\n",
      "        [ 1.3783, -0.0412,  0.1333, -0.0043,  0.3672],\n",
      "        [-0.3018,  0.4574, -0.7103,  0.7805,  1.6531]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7280],\n",
      "        [ 0.6280],\n",
      "        [-0.9641],\n",
      "        [ 4.8793]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4737, -0.6523, -0.0186,  0.9334, -0.6962],\n",
      "        [ 0.5321,  0.6116,  0.4657,  1.6062,  0.5767],\n",
      "        [-0.9580,  0.3071, -2.6875,  0.2062,  0.8783],\n",
      "        [-1.1779, -0.2144, -1.0220, -1.3767,  1.0938]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0630, -0.0842,  0.4186,  0.3786, -0.0145],\n",
      "        [-0.2639, -0.1920, -0.5859,  0.5483, -0.9330],\n",
      "        [ 0.0541, -1.0276, -0.6039, -1.1340, -0.4959],\n",
      "        [ 0.0858, -0.7763, -0.4765, -0.0692, -0.8821]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4737, -0.6523, -0.0186,  0.9334, -0.6962],\n",
      "        [ 0.5321,  0.6116,  0.4657,  1.6062,  0.5767],\n",
      "        [-0.9580,  0.3071, -2.6875,  0.2062,  0.8783],\n",
      "        [-1.1779, -0.2144, -1.0220, -1.3767,  1.0938]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4405],\n",
      "        [-0.1880],\n",
      "        [ 0.5862],\n",
      "        [-0.3172]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5281, -1.3627,  0.5145,  1.6642, -0.1563],\n",
      "        [-1.2267,  1.2966,  0.3031,  1.0492, -0.2165],\n",
      "        [ 1.3983,  0.2249,  0.5328,  0.7211, -0.9260],\n",
      "        [ 0.1025, -0.1817,  0.5595, -0.8946,  0.8013]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1657,  0.0554, -0.3222,  0.2637,  0.9290],\n",
      "        [ 0.1483,  0.9791,  0.2907,  0.2344,  0.1430],\n",
      "        [ 0.1863, -0.5218,  0.3049,  0.0270, -1.0112],\n",
      "        [ 0.2149,  0.6646, -0.2466,  0.7335,  0.3623]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5281, -1.3627,  0.5145,  1.6642, -0.1563],\n",
      "        [-1.2267,  1.2966,  0.3031,  1.0492, -0.2165],\n",
      "        [ 1.3983,  0.2249,  0.5328,  0.7211, -0.9260],\n",
      "        [ 0.1025, -0.1817,  0.5595, -0.8946,  0.8013]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0351],\n",
      "        [ 1.3907],\n",
      "        [ 1.2616],\n",
      "        [-0.6026]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5525, -0.9781,  1.9334, -0.7714, -3.2510],\n",
      "        [-2.0816, -0.6155,  1.0359, -0.5116, -0.5358],\n",
      "        [-1.3319,  0.7164,  1.0301,  0.4666, -1.6489],\n",
      "        [-0.2007,  1.0895,  0.6355,  1.4344, -0.1652]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2484,  1.2987, -0.2362, -0.0582,  0.1572],\n",
      "        [-0.4995, -0.5357, -0.0346,  0.0529, -0.6863],\n",
      "        [-0.2913, -1.3184, -0.1306, -1.0956, -1.5172],\n",
      "        [-0.0170,  0.1145,  1.4500,  0.3461,  1.3524]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5525, -0.9781,  1.9334, -0.7714, -3.2510],\n",
      "        [-2.0816, -0.6155,  1.0359, -0.5116, -0.5358],\n",
      "        [-1.3319,  0.7164,  1.0301,  0.4666, -1.6489],\n",
      "        [-0.2007,  1.0895,  0.6355,  1.4344, -0.1652]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0557],\n",
      "        [ 1.6743],\n",
      "        [ 1.2993],\n",
      "        [ 1.3226]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7127, -0.0684, -0.7120, -0.9316, -0.3594],\n",
      "        [ 0.9390, -0.8875,  0.9344,  1.0189,  1.8970],\n",
      "        [-1.1509,  1.6750, -1.0969, -0.3596, -0.6030],\n",
      "        [ 0.5679, -0.3048,  0.4318, -1.6637, -0.8763]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9446,  0.7839,  1.0729,  1.5499,  1.8659],\n",
      "        [-0.8602, -1.1522, -0.7911, -0.7361, -1.3576],\n",
      "        [-0.4299, -1.2854, -0.9662, -1.4495, -2.4926],\n",
      "        [-0.1803,  0.3096, -0.4259,  0.9015, -0.1061]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7127, -0.0684, -0.7120, -0.9316, -0.3594],\n",
      "        [ 0.9390, -0.8875,  0.9344,  1.0189,  1.8970],\n",
      "        [-1.1509,  1.6750, -1.0969, -0.3596, -0.6030],\n",
      "        [ 0.5679, -0.3048,  0.4318, -1.6637, -0.8763]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.6050],\n",
      "        [-3.8497],\n",
      "        [ 1.4257],\n",
      "        [-1.7876]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1136,  0.5506, -0.3499,  1.9383, -0.1353],\n",
      "        [ 1.1111, -0.0482,  0.4009,  0.3861, -0.4818],\n",
      "        [-0.2855, -0.2005, -0.7680,  0.1296,  0.4189],\n",
      "        [-0.4157, -0.2237, -0.0466, -0.9802, -1.7694]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6763,  1.9024,  2.0655,  1.8908,  3.1898],\n",
      "        [ 0.7377,  0.8133,  1.5640,  1.4297,  1.5992],\n",
      "        [ 0.4250,  0.1824,  0.2357,  0.6792, -0.1492],\n",
      "        [ 0.2550,  0.8415,  1.2565,  1.0113,  1.1102]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1136,  0.5506, -0.3499,  1.9383, -0.1353],\n",
      "        [ 1.1111, -0.0482,  0.4009,  0.3861, -0.4818],\n",
      "        [-0.2855, -0.2005, -0.7680,  0.1296,  0.4189],\n",
      "        [-0.4157, -0.2237, -0.0466, -0.9802, -1.7694]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0154],\n",
      "        [ 1.1891],\n",
      "        [-0.3133],\n",
      "        [-3.3085]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6913,  0.0244,  0.9546, -0.3823,  0.6163],\n",
      "        [ 0.0659, -0.7308,  1.3495,  0.0113,  1.0149],\n",
      "        [ 1.0212, -0.3691, -0.7846,  0.5040,  0.6265],\n",
      "        [ 0.2568,  1.1838,  1.8570, -1.0802,  0.1025]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0915, -0.2548, -0.1726, -0.4207,  0.1066],\n",
      "        [ 0.5950,  0.7770,  1.1021,  0.6988,  1.2144],\n",
      "        [-0.2908,  0.2165,  0.1840,  0.3511, -0.4121],\n",
      "        [ 1.4150,  2.0491,  2.3399,  1.7618,  2.2957]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6913,  0.0244,  0.9546, -0.3823,  0.6163],\n",
      "        [ 0.0659, -0.7308,  1.3495,  0.0113,  1.0149],\n",
      "        [ 1.0212, -0.3691, -0.7846,  0.5040,  0.6265],\n",
      "        [ 0.2568,  1.1838,  1.8570, -1.0802,  0.1025]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1188],\n",
      "        [ 2.1989],\n",
      "        [-0.6025],\n",
      "        [ 5.4665]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1628,  0.0962, -0.4314, -0.4882,  0.5891],\n",
      "        [-0.8482,  2.1744, -0.3317, -0.0517, -0.7465],\n",
      "        [-0.7189,  0.2493,  1.3677, -3.0596, -1.2667],\n",
      "        [-0.2962,  0.9304, -0.8140, -1.2146, -0.6189]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0699,  0.1224,  0.3103,  0.2626,  0.2840],\n",
      "        [-0.5725, -1.0390, -0.4085, -0.7632, -0.7065],\n",
      "        [-0.3514, -0.1301,  0.5753,  0.3800, -0.3893],\n",
      "        [-0.2095, -0.2298, -0.2075,  0.4678,  0.3158]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1628,  0.0962, -0.4314, -0.4882,  0.5891],\n",
      "        [-0.8482,  2.1744, -0.3317, -0.0517, -0.7465],\n",
      "        [-0.7189,  0.2493,  1.3677, -3.0596, -1.2667],\n",
      "        [-0.2962,  0.9304, -0.8140, -1.2146, -0.6189]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0716],\n",
      "        [-1.0712],\n",
      "        [ 0.3375],\n",
      "        [-0.7464]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5212,  1.0529,  1.0407,  0.8804,  1.1573],\n",
      "        [-0.1632,  0.5584,  2.1068, -0.1759,  1.1742],\n",
      "        [-0.3647, -1.8459,  0.2226, -1.1117,  2.0225],\n",
      "        [ 0.7336, -0.5416, -0.4891, -1.1186,  0.7145]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8006,  0.2220,  0.0989, -0.1136, -0.0674],\n",
      "        [-0.1850,  0.3234,  0.6720,  0.2967,  0.1448],\n",
      "        [ 0.3193, -0.0950,  0.2311, -0.4585, -1.0610],\n",
      "        [-0.0572,  1.5505,  1.0188,  0.4920,  0.2746]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5212,  1.0529,  1.0407,  0.8804,  1.1573],\n",
      "        [-0.1632,  0.5584,  2.1068, -0.1759,  1.1742],\n",
      "        [-0.3647, -1.8459,  0.2226, -1.1117,  2.0225],\n",
      "        [ 0.7336, -0.5416, -0.4891, -1.1186,  0.7145]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5759],\n",
      "        [ 1.7443],\n",
      "        [-1.5258],\n",
      "        [-1.7343]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0878, -2.8689,  1.4484,  1.0547,  0.6986],\n",
      "        [ 0.0590, -1.0486,  0.3574,  1.5186,  1.0689],\n",
      "        [-1.3606,  0.8314, -0.0647, -0.9035,  0.8444],\n",
      "        [ 1.2866,  0.3813,  0.7796, -0.3555,  0.3608]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2394, -0.1791, -0.0006,  0.2913, -0.0424],\n",
      "        [-0.7987, -0.1658, -0.8366, -0.5686, -1.5067],\n",
      "        [ 0.3207,  0.7558,  0.5225,  0.1023,  1.0422],\n",
      "        [ 1.3288,  1.6616,  1.5375,  1.4876,  1.4257]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0878, -2.8689,  1.4484,  1.0547,  0.6986],\n",
      "        [ 0.0590, -1.0486,  0.3574,  1.5186,  1.0689],\n",
      "        [-1.3606,  0.8314, -0.0647, -0.9035,  0.8444],\n",
      "        [ 1.2866,  0.3813,  0.7796, -0.3555,  0.3608]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8116],\n",
      "        [-2.6463],\n",
      "        [ 0.9457],\n",
      "        [ 3.5274]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7001, -0.4741,  1.0961, -0.1268, -0.2071],\n",
      "        [ 0.0766, -0.4586, -0.4190, -0.5263,  0.0253],\n",
      "        [-0.6796,  1.1267,  0.3118, -0.4893, -2.0876],\n",
      "        [-0.8422,  0.0711, -1.1266, -0.9832,  2.3547]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3588, -0.4353,  0.0101,  0.5703,  0.2623],\n",
      "        [ 0.2983,  0.6852,  0.8334,  1.1309,  0.6788],\n",
      "        [ 0.5267,  0.3372,  0.2545,  0.2858,  1.0075],\n",
      "        [ 0.0286,  0.2214, -0.0886,  0.0395, -0.1536]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7001, -0.4741,  1.0961, -0.1268, -0.2071],\n",
      "        [ 0.0766, -0.4586, -0.4190, -0.5263,  0.0253],\n",
      "        [-0.6796,  1.1267,  0.3118, -0.4893, -2.0876],\n",
      "        [-0.8422,  0.0711, -1.1266, -0.9832,  2.3547]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1605],\n",
      "        [-1.2186],\n",
      "        [-2.1417],\n",
      "        [-0.3089]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3325,  0.7817,  1.3436,  0.9260,  1.3056],\n",
      "        [ 1.3641,  0.2017, -0.5205,  0.3827,  0.5218],\n",
      "        [-0.6797,  0.3795, -1.2255,  0.2315, -0.3619],\n",
      "        [ 0.4114, -1.3908,  0.1835,  0.0732,  1.0924]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6266,  0.4530,  0.1312,  0.4409, -0.6095],\n",
      "        [ 1.2062,  1.1374,  1.4422,  0.7064,  1.7235],\n",
      "        [ 0.8935,  1.1939,  1.0208,  1.1507,  1.7050],\n",
      "        [ 0.5214,  1.5851,  1.1430,  0.4664,  0.3463]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3325,  0.7817,  1.3436,  0.9260,  1.3056],\n",
      "        [ 1.3641,  0.2017, -0.5205,  0.3827,  0.5218],\n",
      "        [-0.6797,  0.3795, -1.2255,  0.2315, -0.3619],\n",
      "        [ 0.4114, -1.3908,  0.1835,  0.0732,  1.0924]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0655],\n",
      "        [ 2.2940],\n",
      "        [-1.7559],\n",
      "        [-1.3680]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6147,  1.4355, -0.3821,  0.1430,  1.0318],\n",
      "        [ 0.8244, -0.0577,  0.9976,  0.0285,  1.3524],\n",
      "        [-1.3644,  0.5309,  0.3791,  2.2611,  0.1879],\n",
      "        [-1.7317, -1.7768, -0.3068, -0.5046,  1.3733]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2407, -0.0593,  0.1213,  0.0094,  0.0930],\n",
      "        [-0.5791, -0.2520,  0.0671,  0.1159, -0.3326],\n",
      "        [ 1.5003,  2.1589,  1.7405,  0.8115,  1.9830],\n",
      "        [ 0.2973,  1.9817,  1.4918,  1.9639,  2.1014]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6147,  1.4355, -0.3821,  0.1430,  1.0318],\n",
      "        [ 0.8244, -0.0577,  0.9976,  0.0285,  1.3524],\n",
      "        [-1.3644,  0.5309,  0.3791,  2.2611,  0.1879],\n",
      "        [-1.7317, -1.7768, -0.3068, -0.5046,  1.3733]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1138],\n",
      "        [-0.8424],\n",
      "        [ 1.9666],\n",
      "        [-2.5985]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1609,  0.7948, -1.0529,  0.4713,  1.2138],\n",
      "        [-2.1876,  1.3313, -1.1887, -0.2023,  0.6238],\n",
      "        [ 0.7735,  2.2773,  1.7495,  1.1397,  0.7425],\n",
      "        [ 0.1797,  3.0836,  0.8535,  0.7944,  0.5198]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2525,  0.4073,  0.2252, -0.2557, -0.1026],\n",
      "        [ 0.0583,  0.8452,  0.3113,  0.5039,  0.7865],\n",
      "        [ 0.6643,  0.5742,  0.4587,  0.6254,  1.0116],\n",
      "        [ 0.4194, -0.2238,  0.2263,  0.1264,  0.0647]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1609,  0.7948, -1.0529,  0.4713,  1.2138],\n",
      "        [-2.1876,  1.3313, -1.1887, -0.2023,  0.6238],\n",
      "        [ 0.7735,  2.2773,  1.7495,  1.1397,  0.7425],\n",
      "        [ 0.1797,  3.0836,  0.8535,  0.7944,  0.5198]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1991],\n",
      "        [ 1.0164],\n",
      "        [ 4.0879],\n",
      "        [-0.2875]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8959, -0.5016, -0.2228, -0.2523,  1.0630],\n",
      "        [-0.4679, -0.3205,  0.3166,  0.7011, -0.2144],\n",
      "        [ 0.6050,  0.6976,  0.0718,  2.8812, -0.4423],\n",
      "        [ 0.8848, -0.3001,  0.4332, -0.7079,  1.5217]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5518, -0.3027,  0.1016,  0.1282, -0.1559],\n",
      "        [-0.0640, -0.1286,  0.0382, -0.0589,  0.1312],\n",
      "        [-0.5748, -1.1564, -1.0572, -1.9745, -2.1680],\n",
      "        [ 0.3663, -0.3485,  0.2534, -0.2404, -0.1492]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8959, -0.5016, -0.2228, -0.2523,  1.0630],\n",
      "        [-0.4679, -0.3205,  0.3166,  0.7011, -0.2144],\n",
      "        [ 0.6050,  0.6976,  0.0718,  2.8812, -0.4423],\n",
      "        [ 0.8848, -0.3001,  0.4332, -0.7079,  1.5217]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4255],\n",
      "        [ 0.0138],\n",
      "        [-5.9603],\n",
      "        [ 0.4817]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4220, -1.4938, -1.5350,  1.1534,  0.5938],\n",
      "        [ 0.5933, -1.5857,  0.2018,  0.4186,  0.9528],\n",
      "        [ 2.4922,  0.3762,  0.7687, -1.2436,  0.3777],\n",
      "        [ 0.1342, -0.0178,  1.9649,  1.2410,  1.2126]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3091,  0.7953,  0.0080,  0.1356,  0.3834],\n",
      "        [-0.4430,  0.6096,  0.8329,  0.3248,  0.4990],\n",
      "        [ 1.0622,  1.7258,  1.8352,  1.1615,  2.7319],\n",
      "        [ 0.0558, -0.6815,  0.0496, -0.2145,  0.1697]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4220, -1.4938, -1.5350,  1.1534,  0.5938],\n",
      "        [ 0.5933, -1.5857,  0.2018,  0.4186,  0.9528],\n",
      "        [ 2.4922,  0.3762,  0.7687, -1.2436,  0.3777],\n",
      "        [ 0.1342, -0.0178,  1.9649,  1.2410,  1.2126]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2559],\n",
      "        [-0.4500],\n",
      "        [ 4.2945],\n",
      "        [ 0.0566]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2777, -0.0608,  1.8422,  1.8900,  1.7265],\n",
      "        [-0.8098, -1.5908,  0.9201,  0.7511, -1.1094],\n",
      "        [-0.4422,  0.7044,  0.1433, -0.7127, -0.4272],\n",
      "        [-1.8549,  1.5783, -0.0562, -0.8527,  1.3205]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4257,  0.2292,  0.7065,  0.4097,  1.4957],\n",
      "        [ 0.3689,  0.7022,  0.7509,  1.0138,  0.4635],\n",
      "        [-0.7041,  0.9398, -0.0144,  0.2390,  0.0179],\n",
      "        [-0.2186,  0.4908, -0.0022,  0.0789, -0.4052]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2777, -0.0608,  1.8422,  1.8900,  1.7265],\n",
      "        [-0.8098, -1.5908,  0.9201,  0.7511, -1.1094],\n",
      "        [-0.4422,  0.7044,  0.1433, -0.7127, -0.4272],\n",
      "        [-1.8549,  1.5783, -0.0562, -0.8527,  1.3205]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.1004],\n",
      "        [-0.4776],\n",
      "        [ 0.7933],\n",
      "        [ 0.5778]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8963,  0.8230, -0.7391,  0.4010, -1.2741],\n",
      "        [-0.2918, -1.3373,  1.1464, -1.3029,  1.1763],\n",
      "        [-1.0540,  0.2960, -0.1480, -0.3179, -0.3329],\n",
      "        [ 0.8651,  0.1633,  0.3272, -0.1711, -0.7832]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9204, -1.4207, -2.1669, -1.8191, -2.9120],\n",
      "        [ 0.1912,  1.0128, -0.0815,  0.6586,  0.9242],\n",
      "        [ 0.1661,  1.0357,  0.8894,  0.7270,  0.7491],\n",
      "        [-0.2700,  0.1896,  0.6191, -0.0134,  0.1279]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8963,  0.8230, -0.7391,  0.4010, -1.2741],\n",
      "        [-0.2918, -1.3373,  1.1464, -1.3029,  1.1763],\n",
      "        [-1.0540,  0.2960, -0.1480, -0.3179, -0.3329],\n",
      "        [ 0.8651,  0.1633,  0.3272, -0.1711, -0.7832]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5884],\n",
      "        [-1.2745],\n",
      "        [-0.4806],\n",
      "        [-0.0979]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4320,  0.9475,  0.5450, -0.1452,  0.4894],\n",
      "        [ 1.2358,  0.7245,  1.2487,  0.8866,  1.0470],\n",
      "        [ 0.8328, -1.4688,  0.3431, -0.8365,  0.7454],\n",
      "        [ 0.5346,  0.8187,  0.8464,  1.3189, -0.7742]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.9961, -1.9634, -2.5796, -2.2731, -5.0190],\n",
      "        [ 1.2730,  0.7689,  0.6404,  0.3151,  1.0616],\n",
      "        [ 0.2263,  0.7132,  1.4094,  0.9395,  1.0693],\n",
      "        [-0.1740, -0.0228, -0.5005,  0.5504,  0.1711]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4320,  0.9475,  0.5450, -0.1452,  0.4894],\n",
      "        [ 1.2358,  0.7245,  1.2487,  0.8866,  1.0470],\n",
      "        [ 0.8328, -1.4688,  0.3431, -0.8365,  0.7454],\n",
      "        [ 0.5346,  0.8187,  0.8464,  1.3189, -0.7742]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-6.2549],\n",
      "        [ 4.3207],\n",
      "        [-0.3643],\n",
      "        [ 0.0581]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2332, -0.5832,  0.2617, -1.2147,  1.4316],\n",
      "        [-0.1896, -0.2430,  1.1732, -0.0179,  0.3736],\n",
      "        [ 0.3204,  0.2688, -0.9953, -0.8396, -0.4162],\n",
      "        [ 0.2670, -0.9345, -0.6720,  0.3708,  0.1578]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0775,  0.1750,  0.1831, -0.1712,  0.2146],\n",
      "        [-0.8707, -1.3007,  0.0761, -1.5515, -2.7384],\n",
      "        [ 0.5897,  1.4450,  0.8182,  0.8161,  0.8083],\n",
      "        [-0.0641, -0.3074,  0.1285,  0.3631, -0.3147]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2332, -0.5832,  0.2617, -1.2147,  1.4316],\n",
      "        [-0.1896, -0.2430,  1.1732, -0.0179,  0.3736],\n",
      "        [ 0.3204,  0.2688, -0.9953, -0.8396, -0.4162],\n",
      "        [ 0.2670, -0.9345, -0.6720,  0.3708,  0.1578]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4792],\n",
      "        [-0.4250],\n",
      "        [-1.2586],\n",
      "        [ 0.2687]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1953,  0.2336, -0.2259, -0.4097,  1.6015],\n",
      "        [-0.7866, -0.1490,  0.5690,  0.8092,  0.4844],\n",
      "        [-1.7633, -0.5782,  0.5272,  1.7039,  0.5798],\n",
      "        [-0.6555, -0.1608,  0.7597,  1.0450, -0.5518]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5176, -0.5941,  0.4259,  0.4537,  0.2070],\n",
      "        [-0.8216, -0.6203, -0.6541, -0.7593, -1.8539],\n",
      "        [ 0.9730,  0.8522,  1.6858,  1.3251,  1.0994],\n",
      "        [ 0.4798,  0.0022, -0.0422,  0.0364, -0.0106]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1953,  0.2336, -0.2259, -0.4097,  1.6015],\n",
      "        [-0.7866, -0.1490,  0.5690,  0.8092,  0.4844],\n",
      "        [-1.7633, -0.5782,  0.5272,  1.7039,  0.5798],\n",
      "        [-0.6555, -0.1608,  0.7597,  1.0450, -0.5518]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1906],\n",
      "        [-1.1459],\n",
      "        [ 1.5757],\n",
      "        [-0.3030]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0453,  0.1951,  0.0714, -0.0354,  1.0635],\n",
      "        [-0.0120,  1.1239,  0.5295, -0.7963,  0.6035],\n",
      "        [-0.2147, -0.3724,  0.4610,  2.2657, -0.5358],\n",
      "        [ 0.4521,  1.4908,  1.7698,  0.5074, -1.7919]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2764,  0.0360,  0.1154,  0.8133,  0.1077],\n",
      "        [ 0.3454, -0.1912, -0.2537, -0.0745, -0.9056],\n",
      "        [ 0.1304,  0.5467,  0.2570,  0.3148,  0.1611],\n",
      "        [ 0.4023, -0.7739, -0.4022, -0.4587,  1.1265]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0453,  0.1951,  0.0714, -0.0354,  1.0635],\n",
      "        [-0.0120,  1.1239,  0.5295, -0.7963,  0.6035],\n",
      "        [-0.2147, -0.3724,  0.4610,  2.2657, -0.5358],\n",
      "        [ 0.4521,  1.4908,  1.7698,  0.5074, -1.7919]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4643],\n",
      "        [-0.8406],\n",
      "        [ 0.5139],\n",
      "        [-3.9349]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3989,  0.0595,  0.1255, -1.0019,  1.5714],\n",
      "        [-0.6443,  1.4944,  1.9011,  1.5052,  1.6029],\n",
      "        [-0.6817,  1.1751,  0.2820,  0.7390,  1.0163],\n",
      "        [-0.6559, -0.7141,  0.9129, -0.0185,  1.5052]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4142, -0.2714, -0.2743, -0.2094,  0.6182],\n",
      "        [ 0.3148, -1.0921, -0.3582, -0.3908,  0.6581],\n",
      "        [ 0.1380,  0.0567,  0.3746,  0.1072, -0.2301],\n",
      "        [ 1.2083,  2.0213,  1.5604,  1.3309,  1.7116]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3989,  0.0595,  0.1255, -1.0019,  1.5714],\n",
      "        [-0.6443,  1.4944,  1.9011,  1.5052,  1.6029],\n",
      "        [-0.6817,  1.1751,  0.2820,  0.7390,  1.0163],\n",
      "        [-0.6559, -0.7141,  0.9129, -0.0185,  1.5052]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9654],\n",
      "        [-2.0493],\n",
      "        [-0.0764],\n",
      "        [ 1.7400]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2502,  0.6107,  1.7335,  0.1170, -0.3498],\n",
      "        [ 1.1738,  1.3619,  0.1068, -1.0668, -0.0799],\n",
      "        [ 1.1948,  0.7071,  0.1682,  0.4338,  0.0042],\n",
      "        [ 1.1532,  0.8942, -1.9557, -0.4656, -0.8743]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3190, -0.5362, -0.0046,  0.2771, -0.6693],\n",
      "        [ 0.6081,  1.0579,  1.2004,  0.6913,  1.3960],\n",
      "        [ 0.7203, -0.3474,  0.6916,  0.0635,  0.6138],\n",
      "        [ 0.3382,  0.9513,  1.1528,  0.8631,  1.7424]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2502,  0.6107,  1.7335,  0.1170, -0.3498],\n",
      "        [ 1.1738,  1.3619,  0.1068, -1.0668, -0.0799],\n",
      "        [ 1.1948,  0.7071,  0.1682,  0.4338,  0.0042],\n",
      "        [ 1.1532,  0.8942, -1.9557, -0.4656, -0.8743]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0108],\n",
      "        [ 1.4337],\n",
      "        [ 0.7614],\n",
      "        [-2.9391]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1354,  1.4371,  0.8560, -0.8921,  0.0686],\n",
      "        [-0.8678,  0.3238, -0.5361, -0.0489,  1.0494],\n",
      "        [ 0.9953, -0.2874,  0.3656,  0.0324, -0.1982],\n",
      "        [-1.2419,  1.2812, -0.1872, -0.2572,  0.0098]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0842,  0.1131, -0.1295,  0.5130, -0.4709],\n",
      "        [ 0.5033, -0.3563,  0.1567,  0.0975, -0.0331],\n",
      "        [ 0.1226, -0.1464,  0.1034,  0.2045, -0.1523],\n",
      "        [ 0.3052, -0.1377,  0.7444,  0.3668, -0.1028]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1354,  1.4371,  0.8560, -0.8921,  0.0686],\n",
      "        [-0.8678,  0.3238, -0.5361, -0.0489,  1.0494],\n",
      "        [ 0.9953, -0.2874,  0.3656,  0.0324, -0.1982],\n",
      "        [-1.2419,  1.2812, -0.1872, -0.2572,  0.0098]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3426],\n",
      "        [-0.6757],\n",
      "        [ 0.2387],\n",
      "        [-0.7902]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0006, -0.0590, -0.7833,  0.3003,  0.0596],\n",
      "        [-1.2867,  1.2091, -1.1444,  0.1548, -0.8906],\n",
      "        [ 0.7583, -0.3428,  1.1603,  2.8443,  0.4329],\n",
      "        [ 0.4666,  1.3235, -1.0912, -0.3272, -1.2874]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1438,  0.1840, -0.6522, -0.6669, -0.4382],\n",
      "        [ 0.4603,  0.2470, -0.0838,  0.3478,  0.3777],\n",
      "        [-0.0812, -0.4032,  0.6642,  0.6370, -0.3599],\n",
      "        [ 0.7688, -0.5371,  0.2061, -0.4099,  0.6625]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0006, -0.0590, -0.7833,  0.3003,  0.0596],\n",
      "        [-1.2867,  1.2091, -1.1444,  0.1548, -0.8906],\n",
      "        [ 0.7583, -0.3428,  1.1603,  2.8443,  0.4329],\n",
      "        [ 0.4666,  1.3235, -1.0912, -0.3272, -1.2874]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2737],\n",
      "        [-0.4802],\n",
      "        [ 2.5034],\n",
      "        [-1.2958]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3500,  0.0248,  1.5868, -0.3982, -1.6631],\n",
      "        [ 0.2485,  0.3828,  1.3801, -0.4288,  0.2913],\n",
      "        [ 1.2456, -1.0517, -0.6987, -1.0262, -0.6196],\n",
      "        [ 1.8730,  0.9911, -1.4701, -0.8979, -0.7617]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3569, -0.0824, -0.3641, -0.5786,  0.2959],\n",
      "        [-0.1103, -0.6868, -0.0741, -0.4185, -0.1912],\n",
      "        [-0.6833, -0.7917, -1.2194, -1.5241, -1.3880],\n",
      "        [ 0.5004,  0.1188,  0.3919, -0.0729,  1.3401]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3500,  0.0248,  1.5868, -0.3982, -1.6631],\n",
      "        [ 0.2485,  0.3828,  1.3801, -0.4288,  0.2913],\n",
      "        [ 1.2456, -1.0517, -0.6987, -1.0262, -0.6196],\n",
      "        [ 1.8730,  0.9911, -1.4701, -0.8979, -0.7617]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7166],\n",
      "        [-0.2687],\n",
      "        [ 3.2574],\n",
      "        [-0.4764]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0029,  0.6583,  1.8575,  0.7470,  0.0986],\n",
      "        [ 0.0711, -1.0281, -1.3332, -0.0101,  1.5106],\n",
      "        [-0.3895,  1.0050,  0.6665, -1.1802, -1.4295],\n",
      "        [ 0.1512, -0.1708,  0.6602,  0.9030, -0.9794]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3622, -0.3446,  0.0784,  0.6776, -0.1987],\n",
      "        [ 0.1714, -0.8377,  1.0468, -0.8039, -0.4930],\n",
      "        [-1.3729, -1.3684, -1.5660, -2.6405, -3.8588],\n",
      "        [ 0.2015,  0.6540,  0.4357,  0.7079,  0.6854]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0029,  0.6583,  1.8575,  0.7470,  0.0986],\n",
      "        [ 0.0711, -1.0281, -1.3332, -0.0101,  1.5106],\n",
      "        [-0.3895,  1.0050,  0.6665, -1.1802, -1.4295],\n",
      "        [ 0.1512, -0.1708,  0.6602,  0.9030, -0.9794]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0422],\n",
      "        [-1.2588],\n",
      "        [ 6.7485],\n",
      "        [ 0.1744]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9604,  0.2577,  1.2062,  2.5954, -0.4610],\n",
      "        [ 0.4996, -0.1015, -0.3663, -0.1876,  0.0932],\n",
      "        [-0.5691, -0.2802,  0.2850,  1.3067,  0.1089],\n",
      "        [-0.5711, -0.6390,  0.8344,  0.6900, -0.3039]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2795, -0.4736, -0.5797, -0.2774,  0.0739],\n",
      "        [ 0.5833,  0.4012,  0.5655,  0.1189,  1.0635],\n",
      "        [ 0.0828, -0.1962, -0.0104, -0.4834,  0.0588],\n",
      "        [ 0.7755,  0.0811,  0.1836, -0.2539,  0.7864]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9604,  0.2577,  1.2062,  2.5954, -0.4610],\n",
      "        [ 0.4996, -0.1015, -0.3663, -0.1876,  0.0932],\n",
      "        [-0.5691, -0.2802,  0.2850,  1.3067,  0.1089],\n",
      "        [-0.5711, -0.6390,  0.8344,  0.6900, -0.3039]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8437],\n",
      "        [ 0.1204],\n",
      "        [-0.6204],\n",
      "        [-0.7556]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6068, -0.6700,  0.6579,  0.5512, -0.1999],\n",
      "        [-0.7232, -0.7694,  1.6761,  1.2365, -0.1354],\n",
      "        [ 1.2631,  0.0935,  0.3592,  0.6135, -0.2139],\n",
      "        [ 0.0935, -0.1048, -1.4302,  0.2332,  0.1519]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2027,  1.2751,  1.5790,  1.4114,  1.2773],\n",
      "        [ 0.4480, -0.0303, -0.2115, -0.4875, -0.0718],\n",
      "        [ 0.7113, -0.3958, -0.5744, -0.5870,  0.2620],\n",
      "        [ 0.0329,  0.0743,  0.5750,  0.4883,  1.0696]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6068, -0.6700,  0.6579,  0.5512, -0.1999],\n",
      "        [-0.7232, -0.7694,  1.6761,  1.2365, -0.1354],\n",
      "        [ 1.2631,  0.0935,  0.3592,  0.6135, -0.2139],\n",
      "        [ 0.0935, -0.1048, -1.4302,  0.2332,  0.1519]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0329],\n",
      "        [-1.2483],\n",
      "        [ 0.2390],\n",
      "        [-0.5506]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3273, -0.2332,  0.1406, -0.8223, -0.8711],\n",
      "        [-0.0453, -0.9054,  1.4151,  1.4561, -0.4476],\n",
      "        [-0.1593, -0.3144, -1.2378, -1.1585, -1.0029],\n",
      "        [ 1.3118,  1.5238,  0.4465, -0.3500,  2.2263]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1020,  0.0374,  0.5533, -0.1652,  0.5516],\n",
      "        [ 0.7039,  0.3750, -0.1713,  0.2738,  0.4380],\n",
      "        [ 0.2018, -0.4150,  0.1333, -0.5040, -0.0656],\n",
      "        [ 0.4038, -0.1836,  0.2591, -0.3490,  0.3829]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3273, -0.2332,  0.1406, -0.8223, -0.8711],\n",
      "        [-0.0453, -0.9054,  1.4151,  1.4561, -0.4476],\n",
      "        [-0.1593, -0.3144, -1.2378, -1.1585, -1.0029],\n",
      "        [ 1.3118,  1.5238,  0.4465, -0.3500,  2.2263]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4109],\n",
      "        [-0.4112],\n",
      "        [ 0.5829],\n",
      "        [ 1.3402]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9617,  0.6064, -1.3938,  1.4023, -0.8426],\n",
      "        [ 1.0543,  0.8578, -0.4702,  2.4922,  1.8241],\n",
      "        [-0.5858, -0.9686,  0.8335,  0.9921,  1.0424],\n",
      "        [ 0.1136,  0.3273,  0.1375, -0.7170,  2.8502]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6468, -0.5492, -0.0534,  0.0667,  0.2341],\n",
      "        [ 0.9514, -0.1776,  1.3574,  0.4230,  0.8688],\n",
      "        [ 0.2891,  0.0792,  0.2039,  0.1244,  0.3618],\n",
      "        [ 0.5563, -1.0428, -0.5440, -0.5555, -1.3989]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9617,  0.6064, -1.3938,  1.4023, -0.8426],\n",
      "        [ 1.0543,  0.8578, -0.4702,  2.4922,  1.8241],\n",
      "        [-0.5858, -0.9686,  0.8335,  0.9921,  1.0424],\n",
      "        [ 0.1136,  0.3273,  0.1375, -0.7170,  2.8502]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9843],\n",
      "        [ 2.8517],\n",
      "        [ 0.4245],\n",
      "        [-3.9416]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0207,  0.2535, -0.3878,  0.4207, -0.5324],\n",
      "        [-1.2739,  0.9231,  0.9337,  1.1400,  0.1941],\n",
      "        [ 0.0204, -0.6737,  1.1607, -1.0138,  1.6788],\n",
      "        [ 1.1361, -0.3000,  1.2191,  1.3686,  0.2527]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1488,  0.3754,  0.6049,  0.5574,  0.8067],\n",
      "        [-0.2923, -0.6676, -1.6484, -1.1535, -1.2926],\n",
      "        [-0.2956,  0.6920, -0.1289,  0.6363,  0.2619],\n",
      "        [ 1.5198,  1.7354,  1.2244,  1.5603,  1.9822]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0207,  0.2535, -0.3878,  0.4207, -0.5324],\n",
      "        [-1.2739,  0.9231,  0.9337,  1.1400,  0.1941],\n",
      "        [ 0.0204, -0.6737,  1.1607, -1.0138,  1.6788],\n",
      "        [ 1.1361, -0.3000,  1.2191,  1.3686,  0.2527]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8381],\n",
      "        [-3.3489],\n",
      "        [-0.8274],\n",
      "        [ 5.3349]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2723,  0.3942,  1.3083, -1.7395, -0.3550],\n",
      "        [ 0.7726, -0.2734,  0.6493,  0.3904, -0.0627],\n",
      "        [-0.1330,  0.9183, -0.2020, -0.7583, -0.0019],\n",
      "        [-0.3317,  1.4949,  0.7405, -0.4628, -1.3548]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0194, -0.6641, -0.2144, -0.8944, -0.2217],\n",
      "        [ 1.1622, -0.1175,  1.0955,  0.6096,  0.9743],\n",
      "        [ 0.6753,  1.1579, -0.0735,  0.0465,  0.9244],\n",
      "        [-0.7400, -0.9741, -1.1579, -0.6637, -1.1749]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2723,  0.3942,  1.3083, -1.7395, -0.3550],\n",
      "        [ 0.7726, -0.2734,  0.6493,  0.3904, -0.0627],\n",
      "        [-0.1330,  0.9183, -0.2020, -0.7583, -0.0019],\n",
      "        [-0.3317,  1.4949,  0.7405, -0.4628, -1.3548]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0675],\n",
      "        [ 1.8183],\n",
      "        [ 0.9513],\n",
      "        [-0.1693]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0451,  1.3962,  0.0794, -2.3612,  1.5873],\n",
      "        [ 0.2118, -1.6490, -1.6262,  1.8380, -2.2056],\n",
      "        [ 0.9277,  0.8518, -0.6083,  1.2102,  0.2758],\n",
      "        [-0.6621,  0.4330,  0.5483,  1.5608,  0.5324]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1035, -0.4417, -0.8959, -0.0638, -0.8198],\n",
      "        [ 0.2791, -0.2909, -0.2172, -1.5308, -1.3440],\n",
      "        [ 0.1294, -0.1286,  1.0062,  0.3922, -0.2565],\n",
      "        [-0.1903, -0.5456,  0.2108,  0.4942, -0.8257]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0451,  1.3962,  0.0794, -2.3612,  1.5873],\n",
      "        [ 0.2118, -1.6490, -1.6262,  1.8380, -2.2056],\n",
      "        [ 0.9277,  0.8518, -0.6083,  1.2102,  0.2758],\n",
      "        [-0.6621,  0.4330,  0.5483,  1.5608,  0.5324]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8431],\n",
      "        [ 1.0427],\n",
      "        [-0.1976],\n",
      "        [ 0.3372]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5583,  1.5584, -1.5602, -0.6403, -1.4366],\n",
      "        [-0.5385, -0.4816,  0.6763, -0.2360,  1.9187],\n",
      "        [-0.5183, -0.0290,  2.1084,  1.3974, -0.7358],\n",
      "        [ 1.5558,  1.8926, -0.8322,  1.2851,  0.8171]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2256,  0.5750,  0.6520,  1.3230,  0.9890],\n",
      "        [-0.1982, -0.6440, -0.8149, -1.3765, -1.6628],\n",
      "        [-0.2029, -0.1394,  0.5963, -0.1803,  0.2307],\n",
      "        [-0.2109,  0.0887,  0.5530, -0.0725, -0.0540]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5583,  1.5584, -1.5602, -0.6403, -1.4366],\n",
      "        [-0.5385, -0.4816,  0.6763, -0.2360,  1.9187],\n",
      "        [-0.5183, -0.0290,  2.1084,  1.3974, -0.7358],\n",
      "        [ 1.5558,  1.8926, -0.8322,  1.2851,  0.8171]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2630],\n",
      "        [-2.9996],\n",
      "        [ 0.9447],\n",
      "        [-0.7577]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9463,  0.8381, -1.1166, -0.0346,  0.8346],\n",
      "        [-1.3697,  0.0766,  1.0997,  0.6926,  1.1143],\n",
      "        [ 0.0374,  0.3247,  0.2427,  1.9061, -0.5905],\n",
      "        [ 0.8216, -0.1952,  0.8490,  0.1658,  1.7282]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1738,  2.3604,  1.0197,  0.8534,  2.0421],\n",
      "        [ 0.7366,  0.8495,  0.5194,  0.9871,  1.0512],\n",
      "        [-0.0058,  0.1888, -0.3247, -0.1096, -0.5568],\n",
      "        [-0.4757,  0.9009,  0.5451,  0.2091,  0.0440]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9463,  0.8381, -1.1166, -0.0346,  0.8346],\n",
      "        [-1.3697,  0.0766,  1.0997,  0.6926,  1.1143],\n",
      "        [ 0.0374,  0.3247,  0.2427,  1.9061, -0.5905],\n",
      "        [ 0.8216, -0.1952,  0.8490,  0.1658,  1.7282]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2298],\n",
      "        [ 1.4824],\n",
      "        [ 0.1021],\n",
      "        [ 0.0069]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9022,  1.5744, -0.8815,  0.4921,  1.4643],\n",
      "        [ 0.6714,  0.8788,  2.4406,  1.0206,  1.6064],\n",
      "        [-0.7309,  0.6689,  0.6479,  0.5810, -0.1489],\n",
      "        [-1.7113,  1.6455, -0.0395, -0.0173, -1.5169]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5983,  1.6574,  1.1020,  1.5468,  2.0221],\n",
      "        [ 0.2486, -0.4689,  0.0147,  0.0933, -0.4842],\n",
      "        [-0.1352,  0.2881, -0.2939,  0.3232,  0.2610],\n",
      "        [ 0.5500,  0.2390,  0.4695,  0.7059,  0.4577]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9022,  1.5744, -0.8815,  0.4921,  1.4643],\n",
      "        [ 0.6714,  0.8788,  2.4406,  1.0206,  1.6064],\n",
      "        [-0.7309,  0.6689,  0.6479,  0.5810, -0.1489],\n",
      "        [-1.7113,  1.6455, -0.0395, -0.0173, -1.5169]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.8204],\n",
      "        [-0.8919],\n",
      "        [ 0.2500],\n",
      "        [-1.2729]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5848,  1.3104, -0.5919, -0.3929,  0.2157],\n",
      "        [ 0.8910, -1.5343,  0.1926, -0.0309,  0.5525],\n",
      "        [ 0.9595,  0.6933, -0.4430, -0.5190, -1.1386],\n",
      "        [-0.6015, -1.1110,  0.8478, -0.8279,  2.2364]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9904, -1.4417, -1.0557, -1.0097, -1.6387],\n",
      "        [ 0.4470, -0.1327,  0.3906,  0.1936, -0.2062],\n",
      "        [ 0.1781,  0.1177,  0.0662,  0.3391,  0.2662],\n",
      "        [ 0.3374,  0.9617,  1.0421,  0.9863,  0.8773]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5848,  1.3104, -0.5919, -0.3929,  0.2157],\n",
      "        [ 0.8910, -1.5343,  0.1926, -0.0309,  0.5525],\n",
      "        [ 0.9595,  0.6933, -0.4430, -0.5190, -1.1386],\n",
      "        [-0.6015, -1.1110,  0.8478, -0.8279,  2.2364]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6418],\n",
      "        [ 0.5572],\n",
      "        [-0.2559],\n",
      "        [ 0.7576]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1162,  0.1312, -0.8685, -2.0228,  0.1537],\n",
      "        [-0.1974, -0.0003, -0.2601,  0.3936, -0.0355],\n",
      "        [ 1.6047,  0.4880,  0.1247, -0.6071,  0.0183],\n",
      "        [-0.5865, -1.4771, -0.9600,  0.9722, -1.0458]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0007, -0.1838, -0.4343, -0.1684, -0.8729],\n",
      "        [ 0.4426, -0.5167, -0.2499, -0.0971, -0.4273],\n",
      "        [ 0.2564,  0.3772, -0.7079,  0.2375,  0.2069],\n",
      "        [ 1.0328,  0.8567,  0.5912,  0.8089,  1.2085]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1162,  0.1312, -0.8685, -2.0228,  0.1537],\n",
      "        [-0.1974, -0.0003, -0.2601,  0.3936, -0.0355],\n",
      "        [ 1.6047,  0.4880,  0.1247, -0.6071,  0.0183],\n",
      "        [-0.5865, -1.4771, -0.9600,  0.9722, -1.0458]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5603],\n",
      "        [-0.0452],\n",
      "        [ 0.3669],\n",
      "        [-2.9162]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0096, -0.1579,  1.5634, -0.2943,  0.3606],\n",
      "        [-0.8780, -0.3009,  1.4235,  0.3065, -0.1394],\n",
      "        [-0.7476,  1.8078,  0.4703,  1.6458,  0.0501],\n",
      "        [ 1.4734,  0.8187,  0.9781,  0.8599, -0.0654]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4989, -0.1769, -0.5727,  0.3284, -1.1584],\n",
      "        [ 0.4085, -0.3031, -0.1925, -0.1463, -0.6909],\n",
      "        [ 0.0208,  0.2661, -0.4265,  0.0146,  0.2028],\n",
      "        [ 1.5137,  1.5682,  2.0776,  0.9314,  1.7896]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0096, -0.1579,  1.5634, -0.2943,  0.3606],\n",
      "        [-0.8780, -0.3009,  1.4235,  0.3065, -0.1394],\n",
      "        [-0.7476,  1.8078,  0.4703,  1.6458,  0.0501],\n",
      "        [ 1.4734,  0.8187,  0.9781,  0.8599, -0.0654]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3866],\n",
      "        [-0.4900],\n",
      "        [ 0.2992],\n",
      "        [ 6.2300]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.7920,  0.5509,  1.4690,  0.0393,  0.2007],\n",
      "        [-0.0513,  0.4856, -0.9506, -0.2090,  0.3991],\n",
      "        [-0.5073, -0.3961, -0.8617, -0.2833, -0.5633],\n",
      "        [ 0.4715, -0.8136, -0.4463,  0.7291,  1.6191]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6236,  0.9056,  1.5822,  0.3742,  1.1489],\n",
      "        [ 0.7667, -0.4755,  0.2292, -0.3612, -0.5620],\n",
      "        [-0.1818,  0.0840,  0.2537,  0.1814, -0.2032],\n",
      "        [-0.9465, -1.7016, -0.9079, -1.6280, -2.1742]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7920,  0.5509,  1.4690,  0.0393,  0.2007],\n",
      "        [-0.0513,  0.4856, -0.9506, -0.2090,  0.3991],\n",
      "        [-0.5073, -0.3961, -0.8617, -0.2833, -0.5633],\n",
      "        [ 0.4715, -0.8136, -0.4463,  0.7291,  1.6191]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5745],\n",
      "        [-0.6369],\n",
      "        [-0.0966],\n",
      "        [-3.3638]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0973, -0.2945, -0.5390,  2.0055, -0.5887],\n",
      "        [ 1.5107,  0.3241,  0.4288,  1.3804,  0.5315],\n",
      "        [ 0.3774, -2.4106, -0.1151, -1.4147, -0.2676],\n",
      "        [-1.1042,  0.1274,  1.1481,  0.9758, -0.0383]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2829, -1.3836, -0.8692, -0.7470, -1.6837],\n",
      "        [ 0.3763, -0.6005, -0.5584, -0.1448, -1.0532],\n",
      "        [ 0.3811,  0.7666,  0.2732,  0.7296,  0.5576],\n",
      "        [ 0.1505,  1.1282,  1.5346,  0.9165,  0.8799]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0973, -0.2945, -0.5390,  2.0055, -0.5887],\n",
      "        [ 1.5107,  0.3241,  0.4288,  1.3804,  0.5315],\n",
      "        [ 0.3774, -2.4106, -0.1151, -1.4147, -0.2676],\n",
      "        [-1.1042,  0.1274,  1.1481,  0.9758, -0.0383]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9623],\n",
      "        [-0.6253],\n",
      "        [-2.9170],\n",
      "        [ 2.6000]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6474, -0.5079,  0.9717, -3.0843, -0.0099],\n",
      "        [-0.2312, -0.1667,  0.5587, -0.4868,  0.4589],\n",
      "        [ 0.1102, -0.0335, -0.8512, -0.2023, -1.1528],\n",
      "        [-1.9502,  0.2288, -0.4058, -0.1513,  0.3722]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6597, -1.3023, -0.2458, -0.6819, -1.1049],\n",
      "        [ 0.3592, -0.6657,  0.5902, -0.0795,  0.1592],\n",
      "        [ 0.5701,  2.0305,  1.2408,  0.9312,  1.6646],\n",
      "        [-0.6366, -0.3639, -0.6694,  0.0332, -0.5770]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6474, -0.5079,  0.9717, -3.0843, -0.0099],\n",
      "        [-0.2312, -0.1667,  0.5587, -0.4868,  0.4589],\n",
      "        [ 0.1102, -0.0335, -0.8512, -0.2023, -1.1528],\n",
      "        [-1.9502,  0.2288, -0.4058, -0.1513,  0.3722]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.6237],\n",
      "        [ 0.4694],\n",
      "        [-3.1688],\n",
      "        [ 1.2100]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4157,  0.5977,  0.8476, -0.4547, -0.2940],\n",
      "        [-0.3861, -0.3091,  0.6756, -0.0850,  0.9388],\n",
      "        [-2.5575, -2.8572, -0.1170,  0.8309,  2.1023],\n",
      "        [ 1.3738,  2.4551,  0.6959,  1.4807,  1.4616]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1475, -1.2807, -1.7136, -2.6839, -4.0912],\n",
      "        [ 0.1554,  0.0084, -0.6381,  0.7033, -0.2848],\n",
      "        [ 1.6536,  3.1501,  2.4510,  1.8081,  3.2857],\n",
      "        [-0.2795,  0.4274, -0.3719, -0.4819, -1.0187]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4157,  0.5977,  0.8476, -0.4547, -0.2940],\n",
      "        [-0.3861, -0.3091,  0.6756, -0.0850,  0.9388],\n",
      "        [-2.5575, -2.8572, -0.1170,  0.8309,  2.1023],\n",
      "        [ 1.3738,  2.4551,  0.6959,  1.4807,  1.4616]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2716],\n",
      "        [-0.8209],\n",
      "        [-5.1061],\n",
      "        [-1.7961]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4582, -1.5930, -0.8826, -1.6867,  2.1364],\n",
      "        [-0.9135, -1.9339, -0.8381,  0.3530, -1.5037],\n",
      "        [-1.0748,  1.8491,  1.4316,  0.5413,  0.3194],\n",
      "        [ 0.3804,  1.3087, -0.1162, -0.4287,  1.2919]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0144,  0.0024,  0.4544, -0.5007, -0.1061],\n",
      "        [ 0.6702,  0.3820,  0.2498, -0.2949, -0.5908],\n",
      "        [ 0.3420,  0.2158,  0.0107, -0.7237,  0.0968],\n",
      "        [-0.0609,  0.6430,  0.4705,  0.7168,  0.7146]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4582, -1.5930, -0.8826, -1.6867,  2.1364],\n",
      "        [-0.9135, -1.9339, -0.8381,  0.3530, -1.5037],\n",
      "        [-1.0748,  1.8491,  1.4316,  0.5413,  0.3194],\n",
      "        [ 0.3804,  1.3087, -0.1162, -0.4287,  1.2919]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1921],\n",
      "        [-0.7761],\n",
      "        [-0.3141],\n",
      "        [ 1.3795]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1929, -0.8327, -0.2274,  0.7447, -1.0991],\n",
      "        [ 0.9545, -0.0219, -0.2949, -0.1306, -1.1048],\n",
      "        [ 0.7577, -1.1678, -0.0965, -0.1998, -0.9700],\n",
      "        [ 0.5675, -1.8385, -1.3067,  0.7167, -0.5159]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0249,  0.3423, -0.2002,  0.1177, -0.3892],\n",
      "        [ 0.9756, -0.0096,  0.7348, -0.4589,  0.1158],\n",
      "        [ 0.3064,  0.0242,  0.6586,  0.0591,  0.3135],\n",
      "        [ 0.1017,  0.4947,  0.4313, -0.2207, -0.1943]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1929, -0.8327, -0.2274,  0.7447, -1.0991],\n",
      "        [ 0.9545, -0.0219, -0.2949, -0.1306, -1.1048],\n",
      "        [ 0.7577, -1.1678, -0.0965, -0.1998, -0.9700],\n",
      "        [ 0.5675, -1.8385, -1.3067,  0.7167, -0.5159]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3056],\n",
      "        [ 0.6468],\n",
      "        [-0.1756],\n",
      "        [-1.4734]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.4780, -0.3231, -0.2859,  1.2301, -0.1504],\n",
      "        [-0.4968, -0.3949, -0.9891,  1.4320,  0.2881],\n",
      "        [ 0.7861, -0.1312, -1.0550,  0.7530, -0.3569],\n",
      "        [-0.9821,  1.0389,  0.9209, -0.7700, -1.4580]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3188,  0.1903,  0.1115, -0.4545, -0.3106],\n",
      "        [ 0.2722, -0.3127, -0.1592, -0.1884, -0.5171],\n",
      "        [ 0.1761,  0.0927, -0.7714,  0.0543,  0.1421],\n",
      "        [ 0.7040,  0.9529,  1.9557,  1.3810,  1.8631]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.4780, -0.3231, -0.2859,  1.2301, -0.1504],\n",
      "        [-0.4968, -0.3949, -0.9891,  1.4320,  0.2881],\n",
      "        [ 0.7861, -0.1312, -1.0550,  0.7530, -0.3569],\n",
      "        [-0.9821,  1.0389,  0.9209, -0.7700, -1.4580]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1842],\n",
      "        [-0.2730],\n",
      "        [ 0.9303],\n",
      "        [-1.6803]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7999,  0.4355,  0.8142,  0.3265,  0.3316],\n",
      "        [-1.6014,  1.5436, -1.1234,  0.1975,  0.6654],\n",
      "        [ 1.0520,  0.6175, -0.9650, -1.8810, -0.3077],\n",
      "        [ 1.2568, -0.5806,  0.8100,  1.4115,  1.3077]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0191, -0.0399,  0.2108, -0.6621, -0.0522],\n",
      "        [ 0.0549,  0.6627, -0.9612, -0.5130, -0.2573],\n",
      "        [-0.7030, -0.5606, -0.2156,  0.2502, -0.4257],\n",
      "        [ 0.8335,  0.6476,  0.4613,  0.7839,  0.9568]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7999,  0.4355,  0.8142,  0.3265,  0.3316],\n",
      "        [-1.6014,  1.5436, -1.1234,  0.1975,  0.6654],\n",
      "        [ 1.0520,  0.6175, -0.9650, -1.8810, -0.3077],\n",
      "        [ 1.2568, -0.5806,  0.8100,  1.4115,  1.3077]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0946],\n",
      "        [ 1.7424],\n",
      "        [-1.2173],\n",
      "        [ 3.4030]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4595,  1.4867, -0.1804, -1.6985,  1.8442],\n",
      "        [-0.3525, -1.1255, -0.8653,  0.5227, -0.5737],\n",
      "        [-0.1221,  0.7359,  0.0957, -0.3889,  0.0771],\n",
      "        [-0.9272,  1.1083,  1.0926,  0.7506,  0.8691]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1169, -0.4306, -0.8523,  0.1062,  0.4729],\n",
      "        [-0.2406, -0.0970, -0.6998, -0.9999, -1.5881],\n",
      "        [ 0.8757,  0.1521,  0.2504, -0.3638,  0.8504],\n",
      "        [-0.3672, -0.4308,  0.3452,  0.4352, -0.8644]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4595,  1.4867, -0.1804, -1.6985,  1.8442],\n",
      "        [-0.3525, -1.1255, -0.8653,  0.5227, -0.5737],\n",
      "        [-0.1221,  0.7359,  0.0957, -0.3889,  0.0771],\n",
      "        [-0.9272,  1.1083,  1.0926,  0.7506,  0.8691]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0346],\n",
      "        [ 1.1880],\n",
      "        [ 0.2359],\n",
      "        [-0.1844]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2077, -0.1434, -1.2810,  0.2917,  0.2217],\n",
      "        [ 0.7368, -0.9235,  0.5237,  1.9174, -0.0291],\n",
      "        [-0.3865, -0.6147,  1.3039,  1.1254,  1.6460],\n",
      "        [-0.2512, -0.8829, -0.7084,  0.0655, -0.7413]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7121,  0.4040, -0.5100,  0.3157,  0.9972],\n",
      "        [-0.9719, -1.4694, -1.1134, -1.2656, -2.6425],\n",
      "        [-0.2106,  0.1333,  0.5891, -0.8933,  0.2831],\n",
      "        [-0.4429,  0.2610,  1.0484,  1.0707,  0.3704]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2077, -0.1434, -1.2810,  0.2917,  0.2217],\n",
      "        [ 0.7368, -0.9235,  0.5237,  1.9174, -0.0291],\n",
      "        [-0.3865, -0.6147,  1.3039,  1.1254,  1.6460],\n",
      "        [-0.2512, -0.8829, -0.7084,  0.0655, -0.7413]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7608],\n",
      "        [-2.2921],\n",
      "        [ 0.2282],\n",
      "        [-1.0662]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5578,  0.5491,  1.2360, -1.8702,  0.6075],\n",
      "        [-0.1590,  0.4948, -0.7787, -0.0197,  1.2898],\n",
      "        [ 0.4061, -0.0607, -0.6643,  1.7456, -0.9579],\n",
      "        [-2.4038, -1.3825,  0.7911,  1.5683,  1.1393]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2229, -0.2751,  0.1198,  0.5584, -0.5556],\n",
      "        [ 0.4968, -0.4296,  0.0514, -0.2582,  0.3301],\n",
      "        [-0.0284, -0.1684,  0.5249, -0.6026,  0.2841],\n",
      "        [-0.2246,  0.6920,  0.9265,  1.1887,  1.1260]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5578,  0.5491,  1.2360, -1.8702,  0.6075],\n",
      "        [-0.1590,  0.4948, -0.7787, -0.0197,  1.2898],\n",
      "        [ 0.4061, -0.0607, -0.6643,  1.7456, -0.9579],\n",
      "        [-2.4038, -1.3825,  0.7911,  1.5683,  1.1393]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2603],\n",
      "        [ 0.0993],\n",
      "        [-1.6742],\n",
      "        [ 3.4634]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1472,  1.2378, -0.6260, -0.6480, -0.8391],\n",
      "        [-0.0591,  0.3943,  0.5319,  2.1122, -0.2488],\n",
      "        [ 1.1336, -0.2184, -0.1995,  0.0939, -0.1536],\n",
      "        [-0.7329,  1.1325,  2.4877, -0.2066, -0.7359]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4272, -0.0553,  0.8471,  0.7675,  0.9698],\n",
      "        [ 0.0906, -0.7312, -0.9450, -0.4811, -0.2669],\n",
      "        [ 0.5075,  0.5602,  0.7933,  1.2396,  0.3323],\n",
      "        [-0.8756, -0.1921, -1.1544, -0.7427, -1.6512]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1472,  1.2378, -0.6260, -0.6480, -0.8391],\n",
      "        [-0.0591,  0.3943,  0.5319,  2.1122, -0.2488],\n",
      "        [ 1.1336, -0.2184, -0.1995,  0.0939, -0.1536],\n",
      "        [-0.7329,  1.1325,  2.4877, -0.2066, -0.7359]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9726],\n",
      "        [-1.7460],\n",
      "        [ 0.3601],\n",
      "        [-1.0790]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6348,  0.3689,  0.6064,  0.8273, -0.1703],\n",
      "        [ 0.1927,  0.4124,  0.3540,  0.7434, -0.0264],\n",
      "        [-0.8639, -1.2114, -1.6998,  1.0308,  0.6345],\n",
      "        [-0.2592,  0.5260,  1.3930, -0.6372,  2.3811]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6205,  1.3480,  0.4108,  1.0441,  2.0014],\n",
      "        [ 0.8472,  0.2825,  0.0862, -0.4302,  0.8561],\n",
      "        [ 0.6716,  0.3811, -0.1481,  0.3075,  0.7124],\n",
      "        [-0.6577, -0.3822, -0.6956, -0.0705, -0.7050]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6348,  0.3689,  0.6064,  0.8273, -0.1703],\n",
      "        [ 0.1927,  0.4124,  0.3540,  0.7434, -0.0264],\n",
      "        [-0.8639, -1.2114, -1.6998,  1.0308,  0.6345],\n",
      "        [-0.2592,  0.5260,  1.3930, -0.6372,  2.3811]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8754],\n",
      "        [-0.0322],\n",
      "        [-0.0211],\n",
      "        [-2.6333]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1846,  0.0622,  0.7166,  1.1034, -1.3162],\n",
      "        [-0.0953, -0.9199, -1.0240, -0.9231,  0.2297],\n",
      "        [-0.9125, -0.8687,  0.5468, -0.7256, -0.8442],\n",
      "        [-0.1899,  0.7141, -1.6356,  1.1574,  1.0051]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9763,  0.4565,  0.9083, -0.1410,  0.7817],\n",
      "        [ 0.5552, -0.0892,  0.7111, -0.1073,  0.6283],\n",
      "        [ 0.6548,  0.2112, -0.0829,  0.0497,  0.1369],\n",
      "        [ 0.5033,  0.9299,  1.9218,  0.7815,  1.6214]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1846,  0.0622,  0.7166,  1.1034, -1.3162],\n",
      "        [-0.0953, -0.9199, -1.0240, -0.9231,  0.2297],\n",
      "        [-0.9125, -0.8687,  0.5468, -0.7256, -0.8442],\n",
      "        [-0.1899,  0.7141, -1.6356,  1.1574,  1.0051]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6853],\n",
      "        [-0.4557],\n",
      "        [-0.9780],\n",
      "        [-0.0407]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9011,  1.2959,  0.7903,  0.3282,  0.3038],\n",
      "        [-0.4525, -1.6290,  0.0731,  0.0861,  2.0463],\n",
      "        [-0.9436,  0.8959, -1.7146,  0.9917, -1.1160],\n",
      "        [ 1.0394, -0.2797, -0.5721,  0.9201, -0.4990]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7215,  1.0416, -0.0966,  0.8319,  1.2660],\n",
      "        [ 0.4418, -0.0603,  0.4968, -0.4003,  0.5611],\n",
      "        [ 0.2508, -0.0706,  0.6991,  0.2867,  1.0584],\n",
      "        [ 0.3721,  1.4681,  1.4096,  1.2375,  0.8214]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9011,  1.2959,  0.7903,  0.3282,  0.3038],\n",
      "        [-0.4525, -1.6290,  0.0731,  0.0861,  2.0463],\n",
      "        [-0.9436,  0.8959, -1.7146,  0.9917, -1.1160],\n",
      "        [ 1.0394, -0.2797, -0.5721,  0.9201, -0.4990]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5813],\n",
      "        [ 1.0484],\n",
      "        [-2.3954],\n",
      "        [-0.1016]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2494, -0.1169,  0.4902, -0.0132, -0.9421],\n",
      "        [ 1.3988,  0.1269, -1.3147,  0.5918,  2.3569],\n",
      "        [-0.7113, -0.7172,  0.6064,  1.0935, -0.0672],\n",
      "        [-1.2405,  1.6020,  0.3701, -0.1728,  1.9333]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1678, -0.8864, -0.7961, -0.4166, -1.3419],\n",
      "        [ 0.5251, -0.3584, -0.6015, -1.0173, -0.1243],\n",
      "        [ 1.4275,  1.2093,  1.1366,  1.2682,  1.7557],\n",
      "        [-0.2816,  0.9730,  1.3924,  0.6921,  0.8101]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2494, -0.1169,  0.4902, -0.0132, -0.9421],\n",
      "        [ 1.3988,  0.1269, -1.3147,  0.5918,  2.3569],\n",
      "        [-0.7113, -0.7172,  0.6064,  1.0935, -0.0672],\n",
      "        [-1.2405,  1.6020,  0.3701, -0.1728,  1.9333]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1927],\n",
      "        [ 0.5847],\n",
      "        [ 0.0753],\n",
      "        [ 3.8700]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5105,  1.2124, -0.5333, -0.5699, -1.0715],\n",
      "        [ 0.8957, -0.5506,  0.3243,  0.4319, -0.2432],\n",
      "        [-0.0021, -1.1112,  1.4514, -1.1266, -0.1557],\n",
      "        [ 0.3211, -0.7491, -0.4864,  0.6919,  2.7481]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5623, -1.3653, -0.6833, -0.7969, -1.2601],\n",
      "        [ 0.4469, -0.2022, -0.3196, -0.2832, -0.8971],\n",
      "        [ 0.9281,  1.9151,  1.2978,  0.1844,  2.0750],\n",
      "        [-0.7108, -0.8711, -1.3342, -0.9005, -1.5402]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5105,  1.2124, -0.5333, -0.5699, -1.0715],\n",
      "        [ 0.8957, -0.5506,  0.3243,  0.4319, -0.2432],\n",
      "        [-0.0021, -1.1112,  1.4514, -1.1266, -0.1557],\n",
      "        [ 0.3211, -0.7491, -0.4864,  0.6919,  2.7481]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8005],\n",
      "        [ 0.5038],\n",
      "        [-0.7773],\n",
      "        [-3.7823]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5253,  1.5215,  0.3891, -0.4776,  1.5971],\n",
      "        [-1.1855, -2.3994, -0.7136, -1.7355, -1.3688],\n",
      "        [-0.5848,  0.1688,  0.3793, -0.9892, -1.7133],\n",
      "        [ 0.1563,  0.1652,  1.9484, -2.0964, -1.6991]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6746, -1.0095, -1.0667, -1.5542, -2.0033],\n",
      "        [-0.2691, -0.4321, -1.0193, -0.7532, -1.1786],\n",
      "        [ 0.4845,  0.0909, -0.2331,  0.2162,  0.3683],\n",
      "        [ 0.9226,  0.4384,  0.6718,  0.8380,  0.9454]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5253,  1.5215,  0.3891, -0.4776,  1.5971],\n",
      "        [-1.1855, -2.3994, -0.7136, -1.7355, -1.3688],\n",
      "        [-0.5848,  0.1688,  0.3793, -0.9892, -1.7133],\n",
      "        [ 0.1563,  0.1652,  1.9484, -2.0964, -1.6991]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.3791],\n",
      "        [ 5.0035],\n",
      "        [-1.2012],\n",
      "        [-1.8377]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6940, -0.2926,  0.5151,  0.0519,  1.1421],\n",
      "        [-0.8733,  0.6618,  0.0328,  0.6462, -0.2516],\n",
      "        [ 1.1485, -1.0119,  0.3697,  3.2384,  2.1301],\n",
      "        [ 0.3018,  1.5290,  1.5368, -2.3894,  0.3843]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4092,  0.5104,  1.1331,  0.4240,  1.2672],\n",
      "        [-1.3475, -1.5075, -1.6738, -1.9345, -4.4534],\n",
      "        [ 0.4964,  0.5569,  0.0987,  0.8106,  1.2717],\n",
      "        [ 1.0728,  1.8710,  1.2589,  1.1946,  2.2046]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6940, -0.2926,  0.5151,  0.0519,  1.1421],\n",
      "        [-0.8733,  0.6618,  0.0328,  0.6462, -0.2516],\n",
      "        [ 1.1485, -1.0119,  0.3697,  3.2384,  2.1301],\n",
      "        [ 0.3018,  1.5290,  1.5368, -2.3894,  0.3843]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1877],\n",
      "        [-0.0055],\n",
      "        [ 5.3772],\n",
      "        [ 3.1120]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1976,  0.6048,  1.2804,  0.5020, -0.7015],\n",
      "        [-0.3484,  1.6486,  0.4713,  0.9020,  1.1543],\n",
      "        [-1.1524,  0.7831,  0.2121, -0.9527,  1.2294],\n",
      "        [-0.2105, -1.0572,  1.1043, -1.1076,  0.4024]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7221,  0.0232, -0.3243, -0.6818, -1.2245],\n",
      "        [ 0.2124, -0.5033,  0.0232,  0.3978,  0.0381],\n",
      "        [-1.7643, -1.1832, -1.9067, -2.2359, -4.2265],\n",
      "        [-0.2578,  0.7030,  0.6197,  0.2868, -0.1163]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1976,  0.6048,  1.2804,  0.5020, -0.7015],\n",
      "        [-0.3484,  1.6486,  0.4713,  0.9020,  1.1543],\n",
      "        [-1.1524,  0.7831,  0.2121, -0.9527,  1.2294],\n",
      "        [-0.2105, -1.0572,  1.1043, -1.1076,  0.4024]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0270],\n",
      "        [-0.4900],\n",
      "        [-2.3635],\n",
      "        [-0.3690]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2379, -2.1818, -1.0314,  0.4281, -0.1436],\n",
      "        [ 0.8428, -0.2161, -0.9985, -0.7934, -0.8516],\n",
      "        [ 1.3005, -0.6180,  0.8540,  0.3083,  1.0522],\n",
      "        [ 0.9603,  0.7589, -0.6523,  2.0010,  0.5358]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5486,  0.2663, -0.8014,  0.5391, -0.8086],\n",
      "        [ 0.4062, -0.1240, -0.2042,  0.1730, -0.0412],\n",
      "        [-0.4476, -1.0423, -1.3543, -1.0279, -2.1028],\n",
      "        [ 0.4661,  0.5399,  0.5348,  0.5382,  0.6586]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2379, -2.1818, -1.0314,  0.4281, -0.1436],\n",
      "        [ 0.8428, -0.2161, -0.9985, -0.7934, -0.8516],\n",
      "        [ 1.3005, -0.6180,  0.8540,  0.3083,  1.0522],\n",
      "        [ 0.9603,  0.7589, -0.6523,  2.0010,  0.5358]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7230],\n",
      "        [ 0.4708],\n",
      "        [-3.6240],\n",
      "        [ 1.9384]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6660,  1.2015, -1.0345, -0.5198,  1.0674],\n",
      "        [-0.9997,  0.6149,  0.6576,  0.1344, -0.0525],\n",
      "        [-0.1892, -0.0615, -0.7305,  1.0646,  0.3928],\n",
      "        [-1.3097,  1.1604, -1.0888, -0.1174,  0.7814]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2928, -0.3663, -0.1036, -0.3009, -0.9487],\n",
      "        [ 0.5683, -0.0460, -0.0296, -0.8610, -0.1248],\n",
      "        [ 0.5839,  1.0773,  0.0534,  0.0255,  0.1337],\n",
      "        [-0.5016,  0.1795, -0.3914, -0.3531, -1.7687]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6660,  1.2015, -1.0345, -0.5198,  1.0674],\n",
      "        [-0.9997,  0.6149,  0.6576,  0.1344, -0.0525],\n",
      "        [-0.1892, -0.0615, -0.7305,  1.0646,  0.3928],\n",
      "        [-1.3097,  1.1604, -1.0888, -0.1174,  0.7814]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3841],\n",
      "        [-0.7250],\n",
      "        [-0.1360],\n",
      "        [-0.0493]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3063,  0.8624,  1.9336, -0.5035,  0.8549],\n",
      "        [-0.7074, -0.7807, -0.8155,  2.4222, -0.9698],\n",
      "        [ 0.9589,  0.1859, -0.0244,  1.5426,  0.6858],\n",
      "        [ 0.7196, -2.2051, -2.4253,  0.0513,  0.8427]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0074,  0.3535, -0.5811,  0.0279,  0.7114],\n",
      "        [ 0.4588, -0.0383,  0.3496,  0.3112,  0.3875],\n",
      "        [ 0.2041, -0.4356,  0.1364, -0.0343, -0.0700],\n",
      "        [-0.2711, -0.1324,  0.5079,  0.3095, -0.3317]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3063,  0.8624,  1.9336, -0.5035,  0.8549],\n",
      "        [-0.7074, -0.7807, -0.8155,  2.4222, -0.9698],\n",
      "        [ 0.9589,  0.1859, -0.0244,  1.5426,  0.6858],\n",
      "        [ 0.7196, -2.2051, -2.4253,  0.0513,  0.8427]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2269],\n",
      "        [-0.2017],\n",
      "        [ 0.0106],\n",
      "        [-1.3985]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2971,  1.0133, -0.2837,  2.6025,  0.9495],\n",
      "        [-0.2566,  0.9244,  1.7370, -0.7642,  0.0876],\n",
      "        [-1.3733,  0.2878,  0.3032, -0.7223,  0.0805],\n",
      "        [ 1.5991,  0.4691,  0.7867, -0.5857,  1.0979]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1574,  0.4457,  0.3511,  0.1225, -0.6299],\n",
      "        [ 0.3786, -0.1025, -1.5421, -0.0460, -0.2974],\n",
      "        [ 0.3722, -0.8941, -0.4434, -0.4901, -0.7355],\n",
      "        [-0.0390,  0.7433,  1.4149,  0.7597,  0.2826]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2971,  1.0133, -0.2837,  2.6025,  0.9495],\n",
      "        [-0.2566,  0.9244,  1.7370, -0.7642,  0.0876],\n",
      "        [-1.3733,  0.2878,  0.3032, -0.7223,  0.0805],\n",
      "        [ 1.5991,  0.4691,  0.7867, -0.5857,  1.0979]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2769],\n",
      "        [-2.8614],\n",
      "        [-0.6082],\n",
      "        [ 1.2646]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6005,  0.3258, -0.0480, -0.3877,  0.0079],\n",
      "        [ 0.3355, -0.7126, -0.7561, -0.1990,  0.9290],\n",
      "        [ 0.7850,  1.1767,  2.2797,  0.4415, -1.1709],\n",
      "        [ 0.5185, -0.9804, -2.0969,  0.9036, -0.1384]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0236, -0.1334,  0.5525, -0.0911,  0.4278],\n",
      "        [ 1.2586,  0.7681,  0.8386,  1.2441,  1.3979],\n",
      "        [ 0.5568, -0.9904, -1.0450, -0.9911, -1.0070],\n",
      "        [-0.0732,  0.6056, -0.4235,  0.5524,  0.1581]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6005,  0.3258, -0.0480, -0.3877,  0.0079],\n",
      "        [ 0.3355, -0.7126, -0.7561, -0.1990,  0.9290],\n",
      "        [ 0.7850,  1.1767,  2.2797,  0.4415, -1.1709],\n",
      "        [ 0.5185, -0.9804, -2.0969,  0.9036, -0.1384]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0454],\n",
      "        [ 0.2918],\n",
      "        [-2.3691],\n",
      "        [ 0.7335]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2363, -0.4367, -0.2022, -0.4645,  0.2179],\n",
      "        [-1.4625, -0.1112,  0.5130,  0.1718,  0.3619],\n",
      "        [-0.4501, -1.8745,  1.7677, -1.2728, -0.1279],\n",
      "        [-0.4992, -1.5160,  0.4927, -0.1026,  0.6380]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0338,  0.0715,  0.0660,  0.2976, -0.2750],\n",
      "        [ 0.6510,  1.3245,  1.7655,  1.4649,  1.5594],\n",
      "        [ 0.7318,  0.4419,  1.1323,  0.1410,  1.5656],\n",
      "        [ 0.3023,  0.2794, -0.0672,  0.5954,  0.4868]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2363, -0.4367, -0.2022, -0.4645,  0.2179],\n",
      "        [-1.4625, -0.1112,  0.5130,  0.1718,  0.3619],\n",
      "        [-0.4501, -1.8745,  1.7677, -1.2728, -0.1279],\n",
      "        [-0.4992, -1.5160,  0.4927, -0.1026,  0.6380]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2348],\n",
      "        [ 0.6223],\n",
      "        [ 0.4642],\n",
      "        [-0.3580]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2469,  0.5107, -0.5137, -0.5357, -0.4851],\n",
      "        [-0.4207,  0.5136, -0.9629,  0.4166,  0.3041],\n",
      "        [-0.0360,  0.1086, -0.1989, -0.0315, -1.0226],\n",
      "        [ 0.2350, -0.1808, -1.6145, -0.5755, -0.5082]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2449, -0.5354,  0.3595,  0.1296, -0.2863],\n",
      "        [ 0.6872,  0.6894,  0.1285,  0.9064,  0.4074],\n",
      "        [ 0.5973, -0.2881, -0.8071, -0.8930,  0.3342],\n",
      "        [-0.0194,  0.1957,  0.4503,  0.1565,  0.5672]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2469,  0.5107, -0.5137, -0.5357, -0.4851],\n",
      "        [-0.4207,  0.5136, -0.9629,  0.4166,  0.3041],\n",
      "        [-0.0360,  0.1086, -0.1989, -0.0315, -1.0226],\n",
      "        [ 0.2350, -0.1808, -1.6145, -0.5755, -0.5082]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4492],\n",
      "        [ 0.4427],\n",
      "        [-0.2059],\n",
      "        [-1.1453]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9010,  0.5702, -0.6057,  0.1213,  0.1813],\n",
      "        [-1.0832,  0.5050,  0.8651, -0.1512,  0.5854],\n",
      "        [-0.7167,  1.4483,  0.4908, -1.0355,  0.4044],\n",
      "        [-0.5340,  1.4099,  2.0124, -0.9789,  0.2801]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4831, -0.0343, -0.0876, -0.3534, -0.3279],\n",
      "        [ 0.7076,  0.4319,  0.0566,  0.2486,  0.9821],\n",
      "        [ 0.8321, -0.5780,  0.1124, -1.0009, -1.3785],\n",
      "        [ 0.6317,  0.7958,  1.1689,  0.5164,  1.7777]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9010,  0.5702, -0.6057,  0.1213,  0.1813],\n",
      "        [-1.0832,  0.5050,  0.8651, -0.1512,  0.5854],\n",
      "        [-0.7167,  1.4483,  0.4908, -1.0355,  0.4044],\n",
      "        [-0.5340,  1.4099,  2.0124, -0.9789,  0.2801]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9873],\n",
      "        [ 0.0379],\n",
      "        [-0.8994],\n",
      "        [ 3.1295]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5026,  0.4156,  0.0088,  0.3625,  0.5155],\n",
      "        [-0.1700,  1.7889, -0.9550,  1.3228, -1.4368],\n",
      "        [ 1.1821, -0.7002,  0.4360,  0.2648, -0.3044],\n",
      "        [-1.5923, -1.3326, -1.8210,  0.5077, -0.5952]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3110, -0.4272, -0.0014, -0.3012, -0.0931],\n",
      "        [ 0.1049,  0.6655,  0.9178,  0.5119,  0.3713],\n",
      "        [ 0.9348, -0.4740, -0.4880, -0.1911, -0.1915],\n",
      "        [-0.6049, -0.4362, -0.3278, -0.7136, -1.3875]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5026,  0.4156,  0.0088,  0.3625,  0.5155],\n",
      "        [-0.1700,  1.7889, -0.9550,  1.3228, -1.4368],\n",
      "        [ 1.1821, -0.7002,  0.4360,  0.2648, -0.3044],\n",
      "        [-1.5923, -1.3326, -1.8210,  0.5077, -0.5952]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8021],\n",
      "        [ 0.4399],\n",
      "        [ 1.2319],\n",
      "        [ 2.6050]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7946, -0.9885, -0.3688,  1.2482, -0.5059],\n",
      "        [-1.0972,  0.1945,  0.9527,  0.8976,  2.5388],\n",
      "        [-0.8744,  0.5610,  0.0865,  1.0358,  1.0370],\n",
      "        [ 0.0603,  2.0708,  0.3771,  0.1023,  0.3081]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2236,  0.3437,  0.6547,  0.1766,  0.5944],\n",
      "        [-0.0556,  0.7056,  0.6051,  0.4975,  0.4592],\n",
      "        [-0.2307, -1.1079, -1.0886, -1.2903, -1.3323],\n",
      "        [-1.1724, -1.7090, -1.2159, -1.2577, -3.4320]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7946, -0.9885, -0.3688,  1.2482, -0.5059],\n",
      "        [-1.0972,  0.1945,  0.9527,  0.8976,  2.5388],\n",
      "        [-0.8744,  0.5610,  0.0865,  1.0358,  1.0370],\n",
      "        [ 0.0603,  2.0708,  0.3771,  0.1023,  0.3081]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2603],\n",
      "        [ 2.3871],\n",
      "        [-3.2321],\n",
      "        [-5.2542]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2950,  1.2243,  1.1010,  1.0080,  2.2468],\n",
      "        [-1.2500, -1.1443, -0.3855,  0.0889, -0.2767],\n",
      "        [-0.0440, -0.4972,  0.0984,  2.4406,  0.9990],\n",
      "        [-2.1895,  0.6504,  0.9715,  0.9828,  1.0433]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2962,  0.2760, -0.3003, -0.0578, -0.2069],\n",
      "        [-0.7938, -0.2264, -0.7485, -0.5221, -1.8813],\n",
      "        [ 1.0650,  0.9286,  0.1635,  0.5533,  0.7603],\n",
      "        [ 0.1579,  0.2287,  0.3713,  0.3726,  1.3748]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2950,  1.2243,  1.1010,  1.0080,  2.2468],\n",
      "        [-1.2500, -1.1443, -0.3855,  0.0889, -0.2767],\n",
      "        [-0.0440, -0.4972,  0.0984,  2.4406,  0.9990],\n",
      "        [-2.1895,  0.6504,  0.9715,  0.9828,  1.0433]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6033],\n",
      "        [ 2.0139],\n",
      "        [ 1.6175],\n",
      "        [ 1.9643]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1737,  0.0182, -0.4737, -0.9381,  0.5145],\n",
      "        [-0.2653,  0.8931,  0.3594, -0.8733,  0.6982],\n",
      "        [ 1.0729,  0.3213,  0.7271, -0.7222,  0.8360],\n",
      "        [ 0.0259,  0.3569,  0.9148,  0.1705,  0.1687]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6732, -0.3723, -0.5162, -0.4932, -0.1680],\n",
      "        [-1.1293, -1.2517, -1.0585, -1.3696, -2.6003],\n",
      "        [ 0.5468, -0.4320, -0.3307, -0.2193, -0.5799],\n",
      "        [-0.6273, -0.2682, -0.8084, -0.1508, -0.8657]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1737,  0.0182, -0.4737, -0.9381,  0.5145],\n",
      "        [-0.2653,  0.8931,  0.3594, -0.8733,  0.6982],\n",
      "        [ 1.0729,  0.3213,  0.7271, -0.7222,  0.8360],\n",
      "        [ 0.0259,  0.3569,  0.9148,  0.1705,  0.1687]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4971],\n",
      "        [-1.8183],\n",
      "        [-0.1190],\n",
      "        [-1.0233]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4432,  0.8950,  0.5951,  0.1485, -2.8496],\n",
      "        [-0.0297, -1.5246,  0.7791,  0.7270, -0.3888],\n",
      "        [ 0.7567,  1.2781, -1.0581, -0.5703,  0.6156],\n",
      "        [-0.0730,  0.8535,  0.8172, -0.9545,  2.4593]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2202, -0.9860, -0.7148, -0.4198, -0.3135],\n",
      "        [-0.4352, -0.4706, -0.4329, -0.1531, -1.1476],\n",
      "        [ 0.3956, -0.6187, -1.4115, -0.3977, -0.5156],\n",
      "        [-0.4095,  0.3315,  0.4509, -1.0552,  0.1066]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4432,  0.8950,  0.5951,  0.1485, -2.8496],\n",
      "        [-0.0297, -1.5246,  0.7791,  0.7270, -0.3888],\n",
      "        [ 0.7567,  1.2781, -1.0581, -0.5703,  0.6156],\n",
      "        [-0.0730,  0.8535,  0.8172, -0.9545,  2.4593]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5744],\n",
      "        [ 0.7281],\n",
      "        [ 0.9114],\n",
      "        [ 1.9507]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0996,  1.6893, -0.6240, -0.5874, -0.5503],\n",
      "        [ 0.4412,  1.4870,  0.0946, -1.1934,  0.4812],\n",
      "        [-0.1383,  1.0234,  1.3398, -1.7602,  0.8612],\n",
      "        [ 0.3010,  0.4055, -0.0200, -1.3328, -0.9588]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.5542, -0.7662, -0.0785, -1.3450, -0.7078],\n",
      "        [-0.8767,  0.0752, -0.0935, -0.1324, -0.2050],\n",
      "        [-0.0772, -1.0097, -1.0916, -0.6886, -1.5832],\n",
      "        [-1.0414, -0.5165, -1.3905, -0.4422, -1.1225]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0996,  1.6893, -0.6240, -0.5874, -0.5503],\n",
      "        [ 0.4412,  1.4870,  0.0946, -1.1934,  0.4812],\n",
      "        [-0.1383,  1.0234,  1.3398, -1.7602,  0.8612],\n",
      "        [ 0.3010,  0.4055, -0.0200, -1.3328, -0.9588]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0106],\n",
      "        [-0.2244],\n",
      "        [-2.6366],\n",
      "        [ 1.1706]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4291, -0.5843,  0.4939, -0.5701, -0.5489],\n",
      "        [ 0.3711, -0.8092,  1.6472, -2.5285,  0.2248],\n",
      "        [ 0.4032, -0.1879,  0.6705, -0.7132,  0.2178],\n",
      "        [-1.5229,  0.1873,  0.7622, -0.5251,  0.3485]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3750, -0.8103, -0.5364, -0.4280, -0.7087],\n",
      "        [-0.0692,  0.5675,  1.1747,  0.6051,  0.5194],\n",
      "        [ 1.1567,  0.0014,  0.7980, -0.2265,  0.8830],\n",
      "        [-1.0251, -1.6406, -1.6816, -1.0776, -2.1596]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4291, -0.5843,  0.4939, -0.5701, -0.5489],\n",
      "        [ 0.3711, -0.8092,  1.6472, -2.5285,  0.2248],\n",
      "        [ 0.4032, -0.1879,  0.6705, -0.7132,  0.2178],\n",
      "        [-1.5229,  0.1873,  0.7622, -0.5251,  0.3485]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6806],\n",
      "        [ 0.0367],\n",
      "        [ 1.3550],\n",
      "        [-0.2146]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2661,  0.1109,  0.3457,  1.6050, -1.1469],\n",
      "        [ 0.0249, -0.9679, -0.1669, -2.1679, -2.2365],\n",
      "        [ 0.6667, -1.3799, -1.0219,  0.2677, -0.8214],\n",
      "        [ 1.3721,  0.8643, -0.2855,  1.2309,  0.6659]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0360, -0.6495, -0.8855, -0.3238, -1.4554],\n",
      "        [-0.4796,  0.4699,  1.1056,  0.3101,  0.1264],\n",
      "        [ 0.6941,  0.1471, -0.2827, -0.0038,  0.1496],\n",
      "        [ 0.0763,  0.0635, -0.6572, -0.3829, -0.4761]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2661,  0.1109,  0.3457,  1.6050, -1.1469],\n",
      "        [ 0.0249, -0.9679, -0.1669, -2.1679, -2.2365],\n",
      "        [ 0.6667, -1.3799, -1.0219,  0.2677, -0.8214],\n",
      "        [ 1.3721,  0.8643, -0.2855,  1.2309,  0.6659]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7618],\n",
      "        [-1.6064],\n",
      "        [ 0.4247],\n",
      "        [-0.4411]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9163, -0.9808,  2.8936, -0.8563, -1.0656],\n",
      "        [-2.5851,  0.1646, -0.1144,  0.4035, -0.0330],\n",
      "        [ 0.0006,  1.1526, -0.8278,  0.2836,  1.0872],\n",
      "        [-1.5989, -0.3657, -0.8551,  0.0661,  0.1115]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3393, -0.6397, -0.4084, -0.4767, -1.6251],\n",
      "        [ 0.4744,  0.1055,  0.7105,  0.7114,  0.6322],\n",
      "        [ 0.1826, -0.6249, -0.2569, -0.8000, -0.3227],\n",
      "        [ 0.4851,  0.2292, -0.1095, -0.1277, -0.1254]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9163, -0.9808,  2.8936, -0.8563, -1.0656],\n",
      "        [-2.5851,  0.1646, -0.1144,  0.4035, -0.0330],\n",
      "        [ 0.0006,  1.1526, -0.8278,  0.2836,  1.0872],\n",
      "        [-1.5989, -0.3657, -0.8551,  0.0661,  0.1115]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2356],\n",
      "        [-1.0243],\n",
      "        [-1.0852],\n",
      "        [-0.7882]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1752,  1.4609,  0.0907,  0.3268,  0.2232],\n",
      "        [ 0.7054, -0.4118, -0.0593, -0.0668, -0.0606],\n",
      "        [-1.0127, -0.6385,  2.7765, -0.9684, -0.3836],\n",
      "        [ 0.4922, -0.4279, -0.9695,  2.3654,  0.7159]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0484, -1.6658, -2.0077, -1.3747, -2.2705],\n",
      "        [ 0.6722,  0.0384,  1.3334,  0.8534,  1.3470],\n",
      "        [ 0.6656, -0.0017, -0.0463, -0.0951, -0.1706],\n",
      "        [ 0.5970, -0.2878,  0.1199, -0.5568,  0.3227]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1752,  1.4609,  0.0907,  0.3268,  0.2232],\n",
      "        [ 0.7054, -0.4118, -0.0593, -0.0668, -0.0606],\n",
      "        [-1.0127, -0.6385,  2.7765, -0.9684, -0.3836],\n",
      "        [ 0.4922, -0.4279, -0.9695,  2.3654,  0.7159]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.8038],\n",
      "        [ 0.2405],\n",
      "        [-0.6440],\n",
      "        [-0.7853]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6429, -0.1596,  0.3637, -0.2060,  2.4905],\n",
      "        [-0.6076,  0.0521,  0.4567,  0.7528, -0.6268],\n",
      "        [ 0.3038, -1.6435,  0.2669, -0.5698, -0.5330],\n",
      "        [ 1.0366,  0.9774,  0.5841, -0.5955,  0.5269]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8020,  0.9453, -0.0732,  0.3277,  1.1029],\n",
      "        [ 0.2653,  0.6785,  0.9852,  1.3759,  1.4591],\n",
      "        [ 0.3760, -0.1329,  0.6413,  0.4839,  0.4972],\n",
      "        [ 0.6912, -0.6172,  0.3376,  0.3393, -0.0786]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6429, -0.1596,  0.3637, -0.2060,  2.4905],\n",
      "        [-0.6076,  0.0521,  0.4567,  0.7528, -0.6268],\n",
      "        [ 0.3038, -1.6435,  0.2669, -0.5698, -0.5330],\n",
      "        [ 1.0366,  0.9774,  0.5841, -0.5955,  0.5269]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0174],\n",
      "        [ 0.4455],\n",
      "        [-0.0370],\n",
      "        [ 0.0670]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3586,  0.1264,  2.0401,  1.4715, -0.5142],\n",
      "        [-0.6695,  0.2908, -0.3736, -0.1923, -0.9943],\n",
      "        [ 0.3173,  2.1029, -0.0940, -0.7637, -1.6938],\n",
      "        [-0.9930,  1.0364,  0.1040, -1.7610,  2.3259]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6270, -1.0766, -0.1207, -1.1792, -1.5936],\n",
      "        [ 0.1670,  1.0128,  0.1698,  0.0082,  0.6175],\n",
      "        [ 0.4671,  0.1934, -0.2838, -0.1217, -0.0620],\n",
      "        [ 0.7241,  0.1081, -0.1466, -0.7211,  0.0511]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3586,  0.1264,  2.0401,  1.4715, -0.5142],\n",
      "        [-0.6695,  0.2908, -0.3736, -0.1923, -0.9943],\n",
      "        [ 0.3173,  2.1029, -0.0940, -0.7637, -1.6938],\n",
      "        [-0.9930,  1.0364,  0.1040, -1.7610,  2.3259]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1500],\n",
      "        [-0.4963],\n",
      "        [ 0.7794],\n",
      "        [ 0.7664]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7123, -0.0943,  1.1091,  0.1872,  0.6898],\n",
      "        [-0.8771, -1.6598, -1.9799,  0.4543,  1.1914],\n",
      "        [ 0.4443,  0.8297, -2.3611, -0.5865,  1.1307],\n",
      "        [ 0.0048,  1.5344,  0.8070,  1.3570,  0.0174]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8016, -0.9790, -0.9996, -0.5494,  0.0189],\n",
      "        [-0.0340,  0.3524,  0.4211,  0.1543,  0.4150],\n",
      "        [ 0.5238, -0.5069, -0.2971, -0.0968,  0.0732],\n",
      "        [-0.3867, -0.3054,  0.3490, -0.2938, -0.5939]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7123, -0.0943,  1.1091,  0.1872,  0.6898],\n",
      "        [-0.8771, -1.6598, -1.9799,  0.4543,  1.1914],\n",
      "        [ 0.4443,  0.8297, -2.3611, -0.5865,  1.1307],\n",
      "        [ 0.0048,  1.5344,  0.8070,  1.3570,  0.0174]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5352],\n",
      "        [-0.8244],\n",
      "        [ 0.6531],\n",
      "        [-0.5979]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1670,  0.2044,  0.3086, -0.5339,  0.6412],\n",
      "        [ 0.2439, -0.8342, -0.1442,  1.2801,  1.0068],\n",
      "        [ 0.0058, -0.7368,  0.4171,  0.8917, -0.5795],\n",
      "        [-0.5046, -1.3206, -0.5419,  0.7802,  0.0267]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2451, -0.7497, -0.8653, -1.2159, -0.3135],\n",
      "        [ 0.3963,  0.6963,  0.6056,  0.5581,  1.1341],\n",
      "        [ 0.1809, -0.3408, -0.1316, -0.4555, -0.2845],\n",
      "        [ 0.5017, -0.0481, -0.3754, -0.8974, -0.3229]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1670,  0.2044,  0.3086, -0.5339,  0.6412],\n",
      "        [ 0.2439, -0.8342, -0.1442,  1.2801,  1.0068],\n",
      "        [ 0.0058, -0.7368,  0.4171,  0.8917, -0.5795],\n",
      "        [-0.5046, -1.3206, -0.5419,  0.7802,  0.0267]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0131],\n",
      "        [ 1.2847],\n",
      "        [-0.0440],\n",
      "        [-0.6951]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1534, -1.2492,  0.5002,  2.1790,  1.1176],\n",
      "        [ 0.1292, -1.1283,  1.2545, -0.9261,  1.1005],\n",
      "        [-1.4957, -0.6418, -1.2988,  0.2336,  1.5672],\n",
      "        [-0.7522, -0.5447, -0.0970,  0.4202,  0.1816]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3825, -0.7234, -0.7852, -0.1842, -0.6996],\n",
      "        [-0.3107, -0.1158, -0.2436, -0.1150, -0.9721],\n",
      "        [ 0.8257,  0.0975,  0.2220,  0.6453,  0.3660],\n",
      "        [ 0.4259, -0.2255, -0.4504, -0.9770,  0.1407]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1534, -1.2492,  0.5002,  2.1790,  1.1176],\n",
      "        [ 0.1292, -1.1283,  1.2545, -0.9261,  1.1005],\n",
      "        [-1.4957, -0.6418, -1.2988,  0.2336,  1.5672],\n",
      "        [-0.7522, -0.5447, -0.0970,  0.4202,  0.1816]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1134],\n",
      "        [-1.1784],\n",
      "        [-0.8615],\n",
      "        [-0.5388]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2005, -1.0394,  0.4901, -0.3270,  1.5504],\n",
      "        [ 0.2429,  0.3923, -0.5878, -1.3281, -0.2445],\n",
      "        [-1.0391, -0.1106, -0.8438, -0.8530, -0.7253],\n",
      "        [-0.8173,  1.7439,  0.0370,  2.4813,  0.8528]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5177, -0.8577, -1.1527, -0.5972, -0.3978],\n",
      "        [ 0.6189,  1.0239,  0.5805,  0.2741,  0.4239],\n",
      "        [ 0.2748,  0.0905, -0.3047,  1.5995,  0.2036],\n",
      "        [ 0.4355, -0.6137,  0.6475, -0.9072, -0.1855]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2005, -1.0394,  0.4901, -0.3270,  1.5504],\n",
      "        [ 0.2429,  0.3923, -0.5878, -1.3281, -0.2445],\n",
      "        [-1.0391, -0.1106, -0.8438, -0.8530, -0.7253],\n",
      "        [-0.8173,  1.7439,  0.0370,  2.4813,  0.8528]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7164],\n",
      "        [-0.2569],\n",
      "        [-1.5505],\n",
      "        [-3.8113]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0546,  0.5961, -0.8749, -0.1596,  0.5884],\n",
      "        [-0.7945,  0.0150,  2.4987,  0.3846, -0.0412],\n",
      "        [-0.3527, -1.2882,  1.4606,  1.2326,  0.4126],\n",
      "        [ 0.0201, -1.1182,  0.0070, -0.0707,  0.7877]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3678, -1.3900, -1.2714, -0.6293,  0.1201],\n",
      "        [ 0.6128,  0.8452, -0.3535, -0.4006,  0.3975],\n",
      "        [ 1.5779,  0.6579, -0.2180,  1.3631,  1.4505],\n",
      "        [ 1.6462,  1.1572,  1.6601,  1.0965,  1.7828]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0546,  0.5961, -0.8749, -0.1596,  0.5884],\n",
      "        [-0.7945,  0.0150,  2.4987,  0.3846, -0.0412],\n",
      "        [-0.3527, -1.2882,  1.4606,  1.2326,  0.4126],\n",
      "        [ 0.0201, -1.1182,  0.0070, -0.0707,  0.7877]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4347],\n",
      "        [-1.5280],\n",
      "        [ 0.5562],\n",
      "        [ 0.0775]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5858,  2.7103, -1.2631,  1.2572,  0.2774],\n",
      "        [-1.2114, -1.1537,  0.6720,  1.0000, -1.0767],\n",
      "        [-1.5959, -0.5630, -1.0890, -0.3601,  1.4144],\n",
      "        [-0.6554, -1.3901,  0.2568,  0.2505, -0.3445]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4133, -0.8843, -1.1720, -0.7712, -1.3054],\n",
      "        [ 1.3435,  0.8928,  0.4865,  1.2698,  0.8235],\n",
      "        [ 0.8955,  0.7215,  0.5870,  0.3337,  0.3828],\n",
      "        [ 1.0804,  1.5019,  1.7217,  1.4504,  1.5615]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5858,  2.7103, -1.2631,  1.2572,  0.2774],\n",
      "        [-1.2114, -1.1537,  0.6720,  1.0000, -1.0767],\n",
      "        [-1.5959, -0.5630, -1.0890, -0.3601,  1.4144],\n",
      "        [-0.6554, -1.3901,  0.2568,  0.2505, -0.3445]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4902],\n",
      "        [-1.9474],\n",
      "        [-2.0533],\n",
      "        [-2.5283]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2458,  0.3762, -0.1521,  1.8639,  0.1591],\n",
      "        [-0.7840,  2.4182,  1.5182, -1.1250, -0.1060],\n",
      "        [-1.2676, -0.0674, -0.2396, -0.4313, -0.9780],\n",
      "        [-0.2209, -0.3533, -1.2476, -0.2135, -0.6362]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1891,  0.7745,  0.0683,  0.6288,  0.7872],\n",
      "        [ 1.6115,  1.7831,  1.9841,  1.1900,  2.0667],\n",
      "        [-0.4672,  0.1471,  0.2497,  0.4011,  0.3653],\n",
      "        [ 0.0354,  0.2243, -0.3108, -0.4368, -0.2880]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2458,  0.3762, -0.1521,  1.8639,  0.1591],\n",
      "        [-0.7840,  2.4182,  1.5182, -1.1250, -0.1060],\n",
      "        [-1.2676, -0.0674, -0.2396, -0.4313, -0.9780],\n",
      "        [-0.2209, -0.3533, -1.2476, -0.2135, -0.6362]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2860],\n",
      "        [ 4.5031],\n",
      "        [-0.0077],\n",
      "        [ 0.5771]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3571, -0.1722,  1.2497,  1.0151,  2.8380],\n",
      "        [-0.1779,  0.0366, -1.2129, -0.4090,  1.5606],\n",
      "        [-0.4754, -0.8599,  0.8736,  0.9344, -0.8318],\n",
      "        [ 0.0258, -0.1605,  1.0902, -0.8466,  0.7327]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6091,  0.2377, -0.3548, -0.3076, -0.5062],\n",
      "        [-0.2816, -1.2080, -0.5292, -0.7164, -1.2672],\n",
      "        [ 0.1295,  0.6063,  0.0125,  0.4583,  0.1742],\n",
      "        [ 0.1256, -0.1895,  0.3249,  0.3295, -0.6277]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3571, -0.1722,  1.2497,  1.0151,  2.8380],\n",
      "        [-0.1779,  0.0366, -1.2129, -0.4090,  1.5606],\n",
      "        [-0.4754, -0.8599,  0.8736,  0.9344, -0.8318],\n",
      "        [ 0.0258, -0.1605,  1.0902, -0.8466,  0.7327]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4507],\n",
      "        [-1.0370],\n",
      "        [-0.2887],\n",
      "        [-0.3510]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1372, -0.5435,  0.9474,  0.7977,  0.6976],\n",
      "        [ 0.0591, -0.9948, -0.7473,  0.1833, -0.1120],\n",
      "        [ 0.1734,  0.5940,  2.8951, -0.3520, -2.2945],\n",
      "        [-0.5925,  1.1753,  1.0620, -0.3136, -0.1217]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4013,  1.1355,  0.4835,  0.1313,  0.8923],\n",
      "        [ 0.0757, -0.0571,  0.9730,  0.1287, -0.2875],\n",
      "        [ 0.2078, -0.1552, -0.1805, -0.2737, -0.0515],\n",
      "        [ 0.0735, -0.0976, -0.3815, -0.1675, -0.3243]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1372, -0.5435,  0.9474,  0.7977,  0.6976],\n",
      "        [ 0.0591, -0.9948, -0.7473,  0.1833, -0.1120],\n",
      "        [ 0.1734,  0.5940,  2.8951, -0.3520, -2.2945],\n",
      "        [-0.5925,  1.1753,  1.0620, -0.3136, -0.1217]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7603],\n",
      "        [-0.6101],\n",
      "        [-0.3641],\n",
      "        [-0.4715]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5704,  0.3382, -0.8964, -0.1425,  2.4252],\n",
      "        [ 1.9918, -1.3844, -0.6063, -0.8929,  0.7458],\n",
      "        [ 0.4115, -0.4729,  0.5221,  1.3129, -1.6644],\n",
      "        [ 0.8681,  0.3503, -1.4588,  1.7036,  0.4044]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5868, -0.4315,  0.4878,  0.0652,  0.8385],\n",
      "        [ 0.9521,  1.1463,  0.5133,  0.5114,  0.5426],\n",
      "        [ 0.2026, -0.6771,  0.3915, -0.1349, -0.3832],\n",
      "        [ 0.5759,  0.0242, -0.0764, -0.3331,  0.1281]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5704,  0.3382, -0.8964, -0.1425,  2.4252],\n",
      "        [ 1.9918, -1.3844, -0.6063, -0.8929,  0.7458],\n",
      "        [ 0.4115, -0.4729,  0.5221,  1.3129, -1.6644],\n",
      "        [ 0.8681,  0.3503, -1.4588,  1.7036,  0.4044]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7757],\n",
      "        [-0.0537],\n",
      "        [ 1.0687],\n",
      "        [ 0.1042]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4170, -0.3058,  0.7960, -0.3045,  1.1217],\n",
      "        [ 0.8655, -1.8373,  0.1594,  0.6352,  0.7532],\n",
      "        [ 1.1927, -1.2564,  1.3944,  0.1720,  0.2902],\n",
      "        [-0.8576,  1.4325,  0.0415, -1.0526,  2.2317]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4472, -1.1190, -0.8736, -0.5194, -1.1991],\n",
      "        [ 0.3169,  0.1262, -0.6098,  0.5659,  0.6193],\n",
      "        [-0.4706, -0.0358, -0.1210, -0.2382, -0.5889],\n",
      "        [ 0.8384,  0.1188, -0.0541, -0.0411,  0.1232]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4170, -0.3058,  0.7960, -0.3045,  1.1217],\n",
      "        [ 0.8655, -1.8373,  0.1594,  0.6352,  0.7532],\n",
      "        [ 1.1927, -1.2564,  1.3944,  0.1720,  0.2902],\n",
      "        [-0.8576,  1.4325,  0.0415, -1.0526,  2.2317]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1737],\n",
      "        [ 0.7710],\n",
      "        [-0.8969],\n",
      "        [-0.2328]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5476,  0.5877,  0.2739,  0.0266,  0.9287],\n",
      "        [ 1.2319,  1.0038,  0.2341, -2.2229,  0.5066],\n",
      "        [ 0.3032,  0.8406,  0.2037,  1.0250, -0.6669],\n",
      "        [ 0.7219, -1.8437,  0.5868,  2.7457,  0.8468]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9133,  0.2086,  0.7106,  0.6338,  0.7245],\n",
      "        [ 0.5035,  0.3412,  0.6425,  0.7189,  0.2477],\n",
      "        [-0.3188,  0.2984, -0.1798, -0.7758, -0.0936],\n",
      "        [ 0.1892,  0.0936, -0.4878, -0.3077, -0.5683]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5476,  0.5877,  0.2739,  0.0266,  0.9287],\n",
      "        [ 1.2319,  1.0038,  0.2341, -2.2229,  0.5066],\n",
      "        [ 0.3032,  0.8406,  0.2037,  1.0250, -0.6669],\n",
      "        [ 0.7219, -1.8437,  0.5868,  2.7457,  0.8468]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5070],\n",
      "        [-0.3594],\n",
      "        [-0.6153],\n",
      "        [-1.6482]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1761,  0.1344,  1.7109,  0.1949, -2.0883],\n",
      "        [-0.4170, -2.0499, -0.9069, -0.6990,  2.0224],\n",
      "        [ 0.7508,  1.3217,  1.6736, -0.3190,  0.4983],\n",
      "        [-1.5472,  1.4435, -1.4430, -0.0351, -0.2495]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0101, -0.4308, -1.0625, -0.4894, -0.3685],\n",
      "        [ 0.2288,  0.9934,  1.2436,  0.5119,  0.9773],\n",
      "        [ 0.5895, -0.1298, -0.5007, -0.7731, -0.2793],\n",
      "        [ 1.2077,  0.9200,  0.4434,  0.5589,  0.7974]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1761,  0.1344,  1.7109,  0.1949, -2.0883],\n",
      "        [-0.4170, -2.0499, -0.9069, -0.6990,  2.0224],\n",
      "        [ 0.7508,  1.3217,  1.6736, -0.3190,  0.4983],\n",
      "        [-1.5472,  1.4435, -1.4430, -0.0351, -0.2495]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1999],\n",
      "        [-1.6409],\n",
      "        [-0.4595],\n",
      "        [-1.3990]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.2879,  0.2835,  0.6630,  1.3999, -1.5069],\n",
      "        [-0.9213, -0.9907,  1.2469, -0.0198,  0.5652],\n",
      "        [ 0.1808, -0.4339,  1.4628,  1.3910, -0.2662],\n",
      "        [-0.1926, -1.6195, -1.2108, -0.2764,  0.6116]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2442, -0.1057, -0.2148, -0.4905,  0.5164],\n",
      "        [ 1.1696,  1.2623,  1.9263,  1.1078,  1.3867],\n",
      "        [ 0.1012, -0.9503, -0.7043, -0.8066,  0.1117],\n",
      "        [ 1.4927,  0.6422,  1.1880,  0.9512,  1.5168]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.2879,  0.2835,  0.6630,  1.3999, -1.5069],\n",
      "        [-0.9213, -0.9907,  1.2469, -0.0198,  0.5652],\n",
      "        [ 0.1808, -0.4339,  1.4628,  1.3910, -0.2662],\n",
      "        [-0.1926, -1.6195, -1.2108, -0.2764,  0.6116]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.4838],\n",
      "        [ 0.8357],\n",
      "        [-1.7514],\n",
      "        [-2.1013]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2519,  0.1275, -0.3787, -0.5319, -0.4584],\n",
      "        [ 0.1326,  2.5101,  0.3723,  0.4269,  1.2553],\n",
      "        [ 0.4230,  0.8459,  1.2595,  0.0153, -0.8891],\n",
      "        [-0.4975,  0.1413,  0.9452,  0.4106, -1.0256]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.8665,  2.1071,  2.1443,  1.5690,  2.5276],\n",
      "        [ 0.2326,  0.7819,  0.6370,  0.9630,  1.0557],\n",
      "        [ 0.5543,  0.7850,  0.1974,  0.8130,  1.9587],\n",
      "        [ 1.9930,  2.1719,  1.3125,  1.4527,  1.8512]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2519,  0.1275, -0.3787, -0.5319, -0.4584],\n",
      "        [ 0.1326,  2.5101,  0.3723,  0.4269,  1.2553],\n",
      "        [ 0.4230,  0.8459,  1.2595,  0.0153, -0.8891],\n",
      "        [-0.4975,  0.1413,  0.9452,  0.4106, -1.0256]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.0068],\n",
      "        [ 3.9669],\n",
      "        [-0.5820],\n",
      "        [-0.7459]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4971,  0.9063, -0.0275, -1.6227, -1.0577],\n",
      "        [-1.2928,  0.3315,  0.2526, -0.2079,  0.9446],\n",
      "        [-0.5473,  1.2402, -0.0327,  1.2812, -0.3990],\n",
      "        [ 1.9026,  0.5136,  1.0955,  0.1519,  1.3923]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0525,  0.1940,  0.1065,  0.0875,  0.0857],\n",
      "        [-0.7555, -0.4885, -0.7568, -1.5004, -1.8987],\n",
      "        [ 1.0470,  0.3063,  0.2036, -0.2586,  1.4952],\n",
      "        [ 0.3090, -0.1221, -0.2044,  0.6365,  0.2612]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4971,  0.9063, -0.0275, -1.6227, -1.0577],\n",
      "        [-1.2928,  0.3315,  0.2526, -0.2079,  0.9446],\n",
      "        [-0.5473,  1.2402, -0.0327,  1.2812, -0.3990],\n",
      "        [ 1.9026,  0.5136,  1.0955,  0.1519,  1.3923]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0189],\n",
      "        [-0.8579],\n",
      "        [-1.1277],\n",
      "        [ 0.7617]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6705,  0.4516,  0.4384,  2.1908, -0.8782],\n",
      "        [-0.9299,  0.3581,  0.9758,  2.4688, -0.5089],\n",
      "        [-1.2994,  1.6353, -0.4474, -0.5959, -1.3784],\n",
      "        [ 0.4460, -0.7901,  0.4214,  2.0522,  1.1958]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0454, -0.1168, -0.4419, -0.3429, -0.3145],\n",
      "        [-0.4560,  0.1987, -1.2121,  0.0733, -0.6659],\n",
      "        [ 1.1603,  0.6595,  0.9830,  1.2296,  1.4939],\n",
      "        [-0.4285,  0.2916,  0.0005,  0.0768, -0.5467]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6705,  0.4516,  0.4384,  2.1908, -0.8782],\n",
      "        [-0.9299,  0.3581,  0.9758,  2.4688, -0.5089],\n",
      "        [-1.2994,  1.6353, -0.4474, -0.5959, -1.3784],\n",
      "        [ 0.4460, -0.7901,  0.4214,  2.0522,  1.1958]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6911],\n",
      "        [-0.1676],\n",
      "        [-3.6610],\n",
      "        [-0.9175]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7112, -1.2846, -2.2009,  0.0910,  1.3194],\n",
      "        [-0.1775,  0.3709,  2.7137,  2.6435, -0.6752],\n",
      "        [-0.2158,  0.4047, -1.5473,  1.5201, -0.0637],\n",
      "        [ 0.7445, -0.5432,  0.5109, -0.2815, -0.7265]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1991,  0.1694,  0.5495, -0.2455,  0.4362],\n",
      "        [-0.3984,  0.5224,  0.2941,  0.2874, -0.0146],\n",
      "        [ 2.1451,  1.6510,  2.4389,  2.4667,  2.9953],\n",
      "        [ 0.8120,  0.0517,  0.1256,  0.1441,  0.4229]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7112, -1.2846, -2.2009,  0.0910,  1.3194],\n",
      "        [-0.1775,  0.3709,  2.7137,  2.6435, -0.6752],\n",
      "        [-0.2158,  0.4047, -1.5473,  1.5201, -0.0637],\n",
      "        [ 0.7445, -0.5432,  0.5109, -0.2815, -0.7265]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7323],\n",
      "        [ 1.8321],\n",
      "        [-0.0099],\n",
      "        [ 0.2928]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0357,  0.4616,  0.4498,  0.7006,  1.7987],\n",
      "        [-0.0222, -1.3066, -0.3823, -0.1608, -0.0531],\n",
      "        [-1.1931,  0.8047, -0.5627, -0.8429, -0.8444],\n",
      "        [ 0.2756,  1.0280, -0.5724,  0.3313, -0.1383]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1437, -0.0474,  0.4579, -0.5719, -0.2422],\n",
      "        [-0.5756, -0.7119, -0.7054, -0.9561, -1.8648],\n",
      "        [ 0.2553, -0.3144,  0.2773, -0.1507, -0.1950],\n",
      "        [ 0.0189,  0.7000, -0.3003,  0.4239,  0.7749]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0357,  0.4616,  0.4498,  0.7006,  1.7987],\n",
      "        [-0.0222, -1.3066, -0.3823, -0.1608, -0.0531],\n",
      "        [-1.1931,  0.8047, -0.5627, -0.8429, -0.8444],\n",
      "        [ 0.2756,  1.0280, -0.5724,  0.3313, -0.1383]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5032],\n",
      "        [ 1.4653],\n",
      "        [-0.4219],\n",
      "        [ 0.9300]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7837,  1.4304, -2.2828, -1.6115, -1.0979],\n",
      "        [-0.0289, -1.8303, -1.4528,  0.3184, -1.1299],\n",
      "        [-0.5618,  1.7163,  0.4962, -0.3790,  0.8331],\n",
      "        [ 0.1333, -0.4776,  1.4058,  0.9975, -0.1748]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0155, -0.0402, -0.7209, -0.1072,  0.2472],\n",
      "        [-0.7364, -1.0864, -1.5506, -1.5119, -2.1159],\n",
      "        [ 0.4636, -0.4733, -0.2528, -0.7021, -0.4427],\n",
      "        [-0.3264, -0.0450, -0.2188, -0.4876, -0.7112]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7837,  1.4304, -2.2828, -1.6115, -1.0979],\n",
      "        [-0.0289, -1.8303, -1.4528,  0.3184, -1.1299],\n",
      "        [-0.5618,  1.7163,  0.4962, -0.3790,  0.8331],\n",
      "        [ 0.1333, -0.4776,  1.4058,  0.9975, -0.1748]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4772],\n",
      "        [ 6.1720],\n",
      "        [-1.3009],\n",
      "        [-0.6917]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0457, -2.5064,  1.0347,  1.2026, -0.1793],\n",
      "        [ 0.0878, -0.5450,  1.0358,  0.6753, -0.6021],\n",
      "        [-0.1715,  1.6023, -1.1399, -0.2021,  0.4461],\n",
      "        [-1.9004,  1.9958, -1.1745,  0.1232,  0.8268]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1115, -1.1162, -0.8890, -0.9615, -2.0052],\n",
      "        [ 0.0493, -0.0394, -0.5524,  0.2529,  0.2195],\n",
      "        [ 0.6570,  0.4213, -0.3216,  0.6463,  1.0563],\n",
      "        [ 0.2616,  0.1270,  0.2572,  0.4334,  0.1471]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0457, -2.5064,  1.0347,  1.2026, -0.1793],\n",
      "        [ 0.0878, -0.5450,  1.0358,  0.6753, -0.6021],\n",
      "        [-0.1715,  1.6023, -1.1399, -0.2021,  0.4461],\n",
      "        [-1.9004,  1.9958, -1.1745,  0.1232,  0.8268]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1980],\n",
      "        [-0.5078],\n",
      "        [ 1.2696],\n",
      "        [-0.3708]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0389, -0.5272,  0.0492, -0.2865, -0.4050],\n",
      "        [ 0.6932, -0.9281, -0.8308,  0.1860,  0.4221],\n",
      "        [ 1.9219, -0.0141, -0.3847, -1.0049, -1.6833],\n",
      "        [ 1.0238,  0.2335,  1.8562,  0.5689, -0.0399]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5522, -1.5175, -0.9029, -0.5215, -2.0212],\n",
      "        [-0.0439,  0.0234, -0.1677, -0.1121,  0.1384],\n",
      "        [ 0.1841,  1.0694, -0.1704,  0.1527, -0.2064],\n",
      "        [ 0.0352, -0.3438, -0.1367, -0.1152, -0.0474]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0389, -0.5272,  0.0492, -0.2865, -0.4050],\n",
      "        [ 0.6932, -0.9281, -0.8308,  0.1860,  0.4221],\n",
      "        [ 1.9219, -0.0141, -0.3847, -1.0049, -1.6833],\n",
      "        [ 1.0238,  0.2335,  1.8562,  0.5689, -0.0399]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7020],\n",
      "        [ 0.1247],\n",
      "        [ 0.5983],\n",
      "        [-0.3616]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8502, -0.2900,  1.5414,  2.7018, -0.4641],\n",
      "        [-0.1623, -1.8948,  0.3997, -0.3887, -0.4890],\n",
      "        [ 1.0343, -0.3552, -0.3118,  0.9450,  1.5279],\n",
      "        [-1.1829,  0.3968,  0.3018,  2.3680, -0.1100]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0845, -1.5793, -1.2956, -1.5152, -2.6776],\n",
      "        [ 0.0400,  0.0652,  0.2709,  0.4327, -0.3390],\n",
      "        [ 0.0530,  0.0500,  0.2060,  0.0471,  0.0725],\n",
      "        [-0.0225, -0.3072, -0.7302,  0.2450,  0.5561]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8502, -0.2900,  1.5414,  2.7018, -0.4641],\n",
      "        [-0.1623, -1.8948,  0.3997, -0.3887, -0.4890],\n",
      "        [ 1.0343, -0.3552, -0.3118,  0.9450,  1.5279],\n",
      "        [-1.1829,  0.3968,  0.3018,  2.3680, -0.1100]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.4681],\n",
      "        [-0.0241],\n",
      "        [ 0.1280],\n",
      "        [ 0.2033]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8867, -1.8878, -0.2460,  1.0850, -1.4245],\n",
      "        [ 1.3510,  2.0147,  0.8238,  0.4245, -0.7999],\n",
      "        [ 0.2487,  0.8308,  0.4504, -0.3738,  0.5581],\n",
      "        [ 0.6741,  0.1034,  0.5195,  3.5547, -0.3241]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2033, -0.4814, -0.3860, -0.7049, -1.0252],\n",
      "        [ 0.0826,  0.8675,  0.3343, -0.0929,  0.7422],\n",
      "        [-0.1464,  0.0263,  0.3432, -0.0190, -0.0313],\n",
      "        [ 0.2295, -0.1538,  0.8719, -0.5094,  0.4371]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8867, -1.8878, -0.2460,  1.0850, -1.4245],\n",
      "        [ 1.3510,  2.0147,  0.8238,  0.4245, -0.7999],\n",
      "        [ 0.2487,  0.8308,  0.4504, -0.3738,  0.5581],\n",
      "        [ 0.6741,  0.1034,  0.5195,  3.5547, -0.3241]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3158],\n",
      "        [ 1.5016],\n",
      "        [ 0.1296],\n",
      "        [-1.3609]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4410, -0.0029, -0.6008,  1.1795,  1.6586],\n",
      "        [-0.0968,  0.3240,  0.8450, -0.6389, -1.1368],\n",
      "        [-0.5700, -0.0698, -0.8718,  0.8442,  0.3635],\n",
      "        [-1.2283,  0.6082, -0.1412,  2.1165, -0.7982]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4973, -1.3712, -1.4307, -1.0689, -1.4313],\n",
      "        [-0.2307, -0.5720, -0.6398, -0.9123, -0.7925],\n",
      "        [-0.0645, -0.6391,  0.4275, -0.0301, -0.0546],\n",
      "        [ 0.8504,  0.9872,  2.2221, -0.1666,  1.4074]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4410, -0.0029, -0.6008,  1.1795,  1.6586],\n",
      "        [-0.0968,  0.3240,  0.8450, -0.6389, -1.1368],\n",
      "        [-0.5700, -0.0698, -0.8718,  0.8442,  0.3635],\n",
      "        [-1.2283,  0.6082, -0.1412,  2.1165, -0.7982]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9905],\n",
      "        [ 0.7801],\n",
      "        [-0.3366],\n",
      "        [-2.2337]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6584, -0.3725,  0.2889,  0.3313, -0.5285],\n",
      "        [-0.7223, -0.9281,  0.0628, -0.3099,  0.4960],\n",
      "        [-1.1483, -0.2037, -0.3593, -1.3459, -0.2234],\n",
      "        [ 0.7828, -0.2698, -0.6882, -0.8621, -0.1317]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5911, -0.1669,  1.0132, -0.1293,  1.2088],\n",
      "        [-0.2780, -1.4023, -0.5640, -0.8299, -1.1392],\n",
      "        [ 0.5458,  0.0103,  0.1104, -0.7490, -0.1452],\n",
      "        [ 0.8232,  1.4965,  2.0918,  1.1331,  1.6119]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6584, -0.3725,  0.2889,  0.3313, -0.5285],\n",
      "        [-0.7223, -0.9281,  0.0628, -0.3099,  0.4960],\n",
      "        [-1.1483, -0.2037, -0.3593, -1.3459, -0.2234],\n",
      "        [ 0.7828, -0.2698, -0.6882, -0.8621, -0.1317]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7160],\n",
      "        [ 1.1589],\n",
      "        [ 0.3721],\n",
      "        [-2.3880]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1615,  0.1259,  0.3205, -0.4523, -0.9844],\n",
      "        [-0.6190,  1.3927,  1.6374,  0.8126, -1.4843],\n",
      "        [-0.6254, -0.1004, -1.1166,  1.9883,  0.7140],\n",
      "        [ 1.3736,  0.5129,  0.7917,  0.3636, -1.2073]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7117,  0.2903,  0.1864,  0.1557,  1.2115],\n",
      "        [-0.4349, -1.5615, -1.0239, -1.0186, -1.8182],\n",
      "        [-0.3423,  0.2469,  0.3983,  0.2217, -0.1900],\n",
      "        [ 1.5591,  2.2756,  1.7999,  2.2182,  2.4549]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1615,  0.1259,  0.3205, -0.4523, -0.9844],\n",
      "        [-0.6190,  1.3927,  1.6374,  0.8126, -1.4843],\n",
      "        [-0.6254, -0.1004, -1.1166,  1.9883,  0.7140],\n",
      "        [ 1.3736,  0.5129,  0.7917,  0.3636, -1.2073]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2816],\n",
      "        [-1.7110],\n",
      "        [ 0.0497],\n",
      "        [ 2.5764]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2398, -0.8461, -2.0326,  0.0449,  0.2525],\n",
      "        [-0.0643,  0.5146, -2.9675, -1.7391,  2.5700],\n",
      "        [ 0.2424,  1.1576,  1.1842, -1.0350,  0.0328],\n",
      "        [ 0.0354, -0.3894,  1.6171, -0.1592,  0.9037]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.6028,  1.6838,  0.0194,  0.5815,  1.3434],\n",
      "        [-0.7080, -0.9368, -0.9216, -0.1099, -0.9491],\n",
      "        [-0.2359,  0.4485, -0.1120, -0.4497,  0.6350],\n",
      "        [ 0.4939,  0.3164,  0.5824, -0.3463,  0.3999]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2398, -0.8461, -2.0326,  0.0449,  0.2525],\n",
      "        [-0.0643,  0.5146, -2.9675, -1.7391,  2.5700],\n",
      "        [ 0.2424,  1.1576,  1.1842, -1.0350,  0.0328],\n",
      "        [ 0.0354, -0.3894,  1.6171, -0.1592,  0.9037]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7145],\n",
      "        [ 0.0500],\n",
      "        [ 0.8157],\n",
      "        [ 1.2526]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0987,  0.8590, -0.8435,  0.1968, -0.9592],\n",
      "        [-0.9383,  0.8405, -0.2045,  0.4412, -0.7910],\n",
      "        [-0.1583, -0.4740, -0.9645, -0.1459, -1.7986],\n",
      "        [-0.8107, -0.5767,  0.0856,  0.1745,  0.2297]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8102,  0.9924,  1.0619,  0.2648,  1.5542],\n",
      "        [-0.4544, -0.1123, -0.4079, -0.5457, -1.8640],\n",
      "        [-0.0211,  0.3249,  0.5656,  0.2454, -0.0314],\n",
      "        [-0.3383, -0.5997, -0.1735, -0.7188, -0.6343]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0987,  0.8590, -0.8435,  0.1968, -0.9592],\n",
      "        [-0.9383,  0.8405, -0.2045,  0.4412, -0.7910],\n",
      "        [-0.1583, -0.4740, -0.9645, -0.1459, -1.7986],\n",
      "        [-0.8107, -0.5767,  0.0856,  0.1745,  0.2297]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5619],\n",
      "        [ 1.6491],\n",
      "        [-0.6755],\n",
      "        [ 0.3342]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2862, -0.5213,  1.4604,  0.0127,  0.3634],\n",
      "        [ 0.4385,  0.3583,  2.1799,  1.3218, -0.4456],\n",
      "        [-0.7502,  1.8127,  0.1348,  1.6943,  0.0539],\n",
      "        [-2.1281,  0.6892, -0.0864, -0.6509,  0.5997]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.7326,  1.6049,  2.0987,  1.2428,  2.2730],\n",
      "        [-0.6523, -1.4553, -1.3137, -1.1502, -1.1794],\n",
      "        [-0.2124,  0.0535, -0.6874, -0.3073, -0.3411],\n",
      "        [-0.4461, -0.4694,  0.4936, -0.5045, -0.5534]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2862, -0.5213,  1.4604,  0.0127,  0.3634],\n",
      "        [ 0.4385,  0.3583,  2.1799,  1.3218, -0.4456],\n",
      "        [-0.7502,  1.8127,  0.1348,  1.6943,  0.0539],\n",
      "        [-2.1281,  0.6892, -0.0864, -0.6509,  0.5997]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5742],\n",
      "        [-4.6661],\n",
      "        [-0.3753],\n",
      "        [ 0.5796]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2036, -1.6454,  0.9202, -0.3584,  0.2729],\n",
      "        [-0.2262, -0.3567,  0.2236, -0.3776, -1.1252],\n",
      "        [ 1.2505,  0.2091,  0.9639,  0.0304, -1.1899],\n",
      "        [-0.2645, -1.8159,  0.1982,  0.7367,  0.8888]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5233,  0.5268,  1.0293,  0.5567,  0.6588],\n",
      "        [ 0.6652,  1.2458,  0.7113,  0.6938,  1.8643],\n",
      "        [-0.0084, -0.2763,  0.1613, -0.4342, -0.2090],\n",
      "        [-0.2230, -0.3041,  0.7336,  0.4300, -0.4196]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2036, -1.6454,  0.9202, -0.3584,  0.2729],\n",
      "        [-0.2262, -0.3567,  0.2236, -0.3776, -1.1252],\n",
      "        [ 1.2505,  0.2091,  0.9639,  0.0304, -1.1899],\n",
      "        [-0.2645, -1.8159,  0.1982,  0.7367,  0.8888]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1671],\n",
      "        [-2.7955],\n",
      "        [ 0.3227],\n",
      "        [ 0.7004]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7618,  0.7995, -0.3732,  1.4742, -0.5169],\n",
      "        [-0.6670,  0.9664,  0.2766,  1.1681,  0.8305],\n",
      "        [-0.1732, -1.5414,  2.4110,  0.2828,  0.8414],\n",
      "        [ 0.7457,  0.1768, -0.7969,  1.5351, -0.0613]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0898,  0.2872,  0.9176,  0.5540,  0.4611],\n",
      "        [ 1.2081,  1.9715,  1.3680,  1.2008,  2.5374],\n",
      "        [ 0.4438, -0.3069,  0.4119, -0.3141,  0.0556],\n",
      "        [-0.6363,  0.1455,  0.0065, -0.4793, -0.9765]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7618,  0.7995, -0.3732,  1.4742, -0.5169],\n",
      "        [-0.6670,  0.9664,  0.2766,  1.1681,  0.8305],\n",
      "        [-0.1732, -1.5414,  2.4110,  0.2828,  0.8414],\n",
      "        [ 0.7457,  0.1768, -0.7969,  1.5351, -0.0613]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4545],\n",
      "        [ 4.9878],\n",
      "        [ 1.3471],\n",
      "        [-1.1299]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8633,  0.4960,  0.2232,  0.9937,  0.2823],\n",
      "        [-2.7418, -0.3023, -0.7647, -1.3884,  0.8943],\n",
      "        [ 0.3746,  0.1856,  0.8666, -0.0044,  0.2448],\n",
      "        [-0.5843,  0.1004, -0.2161, -1.6117,  0.4808]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2652, -0.1801, -0.6409,  0.1193, -0.0864],\n",
      "        [-0.5775, -0.4547, -0.6213, -0.9482, -0.4445],\n",
      "        [-0.4330, -0.4792, -0.8057, -1.1679, -0.7866],\n",
      "        [ 0.0817, -0.1323, -0.1586,  0.6023,  0.8326]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8633,  0.4960,  0.2232,  0.9937,  0.2823],\n",
      "        [-2.7418, -0.3023, -0.7647, -1.3884,  0.8943],\n",
      "        [ 0.3746,  0.1856,  0.8666, -0.0044,  0.2448],\n",
      "        [-0.5843,  0.1004, -0.2161, -1.6117,  0.4808]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6323],\n",
      "        [ 3.1149],\n",
      "        [-1.1368],\n",
      "        [-0.5971]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6823, -0.7212, -0.4795, -0.1848, -0.0537],\n",
      "        [ 0.8676,  1.7110, -2.2582,  1.6779,  0.0143],\n",
      "        [-0.6188, -0.0491,  1.5871,  1.1788, -1.2039],\n",
      "        [ 0.3896,  0.9615,  0.2788, -1.2599,  1.6379]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1445, -0.1434,  0.4894,  0.3050, -0.3842],\n",
      "        [-1.2252, -1.3817, -1.3916, -1.4938, -2.7967],\n",
      "        [-0.1250, -0.4447,  0.3132,  0.2833,  0.1646],\n",
      "        [ 0.6572, -0.0763,  0.0436,  0.5500,  0.9673]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6823, -0.7212, -0.4795, -0.1848, -0.0537],\n",
      "        [ 0.8676,  1.7110, -2.2582,  1.6779,  0.0143],\n",
      "        [-0.6188, -0.0491,  1.5871,  1.1788, -1.2039],\n",
      "        [ 0.3896,  0.9615,  0.2788, -1.2599,  1.6379]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4101],\n",
      "        [-2.8310],\n",
      "        [ 0.7321],\n",
      "        [ 1.0863]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6597, -0.1624, -1.6341,  0.1331, -1.0765],\n",
      "        [-0.6706,  0.0434,  0.9782,  0.1606,  1.1733],\n",
      "        [ 0.2187,  1.0708, -0.2050, -0.8043,  1.2742],\n",
      "        [-1.2208, -0.3081,  0.2736,  0.3753,  1.7068]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0200, -0.7990, -0.4382,  0.3149,  0.2429],\n",
      "        [ 0.0596, -0.0342, -0.1883, -0.8472, -0.0870],\n",
      "        [ 0.1750, -0.5160,  0.4213,  0.3612, -0.6709],\n",
      "        [-0.0452, -0.5450,  0.3255, -0.3768, -0.7229]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6597, -0.1624, -1.6341,  0.1331, -1.0765],\n",
      "        [-0.6706,  0.0434,  0.9782,  0.1606,  1.1733],\n",
      "        [ 0.2187,  1.0708, -0.2050, -0.8043,  1.2742],\n",
      "        [-1.2208, -0.3081,  0.2736,  0.3753,  1.7068]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6394],\n",
      "        [-0.4637],\n",
      "        [-1.7460],\n",
      "        [-1.0632]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0745, -0.1202,  0.7187,  0.1117, -0.4875],\n",
      "        [-0.3265,  0.7169,  0.2112, -0.4432,  0.2756],\n",
      "        [-1.8984,  1.7916, -0.3779,  0.4732, -0.4851],\n",
      "        [-0.5614,  0.1349, -0.5201,  1.4328, -2.2907]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3142, -0.2396, -0.3679, -0.1840, -0.5241],\n",
      "        [ 0.5522, -0.2626,  0.1439,  0.1072, -0.7521],\n",
      "        [ 0.7212,  0.7309,  0.2949,  0.5571,  1.1560],\n",
      "        [ 0.1652, -0.1545,  0.0225, -0.6572, -0.5558]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0745, -0.1202,  0.7187,  0.1117, -0.4875],\n",
      "        [-0.3265,  0.7169,  0.2112, -0.4432,  0.2756],\n",
      "        [-1.8984,  1.7916, -0.3779,  0.4732, -0.4851],\n",
      "        [-0.5614,  0.1349, -0.5201,  1.4328, -2.2907]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3383],\n",
      "        [-0.5930],\n",
      "        [-0.4683],\n",
      "        [ 0.2062]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3183, -0.0916, -0.9624, -0.2155, -0.5847],\n",
      "        [-0.3399, -0.3878,  0.2100, -0.4894, -1.2063],\n",
      "        [ 0.3653, -0.9952, -2.0690,  0.9935, -0.9268],\n",
      "        [-1.0831,  0.9184,  1.3387,  1.0677, -1.7387]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4322, -0.6535, -0.0573, -0.6145, -0.2735],\n",
      "        [ 0.0705, -0.6893, -1.5005, -0.7351, -0.6175],\n",
      "        [ 0.5469,  0.8394,  1.3989,  0.3771,  0.9881],\n",
      "        [ 0.2299, -0.3930, -0.7740, -0.0032,  0.1294]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3183, -0.0916, -0.9624, -0.2155, -0.5847],\n",
      "        [-0.3399, -0.3878,  0.2100, -0.4894, -1.2063],\n",
      "        [ 0.3653, -0.9952, -2.0690,  0.9935, -0.9268],\n",
      "        [-1.0831,  0.9184,  1.3387,  1.0677, -1.7387]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2697],\n",
      "        [ 1.0328],\n",
      "        [-4.0710],\n",
      "        [-1.8745]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.6314, -1.0755, -0.7906,  0.6829, -0.1482],\n",
      "        [ 1.2043,  0.0932,  1.6362, -1.6172, -0.2092],\n",
      "        [ 1.9560,  0.0800,  0.1737,  0.3815,  1.2046],\n",
      "        [ 0.1859, -0.6986,  0.6180,  0.6608, -0.0451]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3181, -0.7807, -0.4697, -0.0497, -0.0064],\n",
      "        [-0.4206, -0.3157, -0.2716, -0.9857, -0.9873],\n",
      "        [ 1.5305,  1.5876,  1.5525,  1.4783,  3.0403],\n",
      "        [ 0.7858,  1.0587,  0.9447,  1.0220,  1.7396]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.6314, -1.0755, -0.7906,  0.6829, -0.1482],\n",
      "        [ 1.2043,  0.0932,  1.6362, -1.6172, -0.2092],\n",
      "        [ 1.9560,  0.0800,  0.1737,  0.3815,  1.2046],\n",
      "        [ 0.1859, -0.6986,  0.6180,  0.6608, -0.0451]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3409],\n",
      "        [ 0.8202],\n",
      "        [ 7.6167],\n",
      "        [ 0.5872]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0779, -0.3417,  1.8975,  0.0920,  1.8611],\n",
      "        [ 0.7519, -1.3436,  0.9541, -1.0566,  1.0478],\n",
      "        [ 0.0207,  0.0283,  0.9960, -0.8425,  1.4543],\n",
      "        [ 0.8552,  1.3128, -0.2747, -0.8021, -0.4956]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3451, -0.0145,  0.2242,  0.2919,  0.4068],\n",
      "        [ 0.0327, -1.8387, -0.9959, -0.9273, -0.9271],\n",
      "        [-0.8454, -0.5780, -0.4337, -0.3446, -1.9124],\n",
      "        [ 0.3888,  0.3249, -0.2994, -0.0666,  0.3482]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0779, -0.3417,  1.8975,  0.0920,  1.8611],\n",
      "        [ 0.7519, -1.3436,  0.9541, -1.0566,  1.0478],\n",
      "        [ 0.0207,  0.0283,  0.9960, -0.8425,  1.4543],\n",
      "        [ 0.8552,  1.3128, -0.2747, -0.8021, -0.4956]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1875],\n",
      "        [ 1.5533],\n",
      "        [-2.9567],\n",
      "        [ 0.7222]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4025, -0.4778,  1.4101, -0.9667,  1.9151],\n",
      "        [ 1.0006, -0.4660,  1.1378, -0.3857, -0.0591],\n",
      "        [-2.3449,  0.6718,  1.3240,  1.1848, -0.1369],\n",
      "        [ 1.5698,  0.0146,  0.6010,  0.1528, -0.6368]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2279, -0.2667, -0.1105,  0.0535, -0.5203],\n",
      "        [-0.1746, -0.4005, -0.9102, -1.5667, -2.2381],\n",
      "        [ 0.5015,  1.1723,  1.0336,  0.4477,  0.4312],\n",
      "        [ 0.0234, -0.4543, -0.2220, -0.4947, -0.2741]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4025, -0.4778,  1.4101, -0.9667,  1.9151],\n",
      "        [ 1.0006, -0.4660,  1.1378, -0.3857, -0.0591],\n",
      "        [-2.3449,  0.6718,  1.3240,  1.1848, -0.1369],\n",
      "        [ 1.5698,  0.0146,  0.6010,  0.1528, -0.6368]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1682],\n",
      "        [-0.2871],\n",
      "        [ 1.4514],\n",
      "        [-0.0044]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6438, -0.0603,  0.3387, -0.7327, -0.2799],\n",
      "        [ 0.3441,  0.9987,  1.5491,  0.9052,  0.8588],\n",
      "        [-0.1438, -0.5825,  0.6695,  0.0470,  1.8061],\n",
      "        [ 1.3359,  0.4755,  0.5621, -0.1982,  0.2369]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4873,  0.0143, -0.1706, -0.2210,  0.3789],\n",
      "        [-0.3430, -0.3788, -0.6697, -1.0940, -2.0118],\n",
      "        [ 0.1707,  0.8528,  1.7010,  0.7316,  0.1276],\n",
      "        [ 0.0907,  0.5173, -0.3201, -0.2750, -0.0219]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6438, -0.0603,  0.3387, -0.7327, -0.2799],\n",
      "        [ 0.3441,  0.9987,  1.5491,  0.9052,  0.8588],\n",
      "        [-0.1438, -0.5825,  0.6695,  0.0470,  1.8061],\n",
      "        [ 1.3359,  0.4755,  0.5621, -0.1982,  0.2369]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8038],\n",
      "        [-4.2517],\n",
      "        [ 0.8823],\n",
      "        [ 0.2366]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6284,  0.5804,  0.4197, -0.2808, -0.9511],\n",
      "        [-0.3649,  0.3386, -0.6101,  1.9270,  0.7723],\n",
      "        [-0.9755, -0.8456,  1.0101,  1.7451, -0.1807],\n",
      "        [ 1.0981,  1.9863, -1.1512, -1.0990, -0.2948]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5560,  0.4123, -0.2649, -0.8417, -0.0482],\n",
      "        [ 0.4820,  1.6164,  1.1407,  0.9863,  1.2972],\n",
      "        [ 0.3677,  0.4083,  0.6784,  0.4896,  0.1314],\n",
      "        [ 0.0553,  0.6513,  0.2909,  0.7017,  0.1301]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6284,  0.5804,  0.4197, -0.2808, -0.9511],\n",
      "        [-0.3649,  0.3386, -0.6101,  1.9270,  0.7723],\n",
      "        [-0.9755, -0.8456,  1.0101,  1.7451, -0.1807],\n",
      "        [ 1.0981,  1.9863, -1.1512, -1.0990, -0.2948]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4951],\n",
      "        [ 2.5781],\n",
      "        [ 0.8120],\n",
      "        [ 0.2099]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3391, -0.3465,  1.8234,  0.5754, -0.6283],\n",
      "        [-2.3792,  0.8022,  1.5439, -1.1230,  1.1280],\n",
      "        [-1.2304, -1.4939,  0.9529, -1.7450, -0.1104],\n",
      "        [ 1.7992,  0.2456,  0.7426,  0.5840, -0.6211]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6625, -1.2608,  0.2206, -0.1728,  0.3007],\n",
      "        [-0.4121, -0.2149, -0.9941, -1.1469, -1.5542],\n",
      "        [-0.4813,  0.8108,  1.0377,  1.6356, -0.0503],\n",
      "        [-0.0132, -0.0032, -0.4845, -0.3769, -0.0259]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3391, -0.3465,  1.8234,  0.5754, -0.6283],\n",
      "        [-2.3792,  0.8022,  1.5439, -1.1230,  1.1280],\n",
      "        [-1.2304, -1.4939,  0.9529, -1.7450, -0.1104],\n",
      "        [ 1.7992,  0.2456,  0.7426,  0.5840, -0.6211]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7753],\n",
      "        [-1.1921],\n",
      "        [-2.4789],\n",
      "        [-0.5883]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2085,  1.1511,  0.3503, -1.3592,  1.3104],\n",
      "        [-2.5114, -1.2368, -1.4097,  0.2887, -1.2967],\n",
      "        [-1.5770,  0.6471, -0.3717,  0.4866,  1.0947],\n",
      "        [ 0.3981,  1.2837,  0.7675,  0.3642, -0.6417]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3048, -0.9037, -0.2546, -0.6131, -1.1819],\n",
      "        [ 0.3641, -0.9636, -0.5522, -0.7327, -0.5396],\n",
      "        [ 1.1028,  1.1764,  1.8381,  1.2749,  1.6853],\n",
      "        [ 0.3861, -0.5738,  0.0014, -0.7843,  0.3212]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2085,  1.1511,  0.3503, -1.3592,  1.3104],\n",
      "        [-2.5114, -1.2368, -1.4097,  0.2887, -1.2967],\n",
      "        [-1.5770,  0.6471, -0.3717,  0.4866,  1.0947],\n",
      "        [ 0.3981,  1.2837,  0.7675,  0.3642, -0.6417]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2132],\n",
      "        [ 1.5440],\n",
      "        [ 0.8043],\n",
      "        [-1.0735]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0593, -0.3926, -0.2671,  0.3189, -0.6966],\n",
      "        [-0.7374, -0.5966, -1.8258,  0.0495,  0.3010],\n",
      "        [ 0.9118, -1.4766, -1.0976,  0.3355,  1.2455],\n",
      "        [ 2.2379, -0.6957, -0.6242,  1.9779, -1.2722]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8013,  0.6607,  0.8614,  0.2580,  0.9721],\n",
      "        [-0.7366, -1.6024, -0.8060, -1.4653, -2.0727],\n",
      "        [ 0.7729,  1.3848,  1.0444,  1.0491,  1.9798],\n",
      "        [ 0.5018,  0.4595,  0.5983,  0.2470,  1.0910]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0593, -0.3926, -0.2671,  0.3189, -0.6966],\n",
      "        [-0.7374, -0.5966, -1.8258,  0.0495,  0.3010],\n",
      "        [ 0.9118, -1.4766, -1.0976,  0.3355,  1.2455],\n",
      "        [ 2.2379, -0.6957, -0.6242,  1.9779, -1.2722]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9332],\n",
      "        [ 2.2743],\n",
      "        [ 0.3313],\n",
      "        [-0.4698]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4717,  0.8377, -1.0501,  0.3406,  0.3864],\n",
      "        [ 0.3701, -1.4732,  0.3362, -0.2333,  1.6313],\n",
      "        [-1.0705, -0.3572,  0.1634,  0.0454, -1.7305],\n",
      "        [-0.5463,  0.1580,  0.8874,  0.7845, -2.6212]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0512,  1.5308,  0.8994,  0.9884,  2.1829],\n",
      "        [-0.4244, -1.2388, -1.8098, -1.8425, -3.7653],\n",
      "        [ 0.5973,  1.2791,  0.6287,  1.3057,  1.0655],\n",
      "        [ 0.4062, -0.4543, -0.1678,  0.0188, -0.1088]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4717,  0.8377, -1.0501,  0.3406,  0.3864],\n",
      "        [ 0.3701, -1.4732,  0.3362, -0.2333,  1.6313],\n",
      "        [-1.0705, -0.3572,  0.1634,  0.0454, -1.7305],\n",
      "        [-0.5463,  0.1580,  0.8874,  0.7845, -2.6212]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0220],\n",
      "        [-4.6530],\n",
      "        [-2.7780],\n",
      "        [-0.1426]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5631,  0.2209,  0.4000,  1.9415,  1.3528],\n",
      "        [-0.7806,  0.2424,  1.0046, -0.9052, -0.2254],\n",
      "        [-1.1695, -0.7203, -0.1349,  1.0341, -0.2297],\n",
      "        [ 1.3071, -0.8902,  1.5286, -1.7922,  0.6926]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8615,  1.2089,  1.0003,  0.8217,  0.6252],\n",
      "        [-0.2473,  0.7391,  0.5484, -0.1816,  0.1355],\n",
      "        [ 0.2064,  0.0363,  0.0237, -0.3173,  0.0400],\n",
      "        [ 0.1480, -0.4876,  0.1219,  0.1579, -0.2518]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5631,  0.2209,  0.4000,  1.9415,  1.3528],\n",
      "        [-0.7806,  0.2424,  1.0046, -0.9052, -0.2254],\n",
      "        [-1.1695, -0.7203, -0.1349,  1.0341, -0.2297],\n",
      "        [ 1.3071, -0.8902,  1.5286, -1.7922,  0.6926]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7615],\n",
      "        [ 1.0570],\n",
      "        [-0.6080],\n",
      "        [ 0.3564]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3843,  0.2975, -1.3622,  0.2861,  0.5172],\n",
      "        [-0.4494, -2.2277,  1.5321, -1.4279,  1.2409],\n",
      "        [ 0.9140,  0.1709,  1.4464, -0.3600,  1.2045],\n",
      "        [-0.4069, -0.8759,  0.9835,  0.2918,  0.3383]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0576,  0.0098,  1.6445,  0.7219, -0.0859],\n",
      "        [-0.2388, -0.0643,  0.3240, -1.1396, -0.9536],\n",
      "        [-0.0690, -0.2163,  0.0453, -0.9640, -0.2519],\n",
      "        [ 0.5578, -0.1330, -0.1320, -0.3139, -0.2123]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3843,  0.2975, -1.3622,  0.2861,  0.5172],\n",
      "        [-0.4494, -2.2277,  1.5321, -1.4279,  1.2409],\n",
      "        [ 0.9140,  0.1709,  1.4464, -0.3600,  1.2045],\n",
      "        [-0.4069, -0.8759,  0.9835,  0.2918,  0.3383]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0529],\n",
      "        [ 1.1906],\n",
      "        [ 0.0091],\n",
      "        [-0.4037]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4969,  1.1243,  0.0740,  2.3831,  0.8140],\n",
      "        [-0.3567, -0.3715,  0.4165,  1.8092,  0.0942],\n",
      "        [-0.0080, -0.2279, -0.3715,  1.3491,  0.7158],\n",
      "        [-0.8311,  1.8229, -0.8488, -0.3847,  0.3053]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1466,  1.3332,  1.5139,  0.4683,  1.3165],\n",
      "        [-0.3306, -0.4569, -0.3583, -0.4618, -1.2669],\n",
      "        [ 0.0202, -0.0622,  0.2083, -0.5778,  0.0003],\n",
      "        [ 0.3162, -0.4084, -0.4804, -0.3284, -0.3445]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4969,  1.1243,  0.0740,  2.3831,  0.8140],\n",
      "        [-0.3567, -0.3715,  0.4165,  1.8092,  0.0942],\n",
      "        [-0.0080, -0.2279, -0.3715,  1.3491,  0.7158],\n",
      "        [-0.8311,  1.8229, -0.8488, -0.3847,  0.3053]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.2288],\n",
      "        [-0.8164],\n",
      "        [-0.8426],\n",
      "        [-0.5783]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7179, -0.1973,  0.3347,  1.3367,  0.3003],\n",
      "        [ 1.0032,  1.7190, -0.3257,  0.4763, -0.5398],\n",
      "        [-2.0391, -0.9961,  0.2829, -0.4616,  0.6074],\n",
      "        [-1.2213, -1.1313,  0.5528, -0.0209, -0.5941]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4525,  0.1052, -0.1961, -0.2791, -0.4821],\n",
      "        [-0.1524,  0.1501, -0.7467,  0.2462, -0.1524],\n",
      "        [ 0.3790,  0.0805,  0.5417, -0.4690,  0.7704],\n",
      "        [ 0.3615, -0.5474,  0.0242,  0.1158,  0.0259]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7179, -0.1973,  0.3347,  1.3367,  0.3003],\n",
      "        [ 1.0032,  1.7190, -0.3257,  0.4763, -0.5398],\n",
      "        [-2.0391, -0.9961,  0.2829, -0.4616,  0.6074],\n",
      "        [-1.2213, -1.1313,  0.5528, -0.0209, -0.5941]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9291],\n",
      "        [ 0.5479],\n",
      "        [-0.0151],\n",
      "        [ 0.1732]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9010,  0.8260, -1.6179,  0.9282,  1.2716],\n",
      "        [ 0.2433,  0.7383,  0.8397,  0.6247, -1.1522],\n",
      "        [-0.8172, -0.1027, -0.1598,  0.1555,  0.2505],\n",
      "        [-1.0470, -0.9810,  1.4468,  0.7220,  1.2803]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2088,  0.6774,  0.3984,  0.6083,  0.9202],\n",
      "        [-0.3180,  0.3150,  0.1240,  0.0021,  0.3026],\n",
      "        [ 0.4986, -0.3900,  0.2653, -0.1225, -0.6769],\n",
      "        [ 0.4101, -0.0961, -0.9686,  0.1520, -0.5207]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9010,  0.8260, -1.6179,  0.9282,  1.2716],\n",
      "        [ 0.2433,  0.7383,  0.8397,  0.6247, -1.1522],\n",
      "        [-0.8172, -0.1027, -0.1598,  0.1555,  0.2505],\n",
      "        [-1.0470, -0.9810,  1.4468,  0.7220,  1.2803]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4617],\n",
      "        [-0.0879],\n",
      "        [-0.5984],\n",
      "        [-2.2934]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0092,  0.5437,  0.4817, -1.3183,  0.4692],\n",
      "        [-0.2963, -1.2682, -0.7822,  0.2511,  0.0810],\n",
      "        [ 0.3798,  0.7541,  1.7541, -1.0773,  1.1431],\n",
      "        [ 1.6867, -0.1486,  0.7224, -0.0896,  0.2065]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3760,  0.5730, -0.2907,  0.1337, -0.0369],\n",
      "        [-0.7175,  0.6560, -0.1334,  0.5990, -0.1627],\n",
      "        [ 0.2610, -0.7647,  0.2647,  0.3433, -0.2406],\n",
      "        [ 0.5471,  1.1798,  1.5313,  1.2141,  1.7818]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0092,  0.5437,  0.4817, -1.3183,  0.4692],\n",
      "        [-0.2963, -1.2682, -0.7822,  0.2511,  0.0810],\n",
      "        [ 0.3798,  0.7541,  1.7541, -1.0773,  1.1431],\n",
      "        [ 1.6867, -0.1486,  0.7224, -0.0896,  0.2065]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0186],\n",
      "        [-0.3778],\n",
      "        [-0.6580],\n",
      "        [ 2.1128]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2252,  2.2737,  1.3378, -0.1294,  0.8857],\n",
      "        [-0.0801,  0.5321,  0.6934,  0.3668,  0.6767],\n",
      "        [-0.0521,  0.4189, -0.2801, -0.1637, -0.7471],\n",
      "        [ 0.6585,  1.0728,  2.0953, -0.4518,  0.6179]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0280,  0.1229,  1.3791,  0.6042,  0.2519],\n",
      "        [ 0.4501,  0.0090, -0.5099, -0.0860, -0.3907],\n",
      "        [ 0.5614, -0.1954, -0.2852, -0.3360, -0.0576],\n",
      "        [ 0.2215, -0.4012, -0.3327, -0.3832, -0.5691]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2252,  2.2737,  1.3378, -0.1294,  0.8857],\n",
      "        [-0.0801,  0.5321,  0.6934,  0.3668,  0.6767],\n",
      "        [-0.0521,  0.4189, -0.2801, -0.1637, -0.7471],\n",
      "        [ 0.6585,  1.0728,  2.0953, -0.4518,  0.6179]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2350],\n",
      "        [-0.6808],\n",
      "        [ 0.0668],\n",
      "        [-1.1601]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3861,  1.1018, -0.7974,  0.3204,  0.4633],\n",
      "        [-0.7883,  0.6723,  0.0125, -0.3204,  0.2836],\n",
      "        [ 2.0402,  0.6983,  1.1593,  0.7896,  1.0009],\n",
      "        [-1.5383, -0.9242,  0.4297,  1.5307,  0.7445]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7184,  0.3468, -0.0626, -0.8188, -0.9515],\n",
      "        [-0.0141, -1.1116, -0.0687, -0.0018, -1.2234],\n",
      "        [ 0.7640, -0.1457, -0.4251, -0.3388, -0.6039],\n",
      "        [ 0.5517, -0.3371,  0.3582,  0.3862,  0.6392]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3861,  1.1018, -0.7974,  0.3204,  0.4633],\n",
      "        [-0.7883,  0.6723,  0.0125, -0.3204,  0.2836],\n",
      "        [ 2.0402,  0.6983,  1.1593,  0.7896,  1.0009],\n",
      "        [-1.5383, -0.9242,  0.4297,  1.5307,  0.7445]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0062],\n",
      "        [-1.0835],\n",
      "        [ 0.0922],\n",
      "        [ 0.6839]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3896, -0.1032, -0.4576, -0.5471,  1.5495],\n",
      "        [ 1.3470,  1.0967,  1.2359, -0.8477,  2.1038],\n",
      "        [ 0.0899,  0.6413, -0.4679, -1.6414, -0.7276],\n",
      "        [-3.0108, -0.3620,  0.3570,  0.9280, -0.9188]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1981,  0.0367, -0.3839,  0.2723, -0.2006],\n",
      "        [ 0.6606,  0.5159,  0.3574, -0.1643,  0.7223],\n",
      "        [ 0.5373,  0.1638, -0.5594, -0.5919, -0.1974],\n",
      "        [ 0.2934, -0.0048, -0.1551,  0.6027, -0.0212]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3896, -0.1032, -0.4576, -0.5471,  1.5495],\n",
      "        [ 1.3470,  1.0967,  1.2359, -0.8477,  2.1038],\n",
      "        [ 0.0899,  0.6413, -0.4679, -1.6414, -0.7276],\n",
      "        [-3.0108, -0.3620,  0.3570,  0.9280, -0.9188]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3652],\n",
      "        [ 3.5562],\n",
      "        [ 1.5303],\n",
      "        [-0.3583]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0152,  1.9360,  1.9362,  1.8238, -1.2785],\n",
      "        [-0.9384, -0.3773,  0.5052, -1.3015, -0.9266],\n",
      "        [-0.3180, -0.8132,  2.2644, -1.0927, -0.2290],\n",
      "        [-0.4126,  0.0437,  1.5605, -0.5642, -0.6164]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1055,  1.3132,  0.8499,  0.8519,  0.2783],\n",
      "        [-0.7446, -0.8623, -1.8598, -1.7248, -2.9768],\n",
      "        [-0.4050, -0.5084, -0.8174, -0.8897, -1.3022],\n",
      "        [ 0.5518,  0.2021,  0.1735, -0.0376, -0.0703]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0152,  1.9360,  1.9362,  1.8238, -1.2785],\n",
      "        [-0.9384, -0.3773,  0.5052, -1.3015, -0.9266],\n",
      "        [-0.3180, -0.8132,  2.2644, -1.0927, -0.2290],\n",
      "        [-0.4126,  0.0437,  1.5605, -0.5642, -0.6164]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.4931],\n",
      "        [ 5.0877],\n",
      "        [-0.0382],\n",
      "        [ 0.1165]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7700,  1.5069,  0.0222, -0.7736,  0.5157],\n",
      "        [ 0.0555, -1.3560,  0.7117,  0.0310, -0.0895],\n",
      "        [-1.5610, -1.4235, -0.2374, -1.7230,  1.2654],\n",
      "        [ 0.1063, -0.7143,  0.9892,  0.6018,  0.9116]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9540, -1.6895, -1.2118, -2.1044, -3.3699],\n",
      "        [ 0.0179,  0.6966, -0.1926, -0.1567, -0.1334],\n",
      "        [ 0.6167, -0.8009, -0.2385, -0.7687, -1.3059],\n",
      "        [ 0.8714,  0.3480,  0.3110,  0.6649,  0.2051]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7700,  1.5069,  0.0222, -0.7736,  0.5157],\n",
      "        [ 0.0555, -1.3560,  0.7117,  0.0310, -0.0895],\n",
      "        [-1.5610, -1.4235, -0.2374, -1.7230,  1.2654],\n",
      "        [ 0.1063, -0.7143,  0.9892,  0.6018,  0.9116]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.4172],\n",
      "        [-1.0736],\n",
      "        [-0.0939],\n",
      "        [ 0.7389]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2975,  0.1264, -0.1587, -1.8773, -0.5729],\n",
      "        [ 0.8188, -0.7188,  0.6611,  1.0892, -0.1922],\n",
      "        [-0.6876, -1.8222,  0.6487,  0.1602, -0.3679],\n",
      "        [ 0.1372, -1.4054,  0.1083,  1.4041,  0.8850]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0982, -0.9971, -0.7769, -0.4657, -1.3255],\n",
      "        [ 0.5027,  1.2656,  0.3396,  0.8537,  0.9009],\n",
      "        [ 0.6296,  0.2317,  0.0254, -0.1953, -0.1893],\n",
      "        [ 0.1361,  0.0488,  0.1183,  0.4518, -0.0561]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2975,  0.1264, -0.1587, -1.8773, -0.5729],\n",
      "        [ 0.8188, -0.7188,  0.6611,  1.0892, -0.1922],\n",
      "        [-0.6876, -1.8222,  0.6487,  0.1602, -0.3679],\n",
      "        [ 0.1372, -1.4054,  0.1083,  1.4041,  0.8850]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3042],\n",
      "        [ 0.4831],\n",
      "        [-0.8003],\n",
      "        [ 0.5477]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1894,  1.1217,  0.1655, -0.1794, -0.9730],\n",
      "        [ 0.0367,  0.3089,  0.4334,  0.3026,  0.3715],\n",
      "        [ 0.5163,  0.0605, -0.0076,  0.2941, -0.2627],\n",
      "        [-1.1058, -2.1240, -0.0990,  0.9832, -1.1392]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9415, -0.7843, -0.8667, -1.3897, -2.3951],\n",
      "        [ 0.4274,  0.5339,  0.2779, -0.2511,  0.1302],\n",
      "        [ 0.0851, -0.7244, -0.5791, -0.7325, -0.1342],\n",
      "        [-0.1374,  0.9370,  0.8152,  0.2358,  0.3404]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1894,  1.1217,  0.1655, -0.1794, -0.9730],\n",
      "        [ 0.0367,  0.3089,  0.4334,  0.3026,  0.3715],\n",
      "        [ 0.5163,  0.0605, -0.0076,  0.2941, -0.2627],\n",
      "        [-1.1058, -2.1240, -0.0990,  0.9832, -1.1392]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.6765],\n",
      "        [ 0.2734],\n",
      "        [-0.1757],\n",
      "        [-2.0748]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3374, -0.7558, -0.9796,  0.5434,  0.1999],\n",
      "        [ 0.4385,  0.4094, -0.2109,  0.2125, -0.3521],\n",
      "        [ 1.1089,  0.2870, -1.1996,  1.7841, -2.0243],\n",
      "        [-0.8789,  1.5996, -1.7195,  0.6114,  0.2977]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4443, -0.5088,  0.1471,  0.1054,  0.0544],\n",
      "        [ 0.5896,  0.4988, -0.0345,  0.0080, -0.3812],\n",
      "        [ 0.4021, -0.5147, -0.1390, -0.3202, -0.9591],\n",
      "        [ 0.9252,  1.5319,  1.4808,  0.9666,  0.9252]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3374, -0.7558, -0.9796,  0.5434,  0.1999],\n",
      "        [ 0.4385,  0.4094, -0.2109,  0.2125, -0.3521],\n",
      "        [ 1.1089,  0.2870, -1.1996,  1.7841, -2.0243],\n",
      "        [-0.8789,  1.5996, -1.7195,  0.6114,  0.2977]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1587],\n",
      "        [ 0.6060],\n",
      "        [ 1.8350],\n",
      "        [-0.0427]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5384, -2.3887, -0.4717,  0.5863,  0.5890],\n",
      "        [-1.5833, -0.1152, -0.1405,  0.4076, -0.6633],\n",
      "        [ 2.6963,  1.4507, -0.2850,  0.5956, -1.6723],\n",
      "        [ 0.5082, -0.0162,  1.1403, -1.4798,  0.1554]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2165,  0.1262,  0.5520, -0.1063,  0.0142],\n",
      "        [-0.0183,  0.9013,  0.1222, -0.0436,  0.4428],\n",
      "        [-1.0196, -0.6970, -0.5304, -0.6115, -2.0616],\n",
      "        [ 0.8217,  1.2584,  1.8712,  1.0867,  0.8050]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5384, -2.3887, -0.4717,  0.5863,  0.5890],\n",
      "        [-1.5833, -0.1152, -0.1405,  0.4076, -0.6633],\n",
      "        [ 2.6963,  1.4507, -0.2850,  0.5956, -1.6723],\n",
      "        [ 0.5082, -0.0162,  1.1403, -1.4798,  0.1554]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7322],\n",
      "        [-0.4034],\n",
      "        [-0.5258],\n",
      "        [ 1.0478]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4002,  1.4391, -0.2383, -0.3208, -0.0076],\n",
      "        [-0.8459, -0.5895, -0.6443, -0.8994, -1.3810],\n",
      "        [-0.0510,  0.7120, -0.4634,  1.1921, -0.1334],\n",
      "        [ 0.6954, -0.3903,  2.6116,  2.6797,  0.1396]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4803, -0.1979,  0.4126,  0.3647, -0.6488],\n",
      "        [-0.3373,  0.2537,  0.4607,  0.3071,  0.6375],\n",
      "        [ 0.2388, -1.1461, -0.7901, -0.5068, -1.3008],\n",
      "        [ 0.4309,  1.1040,  0.3906,  0.2737,  1.1012]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4002,  1.4391, -0.2383, -0.3208, -0.0076],\n",
      "        [-0.8459, -0.5895, -0.6443, -0.8994, -1.3810],\n",
      "        [-0.0510,  0.7120, -0.4634,  1.1921, -0.1334],\n",
      "        [ 0.6954, -0.3903,  2.6116,  2.6797,  0.1396]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6875],\n",
      "        [-1.3179],\n",
      "        [-0.8928],\n",
      "        [ 1.7760]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7940, -1.2613, -1.3289,  0.6568, -0.0808],\n",
      "        [-1.7229, -1.1379, -0.4999, -1.3253,  1.6228],\n",
      "        [-1.9804,  0.9717,  1.8860,  0.0254,  2.1276],\n",
      "        [-0.7230, -0.7432, -0.5980,  1.5660, -1.0518]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3852, -0.4901, -0.3246, -0.1409,  0.4784],\n",
      "        [ 1.0372,  0.3684,  0.3910,  0.7721,  1.1314],\n",
      "        [-0.4286, -0.8501, -1.0820, -0.6578, -1.0330],\n",
      "        [ 0.2243,  0.6714,  0.1031,  0.2316,  0.1892]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7940, -1.2613, -1.3289,  0.6568, -0.0808],\n",
      "        [-1.7229, -1.1379, -0.4999, -1.3253,  1.6228],\n",
      "        [-1.9804,  0.9717,  1.8860,  0.0254,  2.1276],\n",
      "        [-0.7230, -0.7432, -0.5980,  1.5660, -1.0518]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6095],\n",
      "        [-1.5890],\n",
      "        [-4.2326],\n",
      "        [-0.5592]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9542,  2.0068,  0.9643,  1.1448, -0.2387],\n",
      "        [ 0.1156, -0.1238,  0.1153,  1.1507, -0.1878],\n",
      "        [-0.5056, -0.5965,  1.3347, -1.7228, -0.0138],\n",
      "        [ 0.7232,  1.1109, -0.8640, -0.7985,  0.2011]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4869, -0.3485, -1.0154, -1.4756, -0.6404],\n",
      "        [ 1.2287,  1.1842,  1.3552,  0.8150,  1.3840],\n",
      "        [ 1.6641,  1.6565,  1.3635,  1.2724,  1.9633],\n",
      "        [ 0.3339,  0.8746,  0.8704,  1.6595,  0.8778]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9542,  2.0068,  0.9643,  1.1448, -0.2387],\n",
      "        [ 0.1156, -0.1238,  0.1153,  1.1507, -0.1878],\n",
      "        [-0.5056, -0.5965,  1.3347, -1.7228, -0.0138],\n",
      "        [ 0.7232,  1.1109, -0.8640, -0.7985,  0.2011]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.7506],\n",
      "        [ 0.8296],\n",
      "        [-2.2287],\n",
      "        [-0.6874]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3607,  0.5355,  1.4933, -0.4039,  0.8474],\n",
      "        [-0.9499, -0.4614,  0.0643,  0.9224, -0.2520],\n",
      "        [-0.6079,  1.1790, -1.3139, -0.2491,  0.6447],\n",
      "        [ 0.0375, -0.3358,  1.1528, -1.0446, -0.4143]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2470,  1.2000,  1.0590,  0.7314,  0.9627],\n",
      "        [ 0.4154,  1.4023,  0.8562,  0.9036,  0.8936],\n",
      "        [ 2.0630,  2.3431,  1.9438,  2.0146,  3.1382],\n",
      "        [ 0.3356,  1.2838,  0.9603,  0.8626,  1.5927]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3607,  0.5355,  1.4933, -0.4039,  0.8474],\n",
      "        [-0.9499, -0.4614,  0.0643,  0.9224, -0.2520],\n",
      "        [-0.6079,  1.1790, -1.3139, -0.2491,  0.6447],\n",
      "        [ 0.0375, -0.3358,  1.1528, -1.0446, -0.4143]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.4413],\n",
      "        [-0.3783],\n",
      "        [ 0.4760],\n",
      "        [-0.8723]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2185, -0.3752,  0.1584, -0.4729,  1.2309],\n",
      "        [-1.4191,  0.7309, -0.4250, -1.3470,  0.1996],\n",
      "        [ 0.3094, -1.3576,  0.1297, -0.7409, -0.2924],\n",
      "        [-1.1769, -0.7783,  1.1977, -0.4201,  1.0403]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0190, -1.2542, -1.0597, -1.5839, -1.5072],\n",
      "        [ 0.2003,  0.5543,  0.3575,  1.0512,  0.9703],\n",
      "        [ 0.3372,  0.0969, -0.5187,  0.2228,  0.1204],\n",
      "        [ 1.0199,  1.1753,  1.0229,  0.7389,  1.1837]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2185, -0.3752,  0.1584, -0.4729,  1.2309],\n",
      "        [-1.4191,  0.7309, -0.4250, -1.3470,  0.1996],\n",
      "        [ 0.3094, -1.3576,  0.1297, -0.7409, -0.2924],\n",
      "        [-1.1769, -0.7783,  1.1977, -0.4201,  1.0403]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0262],\n",
      "        [-1.2534],\n",
      "        [-0.2947],\n",
      "        [ 0.0312]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9908, -0.6730,  0.7090, -0.8351, -0.2852],\n",
      "        [-0.7311, -0.5252,  0.6253,  1.2875,  0.2403],\n",
      "        [-0.5605, -1.5723,  0.9775,  0.5963,  0.5566],\n",
      "        [ 0.5171, -2.4141,  0.1743,  0.9543, -1.3473]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6407, -1.0435, -1.2363, -0.7807, -1.1637],\n",
      "        [ 1.2209,  1.7103,  1.0460,  1.3246,  1.2901],\n",
      "        [ 0.0133, -0.2507, -0.2475,  0.2463, -0.4739],\n",
      "        [ 0.7972,  1.1640,  1.2664,  1.1473,  1.2462]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9908, -0.6730,  0.7090, -0.8351, -0.2852],\n",
      "        [-0.7311, -0.5252,  0.6253,  1.2875,  0.2403],\n",
      "        [-0.5605, -1.5723,  0.9775,  0.5963,  0.5566],\n",
      "        [ 0.5171, -2.4141,  0.1743,  0.9543, -1.3473]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4445],\n",
      "        [ 0.8786],\n",
      "        [ 0.0280],\n",
      "        [-2.7611]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7479, -0.7374,  1.0913,  0.0855, -1.4031],\n",
      "        [ 0.1693,  0.4245, -0.1216,  0.0927,  1.1887],\n",
      "        [ 0.2228,  0.8175,  1.4236,  0.6296, -0.6771],\n",
      "        [ 0.1516, -2.0542,  0.1353,  0.8334,  0.5290]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5561, -0.9822, -1.2380, -0.8887, -2.7833],\n",
      "        [ 0.7553,  0.8213,  1.1637,  1.1039,  0.4486],\n",
      "        [ 0.1512, -0.1710,  0.1443, -0.2387,  0.0366],\n",
      "        [ 0.4790,  0.0156, -0.6548,  0.3734,  0.3023]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7479, -0.7374,  1.0913,  0.0855, -1.4031],\n",
      "        [ 0.1693,  0.4245, -0.1216,  0.0927,  1.1887],\n",
      "        [ 0.2228,  0.8175,  1.4236,  0.6296, -0.6771],\n",
      "        [ 0.1516, -2.0542,  0.1353,  0.8334,  0.5290]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.6184],\n",
      "        [ 0.9706],\n",
      "        [-0.0758],\n",
      "        [ 0.4231]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3260, -0.3721,  1.2705, -1.0670, -0.0935],\n",
      "        [ 1.1846, -0.6295, -0.4753, -0.4339, -0.0807],\n",
      "        [ 0.5730,  1.1602, -1.1672, -0.7037,  2.0439],\n",
      "        [-0.7528, -0.6103, -3.3651, -1.1577, -0.2158]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1105,  0.0952,  0.3683,  0.6424,  0.1347],\n",
      "        [ 0.1674,  0.7832,  0.9661,  0.5272,  1.0484],\n",
      "        [ 0.3143,  0.6021,  0.7638,  0.0680,  0.2066],\n",
      "        [-0.3057, -0.0847,  0.4582,  0.1691,  0.3682]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3260, -0.3721,  1.2705, -1.0670, -0.0935],\n",
      "        [ 1.1846, -0.6295, -0.4753, -0.4339, -0.0807],\n",
      "        [ 0.5730,  1.1602, -1.1672, -0.7037,  2.0439],\n",
      "        [-0.7528, -0.6103, -3.3651, -1.1577, -0.2158]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3016],\n",
      "        [-1.0672],\n",
      "        [ 0.3616],\n",
      "        [-1.5352]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2496, -0.1214,  0.9508,  1.3304, -0.0826],\n",
      "        [-0.6659,  1.3077, -0.5015,  0.7478,  0.9815],\n",
      "        [-2.0601, -0.0394,  0.0489,  1.7521,  0.5275],\n",
      "        [ 0.2284, -0.6880, -0.2760,  1.0260,  1.1157]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3167, -0.0607, -0.3027, -0.1339, -0.1514],\n",
      "        [ 1.0372,  1.6630,  1.2180,  1.3777,  1.4561],\n",
      "        [ 0.5045, -0.3239,  0.0549, -0.1103, -0.1991],\n",
      "        [ 1.1057,  0.4033,  0.5387, -0.3022,  1.0859]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2496, -0.1214,  0.9508,  1.3304, -0.0826],\n",
      "        [-0.6659,  1.3077, -0.5015,  0.7478,  0.9815],\n",
      "        [-2.0601, -0.0394,  0.0489,  1.7521,  0.5275],\n",
      "        [ 0.2284, -0.6880, -0.2760,  1.0260,  1.1157]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3669],\n",
      "        [ 3.3326],\n",
      "        [-1.3221],\n",
      "        [ 0.7278]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4777,  0.9952,  0.8979,  0.1597, -0.7416],\n",
      "        [-0.7925,  0.5033, -0.3892, -1.0155, -0.9006],\n",
      "        [-0.2149,  0.4180,  1.6849,  0.8816,  0.7243],\n",
      "        [ 0.0739,  1.7576, -0.6394,  1.0413, -1.0196]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0399, -0.1296,  0.1738,  0.0243,  0.0260],\n",
      "        [-0.9035, -0.6536,  0.0123, -1.0731, -1.2021],\n",
      "        [ 0.4493,  0.6125, -0.1780,  1.3735,  1.4596],\n",
      "        [-0.0411,  0.4355, -0.4878,  0.1780,  0.6681]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4777,  0.9952,  0.8979,  0.1597, -0.7416],\n",
      "        [-0.7925,  0.5033, -0.3892, -1.0155, -0.9006],\n",
      "        [-0.2149,  0.4180,  1.6849,  0.8816,  0.7243],\n",
      "        [ 0.0739,  1.7576, -0.6394,  1.0413, -1.0196]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0074],\n",
      "        [ 2.5548],\n",
      "        [ 2.1276],\n",
      "        [ 0.5785]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6098, -1.1923, -0.5396,  0.2305,  0.8466],\n",
      "        [-0.8519,  0.8262, -0.6820,  0.9050, -0.7838],\n",
      "        [-0.3502, -0.7614,  1.4775,  2.0575,  0.7316],\n",
      "        [-1.2589, -0.1881,  0.2015,  1.2586,  0.1427]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1235, -0.6507,  0.2566,  0.1561,  0.6268],\n",
      "        [-1.4475, -1.6548, -1.0895, -1.4835, -2.0103],\n",
      "        [-0.3940,  0.0103, -0.3429, -0.9558, -1.1853],\n",
      "        [ 0.0130,  0.3754,  0.4777,  0.3327,  0.4087]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6098, -1.1923, -0.5396,  0.2305,  0.8466],\n",
      "        [-0.8519,  0.8262, -0.6820,  0.9050, -0.7838],\n",
      "        [-0.3502, -0.7614,  1.4775,  2.0575,  0.7316],\n",
      "        [-1.2589, -0.1881,  0.2015,  1.2586,  0.1427]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2792],\n",
      "        [ 0.8421],\n",
      "        [-3.2102],\n",
      "        [ 0.4863]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3400, -0.2004,  0.6768, -1.9366, -0.2590],\n",
      "        [-1.2650,  1.8575, -0.6500,  0.3890, -1.4886],\n",
      "        [-0.3178,  0.5398, -0.4647, -0.1970, -1.3408],\n",
      "        [-1.1876, -1.5429,  0.0846, -0.0164, -0.5368]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0596, -0.8084, -1.0060, -1.0931, -0.9692],\n",
      "        [-1.1117, -1.4821, -1.3122, -1.1901, -3.0419],\n",
      "        [ 1.0163,  1.4237,  1.1076,  0.8444,  1.8511],\n",
      "        [-0.0114,  0.2556,  0.0042,  0.4672,  0.3698]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3400, -0.2004,  0.6768, -1.9366, -0.2590],\n",
      "        [-1.2650,  1.8575, -0.6500,  0.3890, -1.4886],\n",
      "        [-0.3178,  0.5398, -0.4647, -0.1970, -1.3408],\n",
      "        [-1.1876, -1.5429,  0.0846, -0.0164, -0.5368]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8693],\n",
      "        [ 3.5713],\n",
      "        [-2.7175],\n",
      "        [-0.5867]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4341, -0.7060, -0.1691, -1.5241,  0.6031],\n",
      "        [-0.0921,  0.5866,  0.0135,  1.0097,  1.2677],\n",
      "        [-0.2834, -0.9452, -1.5286, -0.6467, -0.4419],\n",
      "        [ 0.1266, -0.4088,  1.0546,  1.0892,  1.7758]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8970, -0.4646, -0.8265, -0.7305, -2.2301],\n",
      "        [-1.3891, -2.0697, -3.3880, -2.5072, -5.0547],\n",
      "        [ 1.9271,  1.8504,  1.4657,  1.4809,  2.0768],\n",
      "        [ 0.0157,  0.3320,  0.2955,  0.9153,  0.7150]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4341, -0.7060, -0.1691, -1.5241,  0.6031],\n",
      "        [-0.0921,  0.5866,  0.0135,  1.0097,  1.2677],\n",
      "        [-0.2834, -0.9452, -1.5286, -0.6467, -0.4419],\n",
      "        [ 0.1266, -0.4088,  1.0546,  1.0892,  1.7758]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[  0.6256],\n",
      "        [-10.0711],\n",
      "        [ -6.4110],\n",
      "        [  2.4444]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4328,  1.0372,  0.5726, -0.6503, -0.8068],\n",
      "        [-2.8535,  0.3183, -0.0018,  0.3203,  0.3679],\n",
      "        [ 0.2286,  1.9455,  0.6133,  1.6869,  0.5260],\n",
      "        [-0.5704,  1.3457,  0.1590, -0.1440,  1.3288]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4766, -1.6792, -1.1758, -1.5907, -2.1345],\n",
      "        [ 0.3432, -0.0236,  0.0924,  0.2211, -0.1866],\n",
      "        [-0.2807, -0.6215, -0.3536, -0.2025,  0.0787],\n",
      "        [-0.8938, -1.2599, -1.3329, -1.1243, -1.1917]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4328,  1.0372,  0.5726, -0.6503, -0.8068],\n",
      "        [-2.8535,  0.3183, -0.0018,  0.3203,  0.3679],\n",
      "        [ 0.2286,  1.9455,  0.6133,  1.6869,  0.5260],\n",
      "        [-0.5704,  1.3457,  0.1590, -0.1440,  1.3288]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3412],\n",
      "        [-0.9850],\n",
      "        [-1.7904],\n",
      "        [-2.8191]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3789,  0.3999, -0.6310, -0.9112, -0.2416],\n",
      "        [-0.8390, -0.4061,  0.2343,  0.1270,  0.1540],\n",
      "        [-0.0859, -2.3387, -0.6745,  0.9465,  0.8009],\n",
      "        [-1.9341,  1.5424,  1.6663,  0.5115, -0.2066]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4831, -1.0396, -1.6811, -0.8863, -1.3150],\n",
      "        [ 0.5193,  0.0390,  0.1613, -0.0807,  0.9177],\n",
      "        [ 0.5639,  0.7782,  0.8452,  1.7458,  0.8982],\n",
      "        [ 0.1459,  0.6243,  0.8486,  0.6385,  0.8784]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3789,  0.3999, -0.6310, -0.9112, -0.2416],\n",
      "        [-0.8390, -0.4061,  0.2343,  0.1270,  0.1540],\n",
      "        [-0.0859, -2.3387, -0.6745,  0.9465,  0.8009],\n",
      "        [-1.9341,  1.5424,  1.6663,  0.5115, -0.2066]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.9534],\n",
      "        [-0.2826],\n",
      "        [-0.0666],\n",
      "        [ 2.2398]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7982, -0.4665, -0.8427,  2.6126,  1.9133],\n",
      "        [ 0.0446,  0.0765,  0.9766, -0.1340,  1.5757],\n",
      "        [-1.4291,  0.1107,  1.1839, -0.4603,  0.2487],\n",
      "        [ 0.1676, -0.3796, -0.8929, -2.1393,  1.2411]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6126, -0.5460, -0.3666,  0.4489, -0.4121],\n",
      "        [ 0.0756, -0.1277,  0.1655, -0.7290,  0.3061],\n",
      "        [ 0.5974,  0.2925,  0.1831,  0.4947,  1.2472],\n",
      "        [-0.1292, -0.7865,  0.2280, -0.0987, -0.7451]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7982, -0.4665, -0.8427,  2.6126,  1.9133],\n",
      "        [ 0.0446,  0.0765,  0.9766, -0.1340,  1.5757],\n",
      "        [-1.4291,  0.1107,  1.1839, -0.4603,  0.2487],\n",
      "        [ 0.1676, -0.3796, -0.8929, -2.1393,  1.2411]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4590],\n",
      "        [ 0.7353],\n",
      "        [-0.5221],\n",
      "        [-0.6403]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5816, -1.9482, -1.5441, -0.1791, -0.2430],\n",
      "        [-1.8776, -0.8765,  0.7735, -1.9438,  0.0667],\n",
      "        [ 1.5062, -0.9419, -0.2242,  0.1620, -1.7096],\n",
      "        [-0.5086,  2.0097,  0.3212, -0.1495, -1.4643]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0809,  0.0132,  1.0826,  0.0214,  0.4879],\n",
      "        [ 0.2042, -0.7478, -0.9624, -0.0383, -0.1659],\n",
      "        [ 1.1193,  0.2277, -0.4435,  0.6313,  0.4197],\n",
      "        [-0.2669,  0.2169,  0.2540, -0.1389,  0.0774]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5816, -1.9482, -1.5441, -0.1791, -0.2430],\n",
      "        [-1.8776, -0.8765,  0.7735, -1.9438,  0.0667],\n",
      "        [ 1.5062, -0.9419, -0.2242,  0.1620, -1.7096],\n",
      "        [-0.5086,  2.0097,  0.3212, -0.1495, -1.4643]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7727],\n",
      "        [-0.4090],\n",
      "        [ 0.9555],\n",
      "        [ 0.5607]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7586,  0.0875,  0.3284, -0.8293,  0.0198],\n",
      "        [-0.6416, -0.7073,  0.4407,  1.8539,  0.7577],\n",
      "        [ 2.0203, -1.1742, -0.0352,  0.5667, -0.1090],\n",
      "        [ 1.1334,  0.2258, -1.1052,  0.4161,  0.1887]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8801,  0.6507,  0.9792,  0.7934,  1.5395],\n",
      "        [-0.0578, -0.0574, -0.0108, -0.0273, -0.9307],\n",
      "        [ 0.6561,  0.2332, -0.1352,  0.8562,  0.2335],\n",
      "        [-0.3168,  0.5089,  0.3233,  0.0254,  0.0682]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7586,  0.0875,  0.3284, -0.8293,  0.0198],\n",
      "        [-0.6416, -0.7073,  0.4407,  1.8539,  0.7577],\n",
      "        [ 2.0203, -1.1742, -0.0352,  0.5667, -0.1090],\n",
      "        [ 1.1334,  0.2258, -1.1052,  0.4161,  0.1887]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9166],\n",
      "        [-0.6828],\n",
      "        [ 1.5162],\n",
      "        [-0.5780]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9737, -2.4171,  1.5784, -0.3945,  0.9459],\n",
      "        [ 0.4172,  0.3830, -0.6261, -1.1415, -0.5057],\n",
      "        [-0.0832, -0.0691, -2.0316, -0.2629,  0.5016],\n",
      "        [-0.1566, -0.3167,  0.6139,  0.0985, -0.1514]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6423,  1.1110,  1.3969,  0.9959,  1.8146],\n",
      "        [ 0.1082, -0.9926, -0.0546, -0.6190,  0.1424],\n",
      "        [-0.4420, -0.0671, -0.6120, -0.3686, -0.7460],\n",
      "        [ 0.3811,  0.4295, -0.3273,  0.2428, -0.1488]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9737, -2.4171,  1.5784, -0.3945,  0.9459],\n",
      "        [ 0.4172,  0.3830, -0.6261, -1.1415, -0.5057],\n",
      "        [-0.0832, -0.0691, -2.0316, -0.2629,  0.5016],\n",
      "        [-0.1566, -0.3167,  0.6139,  0.0985, -0.1514]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2177],\n",
      "        [ 0.3337],\n",
      "        [ 1.0075],\n",
      "        [-0.3502]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6064, -0.2970,  0.5715,  0.4576, -0.2142],\n",
      "        [-1.5451,  0.0685,  0.4864, -1.0478,  1.0161],\n",
      "        [-1.0397,  0.3607, -0.0085, -0.0588, -0.2193],\n",
      "        [ 0.8682, -0.7335, -0.1125, -0.0099, -1.6360]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8084,  1.1475,  0.7537,  0.7789,  1.0596],\n",
      "        [ 0.0942, -0.2894, -0.4756,  0.4163, -0.7453],\n",
      "        [-0.7632, -0.6616, -0.5293, -0.6800, -1.1967],\n",
      "        [-0.3917, -0.3832, -0.0128, -0.1721,  0.3884]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6064, -0.2970,  0.5715,  0.4576, -0.2142],\n",
      "        [-1.5451,  0.0685,  0.4864, -1.0478,  1.0161],\n",
      "        [-1.0397,  0.3607, -0.0085, -0.0588, -0.2193],\n",
      "        [ 0.8682, -0.7335, -0.1125, -0.0099, -1.6360]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2708],\n",
      "        [-1.5902],\n",
      "        [ 0.8618],\n",
      "        [-0.6913]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9301,  1.9244,  2.2142,  1.3067, -0.1182],\n",
      "        [ 0.3579,  1.7422,  0.6238,  1.6053, -1.1741],\n",
      "        [ 1.7256, -0.8991, -0.4606, -0.0472, -1.4771],\n",
      "        [-0.0574, -0.6883,  1.2408,  1.5066,  0.5919]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9112,  0.6140,  0.7492,  0.3647,  1.7287],\n",
      "        [ 0.5206,  1.3911,  1.1068,  1.2564,  1.0874],\n",
      "        [-0.2087, -0.4146, -0.6150, -0.6682, -1.7851],\n",
      "        [ 0.3436, -0.0875, -0.5983, -0.8908, -0.2011]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9301,  1.9244,  2.2142,  1.3067, -0.1182],\n",
      "        [ 0.3579,  1.7422,  0.6238,  1.6053, -1.1741],\n",
      "        [ 1.7256, -0.8991, -0.4606, -0.0472, -1.4771],\n",
      "        [-0.0574, -0.6883,  1.2408,  1.5066,  0.5919]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.8715],\n",
      "        [ 4.0404],\n",
      "        [ 2.9642],\n",
      "        [-2.1630]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2877, -0.6075,  2.6287, -0.7887, -1.0251],\n",
      "        [-0.1430, -0.1588, -0.7137,  2.5644,  1.3263],\n",
      "        [-0.6028, -1.0210, -0.4661,  1.2205,  1.3849],\n",
      "        [-1.3990,  1.3434,  1.0590, -0.0512,  1.3774]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0949, -1.3965, -1.6422, -2.0234, -3.9041],\n",
      "        [-1.0907, -1.2187, -0.5823, -1.4490, -2.2130],\n",
      "        [-1.4962, -1.5018, -1.6611, -1.5479, -3.6924],\n",
      "        [ 0.7978,  0.7226,  0.6899,  0.0820,  1.0002]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2877, -0.6075,  2.6287, -0.7887, -1.0251],\n",
      "        [-0.1430, -0.1588, -0.7137,  2.5644,  1.3263],\n",
      "        [-0.6028, -1.0210, -0.4661,  1.2205,  1.3849],\n",
      "        [-1.3990,  1.3434,  1.0590, -0.0512,  1.3774]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8145],\n",
      "        [-5.8856],\n",
      "        [-3.7932],\n",
      "        [ 1.9588]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9910,  1.5201, -0.0547, -1.1825,  0.0860],\n",
      "        [ 0.0464,  2.4312,  1.0461,  0.5805, -0.2023],\n",
      "        [-1.4176,  0.4875, -0.0656, -1.5750,  1.5346],\n",
      "        [ 1.1361,  1.5137, -1.4513,  0.4221, -0.4635]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.4497, -2.1892, -2.1708, -1.6357, -3.9149],\n",
      "        [ 1.3944,  1.4496,  0.9704,  1.1655,  1.2780],\n",
      "        [-0.5478, -0.2175, -0.2571, -0.7293, -1.0410],\n",
      "        [ 0.1366,  0.3524, -0.1508,  0.4915,  0.0165]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9910,  1.5201, -0.0547, -1.1825,  0.0860],\n",
      "        [ 0.0464,  2.4312,  1.0461,  0.5805, -0.2023],\n",
      "        [-1.4176,  0.4875, -0.0656, -1.5750,  1.5346],\n",
      "        [ 1.1361,  1.5137, -1.4513,  0.4221, -0.4635]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.0482],\n",
      "        [ 5.0221],\n",
      "        [ 0.2386],\n",
      "        [ 1.1073]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0758, -1.6860,  0.4325,  1.2935,  0.5781],\n",
      "        [ 1.4483, -0.0007,  0.5688, -0.9355,  0.3539],\n",
      "        [-0.4149, -0.5071,  0.2913,  0.1437,  0.5904],\n",
      "        [-1.0197, -0.8398, -0.6455,  1.9173, -0.3844]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6411, -0.8046, -1.1910, -1.5254, -2.1999],\n",
      "        [-1.0163, -1.1674, -0.6005, -1.4189, -2.0090],\n",
      "        [-0.0886, -0.6448, -0.1062, -0.3911, -1.0153],\n",
      "        [-0.4361,  0.1652, -0.4778, -0.9401, -0.6891]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0758, -1.6860,  0.4325,  1.2935,  0.5781],\n",
      "        [ 1.4483, -0.0007,  0.5688, -0.9355,  0.3539],\n",
      "        [-0.4149, -0.5071,  0.2913,  0.1437,  0.5904],\n",
      "        [-1.0197, -0.8398, -0.6455,  1.9173, -0.3844]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7137],\n",
      "        [-1.1965],\n",
      "        [-0.3229],\n",
      "        [-0.9233]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.0190, -0.5812, -0.7011,  0.6267,  1.3448],\n",
      "        [ 1.7266,  0.9940,  0.8241,  0.4445,  1.3178],\n",
      "        [ 0.5635, -1.8596,  1.0034, -1.6373,  0.5699],\n",
      "        [ 0.3196,  0.7338, -0.5035,  1.2620,  0.6181]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0526, -1.1086, -0.0832, -0.9029, -1.8189],\n",
      "        [-0.2299, -0.9952,  0.3213, -1.5370, -1.2845],\n",
      "        [-0.5656, -0.9003, -1.1433, -1.1797, -1.4618],\n",
      "        [ 0.2372,  0.0320,  0.0101, -0.1811, -0.0060]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.0190, -0.5812, -0.7011,  0.6267,  1.3448],\n",
      "        [ 1.7266,  0.9940,  0.8241,  0.4445,  1.3178],\n",
      "        [ 0.5635, -1.8596,  1.0034, -1.6373,  0.5699],\n",
      "        [ 0.3196,  0.7338, -0.5035,  1.2620,  0.6181]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2031],\n",
      "        [-3.4973],\n",
      "        [ 1.3067],\n",
      "        [-0.1379]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0269,  0.0621,  0.6467,  0.8457,  0.5844],\n",
      "        [ 1.3586, -0.8443,  1.0896,  1.1227, -0.3346],\n",
      "        [ 1.2100, -0.5042, -0.0610,  1.4045, -0.5121],\n",
      "        [-0.8278, -1.0361, -0.7383,  0.4368, -1.1449]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6107,  0.5648,  0.3574,  0.1793,  0.2716],\n",
      "        [ 1.7132,  1.5910,  1.7861,  0.9058,  1.7646],\n",
      "        [-1.1584, -1.0619, -0.4622, -1.3724, -1.8248],\n",
      "        [ 0.0802, -0.2316, -0.1223,  0.1587, -0.4460]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0269,  0.0621,  0.6467,  0.8457,  0.5844],\n",
      "        [ 1.3586, -0.8443,  1.0896,  1.1227, -0.3346],\n",
      "        [ 1.2100, -0.5042, -0.0610,  1.4045, -0.5121],\n",
      "        [-0.8278, -1.0361, -0.7383,  0.4368, -1.1449]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2038],\n",
      "        [ 3.3568],\n",
      "        [-1.8312],\n",
      "        [ 0.8437]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0746,  0.9296,  2.3783,  0.2413, -0.7516],\n",
      "        [ 0.5230, -1.2709, -0.0933,  0.4727,  0.0870],\n",
      "        [-0.8124,  0.3486, -1.1175, -0.5932,  0.5386],\n",
      "        [-0.2292, -0.6586,  0.2240,  0.4713,  0.7440]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2492, -1.0391, -0.9026, -0.7890, -1.8285],\n",
      "        [-0.1720, -0.4320, -0.4652, -0.2484, -0.7547],\n",
      "        [-0.0589, -1.0757, -0.3310, -0.7622, -1.0040],\n",
      "        [-0.2172,  0.3695,  0.5165,  0.3498, -0.2644]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0746,  0.9296,  2.3783,  0.2413, -0.7516],\n",
      "        [ 0.5230, -1.2709, -0.0933,  0.4727,  0.0870],\n",
      "        [-0.8124,  0.3486, -1.1175, -0.5932,  0.5386],\n",
      "        [-0.2292, -0.6586,  0.2240,  0.4713,  0.7440]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1967],\n",
      "        [ 0.3193],\n",
      "        [-0.0459],\n",
      "        [-0.1097]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4113, -0.9890,  0.3348,  1.5135,  1.3139],\n",
      "        [-2.6966, -0.1487,  0.9440,  0.5550, -0.1375],\n",
      "        [-0.6764, -0.3388, -0.8925, -0.0840, -0.3469],\n",
      "        [-0.2480, -0.0970,  0.7605,  0.1209, -0.6529]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6223, -0.0014, -0.6594, -0.5880,  0.9474],\n",
      "        [-0.4279,  0.3408,  0.3617,  0.1631,  0.0524],\n",
      "        [-0.0140, -1.3179, -1.2850, -1.5741, -2.3562],\n",
      "        [ 0.4844, -0.7903, -0.3340, -0.5165,  0.4652]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4113, -0.9890,  0.3348,  1.5135,  1.3139],\n",
      "        [-2.6966, -0.1487,  0.9440,  0.5550, -0.1375],\n",
      "        [-0.6764, -0.3388, -0.8925, -0.0840, -0.3469],\n",
      "        [-0.2480, -0.0970,  0.7605,  0.1209, -0.6529]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1205],\n",
      "        [ 1.5280],\n",
      "        [ 2.5524],\n",
      "        [-0.6637]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8610,  0.3760, -0.8250,  0.1605,  1.6415],\n",
      "        [ 0.1023, -0.5488,  0.0443,  0.4531, -0.5444],\n",
      "        [ 0.3555, -1.3770, -0.0912,  0.3412, -1.0824],\n",
      "        [ 0.7064, -0.7878, -0.1003,  0.2287, -1.5615]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3500, -0.2928, -0.3988, -0.4891,  0.3293],\n",
      "        [-0.5360,  0.0268, -0.3897, -0.1706, -1.0365],\n",
      "        [ 0.0914, -0.5508, -0.0943,  0.2184, -0.1072],\n",
      "        [ 0.1191,  0.5416,  0.2352, -0.1311,  0.3127]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8610,  0.3760, -0.8250,  0.1605,  1.6415],\n",
      "        [ 0.1023, -0.5488,  0.0443,  0.4531, -0.5444],\n",
      "        [ 0.3555, -1.3770, -0.0912,  0.3412, -1.0824],\n",
      "        [ 0.7064, -0.7878, -0.1003,  0.2287, -1.5615]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3323],\n",
      "        [ 0.4002],\n",
      "        [ 0.9901],\n",
      "        [-0.8843]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0090,  1.2153,  1.0052,  0.1307, -1.3630],\n",
      "        [-1.0376,  1.6129, -0.0946,  0.2333,  1.3934],\n",
      "        [ 2.7553, -1.5366,  1.3613, -0.2353, -1.2167],\n",
      "        [-0.2140,  0.0985,  0.4723, -0.1560,  1.1087]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0998, -0.5516, -1.2403, -1.4576, -1.5484],\n",
      "        [-0.5004, -0.2602, -0.2259, -0.8540, -0.8763],\n",
      "        [-0.4838, -0.8435,  0.8741, -0.4117, -0.8822],\n",
      "        [ 0.2362,  1.2027, -0.2133,  0.1225,  0.6872]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0090,  1.2153,  1.0052,  0.1307, -1.3630],\n",
      "        [-1.0376,  1.6129, -0.0946,  0.2333,  1.3934],\n",
      "        [ 2.7553, -1.5366,  1.3613, -0.2353, -1.2167],\n",
      "        [-0.2140,  0.0985,  0.4723, -0.1560,  1.1087]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0019],\n",
      "        [-1.2994],\n",
      "        [ 2.3234],\n",
      "        [ 0.7100]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0569,  0.3652, -0.0850, -0.3607,  1.6534],\n",
      "        [ 1.0301,  0.9988,  1.3087, -1.3330,  0.7941],\n",
      "        [-0.3701,  1.7203, -0.6342, -0.6583,  0.5264],\n",
      "        [-1.3912, -0.1059,  1.4208, -0.0751, -0.1800]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1046, -1.1757, -1.2450, -1.0048, -1.7746],\n",
      "        [-0.0815, -0.0615,  0.3559, -0.0594, -0.0085],\n",
      "        [-0.9839, -0.1689, -1.7512, -1.3905, -2.7226],\n",
      "        [ 0.5672, -0.3595,  0.3431, -0.1511, -0.8226]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0569,  0.3652, -0.0850, -0.3607,  1.6534],\n",
      "        [ 1.0301,  0.9988,  1.3087, -1.3330,  0.7941],\n",
      "        [-0.3701,  1.7203, -0.6342, -0.6583,  0.5264],\n",
      "        [-1.3912, -0.1059,  1.4208, -0.0751, -0.1800]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9013],\n",
      "        [ 0.3928],\n",
      "        [ 0.6666],\n",
      "        [-0.1041]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8022, -0.1960,  1.4581, -0.7278,  0.8186],\n",
      "        [-1.5070,  1.2286,  1.0391, -0.6820, -0.2336],\n",
      "        [ 1.8450, -0.0598, -0.3157, -0.2844,  0.2874],\n",
      "        [ 0.8626, -0.2065,  0.1937, -0.9595, -0.5390]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0272,  0.0069,  1.1056, -1.0411,  1.0454],\n",
      "        [-0.2771,  0.4312,  0.3806, -0.2362, -0.4592],\n",
      "        [-0.7405, -0.7882, -0.8583, -1.5469, -2.5034],\n",
      "        [ 0.3598, -0.4074, -0.5165,  0.9510,  0.6563]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8022, -0.1960,  1.4581, -0.7278,  0.8186],\n",
      "        [-1.5070,  1.2286,  1.0391, -0.6820, -0.2336],\n",
      "        [ 1.8450, -0.0598, -0.3157, -0.2844,  0.2874],\n",
      "        [ 0.8626, -0.2065,  0.1937, -0.9595, -0.5390]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4001],\n",
      "        [ 1.6112],\n",
      "        [-1.3277],\n",
      "        [-0.9718]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4728,  0.2301,  0.4388, -1.0499, -0.2841],\n",
      "        [-1.8953,  1.8667,  2.1588,  1.8701,  1.7919],\n",
      "        [-0.3151,  2.7242,  1.7623,  0.6288,  0.4888],\n",
      "        [-0.6728, -1.5132, -0.1173, -0.5666,  0.5762]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3621, -1.2283, -0.5229, -0.7300, -1.3342],\n",
      "        [-0.5216, -0.8778, -0.0260, -0.9470, -1.7919],\n",
      "        [-0.2785, -0.5394,  0.0906, -0.5861, -1.4430],\n",
      "        [ 0.8790,  0.5396,  0.3184, -0.1438,  1.1797]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4728,  0.2301,  0.4388, -1.0499, -0.2841],\n",
      "        [-1.8953,  1.8667,  2.1588,  1.8701,  1.7919],\n",
      "        [-0.3151,  2.7242,  1.7623,  0.6288,  0.4888],\n",
      "        [-0.6728, -1.5132, -0.1173, -0.5666,  0.5762]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1001],\n",
      "        [-5.6881],\n",
      "        [-2.2958],\n",
      "        [-0.6841]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3839,  1.9338,  0.6236, -0.6476,  0.7166],\n",
      "        [-1.8816,  0.6320,  0.9384,  1.9921,  0.4251],\n",
      "        [-0.0258, -0.3436,  0.4039, -0.9927,  0.4487],\n",
      "        [-0.1935,  2.0510,  1.2938,  0.9840,  1.3920]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2578, -1.4214, -1.2928, -0.7007, -1.3419],\n",
      "        [ 1.5699,  1.9141,  1.7399,  1.1807,  2.3625],\n",
      "        [ 0.6727, -0.2426, -0.0306, -0.3983,  0.4555],\n",
      "        [ 0.8404,  0.2724,  0.5439,  0.2748,  0.4984]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3839,  1.9338,  0.6236, -0.6476,  0.7166],\n",
      "        [-1.8816,  0.6320,  0.9384,  1.9921,  0.4251],\n",
      "        [-0.0258, -0.3436,  0.4039, -0.9927,  0.4487],\n",
      "        [-0.1935,  2.0510,  1.2938,  0.9840,  1.3920]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.7057],\n",
      "        [ 3.2450],\n",
      "        [ 0.6534],\n",
      "        [ 2.0641]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2333, -0.8282,  0.6470, -0.3609,  2.0244],\n",
      "        [ 1.8586, -0.5061,  2.2113,  1.1107, -0.3872],\n",
      "        [ 0.7269, -0.8928,  1.1955,  0.5093,  1.5874],\n",
      "        [-1.5706,  1.4418,  1.8841, -0.1179,  0.6964]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.9287,  0.7139,  0.4537,  0.1311,  1.4053],\n",
      "        [ 0.8925,  1.0255,  1.0756,  1.2935,  1.3865],\n",
      "        [-0.3412, -0.9143, -0.9789, -0.8850, -1.2022],\n",
      "        [-0.2927, -0.9196, -0.5520, -0.3641, -1.1071]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2333, -0.8282,  0.6470, -0.3609,  2.0244],\n",
      "        [ 1.8586, -0.5061,  2.2113,  1.1107, -0.3872],\n",
      "        [ 0.7269, -0.8928,  1.1955,  0.5093,  1.5874],\n",
      "        [-1.5706,  1.4418,  1.8841, -0.1179,  0.6964]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2833],\n",
      "        [ 4.4182],\n",
      "        [-2.9612],\n",
      "        [-2.6344]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6012,  1.0960, -1.3650,  0.1382,  1.2222],\n",
      "        [-0.5474,  0.8489,  0.3219,  0.4619, -0.4917],\n",
      "        [-0.9787,  1.9291, -0.6643, -0.3357,  0.3014],\n",
      "        [-0.2929,  0.2757,  1.4990,  0.7738, -0.8095]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3333, -0.5839, -0.7234, -1.1997, -0.7559],\n",
      "        [-0.8323, -0.8410, -1.3265, -1.5733, -1.8970],\n",
      "        [ 0.8902,  0.7753,  0.5711,  0.6390,  1.1480],\n",
      "        [ 0.3599,  0.9951,  0.2969,  1.3396,  1.2207]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6012,  1.0960, -1.3650,  0.1382,  1.2222],\n",
      "        [-0.5474,  0.8489,  0.3219,  0.4619, -0.4917],\n",
      "        [-0.9787,  1.9291, -0.6643, -0.3357,  0.3014],\n",
      "        [-0.2929,  0.2757,  1.4990,  0.7738, -0.8095]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5418],\n",
      "        [-0.4792],\n",
      "        [ 0.3766],\n",
      "        [ 0.6625]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1453,  0.0062, -0.5713, -0.7304,  1.3101],\n",
      "        [ 1.1663,  0.7560,  0.0158, -1.3693, -0.1585],\n",
      "        [-0.3958, -1.5037,  1.7359, -0.0137, -0.1190],\n",
      "        [-1.1606,  0.2090,  0.5577,  1.9739, -0.7806]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0663, -0.4848, -0.2688, -0.9548, -0.5005],\n",
      "        [-0.3813, -1.1271, -1.5490, -1.2024, -1.6611],\n",
      "        [ 0.0851, -0.2499, -0.1665,  0.3265,  1.2294],\n",
      "        [ 1.0681,  0.3274, -0.6898,  0.3090,  0.3321]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1453,  0.0062, -0.5713, -0.7304,  1.3101],\n",
      "        [ 1.1663,  0.7560,  0.0158, -1.3693, -0.1585],\n",
      "        [-0.3958, -1.5037,  1.7359, -0.0137, -0.1190],\n",
      "        [-1.1606,  0.2090,  0.5577,  1.9739, -0.7806]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1163],\n",
      "        [ 0.5885],\n",
      "        [-0.0977],\n",
      "        [-1.2053]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4261, -0.3348, -0.0732,  0.9321, -0.3072],\n",
      "        [ 1.4075,  0.7341, -0.0409,  1.5288, -0.4634],\n",
      "        [-0.0230, -0.2564,  0.1162, -0.0598, -0.5865],\n",
      "        [-0.0636,  0.2095,  0.6576, -0.3439, -0.0896]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0849, -0.5367, -0.5111, -0.8100, -0.5922],\n",
      "        [-1.0494, -0.7087, -0.1636, -1.2382, -1.5578],\n",
      "        [ 0.5982, -0.0316, -0.4556, -0.5670,  0.1707],\n",
      "        [ 0.8898,  0.8931,  0.7456,  1.4982,  1.4180]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4261, -0.3348, -0.0732,  0.9321, -0.3072],\n",
      "        [ 1.4075,  0.7341, -0.0409,  1.5288, -0.4634],\n",
      "        [-0.0230, -0.2564,  0.1162, -0.0598, -0.5865],\n",
      "        [-0.0636,  0.2095,  0.6576, -0.3439, -0.0896]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4771],\n",
      "        [-3.1617],\n",
      "        [-0.1248],\n",
      "        [-0.0215]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3648,  0.6005,  0.1220, -0.8809, -1.0729],\n",
      "        [-1.1491, -0.3851, -0.1802, -1.2813,  2.7357],\n",
      "        [-0.1855, -0.0709, -1.2679,  0.3045,  0.4149],\n",
      "        [-0.8742,  0.2135, -0.2536, -0.9065,  1.0710]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1221, -0.2038, -0.4169,  0.0961, -0.6281],\n",
      "        [ 0.0621,  0.5704, -0.1559,  0.4827,  0.8571],\n",
      "        [ 0.7249, -0.9411, -0.5218, -0.5948, -0.2202],\n",
      "        [ 0.1751,  0.2825,  0.1056,  0.6477,  1.5288]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3648,  0.6005,  0.1220, -0.8809, -1.0729],\n",
      "        [-1.1491, -0.3851, -0.1802, -1.2813,  2.7357],\n",
      "        [-0.1855, -0.0709, -1.2679,  0.3045,  0.4149],\n",
      "        [-0.8742,  0.2135, -0.2536, -0.9065,  1.0710]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2493],\n",
      "        [ 1.4632],\n",
      "        [ 0.3213],\n",
      "        [ 0.9306]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8493,  0.9294, -0.2223, -0.8200, -1.1733],\n",
      "        [ 0.0272,  0.0793,  1.6485,  1.6012, -0.6856],\n",
      "        [-0.2908, -0.4041, -0.1460, -1.2261,  0.9534],\n",
      "        [-1.7728, -0.3101,  0.3140,  0.0185,  1.5159]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2810, -0.2101, -0.0310, -0.4608, -0.6061],\n",
      "        [-0.7286, -0.7780, -0.8998,  0.0236, -0.6053],\n",
      "        [ 0.3436, -0.4635,  0.0472, -0.3379, -1.1469],\n",
      "        [ 0.7393,  0.1603,  0.6304,  0.0639,  0.4833]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8493,  0.9294, -0.2223, -0.8200, -1.1733],\n",
      "        [ 0.0272,  0.0793,  1.6485,  1.6012, -0.6856],\n",
      "        [-0.2908, -0.4041, -0.1460, -1.2261,  0.9534],\n",
      "        [-1.7728, -0.3101,  0.3140,  0.0185,  1.5159]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1393],\n",
      "        [-1.1120],\n",
      "        [-0.5986],\n",
      "        [-0.4284]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1408,  0.3463,  0.9195,  0.3167, -1.0529],\n",
      "        [ 1.0900, -0.4907, -0.0975, -0.3430, -1.0390],\n",
      "        [-0.9918, -0.3181,  1.4668,  0.3341,  0.4090],\n",
      "        [-0.7675,  0.9365,  1.2695,  0.4039, -0.4418]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1625, -1.3418, -0.8210, -1.2715, -1.8224],\n",
      "        [-0.1289,  0.2002, -0.2093,  0.0492,  0.5530],\n",
      "        [ 0.3995, -0.5974, -0.9778, -0.7944, -1.1072],\n",
      "        [ 0.5010,  0.0670,  0.6030, -0.4379,  0.6247]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1408,  0.3463,  0.9195,  0.3167, -1.0529],\n",
      "        [ 1.0900, -0.4907, -0.0975, -0.3430, -1.0390],\n",
      "        [-0.9918, -0.3181,  1.4668,  0.3341,  0.4090],\n",
      "        [-0.7675,  0.9365,  1.2695,  0.4039, -0.4418]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0296],\n",
      "        [-0.8097],\n",
      "        [-2.3587],\n",
      "        [-0.0091]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2007, -0.0800,  0.5764, -0.7964, -0.2021],\n",
      "        [-0.0388,  0.7647, -0.5163,  2.2649,  0.2069],\n",
      "        [-0.9315,  1.0975,  1.8510, -0.2500,  2.4273],\n",
      "        [ 0.2082, -1.6796,  0.9266,  0.1298, -1.2771]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4215, -0.3406, -1.4143, -1.0919, -0.8976],\n",
      "        [ 0.1814, -0.0270, -0.4138, -0.3552,  0.9364],\n",
      "        [ 0.8274,  0.8633,  1.2244, -0.0895,  0.9448],\n",
      "        [ 0.5484,  1.1317,  0.3605,  0.4288,  0.2981]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2007, -0.0800,  0.5764, -0.7964, -0.2021],\n",
      "        [-0.0388,  0.7647, -0.5163,  2.2649,  0.2069],\n",
      "        [-0.9315,  1.0975,  1.8510, -0.2500,  2.4273],\n",
      "        [ 0.2082, -1.6796,  0.9266,  0.1298, -1.2771]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7691],\n",
      "        [-0.4248],\n",
      "        [ 4.7587],\n",
      "        [-1.7775]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3744, -1.2858, -0.1207, -1.4190,  1.5673],\n",
      "        [-0.0874, -1.2068,  0.5586, -0.8994, -0.5564],\n",
      "        [ 1.1467, -0.8231,  0.5646, -0.7113, -1.3086],\n",
      "        [-1.3037,  0.4027, -0.1111,  0.1567,  0.2826]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1581, -0.7573, -0.4560, -1.5963, -1.3940],\n",
      "        [ 0.0381, -0.4479, -0.4474, -0.3720, -0.4305],\n",
      "        [-0.7566, -1.0432, -1.6952, -1.0275, -2.6782],\n",
      "        [ 0.6604,  1.4657,  1.1309,  1.2940,  1.3875]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3744, -1.2858, -0.1207, -1.4190,  1.5673],\n",
      "        [-0.0874, -1.2068,  0.5586, -0.8994, -0.5564],\n",
      "        [ 1.1467, -0.8231,  0.5646, -0.7113, -1.3086],\n",
      "        [-1.3037,  0.4027, -0.1111,  0.1567,  0.2826]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8917],\n",
      "        [ 0.8614],\n",
      "        [ 3.2695],\n",
      "        [ 0.1986]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.9695, -0.5801, -0.0644,  1.1432, -0.0920],\n",
      "        [-0.0736, -0.0262, -1.2128,  1.3468,  1.7941],\n",
      "        [-0.1136,  1.2915,  1.6156, -0.7310,  0.5418],\n",
      "        [-0.3608,  0.7861, -0.6542, -0.7037,  1.0041]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5493, -0.4641, -1.4743, -0.2163, -1.6041],\n",
      "        [ 0.0167, -1.0136, -0.6851, -0.7567, -1.3351],\n",
      "        [-1.6458, -2.0699, -2.5811, -2.4450, -5.2927],\n",
      "        [ 0.8261,  0.8255,  0.7803,  1.3148,  1.5915]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.9695, -0.5801, -0.0644,  1.1432, -0.0920],\n",
      "        [-0.0736, -0.0262, -1.2128,  1.3468,  1.7941],\n",
      "        [-0.1136,  1.2915,  1.6156, -0.7310,  0.5418],\n",
      "        [-0.3608,  0.7861, -0.6542, -0.7037,  1.0041]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8957],\n",
      "        [-2.5582],\n",
      "        [-7.7364],\n",
      "        [ 0.5132]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1560, -0.2661, -1.5895, -0.1758,  1.8272],\n",
      "        [-0.1978,  0.4773, -0.2549, -0.4193, -0.7592],\n",
      "        [-0.7594, -0.1412, -0.9315, -0.6424, -1.8398],\n",
      "        [-0.5971,  1.2065, -0.1883,  0.6391,  1.0557]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8873, -1.1419, -1.6704, -1.2920, -2.3863],\n",
      "        [ 1.0299,  1.3736,  1.6214,  0.4635,  0.9828],\n",
      "        [ 0.1985,  0.0276, -0.5407,  0.1260,  0.3108],\n",
      "        [ 0.3205,  0.7922,  0.9637,  0.2803,  0.3838]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1560, -0.2661, -1.5895, -0.1758,  1.8272],\n",
      "        [-0.1978,  0.4773, -0.2549, -0.4193, -0.7592],\n",
      "        [-0.7594, -0.1412, -0.9315, -0.6424, -1.8398],\n",
      "        [-0.5971,  1.2065, -0.1883,  0.6391,  1.0557]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0358],\n",
      "        [-0.9019],\n",
      "        [-0.3038],\n",
      "        [ 1.1672]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8706, -1.8197, -0.5808,  0.4042, -0.9948],\n",
      "        [ 0.4496, -0.0174,  0.1972,  1.3347, -0.5328],\n",
      "        [ 0.9609, -1.7935,  1.2977, -0.3334, -1.3203],\n",
      "        [ 0.2478, -0.4473,  0.2739,  1.9507,  0.3741]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2840, -1.4000, -1.5467, -1.3702, -2.0125],\n",
      "        [ 0.8164,  0.8910,  0.9487,  0.7513,  1.5628],\n",
      "        [ 0.2259,  0.5046,  0.1059,  0.1211, -0.1212],\n",
      "        [ 0.0821,  0.3853,  1.1124,  0.4940,  0.5488]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8706, -1.8197, -0.5808,  0.4042, -0.9948],\n",
      "        [ 0.4496, -0.0174,  0.1972,  1.3347, -0.5328],\n",
      "        [ 0.9609, -1.7935,  1.2977, -0.3334, -1.3203],\n",
      "        [ 0.2478, -0.4473,  0.2739,  1.9507,  0.3741]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.1412],\n",
      "        [ 0.7088],\n",
      "        [-0.4308],\n",
      "        [ 1.3217]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0188,  0.3554, -1.0276, -0.1612,  0.4595],\n",
      "        [ 0.0052, -0.5799,  0.8149,  0.5552, -1.9779],\n",
      "        [-0.0383, -1.3664, -0.5373,  1.0776, -0.8203],\n",
      "        [ 1.2713, -0.9488,  0.9656,  1.5423,  0.2956]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4247, -0.1240,  0.1738, -0.0116, -0.6121],\n",
      "        [ 0.4495, -0.1277, -0.0005, -0.2010,  0.2934],\n",
      "        [ 0.3169,  0.3172, -0.1006, -0.6165,  0.2433],\n",
      "        [-0.5803,  0.0619,  0.7591, -0.2552, -0.4130]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0188,  0.3554, -1.0276, -0.1612,  0.4595],\n",
      "        [ 0.0052, -0.5799,  0.8149,  0.5552, -1.9779],\n",
      "        [-0.0383, -1.3664, -0.5373,  1.0776, -0.8203],\n",
      "        [ 1.2713, -0.9488,  0.9656,  1.5423,  0.2956]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5100],\n",
      "        [-0.6159],\n",
      "        [-1.2554],\n",
      "        [-0.5790]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1039, -1.5993,  1.1136,  0.6267, -1.0254],\n",
      "        [-1.4650, -1.0409,  1.6478, -2.4281, -0.0619],\n",
      "        [-0.6373,  2.1085,  0.9066,  1.2405,  0.1245],\n",
      "        [-1.2693, -0.4344, -0.2738,  1.9104, -0.1213]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1367, -0.6958, -0.5645, -0.2277, -0.3959],\n",
      "        [ 0.7289,  0.7708,  0.7549, -0.2061,  1.4071],\n",
      "        [ 0.5074,  0.7037, -0.3915,  0.0704,  1.4588],\n",
      "        [ 0.1772,  0.1589,  0.6148,  0.2534,  1.5027]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1039, -1.5993,  1.1136,  0.6267, -1.0254],\n",
      "        [-1.4650, -1.0409,  1.6478, -2.4281, -0.0619],\n",
      "        [-0.6373,  2.1085,  0.9066,  1.2405,  0.1245],\n",
      "        [-1.2693, -0.4344, -0.2738,  1.9104, -0.1213]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5964],\n",
      "        [-0.2129],\n",
      "        [ 1.0745],\n",
      "        [-0.1605]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3855,  0.6996, -1.2858,  1.8540, -0.2129],\n",
      "        [ 0.4475,  0.8939,  0.5378,  0.7364,  0.0197],\n",
      "        [-0.2439,  1.7069, -0.6405,  0.6785,  0.5732],\n",
      "        [ 0.0961,  2.0919,  0.6811,  1.9402, -1.1214]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1937, -0.3776, -0.3525,  0.0877,  0.2614],\n",
      "        [ 0.2375, -0.0711, -0.2886,  0.0029,  0.5788],\n",
      "        [-0.0131, -0.0706,  0.0819, -0.6608,  0.3650],\n",
      "        [ 0.3322,  1.0144,  0.5803,  0.3960,  1.1873]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3855,  0.6996, -1.2858,  1.8540, -0.2129],\n",
      "        [ 0.4475,  0.8939,  0.5378,  0.7364,  0.0197],\n",
      "        [-0.2439,  1.7069, -0.6405,  0.6785,  0.5732],\n",
      "        [ 0.0961,  2.0919,  0.6811,  1.9402, -1.1214]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5645],\n",
      "        [-0.0990],\n",
      "        [-0.4088],\n",
      "        [ 1.9860]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0552, -1.1819,  2.8399,  1.7106, -0.2916],\n",
      "        [-2.6034, -1.1626,  1.3004,  0.1581, -0.1104],\n",
      "        [ 0.8850,  0.1894,  0.2726,  1.2502,  0.2712],\n",
      "        [-1.0184, -0.1614,  0.5637,  0.3448,  0.1594]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2507,  0.5561,  0.3640,  0.3412,  0.2582],\n",
      "        [ 0.7368, -0.4522, -0.5760, -0.3717,  0.6987],\n",
      "        [ 0.3317, -0.0587,  0.0274, -0.1052, -0.0973],\n",
      "        [-0.5716, -0.4958, -0.5746,  0.2545, -1.0525]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0552, -1.1819,  2.8399,  1.7106, -0.2916],\n",
      "        [-2.6034, -1.1626,  1.3004,  0.1581, -0.1104],\n",
      "        [ 0.8850,  0.1894,  0.2726,  1.2502,  0.2712],\n",
      "        [-1.0184, -0.1614,  0.5637,  0.3448,  0.1594]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8710],\n",
      "        [-2.2774],\n",
      "        [ 0.1321],\n",
      "        [ 0.2581]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9988, -0.9538,  0.6354,  0.3924, -1.1383],\n",
      "        [-0.6973, -0.7527, -1.2336, -1.6143,  0.9474],\n",
      "        [-1.4140,  0.5642, -0.2777,  1.2414,  1.0682],\n",
      "        [ 1.4060,  2.4684,  0.8031,  0.6400, -0.8641]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2019,  0.0463, -0.2433, -0.2779, -1.1479],\n",
      "        [ 1.1041,  0.6495,  0.2450,  0.7268,  0.7884],\n",
      "        [-0.1859, -0.4601, -0.5929, -0.9837, -0.0887],\n",
      "        [-0.2580,  0.5451,  0.3542,  0.6907,  0.1780]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9988, -0.9538,  0.6354,  0.3924, -1.1383],\n",
      "        [-0.6973, -0.7527, -1.2336, -1.6143,  0.9474],\n",
      "        [-1.4140,  0.5642, -0.2777,  1.2414,  1.0682],\n",
      "        [ 1.4060,  2.4684,  0.8031,  0.6400, -0.8641]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2005],\n",
      "        [-1.9874],\n",
      "        [-1.1480],\n",
      "        [ 1.5555]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9188,  1.0296, -0.7604, -0.5976, -0.2313],\n",
      "        [ 0.6433,  0.6303,  1.2514, -0.5779,  0.3460],\n",
      "        [ 0.9161, -0.1949,  0.7307,  0.5274, -1.0950],\n",
      "        [-0.2696,  0.5958,  0.1883, -0.1530,  0.8719]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6762, -0.4098, -0.3994, -0.2100, -1.2741],\n",
      "        [ 1.3851,  1.6219,  1.7184,  0.7929,  2.3595],\n",
      "        [ 0.7159,  1.0352,  0.5820,  0.9410,  1.5310],\n",
      "        [-0.6210,  0.0023, -0.3441, -0.2069, -1.3149]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9188,  1.0296, -0.7604, -0.5976, -0.2313],\n",
      "        [ 0.6433,  0.6303,  1.2514, -0.5779,  0.3460],\n",
      "        [ 0.9161, -0.1949,  0.7307,  0.5274, -1.0950],\n",
      "        [-0.2696,  0.5958,  0.1883, -0.1530,  0.8719]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9232],\n",
      "        [ 4.4217],\n",
      "        [-0.3008],\n",
      "        [-1.0109]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8403,  0.4554,  0.4234, -1.9576, -0.4888],\n",
      "        [-0.3872,  0.0013,  1.3936, -0.7797, -0.6772],\n",
      "        [-0.3172,  1.2372,  0.3208,  0.1563, -1.4053],\n",
      "        [ 0.5555,  3.0447, -0.2363, -0.4279,  1.5291]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4566, -1.1101, -0.5413, -0.5401, -2.1172],\n",
      "        [-0.6978, -0.0981,  0.7845, -0.3518,  0.1192],\n",
      "        [ 0.1450,  0.4483,  0.7885,  0.1647,  0.8083],\n",
      "        [-0.1469,  0.3603,  0.3545, -0.2438,  1.1555]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8403,  0.4554,  0.4234, -1.9576, -0.4888],\n",
      "        [-0.3872,  0.0013,  1.3936, -0.7797, -0.6772],\n",
      "        [-0.3172,  1.2372,  0.3208,  0.1563, -1.4053],\n",
      "        [ 0.5555,  3.0447, -0.2363, -0.4279,  1.5291]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7412],\n",
      "        [ 1.5568],\n",
      "        [-0.3486],\n",
      "        [ 2.8030]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7792, -0.1781, -0.4200, -0.0375,  0.5174],\n",
      "        [-1.7542,  0.1844,  1.0543, -0.7437,  0.8413],\n",
      "        [-0.3296,  1.5702, -0.4579,  0.0918,  0.3314],\n",
      "        [ 1.2593,  1.2157,  1.3102, -0.5245,  0.5574]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2074, -1.1461, -1.1013, -1.8140, -2.9104],\n",
      "        [-0.5778, -0.4496,  0.0206, -0.5860, -1.0790],\n",
      "        [ 0.5682, -0.6985, -0.4046, -0.2998,  0.1405],\n",
      "        [-0.4794, -0.2296, -1.2413, -1.4294, -1.6259]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7792, -0.1781, -0.4200, -0.0375,  0.5174],\n",
      "        [-1.7542,  0.1844,  1.0543, -0.7437,  0.8413],\n",
      "        [-0.3296,  1.5702, -0.4579,  0.0918,  0.3314],\n",
      "        [ 1.2593,  1.2157,  1.3102, -0.5245,  0.5574]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7119],\n",
      "        [ 0.4805],\n",
      "        [-1.0798],\n",
      "        [-2.6656]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1686, -2.1941, -0.3749,  0.5392,  1.1948],\n",
      "        [ 1.3465,  0.4568,  0.4803,  0.1868, -1.1907],\n",
      "        [-0.0692, -1.3712,  0.4480, -0.8904, -1.2780],\n",
      "        [ 0.7049, -1.2723,  1.8896, -0.0519, -0.4662]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7803, -0.7566, -0.3238, -0.7036, -0.8803],\n",
      "        [ 0.0161, -1.1415, -0.2459, -0.6544, -2.1642],\n",
      "        [ 0.8431,  0.5030,  0.2522, -0.2865,  1.0477],\n",
      "        [ 0.2069,  1.1179, -0.1808,  1.2453,  0.1504]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1686, -2.1941, -0.3749,  0.5392,  1.1948],\n",
      "        [ 1.3465,  0.4568,  0.4803,  0.1868, -1.1907],\n",
      "        [-0.0692, -1.3712,  0.4480, -0.8904, -1.2780],\n",
      "        [ 0.7049, -1.2723,  1.8896, -0.0519, -0.4662]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5613],\n",
      "        [ 1.8368],\n",
      "        [-1.7191],\n",
      "        [-1.7528]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0709,  0.3116,  0.7069,  1.4835, -1.1709],\n",
      "        [-1.4186, -1.2538, -2.1847, -0.8876,  1.0715],\n",
      "        [-0.2934,  1.4031,  1.2618,  1.3622,  1.9012],\n",
      "        [-1.2019,  0.2799, -1.3618, -0.1156,  0.2124]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4156, -0.7513,  0.0069, -1.0909, -0.3520],\n",
      "        [-0.6018, -1.0436, -0.5414, -1.0997, -2.3795],\n",
      "        [ 1.2587,  1.1299,  0.6487,  0.7500,  1.8687],\n",
      "        [ 0.7914,  1.0003,  1.0554,  1.4544,  2.0016]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0709,  0.3116,  0.7069,  1.4835, -1.1709],\n",
      "        [-1.4186, -1.2538, -2.1847, -0.8876,  1.0715],\n",
      "        [-0.2934,  1.4031,  1.2618,  1.3622,  1.9012],\n",
      "        [-1.2019,  0.2799, -1.3618, -0.1156,  0.2124]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8805],\n",
      "        [ 1.7716],\n",
      "        [ 6.6090],\n",
      "        [-1.8515]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 3.2761,  0.7513,  0.6873,  0.6031,  0.2917],\n",
      "        [-1.7221,  0.8572, -0.9046, -1.5152,  0.1982],\n",
      "        [ 0.3130, -1.4808,  0.2623,  0.1536, -1.0823],\n",
      "        [ 0.9845, -0.9173,  2.5577,  1.3221,  0.0364]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3159, -0.1939, -0.0239, -0.2083,  1.0516],\n",
      "        [-1.1392, -1.4333, -1.6198, -2.0337, -3.7007],\n",
      "        [-1.0729, -1.8131, -1.8942, -1.3569, -3.4104],\n",
      "        [ 1.2227,  1.3935,  1.5641,  2.1758,  1.6317]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 3.2761,  0.7513,  0.6873,  0.6031,  0.2917],\n",
      "        [-1.7221,  0.8572, -0.9046, -1.5152,  0.1982],\n",
      "        [ 0.3130, -1.4808,  0.2623,  0.1536, -1.0823],\n",
      "        [ 0.9845, -0.9173,  2.5577,  1.3221,  0.0364]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0540],\n",
      "        [ 4.5462],\n",
      "        [ 5.3351],\n",
      "        [ 6.8620]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0275,  0.7155, -0.2596,  0.0358, -0.6241],\n",
      "        [-0.5478,  1.2193,  1.2296, -0.7883,  0.5072],\n",
      "        [ 0.1354,  0.0195,  1.3031, -0.4215,  0.2606],\n",
      "        [ 0.1667, -1.7340,  0.2656, -0.0113,  0.3579]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3655, -1.1517, -0.3200, -0.5664, -0.1142],\n",
      "        [ 0.0355, -0.6666,  0.0656,  0.1164,  0.1357],\n",
      "        [-2.0962, -3.1997, -4.0212, -3.5657, -6.9874],\n",
      "        [-1.0138, -0.3670, -0.5019, -1.6792, -2.8452]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0275,  0.7155, -0.2596,  0.0358, -0.6241],\n",
      "        [-0.5478,  1.2193,  1.2296, -0.7883,  0.5072],\n",
      "        [ 0.1354,  0.0195,  1.3031, -0.4215,  0.2606],\n",
      "        [ 0.1667, -1.7340,  0.2656, -0.0113,  0.3579]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7001],\n",
      "        [-0.7746],\n",
      "        [-5.9046],\n",
      "        [-0.6653]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2745,  0.4451, -0.9900,  1.1607,  0.2607],\n",
      "        [ 0.8294, -0.1034, -0.5881,  0.3560,  0.8161],\n",
      "        [-0.0419, -0.0240, -0.2803, -0.3030,  2.8252],\n",
      "        [-0.5598, -0.3989,  0.4171, -0.9298, -1.5103]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0963, -0.9050, -0.3737, -0.8237, -0.9598],\n",
      "        [ 0.9168,  0.0142, -0.5634, -0.0208,  0.1332],\n",
      "        [-0.0640,  0.3643,  0.4299, -0.0671,  0.4631],\n",
      "        [-0.6673, -0.2626, -0.8908, -1.2213, -2.1534]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2745,  0.4451, -0.9900,  1.1607,  0.2607],\n",
      "        [ 0.8294, -0.1034, -0.5881,  0.3560,  0.8161],\n",
      "        [-0.0419, -0.0240, -0.2803, -0.3030,  2.8252],\n",
      "        [-0.5598, -0.3989,  0.4171, -0.9298, -1.5103]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1164],\n",
      "        [ 1.1916],\n",
      "        [ 1.2022],\n",
      "        [ 4.4946]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5678, -0.8669, -0.3461,  0.6633,  1.8399],\n",
      "        [ 2.1301,  0.0897,  0.7888,  1.3160,  0.4808],\n",
      "        [-0.7134,  0.4129,  1.7148,  0.7354,  0.6129],\n",
      "        [-0.1627,  0.0385,  0.4197,  2.4603, -0.1094]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2174, -0.8399, -0.8318, -0.7594, -0.4552],\n",
      "        [-0.1268, -1.0512,  0.1911, -0.2224, -0.7098],\n",
      "        [-0.9370, -0.0203, -0.0737, -0.2260, -0.3447],\n",
      "        [-1.9229, -2.6796, -2.0406, -2.7563, -5.0721]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5678, -0.8669, -0.3461,  0.6633,  1.8399],\n",
      "        [ 2.1301,  0.0897,  0.7888,  1.3160,  0.4808],\n",
      "        [-0.7134,  0.4129,  1.7148,  0.7354,  0.6129],\n",
      "        [-0.1627,  0.0385,  0.4197,  2.4603, -0.1094]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2018],\n",
      "        [-0.8477],\n",
      "        [ 0.1562],\n",
      "        [-6.8731]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2154,  1.4313,  1.0269,  1.8799,  0.4350],\n",
      "        [ 0.1224,  0.2425,  1.0340, -0.2261, -0.1587],\n",
      "        [-0.3672,  0.3807,  1.5453, -0.6146,  1.7624],\n",
      "        [ 0.4941,  0.3298,  0.8056,  0.7630, -2.2769]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1105, -0.9074, -0.8051, -0.1174, -1.1201],\n",
      "        [ 0.2693,  0.2514,  0.0875, -0.0651, -0.8961],\n",
      "        [-0.7865,  0.3543,  0.6896, -0.1211, -0.0292],\n",
      "        [ 0.3905, -0.1833, -0.2820, -1.0722, -0.0485]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2154,  1.4313,  1.0269,  1.8799,  0.4350],\n",
      "        [ 0.1224,  0.2425,  1.0340, -0.2261, -0.1587],\n",
      "        [-0.3672,  0.3807,  1.5453, -0.6146,  1.7624],\n",
      "        [ 0.4941,  0.3298,  0.8056,  0.7630, -2.2769]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8098],\n",
      "        [ 0.3413],\n",
      "        [ 1.5124],\n",
      "        [-0.8023]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0430, -0.7483, -0.8518,  0.8046,  1.5596],\n",
      "        [ 0.7481,  0.9179,  1.4626, -1.1184, -0.4403],\n",
      "        [-0.6596, -2.6689,  2.2655, -0.2081,  0.1363],\n",
      "        [ 0.3135, -0.4958, -0.2930,  0.9236,  0.7209]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8341,  0.8248,  0.6720,  0.5325,  1.0460],\n",
      "        [ 0.6082, -0.2254, -0.5160, -0.7683, -0.1984],\n",
      "        [-0.4548, -0.4674, -0.8215, -0.5804, -1.8785],\n",
      "        [ 0.1339, -0.0155, -0.1217,  0.3340,  0.0860]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0430, -0.7483, -0.8518,  0.8046,  1.5596],\n",
      "        [ 0.7481,  0.9179,  1.4626, -1.1184, -0.4403],\n",
      "        [-0.6596, -2.6689,  2.2655, -0.2081,  0.1363],\n",
      "        [ 0.3135, -0.4958, -0.2930,  0.9236,  0.7209]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9061],\n",
      "        [ 0.4400],\n",
      "        [-0.4488],\n",
      "        [ 0.4558]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6108, -0.3720, -0.1169,  1.9794,  0.2122],\n",
      "        [-0.1180,  0.8610,  0.8429,  0.7349,  0.7167],\n",
      "        [ 0.4745,  0.1275,  0.0926, -0.3464, -0.2740],\n",
      "        [-0.8014, -0.5971, -0.4326,  0.9073, -0.9792]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0623, -0.7283, -0.0167, -0.5727, -0.2504],\n",
      "        [-0.0787,  0.1851,  0.0337, -0.1274,  0.1302],\n",
      "        [-0.6185, -0.3772, -0.2758,  0.0962,  0.1596],\n",
      "        [ 0.3662,  0.0949,  0.0304, -0.0673,  0.0857]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6108, -0.3720, -0.1169,  1.9794,  0.2122],\n",
      "        [-0.1180,  0.8610,  0.8429,  0.7349,  0.7167],\n",
      "        [ 0.4745,  0.1275,  0.0926, -0.3464, -0.2740],\n",
      "        [-0.8014, -0.5971, -0.4326,  0.9073, -0.9792]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9519],\n",
      "        [ 0.1967],\n",
      "        [-0.4441],\n",
      "        [-0.5083]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5440,  0.2819,  1.2680,  0.9125, -0.3557],\n",
      "        [ 0.9221,  0.4225, -0.8816,  0.8276, -0.4391],\n",
      "        [-0.8090, -0.6526, -2.2176,  0.5942,  1.5920],\n",
      "        [-1.3622, -0.6924,  0.8819,  0.2898, -0.8577]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8074, -1.0037, -0.0271, -0.7600,  0.3222],\n",
      "        [-0.3026,  0.4311, -0.7323,  0.5820,  0.1774],\n",
      "        [-0.0125, -0.0522, -0.4947, -0.1645,  0.3321],\n",
      "        [ 0.2680, -0.3151, -0.4140,  0.0664, -1.5902]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5440,  0.2819,  1.2680,  0.9125, -0.3557],\n",
      "        [ 0.9221,  0.4225, -0.8816,  0.8276, -0.4391],\n",
      "        [-0.8090, -0.6526, -2.2176,  0.5942,  1.5920],\n",
      "        [-1.3622, -0.6924,  0.8819,  0.2898, -0.8577]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6862],\n",
      "        [ 0.9524],\n",
      "        [ 1.5723],\n",
      "        [ 0.8710]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2465, -0.7329,  1.1816,  1.1454, -0.0802],\n",
      "        [ 0.0708,  1.0500, -1.0185,  0.7530,  0.2167],\n",
      "        [-0.9179,  1.2803,  0.8551, -0.3182,  0.9363],\n",
      "        [ 0.3373,  0.0603,  0.3825,  1.4461,  0.8248]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6249, -0.5264, -0.3121, -0.0322,  0.1670],\n",
      "        [-0.2465, -0.5904,  0.3273, -0.4012, -0.7368],\n",
      "        [-0.3922, -0.5760, -0.3759, -0.2085, -1.4684],\n",
      "        [-0.4203, -0.5987, -0.3661, -0.2071, -1.1510]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2465, -0.7329,  1.1816,  1.1454, -0.0802],\n",
      "        [ 0.0708,  1.0500, -1.0185,  0.7530,  0.2167],\n",
      "        [-0.9179,  1.2803,  0.8551, -0.3182,  0.9363],\n",
      "        [ 0.3373,  0.0603,  0.3825,  1.4461,  0.8248]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1208],\n",
      "        [-1.4325],\n",
      "        [-2.0074],\n",
      "        [-1.5667]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0576,  2.0729,  0.8415, -1.2124,  1.0335],\n",
      "        [ 0.6937, -0.0338, -0.4534, -0.4605, -0.3059],\n",
      "        [-0.9846,  0.0150, -2.4782,  1.5159, -0.3120],\n",
      "        [ 1.5820,  0.6800, -1.6154, -0.9408, -1.0332]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4265, -0.3035, -0.4950, -1.1918, -1.0983],\n",
      "        [ 0.1808,  1.1235, -0.0418,  0.2461,  1.1428],\n",
      "        [-0.2526, -0.1327,  0.4584,  0.3516,  0.3407],\n",
      "        [ 0.9062, -0.1151,  0.3900,  0.7485,  1.3252]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0576,  2.0729,  0.8415, -1.2124,  1.0335],\n",
      "        [ 0.6937, -0.0338, -0.4534, -0.4605, -0.3059],\n",
      "        [-0.9846,  0.0150, -2.4782,  1.5159, -0.3120],\n",
      "        [ 1.5820,  0.6800, -1.6154, -0.9408, -1.0332]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7112],\n",
      "        [-0.3565],\n",
      "        [-0.4627],\n",
      "        [-1.3480]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0577, -0.1256,  0.7471,  1.7257,  1.1354],\n",
      "        [-0.6160,  0.3482,  0.1835, -0.5543,  0.5694],\n",
      "        [-1.5410,  0.9330,  0.5402,  0.6776,  0.7548],\n",
      "        [ 0.3993,  0.0450,  1.5444,  0.3582,  0.9271]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6408, -0.2878, -0.7729, -1.3169, -0.5211],\n",
      "        [ 0.0238, -0.1453,  0.0557,  0.2067,  0.2213],\n",
      "        [ 0.3240,  0.1403,  0.2073, -0.1461,  0.7666],\n",
      "        [ 0.7027,  1.2581,  0.7082,  0.5707,  1.3050]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0577, -0.1256,  0.7471,  1.7257,  1.1354],\n",
      "        [-0.6160,  0.3482,  0.1835, -0.5543,  0.5694],\n",
      "        [-1.5410,  0.9330,  0.5402,  0.6776,  0.7548],\n",
      "        [ 0.3993,  0.0450,  1.5444,  0.3582,  0.9271]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.4424],\n",
      "        [-0.0436],\n",
      "        [ 0.2233],\n",
      "        [ 2.8453]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4428, -0.3616, -0.7593, -1.3837, -0.4139],\n",
      "        [ 0.2085, -0.1836, -2.0010,  1.4638,  1.8516],\n",
      "        [ 0.2524, -0.2210, -0.0064,  0.5735, -0.2138],\n",
      "        [-0.7315, -0.3432,  0.3472, -0.8708,  0.6570]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0988,  1.1313,  1.3569,  0.8687,  1.0713],\n",
      "        [ 0.5475,  0.3476,  0.4776,  0.0149,  0.6278],\n",
      "        [ 0.2421, -0.8361, -0.1979, -0.9198, -1.0158],\n",
      "        [-0.5710, -1.0930, -0.9339, -1.0361, -1.3867]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4428, -0.3616, -0.7593, -1.3837, -0.4139],\n",
      "        [ 0.2085, -0.1836, -2.0010,  1.4638,  1.8516],\n",
      "        [ 0.2524, -0.2210, -0.0064,  0.5735, -0.2138],\n",
      "        [-0.7315, -0.3432,  0.3472, -0.8708,  0.6570]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.6701],\n",
      "        [ 0.2789],\n",
      "        [-0.0632],\n",
      "        [ 0.4596]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7040,  1.2102, -0.2209,  0.9130,  0.3205],\n",
      "        [ 0.7873,  0.4362,  0.2421,  0.1769,  2.2841],\n",
      "        [-0.0266,  0.4434,  1.9339, -0.6876, -0.2837],\n",
      "        [-1.0009,  0.6891,  0.5167,  0.5647,  0.5834]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.2151,  2.6229,  1.7136,  2.6360,  3.6238],\n",
      "        [ 0.1884, -0.0422, -0.1269, -0.2864, -0.2000],\n",
      "        [ 0.1260, -0.5030,  0.0253, -0.4192, -0.2535],\n",
      "        [-0.4687, -0.7775,  0.0205, -1.2062, -0.6247]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7040,  1.2102, -0.2209,  0.9130,  0.3205],\n",
      "        [ 0.7873,  0.4362,  0.2421,  0.1769,  2.2841],\n",
      "        [-0.0266,  0.4434,  1.9339, -0.6876, -0.2837],\n",
      "        [-1.0009,  0.6891,  0.5167,  0.5647,  0.5834]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5894],\n",
      "        [-0.4084],\n",
      "        [ 0.1827],\n",
      "        [-1.1017]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[ 0.6423,  1.6218, -0.0113, -0.2559, -1.1623],\n",
      "        [ 0.0373, -0.2678,  0.9537, -1.6105,  0.2437],\n",
      "        [ 0.9849, -1.1916, -0.6872, -0.8964, -0.3134],\n",
      "        [-1.1821,  1.1274,  1.1110,  0.5302, -1.3681]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1148,  0.0864,  0.6094,  0.3344,  0.3783],\n",
      "        [ 0.0040,  0.2166, -0.5349,  0.0536, -0.1095],\n",
      "        [-0.6476, -1.1820, -0.6895, -0.3808, -0.2885],\n",
      "        [ 0.0747,  0.0701, -0.1083,  0.5456,  0.6540]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6423,  1.6218, -0.0113, -0.2559, -1.1623],\n",
      "        [ 0.0373, -0.2678,  0.9537, -1.6105,  0.2437],\n",
      "        [ 0.9849, -1.1916, -0.6872, -0.8964, -0.3134],\n",
      "        [-1.1821,  1.1274,  1.1110,  0.5302, -1.3681]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3183],\n",
      "        [-0.6810],\n",
      "        [ 1.6763],\n",
      "        [-0.7350]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5428,  1.9372,  0.4989,  0.9496, -1.0573],\n",
      "        [ 0.7096, -0.6059,  1.7351, -0.6211, -0.5498],\n",
      "        [ 1.0540, -0.4844, -0.2402,  1.2765, -0.6382],\n",
      "        [ 1.2740, -2.2466,  0.6664, -0.1311,  0.6784]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0371,  0.1654, -0.8479,  0.1542, -0.2652],\n",
      "        [-0.0167,  0.8472, -0.1341,  0.3401, -0.1606],\n",
      "        [-0.3517, -0.7238, -0.8198, -2.1398, -1.6964],\n",
      "        [ 0.4207,  0.4812, -0.1870,  0.2469,  0.1194]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5428,  1.9372,  0.4989,  0.9496, -1.0573],\n",
      "        [ 0.7096, -0.6059,  1.7351, -0.6211, -0.5498],\n",
      "        [ 1.0540, -0.4844, -0.2402,  1.2765, -0.6382],\n",
      "        [ 1.2740, -2.2466,  0.6664, -0.1311,  0.6784]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3444],\n",
      "        [-0.8808],\n",
      "        [-1.4718],\n",
      "        [-0.6211]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2045, -0.5823,  0.2196,  1.7876,  1.3599],\n",
      "        [-1.3811, -1.1321,  0.3163,  2.1865, -1.1441],\n",
      "        [-0.0091, -0.1272, -0.0230,  1.1750,  3.3723],\n",
      "        [-0.2689,  0.2514, -0.2732, -0.3363,  0.1748]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3496,  0.3575, -0.4359, -1.0277, -0.2573],\n",
      "        [ 0.9464,  0.2000,  0.4683,  0.5859,  1.0116],\n",
      "        [-0.3654, -1.1167, -0.6243, -0.9461, -1.3726],\n",
      "        [ 0.7922, -0.2947,  0.3637, -0.1148,  0.0492]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2045, -0.5823,  0.2196,  1.7876,  1.3599],\n",
      "        [-1.3811, -1.1321,  0.3163,  2.1865, -1.1441],\n",
      "        [-0.0091, -0.1272, -0.0230,  1.1750,  3.3723],\n",
      "        [-0.2689,  0.2514, -0.2732, -0.3363,  0.1748]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4194],\n",
      "        [-1.2617],\n",
      "        [-5.5806],\n",
      "        [-0.3392]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2392,  1.3969, -0.1502,  0.2091, -1.2581],\n",
      "        [-0.2937, -0.3585, -0.1433, -2.1877, -0.0109],\n",
      "        [ 0.0941, -0.8590,  3.2127, -0.8753, -0.2423],\n",
      "        [-0.5449, -1.9926,  0.0938,  0.4695,  0.2252]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0434,  1.1519,  0.6874,  1.2396,  1.7712],\n",
      "        [ 1.1455,  0.2116,  1.0517,  0.8371,  0.7000],\n",
      "        [ 1.4126,  1.6215,  1.8196,  1.3393,  2.8514],\n",
      "        [ 0.3243, -0.1069, -0.4436, -0.6553,  0.0799]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2392,  1.3969, -0.1502,  0.2091, -1.2581],\n",
      "        [-0.2937, -0.3585, -0.1433, -2.1877, -0.0109],\n",
      "        [ 0.0941, -0.8590,  3.2127, -0.8753, -0.2423],\n",
      "        [-0.5449, -1.9926,  0.0938,  0.4695,  0.2252]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7561],\n",
      "        [-2.4020],\n",
      "        [ 2.7228],\n",
      "        [-0.2949]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0975,  1.2481,  0.3841,  1.6910,  0.0075],\n",
      "        [-0.8317, -0.4875, -1.1544, -0.0187,  0.1258],\n",
      "        [-2.0068, -0.6774,  1.0227, -0.4603, -0.2835],\n",
      "        [ 1.0999,  1.7502, -0.6026,  0.5277, -0.4769]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2834,  1.5723,  1.8250,  1.3405,  2.7793],\n",
      "        [ 1.4869,  1.5192,  2.0598,  1.5296,  1.9445],\n",
      "        [ 0.7555,  1.4509,  0.5905,  1.1791,  1.8376],\n",
      "        [ 0.4051, -0.0844, -0.1687, -0.4004, -0.5720]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0975,  1.2481,  0.3841,  1.6910,  0.0075],\n",
      "        [-0.8317, -0.4875, -1.1544, -0.0187,  0.1258],\n",
      "        [-2.0068, -0.6774,  1.0227, -0.4603, -0.2835],\n",
      "        [ 1.0999,  1.7502, -0.6026,  0.5277, -0.4769]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.8259],\n",
      "        [-4.1391],\n",
      "        [-2.9585],\n",
      "        [ 0.4609]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0352,  0.4045,  1.9481,  0.3392,  0.7863],\n",
      "        [ 1.1259, -0.0707,  0.8092,  1.6115, -1.9895],\n",
      "        [ 0.6811,  1.1591, -0.6399,  1.5145,  2.6604],\n",
      "        [ 0.0571,  0.7502, -0.1321,  1.1832,  1.4996]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1036, -0.0940, -0.3400, -0.0516, -1.1226],\n",
      "        [ 0.4191, -0.6640,  0.3683,  0.3938,  0.0649],\n",
      "        [ 1.6036,  2.0787,  1.5218,  1.7420,  2.4209],\n",
      "        [ 0.1062, -0.2531, -0.2653, -0.2118, -0.1636]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0352,  0.4045,  1.9481,  0.3392,  0.7863],\n",
      "        [ 1.1259, -0.0707,  0.8092,  1.6115, -1.9895],\n",
      "        [ 0.6811,  1.1591, -0.6399,  1.5145,  2.6604],\n",
      "        [ 0.0571,  0.7502, -0.1321,  1.1832,  1.4996]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ -1.3897],\n",
      "        [  1.3224],\n",
      "        [ 11.6065],\n",
      "        [ -0.6448]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5939,  0.2037, -0.4396,  0.7517, -1.6734],\n",
      "        [-0.0463,  1.2496,  1.2109,  1.2492, -0.4249],\n",
      "        [ 0.4408,  1.0689,  0.5446,  0.5418,  0.6143],\n",
      "        [ 0.7095,  2.0003,  0.5641,  1.1088, -0.8586]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2719,  1.2249,  0.4704,  0.7068,  1.9219],\n",
      "        [ 0.0751, -0.5599, -1.0773, -0.7527, -0.5351],\n",
      "        [-1.7725, -1.6959, -1.5613, -2.6026, -4.4859],\n",
      "        [ 0.4979, -0.7965, -0.1162, -0.6792, -0.3585]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5939,  0.2037, -0.4396,  0.7517, -1.6734],\n",
      "        [-0.0463,  1.2496,  1.2109,  1.2492, -0.4249],\n",
      "        [ 0.4408,  1.0689,  0.5446,  0.5418,  0.6143],\n",
      "        [ 0.7095,  2.0003,  0.5641,  1.1088, -0.8586]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8036],\n",
      "        [-2.7205],\n",
      "        [-7.6103],\n",
      "        [-1.7507]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0859, -0.4653, -0.0262,  2.0042, -0.1391],\n",
      "        [-0.3599, -0.1540, -1.7275, -0.1534,  0.0417],\n",
      "        [-1.3018,  1.0628,  0.6493, -0.0871,  0.7403],\n",
      "        [ 0.4709,  1.5962,  2.0549,  1.3932, -1.3552]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1068,  1.8801,  1.2001,  0.9265,  2.0398],\n",
      "        [ 0.8287,  1.3264,  1.1448,  0.2906,  1.6183],\n",
      "        [ 1.0128,  1.1074,  0.4404,  0.6775,  0.9512],\n",
      "        [ 0.5972,  0.6486,  0.3185,  0.4619,  1.0904]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0859, -0.4653, -0.0262,  2.0042, -0.1391],\n",
      "        [-0.3599, -0.1540, -1.7275, -0.1534,  0.0417],\n",
      "        [-1.3018,  1.0628,  0.6493, -0.0871,  0.7403],\n",
      "        [ 0.4709,  1.5962,  2.0549,  1.3932, -1.3552]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6417],\n",
      "        [-2.4574],\n",
      "        [ 0.7896],\n",
      "        [ 1.1368]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3267, -1.4605,  1.9316,  1.1432,  0.6233],\n",
      "        [-0.4232,  1.7439, -0.0403, -0.0876,  0.8328],\n",
      "        [-0.5349, -0.5937,  0.7698,  0.9159,  1.2989],\n",
      "        [-0.7454, -0.4034,  1.1332, -0.1343,  1.9322]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2988,  0.1243, -0.0511, -0.1739, -0.2926],\n",
      "        [ 1.3954,  2.3134,  1.7159,  1.2026,  2.8756],\n",
      "        [ 0.0252,  0.1766,  0.5054,  0.6925,  0.6755],\n",
      "        [-0.3363,  0.2994,  0.0219,  0.1387, -0.3343]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3267, -1.4605,  1.9316,  1.1432,  0.6233],\n",
      "        [-0.4232,  1.7439, -0.0403, -0.0876,  0.8328],\n",
      "        [-0.5349, -0.5937,  0.7698,  0.9159,  1.2989],\n",
      "        [-0.7454, -0.4034,  1.1332, -0.1343,  1.9322]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7591],\n",
      "        [ 5.6639],\n",
      "        [ 1.7825],\n",
      "        [-0.5098]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4662, -0.3739, -0.5291,  1.3874,  0.8062],\n",
      "        [-0.5185,  0.5262, -1.4715, -0.6610, -0.4217],\n",
      "        [-0.2055, -0.4706, -0.3514, -0.0062,  1.4825],\n",
      "        [ 0.2724,  0.7222,  0.7966,  0.3247,  1.7718]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3668,  0.1939,  0.0585, -0.2164,  0.2945],\n",
      "        [-0.6035, -0.7790, -0.3076, -0.8487, -1.2988],\n",
      "        [-0.6936, -0.1440, -0.5571, -0.4313, -1.1110],\n",
      "        [ 0.4585, -0.3348, -0.3340, -0.0568, -1.0318]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4662, -0.3739, -0.5291,  1.3874,  0.8062],\n",
      "        [-0.5185,  0.5262, -1.4715, -0.6610, -0.4217],\n",
      "        [-0.2055, -0.4706, -0.3514, -0.0062,  1.4825],\n",
      "        [ 0.2724,  0.7222,  0.7966,  0.3247,  1.7718]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0047],\n",
      "        [ 1.4644],\n",
      "        [-1.2382],\n",
      "        [-2.2295]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6072,  0.1000, -0.1383,  0.6303,  0.4207],\n",
      "        [-0.8495,  0.6568, -0.4191, -0.4389,  1.2527],\n",
      "        [ 0.3232,  2.8823,  0.3816,  1.1422,  1.3209],\n",
      "        [ 0.7858,  0.1228, -1.5635, -0.4797, -0.1387]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4399, -0.1896, -0.3435, -0.1742, -0.2575],\n",
      "        [-1.2597, -1.0814, -1.7745, -0.6504, -2.5730],\n",
      "        [ 0.0826,  0.9528,  0.5130,  0.2658,  0.4231],\n",
      "        [ 0.5150,  0.8995,  1.2148,  1.3038,  1.3977]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6072,  0.1000, -0.1383,  0.6303,  0.4207],\n",
      "        [-0.8495,  0.6568, -0.4191, -0.4389,  1.2527],\n",
      "        [ 0.3232,  2.8823,  0.3816,  1.1422,  1.3209],\n",
      "        [ 0.7858,  0.1228, -1.5635, -0.4797, -0.1387]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0775],\n",
      "        [-1.8341],\n",
      "        [ 3.8311],\n",
      "        [-2.2036]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6978, -0.0725, -0.4376, -1.2020, -0.5543],\n",
      "        [-0.7505, -0.9038, -2.2898, -0.6415,  1.0495],\n",
      "        [-0.4815, -0.6865,  1.0734,  0.8568,  1.4080],\n",
      "        [ 1.0031,  1.1074,  0.5009,  0.7810,  0.7218]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2901, -0.5898, -0.1729, -0.0383, -0.3367],\n",
      "        [-0.3134, -0.5891,  0.1829,  0.1459, -0.6785],\n",
      "        [-1.0620, -0.7205, -1.3544, -1.4741, -3.0842],\n",
      "        [ 1.1843,  1.9416,  1.7915,  1.3477,  2.4545]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6978, -0.0725, -0.4376, -1.2020, -0.5543],\n",
      "        [-0.7505, -0.9038, -2.2898, -0.6415,  1.0495],\n",
      "        [-0.4815, -0.6865,  1.0734,  0.8568,  1.4080],\n",
      "        [ 1.0031,  1.1074,  0.5009,  0.7810,  0.7218]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1486],\n",
      "        [-0.4567],\n",
      "        [-6.0534],\n",
      "        [ 7.0600]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3801, -0.2668,  2.1198, -0.1100,  0.5555],\n",
      "        [ 0.9872, -0.7808, -0.1120, -0.1018,  0.0070],\n",
      "        [-0.5778, -0.1969, -0.9843, -0.3050, -1.0051],\n",
      "        [ 3.0287, -0.5189,  0.0990,  0.4647,  1.1018]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0649,  0.1517, -0.0384,  0.3053, -0.0971],\n",
      "        [ 0.0096,  0.2382,  0.0165, -0.0430, -0.0583],\n",
      "        [ 0.8794,  1.4429,  0.0537,  1.0467,  1.0779],\n",
      "        [-0.5731, -0.8750, -1.0597, -1.0455, -1.9799]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3801, -0.2668,  2.1198, -0.1100,  0.5555],\n",
      "        [ 0.9872, -0.7808, -0.1120, -0.1018,  0.0070],\n",
      "        [-0.5778, -0.1969, -0.9843, -0.3050, -1.0051],\n",
      "        [ 3.0287, -0.5189,  0.0990,  0.4647,  1.1018]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1847],\n",
      "        [-0.1743],\n",
      "        [-2.2477],\n",
      "        [-4.0540]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0234,  0.3392,  3.2149,  0.7091,  1.7787],\n",
      "        [-0.3210,  0.4464,  0.8688,  0.9362,  0.8871],\n",
      "        [ 0.4280, -1.2203, -0.3976,  1.2977, -1.3222],\n",
      "        [-1.1356,  1.1272, -0.5062,  1.9034,  0.2572]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2315,  0.5863,  0.3257, -0.2959, -0.2401],\n",
      "        [ 0.1760,  0.1673, -0.6467,  0.0265,  0.0520],\n",
      "        [ 1.3486,  1.8454,  1.8689,  1.5571,  1.8960],\n",
      "        [ 1.0932,  0.3288,  0.8928,  1.6601,  1.6039]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0234,  0.3392,  3.2149,  0.7091,  1.7787],\n",
      "        [-0.3210,  0.4464,  0.8688,  0.9362,  0.8871],\n",
      "        [ 0.4280, -1.2203, -0.3976,  1.2977, -1.3222],\n",
      "        [-1.1356,  1.1272, -0.5062,  1.9034,  0.2572]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6145],\n",
      "        [-0.4727],\n",
      "        [-2.9039],\n",
      "        [ 2.2496]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2947,  1.6856,  0.1749,  1.7073, -0.9370],\n",
      "        [ 0.2245,  0.6237,  0.7447,  1.4052, -0.1671],\n",
      "        [ 1.2085, -0.8500, -0.5997, -0.3461, -0.8556],\n",
      "        [-1.5840, -0.5339,  0.2116, -0.2195, -1.1224]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2701, -0.9543, -0.4751, -0.2671,  0.4756],\n",
      "        [ 0.6140,  0.1565,  0.1278, -0.4988, -0.7547],\n",
      "        [-0.0163,  0.7057,  0.0007, -0.5361, -0.3544],\n",
      "        [-0.1721, -0.0186, -0.0393, -0.3798, -1.1537]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2947,  1.6856,  0.1749,  1.7073, -0.9370],\n",
      "        [ 0.2245,  0.6237,  0.7447,  1.4052, -0.1671],\n",
      "        [ 1.2085, -0.8500, -0.5997, -0.3461, -0.8556],\n",
      "        [-1.5840, -0.5339,  0.2116, -0.2195, -1.1224]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5137],\n",
      "        [-0.2441],\n",
      "        [-0.1312],\n",
      "        [ 1.6524]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4267,  0.0093, -0.5012,  1.0194, -1.0218],\n",
      "        [-0.4718,  1.1611,  0.4290,  0.0053,  1.3504],\n",
      "        [-0.6715, -1.1194, -0.6542, -0.2035,  0.1354],\n",
      "        [-0.0626,  1.4963,  0.4023,  0.2807, -0.3796]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0563,  1.7796,  1.1907,  1.7294,  1.9489],\n",
      "        [-0.1383, -0.4653, -0.5697, -0.3184, -0.6797],\n",
      "        [ 0.0690,  0.0745,  0.4385, -0.3369,  0.0528],\n",
      "        [ 0.1959, -0.2875, -0.7388, -0.9103, -1.1314]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4267,  0.0093, -0.5012,  1.0194, -1.0218],\n",
      "        [-0.4718,  1.1611,  0.4290,  0.0053,  1.3504],\n",
      "        [-0.6715, -1.1194, -0.6542, -0.2035,  0.1354],\n",
      "        [-0.0626,  1.4963,  0.4023,  0.2807, -0.3796]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3580],\n",
      "        [-1.6390],\n",
      "        [-0.3408],\n",
      "        [-0.5657]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8984, -0.5941, -0.2784, -0.4647,  1.6916],\n",
      "        [-0.2421, -1.9071, -0.6297,  0.9251,  1.8572],\n",
      "        [ 0.6322, -1.4822,  0.1408,  1.4672, -0.3417],\n",
      "        [-0.2807, -0.0559,  0.5607,  0.3341,  0.7826]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9893,  2.0276,  1.6579,  1.2243,  2.0062],\n",
      "        [ 0.8699,  0.3694,  0.4806,  0.3390,  1.2464],\n",
      "        [ 0.6229,  0.3396,  0.0415,  0.2964, -0.3879],\n",
      "        [-0.3405,  0.4018,  0.0330,  0.3389,  0.1653]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8984, -0.5941, -0.2784, -0.4647,  1.6916],\n",
      "        [-0.2421, -1.9071, -0.6297,  0.9251,  1.8572],\n",
      "        [ 0.6322, -1.4822,  0.1408,  1.4672, -0.3417],\n",
      "        [-0.2807, -0.0559,  0.5607,  0.3341,  0.7826]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0474],\n",
      "        [ 1.4106],\n",
      "        [ 0.4637],\n",
      "        [ 0.3341]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7691,  0.5792, -0.6813, -1.4886,  0.2390],\n",
      "        [ 0.0873,  0.5169, -0.5765,  0.2283,  0.7552],\n",
      "        [-0.3990,  0.3529, -0.4047, -0.6714,  1.4577],\n",
      "        [ 0.0051,  1.8435,  0.8213, -0.1595,  0.5857]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0713,  0.7906,  0.6394,  0.8502,  0.4316],\n",
      "        [ 0.1231, -0.4111,  0.0079, -0.3729, -0.7818],\n",
      "        [-0.0673,  0.4686,  0.4815, -0.0825, -0.3020],\n",
      "        [ 0.3996,  0.8403,  0.2079,  0.4586,  0.8486]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7691,  0.5792, -0.6813, -1.4886,  0.2390],\n",
      "        [ 0.0873,  0.5169, -0.5765,  0.2283,  0.7552],\n",
      "        [-0.3990,  0.3529, -0.4047, -0.6714,  1.4577],\n",
      "        [ 0.0051,  1.8435,  0.8213, -0.1595,  0.5857]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2662],\n",
      "        [-0.8818],\n",
      "        [-0.3876],\n",
      "        [ 2.1458]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1440,  1.7729,  2.2898, -0.5177, -0.0821],\n",
      "        [-0.3261,  0.2963,  0.9343, -0.5561, -1.1765],\n",
      "        [-0.8046, -1.2510,  0.2913,  0.0440, -1.4817],\n",
      "        [-1.4884,  0.1929, -0.9741,  0.3624, -0.8082]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6416,  0.9296,  1.0182,  1.3758,  0.9939],\n",
      "        [-0.0103, -0.2369, -0.2499, -0.4841, -0.2892],\n",
      "        [ 0.3492,  0.2688,  0.1973,  0.5104, -0.2107],\n",
      "        [-0.2684, -0.0462, -0.6303, -0.5587, -1.4167]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1440,  1.7729,  2.2898, -0.5177, -0.0821],\n",
      "        [-0.3261,  0.2963,  0.9343, -0.5561, -1.1765],\n",
      "        [-0.8046, -1.2510,  0.2913,  0.0440, -1.4817],\n",
      "        [-1.4884,  0.1929, -0.9741,  0.3624, -0.8082]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4516],\n",
      "        [ 0.3091],\n",
      "        [-0.2251],\n",
      "        [ 1.9469]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7055,  0.2471,  0.3346, -0.6301, -1.1087],\n",
      "        [-0.4405,  0.3857,  0.0717, -0.4573,  1.2489],\n",
      "        [-1.1524,  0.0200,  0.7891,  0.3836, -0.8545],\n",
      "        [-0.0219, -2.4395,  2.7421, -1.6514,  1.0439]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1680, -0.4572,  0.1431,  0.1101, -0.1732],\n",
      "        [ 0.3544, -0.1426,  0.0768, -0.2434, -0.0503],\n",
      "        [ 0.2879, -0.2699, -0.4040, -0.6891, -0.2973],\n",
      "        [-1.1378, -1.2494, -1.2721, -1.1262, -2.5880]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7055,  0.2471,  0.3346, -0.6301, -1.1087],\n",
      "        [-0.4405,  0.3857,  0.0717, -0.4573,  1.2489],\n",
      "        [-1.1524,  0.0200,  0.7891,  0.3836, -0.8545],\n",
      "        [-0.0219, -2.4395,  2.7421, -1.6514,  1.0439]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0609],\n",
      "        [-0.1571],\n",
      "        [-0.6662],\n",
      "        [-1.2573]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3756, -0.6044,  2.0168, -0.6207,  0.6511],\n",
      "        [-0.7872,  0.2924,  0.5350, -0.5398,  0.9681],\n",
      "        [ 0.7967,  0.9065, -0.5024,  0.8865,  0.0404],\n",
      "        [-0.0038,  0.3315, -0.2088,  0.5017, -0.6719]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2353,  0.9734,  0.8460,  1.3682,  1.3743],\n",
      "        [ 0.1416,  0.5019, -0.7194,  0.2362, -0.2816],\n",
      "        [ 0.1057,  0.1529, -0.3204, -0.2192, -0.0795],\n",
      "        [-0.8310, -0.3390, -0.4492, -0.1937, -0.6332]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3756, -0.6044,  2.0168, -0.6207,  0.6511],\n",
      "        [-0.7872,  0.2924,  0.5350, -0.5398,  0.9681],\n",
      "        [ 0.7967,  0.9065, -0.5024,  0.8865,  0.0404],\n",
      "        [-0.0038,  0.3315, -0.2088,  0.5017, -0.6719]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2520],\n",
      "        [-0.7497],\n",
      "        [ 0.1862],\n",
      "        [ 0.3128]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3506,  1.3171,  1.5624,  0.4709,  0.6806],\n",
      "        [-0.0200,  1.2069, -0.1934,  1.2118,  2.3887],\n",
      "        [-1.2659,  0.5361,  1.1648, -0.0214, -0.6260],\n",
      "        [ 0.1648,  0.1896, -0.7496,  0.0800,  0.4441]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0194,  0.6789, -0.3054,  0.4318, -0.2047],\n",
      "        [ 0.5820,  0.4369,  0.7740,  0.1172, -0.0190],\n",
      "        [ 0.3480,  0.4345, -0.1856, -0.0828, -0.4079],\n",
      "        [-0.1002, -0.7157,  0.2086, -0.4726, -1.0025]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3506,  1.3171,  1.5624,  0.4709,  0.6806],\n",
      "        [-0.0200,  1.2069, -0.1934,  1.2118,  2.3887],\n",
      "        [-1.2659,  0.5361,  1.1648, -0.0214, -0.6260],\n",
      "        [ 0.1648,  0.1896, -0.7496,  0.0800,  0.4441]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5073],\n",
      "        [ 0.4625],\n",
      "        [-0.1666],\n",
      "        [-0.7916]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7471,  0.1231,  0.4207,  0.3106,  0.0277],\n",
      "        [ 0.4003,  0.1456, -1.0040,  0.2411, -0.6086],\n",
      "        [ 1.0244,  0.8882, -0.5200,  1.0951, -0.5516],\n",
      "        [-0.3295, -0.1919,  0.2649, -0.0248,  0.7617]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1876,  0.5791,  0.5127,  0.4476,  0.2085],\n",
      "        [-0.0709, -0.2408, -0.0974,  0.4152, -0.4088],\n",
      "        [ 0.6010,  0.3280, -0.6310,  0.3076,  0.0380],\n",
      "        [ 0.1383, -0.2160,  0.1256, -1.4114, -0.2871]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7471,  0.1231,  0.4207,  0.3106,  0.0277],\n",
      "        [ 0.4003,  0.1456, -1.0040,  0.2411, -0.6086],\n",
      "        [ 1.0244,  0.8882, -0.5200,  1.0951, -0.5516],\n",
      "        [-0.3295, -0.1919,  0.2649, -0.0248,  0.7617]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7596],\n",
      "        [ 0.3832],\n",
      "        [ 1.5511],\n",
      "        [-0.1545]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2099,  0.6564,  0.3198, -0.5003,  0.7310],\n",
      "        [ 0.9752,  0.2802,  0.4731,  0.8539, -0.7174],\n",
      "        [-1.5726,  0.3678,  0.6571, -2.4627, -0.3902],\n",
      "        [ 1.1628, -0.2282,  0.0883, -2.2211,  0.8618]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2551,  0.3034,  0.3078, -0.0266,  0.5744],\n",
      "        [ 0.2032, -0.5889, -0.3150, -0.1017,  0.5527],\n",
      "        [-0.2618, -0.5612, -0.6322, -1.0309, -1.2728],\n",
      "        [-0.1453, -0.3062, -0.1858, -0.7816, -0.2953]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2099,  0.6564,  0.3198, -0.5003,  0.7310],\n",
      "        [ 0.9752,  0.2802,  0.4731,  0.8539, -0.7174],\n",
      "        [-1.5726,  0.3678,  0.6571, -2.4627, -0.3902],\n",
      "        [ 1.1628, -0.2282,  0.0883, -2.2211,  0.8618]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4221],\n",
      "        [-0.5992],\n",
      "        [ 2.8252],\n",
      "        [ 1.3660]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1701,  3.0008, -0.5849,  1.3283,  0.5326],\n",
      "        [-2.3686, -1.8418,  1.8585, -0.9970,  0.8807],\n",
      "        [-0.6892, -0.5360,  1.0608,  1.5706, -0.4815],\n",
      "        [-1.0302, -0.1898,  1.7413, -0.4304,  0.5125]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2439,  0.4869,  1.2496,  0.9242,  0.9221],\n",
      "        [-0.2024, -0.0484, -0.3630, -0.5194, -0.6531],\n",
      "        [-0.8707, -1.5815, -0.9233, -2.0346, -2.7736],\n",
      "        [-0.9793, -1.1004, -0.9871, -0.4299, -0.7060]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1701,  3.0008, -0.5849,  1.3283,  0.5326],\n",
      "        [-2.3686, -1.8418,  1.8585, -0.9970,  0.8807],\n",
      "        [-0.6892, -0.5360,  1.0608,  1.5706, -0.4815],\n",
      "        [-1.0302, -0.1898,  1.7413, -0.4304,  0.5125]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4073],\n",
      "        [-0.1636],\n",
      "        [-1.3916],\n",
      "        [-0.6778]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9092, -0.6344,  0.2414,  1.6778,  0.6854],\n",
      "        [-0.3940,  0.5529, -0.8199,  1.9097, -0.3291],\n",
      "        [ 1.2953, -2.6810, -0.5770,  1.5876,  0.5779],\n",
      "        [ 0.7152, -1.2539,  1.2458, -0.2914, -0.9478]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7063,  0.3713,  0.0159, -0.4081, -1.8692],\n",
      "        [ 0.1348, -0.0136, -0.0156,  0.2646, -0.2463],\n",
      "        [-0.9212, -0.8597, -0.9265, -0.4058, -1.4752],\n",
      "        [-0.4551, -0.2108, -0.6521, -0.2748, -0.7343]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9092, -0.6344,  0.2414,  1.6778,  0.6854],\n",
      "        [-0.3940,  0.5529, -0.8199,  1.9097, -0.3291],\n",
      "        [ 1.2953, -2.6810, -0.5770,  1.5876,  0.5779],\n",
      "        [ 0.7152, -1.2539,  1.2458, -0.2914, -0.9478]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5553],\n",
      "        [ 0.5385],\n",
      "        [ 0.1495],\n",
      "        [-0.0975]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9728, -0.6085,  0.0764,  0.6092, -0.3703],\n",
      "        [ 1.6640, -0.5800, -1.3428,  0.1678,  0.0125],\n",
      "        [-1.4742, -1.0925, -0.9388, -0.7578, -0.9554],\n",
      "        [-1.9840, -1.9257,  1.3001, -1.5015,  0.5526]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3262,  0.7924,  0.8963,  0.7672,  0.0541],\n",
      "        [-0.2618, -0.4137,  0.1747,  0.0316,  0.0173],\n",
      "        [-1.0404, -1.5499, -1.5212, -0.8229, -1.6755],\n",
      "        [ 0.5741, -0.5899,  0.2183, -0.4315, -1.2383]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9728, -0.6085,  0.0764,  0.6092, -0.3703],\n",
      "        [ 1.6640, -0.5800, -1.3428,  0.1678,  0.0125],\n",
      "        [-1.4742, -1.0925, -0.9388, -0.7578, -0.9554],\n",
      "        [-1.9840, -1.9257,  1.3001, -1.5015,  0.5526]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6100],\n",
      "        [-0.4248],\n",
      "        [ 6.8797],\n",
      "        [ 0.2441]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3143,  1.3564,  0.9926,  0.3390, -0.2637],\n",
      "        [-0.4399,  0.1615, -0.2272, -0.1818,  0.8337],\n",
      "        [-0.0678,  0.7867, -0.1101, -1.0508,  0.3125],\n",
      "        [-0.2674, -0.5057,  0.5602, -0.5743,  0.6635]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5362,  0.9424,  1.2252,  1.3103,  1.0935],\n",
      "        [ 0.4509,  0.2338, -0.4508,  0.1620, -0.3144],\n",
      "        [-0.0891,  0.1571, -0.3254, -0.4450,  0.2575],\n",
      "        [ 0.1624,  0.0938, -0.0614, -0.3997, -0.0422]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3143,  1.3564,  0.9926,  0.3390, -0.2637],\n",
      "        [-0.4399,  0.1615, -0.2272, -0.1818,  0.8337],\n",
      "        [-0.0678,  0.7867, -0.1101, -1.0508,  0.3125],\n",
      "        [-0.2674, -0.5057,  0.5602, -0.5743,  0.6635]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.8187],\n",
      "        [-0.3498],\n",
      "        [ 0.7136],\n",
      "        [ 0.0763]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1146,  1.3699, -0.7661, -2.2317, -0.7170],\n",
      "        [ 0.9216, -0.3235,  0.6756,  0.6529,  0.5314],\n",
      "        [-0.2027, -0.8810, -1.2457,  2.0532,  0.3942],\n",
      "        [ 0.8171,  1.6437, -0.9448, -1.0382,  0.9505]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7221, -0.3887, -0.1757, -0.8475, -0.7684],\n",
      "        [ 0.1757, -0.1174,  0.6306, -0.6385,  0.0611],\n",
      "        [-0.1941, -0.2838,  0.0175, -0.2246, -0.2017],\n",
      "        [-0.1730,  0.2154,  0.4953, -0.0921, -0.0134]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1146,  1.3699, -0.7661, -2.2317, -0.7170],\n",
      "        [ 0.9216, -0.3235,  0.6756,  0.6529,  0.5314],\n",
      "        [-0.2027, -0.8810, -1.2457,  2.0532,  0.3942],\n",
      "        [ 0.8171,  1.6437, -0.9448, -1.0382,  0.9505]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1272],\n",
      "        [ 0.2415],\n",
      "        [-0.2731],\n",
      "        [-0.1723]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0063,  1.1646,  0.2675,  1.1259,  0.7605],\n",
      "        [-0.4079, -0.8045,  0.2126,  0.0337,  0.3865],\n",
      "        [ 0.5586, -0.5684,  0.3291,  1.9978, -0.1830],\n",
      "        [ 0.2089, -0.0419,  2.5424,  0.6785,  0.3811]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.5960, -1.2564, -1.1376, -2.1041, -2.6404],\n",
      "        [ 0.4606, -0.2723,  0.1110,  0.2331, -0.1591],\n",
      "        [ 0.2357, -0.3184,  0.1313, -0.2998,  0.1909],\n",
      "        [-0.0234, -0.6798, -0.4163, -0.6467, -0.6668]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0063,  1.1646,  0.2675,  1.1259,  0.7605],\n",
      "        [-0.4079, -0.8045,  0.2126,  0.0337,  0.3865],\n",
      "        [ 0.5586, -0.5684,  0.3291,  1.9978, -0.1830],\n",
      "        [ 0.2089, -0.0419,  2.5424,  0.6785,  0.3811]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.5385],\n",
      "        [ 0.0011],\n",
      "        [-0.2780],\n",
      "        [-1.7278]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6530, -0.2955,  1.1282,  0.9128,  0.4078],\n",
      "        [-0.0490,  1.8229, -0.7611, -0.5452, -1.1335],\n",
      "        [-0.5890, -0.5540, -1.2129,  0.7275, -1.0022],\n",
      "        [-0.6864,  0.9051,  0.7445, -0.7228,  1.1588]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1499,  0.7498,  0.5831,  0.7347,  0.0041],\n",
      "        [ 0.4925,  0.1931, -0.2872,  0.2479,  0.3537],\n",
      "        [-0.0933, -0.3529, -0.4306, -0.2577, -0.7537],\n",
      "        [ 0.6657,  0.4945,  0.4690,  0.5034,  0.8135]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6530, -0.2955,  1.1282,  0.9128,  0.4078],\n",
      "        [-0.0490,  1.8229, -0.7611, -0.5452, -1.1335],\n",
      "        [-0.5890, -0.5540, -1.2129,  0.7275, -1.0022],\n",
      "        [-0.6864,  0.9051,  0.7445, -0.7228,  1.1588]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0107],\n",
      "        [ 0.0103],\n",
      "        [ 1.3406],\n",
      "        [ 0.9187]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8538,  1.5688, -0.0183,  0.4333,  0.1760],\n",
      "        [-0.9304,  0.4465, -1.5356,  1.0989,  0.2364],\n",
      "        [-0.9795,  0.4754,  0.4498,  2.1126,  1.0669],\n",
      "        [-0.7261, -1.0362,  0.0436,  0.0514, -1.9995]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6074,  0.2899, -0.5880,  0.2696,  0.6745],\n",
      "        [-0.1640, -0.3377,  0.1307, -0.4117,  0.1758],\n",
      "        [-0.6644, -1.2021, -0.3744, -0.5349, -0.8361],\n",
      "        [ 0.4611, -0.0682, -0.1868, -0.2753, -0.5458]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8538,  1.5688, -0.0183,  0.4333,  0.1760],\n",
      "        [-0.9304,  0.4465, -1.5356,  1.0989,  0.2364],\n",
      "        [-0.9795,  0.4754,  0.4498,  2.1126,  1.0669],\n",
      "        [-0.7261, -1.0362,  0.0436,  0.0514, -1.9995]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1826],\n",
      "        [-0.6099],\n",
      "        [-2.1112],\n",
      "        [ 0.8049]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6033, -1.1166,  1.8176, -1.3685,  0.3557],\n",
      "        [-1.3541,  0.1216, -0.3416,  2.1449,  1.3653],\n",
      "        [-1.3655,  0.8570,  1.7377, -0.1960, -0.4357],\n",
      "        [-1.2115, -0.4592,  0.3385, -0.4183,  1.3007]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1589,  0.1744, -0.4550,  0.4622,  0.3365],\n",
      "        [ 0.1004,  0.0103, -0.3241,  0.4075,  0.4102],\n",
      "        [ 0.7325,  0.6883,  0.7861,  0.2960,  0.9149],\n",
      "        [-0.5247, -0.7986, -1.8361, -0.2140, -0.8532]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6033, -1.1166,  1.8176, -1.3685,  0.3557],\n",
      "        [-1.3541,  0.1216, -0.3416,  2.1449,  1.3653],\n",
      "        [-1.3655,  0.8570,  1.7377, -0.1960, -0.4357],\n",
      "        [-1.2115, -0.4592,  0.3385, -0.4183,  1.3007]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4387],\n",
      "        [ 1.4101],\n",
      "        [ 0.4989],\n",
      "        [-0.6395]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4044,  0.9279,  0.1828,  1.2821,  0.2244],\n",
      "        [-1.2772,  1.1892, -1.7592, -0.6879, -0.6521],\n",
      "        [ 0.2471,  1.0508,  0.3762,  0.3950,  1.4292],\n",
      "        [ 0.7140, -2.5315,  1.1286, -0.5937, -0.8723]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2978,  0.7706,  0.4185,  0.6590,  1.0190],\n",
      "        [-0.0680, -1.2683, -0.4217, -0.3660, -1.1788],\n",
      "        [-0.2043,  0.2423, -0.4611, -0.0905, -0.0485],\n",
      "        [ 0.0270, -0.5607, -1.1556, -0.4686, -0.9073]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4044,  0.9279,  0.1828,  1.2821,  0.2244],\n",
      "        [-1.2772,  1.1892, -1.7592, -0.6879, -0.6521],\n",
      "        [ 0.2471,  1.0508,  0.3762,  0.3950,  1.4292],\n",
      "        [ 0.7140, -2.5315,  1.1286, -0.5937, -0.8723]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4469],\n",
      "        [ 0.3410],\n",
      "        [-0.0744],\n",
      "        [ 1.2040]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4014,  0.6608,  0.0051,  1.7130,  0.9505],\n",
      "        [ 0.7138,  0.0914, -1.9948, -1.7645,  0.9272],\n",
      "        [ 0.6203, -1.2805, -0.4609, -0.2975, -0.2217],\n",
      "        [-1.2661, -0.4683,  1.1352, -1.0358,  1.8718]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.4073, -0.9095,  0.0632, -0.0699, -0.1598],\n",
      "        [-0.1814, -0.8753, -0.3297,  0.2081, -1.0677],\n",
      "        [-0.5286, -0.1431,  0.0958, -0.5480, -0.4956],\n",
      "        [-0.1128, -0.8054, -0.2581, -1.1968, -1.6783]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4014,  0.6608,  0.0051,  1.7130,  0.9505],\n",
      "        [ 0.7138,  0.0914, -1.9948, -1.7645,  0.9272],\n",
      "        [ 0.6203, -1.2805, -0.4609, -0.2975, -0.2217],\n",
      "        [-1.2661, -0.4683,  1.1352, -1.0358,  1.8718]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7088],\n",
      "        [-0.9090],\n",
      "        [ 0.0842],\n",
      "        [-1.6748]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4489, -0.5969, -0.7878, -0.6323,  0.6130],\n",
      "        [-0.3360,  1.9616, -1.5960,  1.2957, -1.2140],\n",
      "        [ 0.0033, -1.6755,  0.9773,  0.3242, -0.9918],\n",
      "        [ 0.5403,  1.4020,  0.2957, -1.4979,  0.4849]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0325, -0.4085, -0.1162, -0.4550,  0.6611],\n",
      "        [ 0.2196, -0.7058,  0.0845, -0.1927, -0.0608],\n",
      "        [ 0.0132, -0.1731, -0.4899,  0.3186, -0.6312],\n",
      "        [ 0.5786, -1.4587, -0.3115, -0.7969, -0.4581]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4489, -0.5969, -0.7878, -0.6323,  0.6130],\n",
      "        [-0.3360,  1.9616, -1.5960,  1.2957, -1.2140],\n",
      "        [ 0.0033, -1.6755,  0.9773,  0.3242, -0.9918],\n",
      "        [ 0.5403,  1.4020,  0.2957, -1.4979,  0.4849]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0756],\n",
      "        [-1.7690],\n",
      "        [ 0.5406],\n",
      "        [-0.8531]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8036, -0.3870,  0.6387, -0.5041, -0.7685],\n",
      "        [-0.5942, -0.7947, -0.9257,  3.4693,  0.7956],\n",
      "        [-0.0179, -0.4049,  0.8543, -1.8398, -0.1517],\n",
      "        [-1.6037, -0.2665,  1.1814, -0.7221,  1.1749]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7266, -0.6918,  0.7028, -0.0447,  0.3127],\n",
      "        [ 0.4405,  0.8971,  0.7828,  0.8250,  1.1932],\n",
      "        [-0.4826, -0.7464, -0.3503,  0.1642, -0.8846],\n",
      "        [ 0.1152, -0.2150, -0.3188, -0.2891, -1.3571]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8036, -0.3870,  0.6387, -0.5041, -0.7685],\n",
      "        [-0.5942, -0.7947, -0.9257,  3.4693,  0.7956],\n",
      "        [-0.0179, -0.4049,  0.8543, -1.8398, -0.1517],\n",
      "        [-1.6037, -0.2665,  1.1814, -0.7221,  1.1749]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8094],\n",
      "        [ 2.1122],\n",
      "        [-0.1563],\n",
      "        [-1.8897]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4761, -0.6443,  2.1774, -0.1793, -2.4098],\n",
      "        [ 1.1939, -1.0216, -0.7896, -0.6104, -1.4945],\n",
      "        [ 0.3420, -0.4548,  0.3201, -1.4193, -1.2277],\n",
      "        [-0.2109,  0.9271,  0.3639,  0.9405,  0.4336]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9745, -0.8933, -1.6388, -1.2469, -2.2380],\n",
      "        [-0.4310, -0.2141, -0.6643,  0.1005, -1.2521],\n",
      "        [ 0.2398, -0.4005, -0.4690, -0.0746, -0.1313],\n",
      "        [ 0.7101,  0.3810,  0.2009, -0.1852,  1.6409]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4761, -0.6443,  2.1774, -0.1793, -2.4098],\n",
      "        [ 1.1939, -1.0216, -0.7896, -0.6104, -1.4945],\n",
      "        [ 0.3420, -0.4548,  0.3201, -1.4193, -1.2277],\n",
      "        [-0.2109,  0.9271,  0.3639,  0.9405,  0.4336]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1598],\n",
      "        [ 2.0384],\n",
      "        [ 0.3812],\n",
      "        [ 0.8138]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1304, -1.1558, -0.1416, -0.5404, -0.6833],\n",
      "        [-1.9580, -0.3039,  0.3318,  0.2358, -1.9350],\n",
      "        [ 1.4325,  2.8020,  1.0895,  1.5757,  1.8017],\n",
      "        [ 0.3024,  0.1085,  0.5142, -0.8024,  1.5025]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6631, -0.1765,  0.1680,  0.1153,  0.3189],\n",
      "        [-0.9371, -0.2043, -0.8120, -1.3749, -2.1752],\n",
      "        [ 0.1001, -0.6968, -0.2072,  0.3401,  0.0860],\n",
      "        [-0.1222, -0.3307, -0.4658, -0.2308, -1.2212]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1304, -1.1558, -0.1416, -0.5404, -0.6833],\n",
      "        [-1.9580, -0.3039,  0.3318,  0.2358, -1.9350],\n",
      "        [ 1.4325,  2.8020,  1.0895,  1.5757,  1.8017],\n",
      "        [ 0.3024,  0.1085,  0.5142, -0.8024,  1.5025]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1864],\n",
      "        [ 5.5123],\n",
      "        [-1.3439],\n",
      "        [-1.9621]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3087, -0.5499,  0.8697, -0.6566, -0.6159],\n",
      "        [ 1.2560,  0.8421,  2.0227, -2.0224,  0.6973],\n",
      "        [-1.3377,  0.6323, -2.1029,  0.0480, -0.6470],\n",
      "        [ 0.6125,  0.9678,  0.0699,  0.2108, -1.0307]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4472, -0.4393, -0.2312,  0.0237, -0.8089],\n",
      "        [-2.4479, -2.2539, -3.3219, -3.0727, -5.1882],\n",
      "        [ 0.6360,  0.8048,  0.4581,  1.2489,  0.5229],\n",
      "        [ 0.7706,  1.0310,  0.3375,  0.4402,  1.9771]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3087, -0.5499,  0.8697, -0.6566, -0.6159],\n",
      "        [ 1.2560,  0.8421,  2.0227, -2.0224,  0.6973],\n",
      "        [-1.3377,  0.6323, -2.1029,  0.0480, -0.6470],\n",
      "        [ 0.6125,  0.9678,  0.0699,  0.2108, -1.0307]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3852],\n",
      "        [-9.0951],\n",
      "        [-1.5835],\n",
      "        [-0.4515]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2470,  2.5939, -1.3183,  1.5617,  1.0973],\n",
      "        [ 1.9373,  0.4054,  0.5504,  0.1181, -1.1509],\n",
      "        [-2.1807, -0.9189,  0.9892,  0.2288, -0.0405],\n",
      "        [ 0.5526, -0.6464, -0.2761, -1.8498,  1.0631]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3199, -0.5516, -0.5944, -0.4202,  0.2058],\n",
      "        [ 0.2560, -0.2082, -0.4483,  0.0357,  0.3351],\n",
      "        [ 0.7888,  1.7495,  1.2299,  0.8806,  1.5610],\n",
      "        [ 0.7658,  0.2589,  1.7701,  0.3120,  0.6145]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2470,  2.5939, -1.3183,  1.5617,  1.0973],\n",
      "        [ 1.9373,  0.4054,  0.5504,  0.1181, -1.1509],\n",
      "        [-2.1807, -0.9189,  0.9892,  0.2288, -0.0405],\n",
      "        [ 0.5526, -0.6464, -0.2761, -1.8498,  1.0631]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4766],\n",
      "        [-0.2166],\n",
      "        [-1.9728],\n",
      "        [-0.1567]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1007,  0.6346,  0.0877,  0.7589, -0.3417],\n",
      "        [-0.3273,  0.7208, -0.1167,  0.8115,  0.6095],\n",
      "        [-0.2567, -0.3247, -1.0127, -0.8506,  0.5265],\n",
      "        [-0.3256,  0.9539,  0.0783, -0.2747, -0.1846]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5644,  0.7984,  0.9435,  0.9323,  1.4936],\n",
      "        [ 0.4242, -0.1697,  0.2462,  0.3656, -0.2523],\n",
      "        [ 1.5321,  1.6965,  2.0994,  1.5778,  2.0819],\n",
      "        [ 0.6458, -0.2600,  0.3405, -0.2038,  0.3812]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1007,  0.6346,  0.0877,  0.7589, -0.3417],\n",
      "        [-0.3273,  0.7208, -0.1167,  0.8115,  0.6095],\n",
      "        [-0.2567, -0.3247, -1.0127, -0.8506,  0.5265],\n",
      "        [-0.3256,  0.9539,  0.0783, -0.2747, -0.1846]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8433],\n",
      "        [-0.1469],\n",
      "        [-3.3160],\n",
      "        [-0.4461]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1789,  1.9087,  0.7980, -0.1389,  0.4489],\n",
      "        [-0.8661, -0.9372, -0.7086,  0.7215,  0.4559],\n",
      "        [-0.4664, -0.8824, -0.0500,  0.6987,  0.1815],\n",
      "        [ 0.5850,  1.0123, -1.7836,  0.6632, -0.9707]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3505,  0.4601,  0.6716, -0.0001,  0.2492],\n",
      "        [ 0.0981,  0.0171,  0.5402, -0.0031, -0.0467],\n",
      "        [ 0.1633,  0.5068,  0.5978,  0.1331,  0.1860],\n",
      "        [ 0.8738,  0.5108,  0.1866,  0.7370,  0.9851]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1789,  1.9087,  0.7980, -0.1389,  0.4489],\n",
      "        [-0.8661, -0.9372, -0.7086,  0.7215,  0.4559],\n",
      "        [-0.4664, -0.8824, -0.0500,  0.6987,  0.1815],\n",
      "        [ 0.5850,  1.0123, -1.7836,  0.6632, -0.9707]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4634],\n",
      "        [-0.5073],\n",
      "        [-0.4265],\n",
      "        [ 0.2279]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8939,  0.2353, -0.0376, -0.8484, -1.1909],\n",
      "        [-1.0853,  1.3620, -0.2896, -1.5867,  0.6947],\n",
      "        [-1.1695, -0.0568, -0.6693, -0.3097,  0.5544],\n",
      "        [-2.1896,  0.5498,  0.4591,  0.2952,  0.8439]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0436, -0.4701, -0.4465, -0.6072, -0.9103],\n",
      "        [ 0.2770,  0.2916, -0.7482, -0.1281,  0.4812],\n",
      "        [ 0.3828, -0.0444,  0.0667, -0.1879, -0.4022],\n",
      "        [ 0.0636, -0.1216, -0.4914,  0.0146, -0.3452]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8939,  0.2353, -0.0376, -0.8484, -1.1909],\n",
      "        [-1.0853,  1.3620, -0.2896, -1.5867,  0.6947],\n",
      "        [-1.1695, -0.0568, -0.6693, -0.3097,  0.5544],\n",
      "        [-2.1896,  0.5498,  0.4591,  0.2952,  0.8439]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4664],\n",
      "        [ 0.8507],\n",
      "        [-0.6546],\n",
      "        [-0.7187]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3028, -0.8629,  1.5308, -0.5483, -0.8678],\n",
      "        [ 0.1900,  0.6097,  1.2358, -0.8804, -0.0197],\n",
      "        [-1.2215, -0.9493,  0.3764, -0.9654, -1.4107],\n",
      "        [-1.5568,  0.4095,  0.1380, -0.1558, -0.4924]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9264, -1.3269, -0.4437,  0.1058, -1.7569],\n",
      "        [-0.0112,  0.0883, -0.4416, -0.5922, -0.8409],\n",
      "        [ 0.6626,  0.1387, -0.5410, -0.6142, -0.3980],\n",
      "        [ 1.3477,  0.4057,  0.1722,  0.4807,  0.2430]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3028, -0.8629,  1.5308, -0.5483, -0.8678],\n",
      "        [ 0.1900,  0.6097,  1.2358, -0.8804, -0.0197],\n",
      "        [-1.2215, -0.9493,  0.3764, -0.9654, -1.4107],\n",
      "        [-1.5568,  0.4095,  0.1380, -0.1558, -0.4924]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7256],\n",
      "        [ 0.0440],\n",
      "        [ 0.0097],\n",
      "        [-2.1026]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.5171,  2.2603, -0.2581, -0.0247,  0.7346],\n",
      "        [ 0.0934, -0.4208,  0.1847,  1.1804, -1.1121],\n",
      "        [-0.0881, -1.4469,  1.0286,  0.5328, -1.8178],\n",
      "        [-0.3314,  0.0848,  1.6708,  1.7904,  0.7851]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.4527, -1.0267, -0.7362, -1.0433, -1.3363],\n",
      "        [ 0.4101,  0.0459,  0.2596,  0.1099, -0.3960],\n",
      "        [ 0.1861, -0.3200, -0.4179,  0.4679,  0.3599],\n",
      "        [ 1.2821,  1.2624,  1.2486,  0.8317,  1.4715]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.5171,  2.2603, -0.2581, -0.0247,  0.7346],\n",
      "        [ 0.0934, -0.4208,  0.1847,  1.1804, -1.1121],\n",
      "        [-0.0881, -1.4469,  1.0286,  0.5328, -1.8178],\n",
      "        [-0.3314,  0.0848,  1.6708,  1.7904,  0.7851]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-6.7432],\n",
      "        [ 0.6370],\n",
      "        [-0.3881],\n",
      "        [ 4.4126]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4964,  0.5537, -0.7212, -1.6439,  0.5055],\n",
      "        [-1.0530,  0.9314, -0.8129,  0.9440,  0.9476],\n",
      "        [-0.3237,  1.2267, -0.6290,  0.5901, -0.5091],\n",
      "        [-2.1086,  1.4510, -0.4521,  0.5291, -0.3065]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5679,  1.5851,  1.6883,  0.9033,  2.3002],\n",
      "        [ 0.0700, -0.1527, -0.1253,  0.3310, -0.2893],\n",
      "        [ 0.0866, -0.8958, -0.2358,  0.3352, -0.3503],\n",
      "        [-0.4522, -0.4697, -1.2565, -1.1491, -2.4262]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4964,  0.5537, -0.7212, -1.6439,  0.5055],\n",
      "        [-1.0530,  0.9314, -0.8129,  0.9440,  0.9476],\n",
      "        [-0.3237,  1.2267, -0.6290,  0.5901, -0.5091],\n",
      "        [-2.1086,  1.4510, -0.4521,  0.5291, -0.3065]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4406],\n",
      "        [-0.0757],\n",
      "        [-0.6025],\n",
      "        [ 0.9756]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0421, -0.5359, -1.5326,  1.0733, -0.2474],\n",
      "        [ 0.4493,  0.2488, -0.7687,  1.8067,  0.0619],\n",
      "        [-0.6132, -0.2434, -0.0993, -0.6573, -0.1558],\n",
      "        [ 0.8752, -0.0092, -0.7410,  0.7325,  1.6653]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4014,  2.4370,  2.1177,  1.5664,  3.1374],\n",
      "        [ 0.3240, -0.4289, -0.3823,  0.0270,  0.1469],\n",
      "        [ 0.2912, -0.4653, -0.1010, -0.0696,  0.1099],\n",
      "        [-0.4894, -1.3675, -1.8714, -1.1234, -1.6192]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0421, -0.5359, -1.5326,  1.0733, -0.2474],\n",
      "        [ 0.4493,  0.2488, -0.7687,  1.8067,  0.0619],\n",
      "        [-0.6132, -0.2434, -0.0993, -0.6573, -0.1558],\n",
      "        [ 0.8752, -0.0092, -0.7410,  0.7325,  1.6653]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.7054],\n",
      "        [ 0.3905],\n",
      "        [-0.0267],\n",
      "        [-2.5483]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8805,  0.1135, -0.4585,  0.4890,  0.1809],\n",
      "        [-0.4465, -0.2992,  0.3726,  0.3574, -0.7044],\n",
      "        [ 0.3722,  1.1103, -0.0126,  0.2770, -2.2163],\n",
      "        [-0.7177, -0.5095, -0.1965,  1.3572,  0.7668]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4486, -0.4380,  0.0712, -0.5843,  0.3516],\n",
      "        [ 0.5590, -0.2772,  0.6108,  0.0644,  0.0193],\n",
      "        [ 0.4628, -0.5578, -0.3474, -0.2960, -1.0818],\n",
      "        [ 0.7405, -0.4198,  0.0467,  0.3707,  0.6823]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8805,  0.1135, -0.4585,  0.4890,  0.1809],\n",
      "        [-0.4465, -0.2992,  0.3726,  0.3574, -0.7044],\n",
      "        [ 0.3722,  1.1103, -0.0126,  0.2770, -2.2163],\n",
      "        [-0.7177, -0.5095, -0.1965,  1.3572,  0.7668]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0905],\n",
      "        [ 0.0704],\n",
      "        [ 1.8730],\n",
      "        [ 0.6997]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1520,  1.2703, -0.5792, -0.1724, -2.6139],\n",
      "        [ 2.5253,  1.4199,  0.1390,  1.3447,  0.1245],\n",
      "        [-0.5432, -0.3010,  1.8300, -0.2551,  0.0206],\n",
      "        [ 1.0002, -0.4855, -1.5545, -0.6472, -0.2980]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7884,  0.5239,  0.4897,  0.0149, -0.0684],\n",
      "        [ 0.2594,  0.5809,  0.6536,  0.3535,  0.0869],\n",
      "        [-0.4096, -0.9295, -1.1978, -0.9401, -1.6331],\n",
      "        [ 0.4645, -0.0484, -0.2745, -0.3268, -0.6518]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1520,  1.2703, -0.5792, -0.1724, -2.6139],\n",
      "        [ 2.5253,  1.4199,  0.1390,  1.3447,  0.1245],\n",
      "        [-0.5432, -0.3010,  1.8300, -0.2551,  0.0206],\n",
      "        [ 1.0002, -0.4855, -1.5545, -0.6472, -0.2980]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1385],\n",
      "        [ 2.0569],\n",
      "        [-1.4834],\n",
      "        [ 1.3206]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6251, -1.5696,  1.8779, -0.5163,  0.0334],\n",
      "        [-0.4491,  1.0207,  2.8433, -0.6903,  1.4977],\n",
      "        [-1.5995,  1.2262, -0.8393, -1.1885, -0.1652],\n",
      "        [-0.1192,  0.9698, -0.0404, -0.5843,  1.2284]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1185, -0.0519,  0.3830,  0.6669,  1.4552],\n",
      "        [-0.5307, -0.7317, -1.3585, -0.3200, -0.6704],\n",
      "        [ 0.5472, -0.8453, -0.4836, -0.0664, -0.8773],\n",
      "        [-0.1957, -0.5378,  0.0737, -0.9105, -1.2167]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6251, -1.5696,  1.8779, -0.5163,  0.0334],\n",
      "        [-0.4491,  1.0207,  2.8433, -0.6903,  1.4977],\n",
      "        [-1.5995,  1.2262, -0.8393, -1.1885, -0.1652],\n",
      "        [-0.1192,  0.9698, -0.0404, -0.5843,  1.2284]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4309],\n",
      "        [-5.1543],\n",
      "        [-1.2821],\n",
      "        [-1.4638]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3766,  0.5966, -2.2882, -0.0123,  0.4368],\n",
      "        [ 2.2895,  0.6200,  0.3099,  0.7451,  1.6603],\n",
      "        [ 0.9322,  2.7955,  1.1658,  1.2696,  0.7677],\n",
      "        [-0.8364, -0.5647,  0.2585,  0.5794, -1.1784]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0553,  0.1559, -0.1132,  0.1855,  0.5000],\n",
      "        [ 1.0658,  1.0511,  1.5310,  1.3042,  2.5399],\n",
      "        [ 0.8910,  0.2544, -0.2640, -0.0884,  0.9518],\n",
      "        [ 0.3544,  0.3033, -0.1242,  0.3352,  0.7249]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3766,  0.5966, -2.2882, -0.0123,  0.4368],\n",
      "        [ 2.2895,  0.6200,  0.3099,  0.7451,  1.6603],\n",
      "        [ 0.9322,  2.7955,  1.1658,  1.2696,  0.7677],\n",
      "        [-0.8364, -0.5647,  0.2585,  0.5794, -1.1784]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5473],\n",
      "        [ 8.7550],\n",
      "        [ 1.8526],\n",
      "        [-1.1599]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9259,  0.6508,  0.2457,  1.6283,  2.1494],\n",
      "        [-1.4659,  0.5095, -0.5715, -0.2590,  1.0547],\n",
      "        [-0.6564,  0.1853,  1.1980,  0.3128, -0.1221],\n",
      "        [ 0.1480,  1.0348,  0.2874,  1.7341, -0.8549]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1064,  0.1407,  0.7170,  1.1209,  0.3928],\n",
      "        [-1.2762, -2.0130, -1.9325, -2.1436, -4.3585],\n",
      "        [-0.0243, -0.8239, -1.2515, -1.5358, -1.3644],\n",
      "        [ 1.1918,  0.9280,  0.9651,  0.2464,  1.2506]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9259,  0.6508,  0.2457,  1.6283,  2.1494],\n",
      "        [-1.4659,  0.5095, -0.5715, -0.2590,  1.0547],\n",
      "        [-0.6564,  0.1853,  1.1980,  0.3128, -0.1221],\n",
      "        [ 0.1480,  1.0348,  0.2874,  1.7341, -0.8549]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.1420],\n",
      "        [-2.0921],\n",
      "        [-1.9497],\n",
      "        [ 0.7723]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5876,  0.6426, -0.0929, -0.2210,  0.0569],\n",
      "        [ 0.9855,  0.9055,  0.3205, -0.0575,  0.0943],\n",
      "        [-0.3596,  0.2302,  1.3874,  1.1680, -3.5884],\n",
      "        [ 0.3810, -0.2495,  0.9050,  0.4718,  1.2151]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6303, -1.3857, -0.7984, -1.0131, -2.7676],\n",
      "        [-0.7130, -1.5484, -1.4140, -1.7533, -2.4056],\n",
      "        [ 0.8527, -0.1201,  0.1261, -0.0673,  0.9684],\n",
      "        [ 0.4725, -0.5394,  0.0121,  0.0506, -0.2670]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5876,  0.6426, -0.0929, -0.2210,  0.0569],\n",
      "        [ 0.9855,  0.9055,  0.3205, -0.0575,  0.0943],\n",
      "        [-0.3596,  0.2302,  1.3874,  1.1680, -3.5884],\n",
      "        [ 0.3810, -0.2495,  0.9050,  0.4718,  1.2151]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3795],\n",
      "        [-2.6841],\n",
      "        [-3.7131],\n",
      "        [ 0.0250]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5757, -1.2374,  1.1607, -0.6412,  1.7167],\n",
      "        [-0.9660,  0.5148, -0.4441,  0.2049,  0.6291],\n",
      "        [-1.6697,  0.2860,  0.0796,  0.3339, -0.5918],\n",
      "        [-0.6137, -0.2935,  0.1183,  0.7647,  0.2650]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6169, -1.4285, -1.5080, -0.1316, -1.4399],\n",
      "        [ 0.4395, -0.1646, -0.0706,  0.1104, -0.1687],\n",
      "        [ 1.3827,  1.1775,  1.7081,  1.0636,  1.8333],\n",
      "        [ 1.4134, -0.1615,  0.6384,  0.7844,  0.5935]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5757, -1.2374,  1.1607, -0.6412,  1.7167],\n",
      "        [-0.9660,  0.5148, -0.4441,  0.2049,  0.6291],\n",
      "        [-1.6697,  0.2860,  0.0796,  0.3339, -0.5918],\n",
      "        [-0.6137, -0.2935,  0.1183,  0.7647,  0.2650]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.7253],\n",
      "        [-0.5614],\n",
      "        [-2.5658],\n",
      "        [ 0.0126]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0544, -0.9245,  0.3321,  0.3615, -0.4035],\n",
      "        [ 0.7395,  1.9964,  0.7863,  0.5260, -1.6065],\n",
      "        [-2.1526, -1.0183,  1.1619,  1.2066,  0.6012],\n",
      "        [-0.7488, -0.9488, -0.5416,  0.7938, -0.8935]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7287, -0.2824, -0.5674, -0.5616,  0.5641],\n",
      "        [ 0.0747,  0.5960, -0.2188,  0.0927, -0.0296],\n",
      "        [ 2.3357,  2.7950,  1.6273,  1.7194,  3.9442],\n",
      "        [ 1.0755,  0.5165, -0.4964,  0.7397,  0.2327]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0544, -0.9245,  0.3321,  0.3615, -0.4035],\n",
      "        [ 0.7395,  1.9964,  0.7863,  0.5260, -1.6065],\n",
      "        [-2.1526, -1.0183,  1.1619,  1.2066,  0.6012],\n",
      "        [-0.7488, -0.9488, -0.5416,  0.7938, -0.8935]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4104],\n",
      "        [ 1.1694],\n",
      "        [-1.5373],\n",
      "        [-0.6473]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6474,  0.4148,  1.8019,  2.3383, -0.0125],\n",
      "        [ 1.1146,  0.6557, -0.5503,  1.8241, -0.1360],\n",
      "        [ 1.1616, -1.2291,  1.2045, -0.5511, -0.5322],\n",
      "        [-0.9410, -0.2020,  0.8121, -1.3662,  0.1297]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1239, -0.7099, -0.5861, -0.3416, -0.7023],\n",
      "        [-0.5368, -0.6338, -0.9337, -0.4736, -0.7588],\n",
      "        [ 0.6842, -0.2437, -0.1565, -0.4185,  0.1924],\n",
      "        [ 0.5316,  0.5898, -0.2116, -0.0265,  0.0405]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6474,  0.4148,  1.8019,  2.3383, -0.0125],\n",
      "        [ 1.1146,  0.6557, -0.5503,  1.8241, -0.1360],\n",
      "        [ 1.1616, -1.2291,  1.2045, -0.5511, -0.5322],\n",
      "        [-0.9410, -0.2020,  0.8121, -1.3662,  0.1297]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2206],\n",
      "        [-1.2608],\n",
      "        [ 1.0341],\n",
      "        [-0.7497]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5738, -0.3376,  1.4070,  1.4565, -0.0191],\n",
      "        [-0.3506,  0.3690,  0.2840,  1.9226, -1.9394],\n",
      "        [ 0.8264, -0.3138,  1.4084, -0.1458, -1.5989],\n",
      "        [-2.1696,  0.4854,  0.9752, -0.1420,  1.0068]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7072,  0.9497,  0.4456, -0.1085,  1.5544],\n",
      "        [ 0.4846,  0.4031,  0.1467, -0.2589,  0.0897],\n",
      "        [-0.1055, -0.4187,  0.4277,  0.3361, -0.7528],\n",
      "        [ 0.9553,  0.0617,  0.2789,  0.5244,  0.5351]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5738, -0.3376,  1.4070,  1.4565, -0.0191],\n",
      "        [-0.3506,  0.3690,  0.2840,  1.9226, -1.9394],\n",
      "        [ 0.8264, -0.3138,  1.4084, -0.1458, -1.5989],\n",
      "        [-2.1696,  0.4854,  0.9752, -0.1420,  1.0068]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2316],\n",
      "        [-0.6513],\n",
      "        [ 1.8012],\n",
      "        [-1.3063]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3863,  0.7163,  1.6244, -0.0646,  2.1425],\n",
      "        [ 0.2528,  0.6119,  0.5596, -0.5490, -0.3436],\n",
      "        [ 0.7033, -0.3382,  1.8762, -0.6635,  0.3981],\n",
      "        [ 0.6983, -0.0298,  0.7341, -0.6273,  0.4874]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1634, -0.9159,  0.2577, -0.3937, -0.1300],\n",
      "        [ 0.4941, -0.1479,  0.3439, -0.4418, -0.3956],\n",
      "        [-1.2390, -0.6251, -0.7309, -1.4524, -1.8291],\n",
      "        [ 1.3652,  1.0887,  0.8255,  1.4764,  0.7933]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3863,  0.7163,  1.6244, -0.0646,  2.1425],\n",
      "        [ 0.2528,  0.6119,  0.5596, -0.5490, -0.3436],\n",
      "        [ 0.7033, -0.3382,  1.8762, -0.6635,  0.3981],\n",
      "        [ 0.6983, -0.0298,  0.7341, -0.6273,  0.4874]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5536],\n",
      "        [ 0.6054],\n",
      "        [-1.7958],\n",
      "        [ 0.9874]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8139,  0.6476,  0.4675, -0.2413,  0.7483],\n",
      "        [-0.5897,  0.1290, -0.2596, -0.6166, -0.3111],\n",
      "        [-0.1575, -0.5093,  0.6217,  1.9529,  1.9747],\n",
      "        [-1.2251, -1.8102, -0.0443,  1.1236, -0.6540]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4177, -0.6555, -0.5777, -0.9941, -0.5699],\n",
      "        [ 0.4288, -0.1886, -0.2073,  0.1845, -0.4292],\n",
      "        [-0.0782, -0.0653,  0.0720,  0.2603,  0.1792],\n",
      "        [ 1.1476,  0.1602,  0.6059,  0.3379,  0.7931]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8139,  0.6476,  0.4675, -0.2413,  0.7483],\n",
      "        [-0.5897,  0.1290, -0.2596, -0.6166, -0.3111],\n",
      "        [-0.1575, -0.5093,  0.6217,  1.9529,  1.9747],\n",
      "        [-1.2251, -1.8102, -0.0443,  1.1236, -0.6540]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5412],\n",
      "        [-0.2036],\n",
      "        [ 0.9526],\n",
      "        [-1.8616]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3138,  1.2089, -0.4426, -0.8353,  0.4503],\n",
      "        [-0.7793,  0.7334, -0.2080, -0.6224, -0.4832],\n",
      "        [-0.3766, -0.0465, -0.4695, -2.4359, -0.4262],\n",
      "        [ 0.1065, -0.9679, -1.7201,  1.2111, -0.3447]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3839, -0.9702, -0.5315, -0.5715, -0.8593],\n",
      "        [ 0.1961, -0.4045, -0.6426, -0.6411, -0.7508],\n",
      "        [-0.4135, -0.2279, -0.1646, -0.4079, -0.9196],\n",
      "        [ 0.4770, -0.0241,  0.4218, -0.4531,  0.2355]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3138,  1.2089, -0.4426, -0.8353,  0.4503],\n",
      "        [-0.7793,  0.7334, -0.2080, -0.6224, -0.4832],\n",
      "        [-0.3766, -0.0465, -0.4695, -2.4359, -0.4262],\n",
      "        [ 0.1065, -0.9679, -1.7201,  1.2111, -0.3447]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3428],\n",
      "        [ 0.4460],\n",
      "        [ 1.6292],\n",
      "        [-1.2814]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2053, -0.2803, -0.1862,  1.9446, -1.0081],\n",
      "        [ 1.4706,  1.0573, -0.4972,  0.8825,  0.8108],\n",
      "        [-0.9641, -1.0802, -0.9345, -0.5538,  0.0427],\n",
      "        [-2.4210, -0.3772,  1.8220, -0.7508, -0.6166]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2156, -0.4958, -0.9871, -0.4121, -0.4230],\n",
      "        [ 0.7790, -0.2503, -0.6627,  0.0053, -0.7741],\n",
      "        [-0.5337, -1.7601, -0.2544, -1.4434, -2.4890],\n",
      "        [ 0.2005,  0.4316,  0.9009,  0.4821,  1.2782]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2053, -0.2803, -0.1862,  1.9446, -1.0081],\n",
      "        [ 1.4706,  1.0573, -0.4972,  0.8825,  0.8108],\n",
      "        [-0.9641, -1.0802, -0.9345, -0.5538,  0.0427],\n",
      "        [-2.4210, -0.3772,  1.8220, -0.7508, -0.6166]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0964],\n",
      "        [ 0.5874],\n",
      "        [ 3.3468],\n",
      "        [-0.1568]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2032, -0.1656,  0.9580,  0.8568,  0.2579],\n",
      "        [ 0.0171,  0.8837, -0.6297,  0.4353,  0.6944],\n",
      "        [ 0.1588, -0.5787, -0.2046,  0.6020, -0.1717],\n",
      "        [-0.3715,  0.8461,  0.5323,  1.5284,  0.4299]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1275, -0.8781, -0.2952, -0.9827, -1.0132],\n",
      "        [-0.1046, -0.2270,  0.3313, -0.0591,  0.2995],\n",
      "        [-1.1562, -1.9055, -2.2660, -2.1927, -4.0592],\n",
      "        [ 0.7297, -0.0151,  0.0551,  0.2806,  0.8334]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2032, -0.1656,  0.9580,  0.8568,  0.2579],\n",
      "        [ 0.0171,  0.8837, -0.6297,  0.4353,  0.6944],\n",
      "        [ 0.1588, -0.5787, -0.2046,  0.6020, -0.1717],\n",
      "        [-0.3715,  0.8461,  0.5323,  1.5284,  0.4299]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0873],\n",
      "        [-0.2288],\n",
      "        [ 0.7596],\n",
      "        [ 0.5327]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0240, -1.7881,  0.2679,  0.3850, -1.2761],\n",
      "        [ 0.8970,  0.9538,  0.2807, -0.3695,  0.9967],\n",
      "        [-0.8261,  0.5416,  0.8443,  1.6002,  0.3028],\n",
      "        [-0.1527, -0.3794, -0.3021, -0.8158,  2.3741]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5747, -0.3659, -1.0688, -0.0149, -0.4221],\n",
      "        [-0.4519, -0.3278,  0.0178,  0.1625, -0.0700],\n",
      "        [-0.3110, -0.2045,  0.1229,  0.1917,  0.5760],\n",
      "        [ 0.8106, -0.2139, -0.0305, -0.2328, -0.0785]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0240, -1.7881,  0.2679,  0.3850, -1.2761],\n",
      "        [ 0.8970,  0.9538,  0.2807, -0.3695,  0.9967],\n",
      "        [-0.8261,  0.5416,  0.8443,  1.6002,  0.3028],\n",
      "        [-0.1527, -0.3794, -0.3021, -0.8158,  2.3741]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9146],\n",
      "        [-0.8428],\n",
      "        [ 0.7312],\n",
      "        [-0.0300]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0916,  0.5045,  0.0620,  0.2958, -0.5536],\n",
      "        [-0.2436, -1.6241,  0.7738, -0.5810, -0.7996],\n",
      "        [ 0.4290,  0.0921, -0.6360,  0.3165,  0.7913],\n",
      "        [-2.4974, -0.3647, -0.3718, -0.3739, -1.3329]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2004, -1.4383, -1.1773, -0.6262, -1.4577],\n",
      "        [ 0.2612,  0.0779, -0.3933, -0.3966,  1.0288],\n",
      "        [ 0.7016, -0.2798,  0.7600,  0.1550,  0.2586],\n",
      "        [ 0.5620, -0.1621,  0.2354, -0.3712,  0.2555]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0916,  0.5045,  0.0620,  0.2958, -0.5536],\n",
      "        [-0.2436, -1.6241,  0.7738, -0.5810, -0.7996],\n",
      "        [ 0.4290,  0.0921, -0.6360,  0.3165,  0.7913],\n",
      "        [-2.4974, -0.3647, -0.3718, -0.3739, -1.3329]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0419],\n",
      "        [-1.0867],\n",
      "        [ 0.0456],\n",
      "        [-1.6336]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0760, -0.8151,  0.7573,  0.2323, -0.9362],\n",
      "        [-0.8101,  1.1513,  1.5146, -0.4341, -1.6951],\n",
      "        [ 0.1501,  0.5029,  1.2975, -0.4569,  0.8614],\n",
      "        [-0.8086, -0.3712, -0.9822,  1.2867,  0.4466]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3810, -0.6245, -0.6319, -0.5450, -0.7693],\n",
      "        [ 0.3440,  0.3698,  0.6430,  0.2575,  1.0112],\n",
      "        [ 0.3492, -0.3946, -0.5457, -0.5727, -0.0386],\n",
      "        [ 0.9821,  1.1563,  1.2564,  1.1383,  1.2497]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0760, -0.8151,  0.7573,  0.2323, -0.9362],\n",
      "        [-0.8101,  1.1513,  1.5146, -0.4341, -1.6951],\n",
      "        [ 0.1501,  0.5029,  1.2975, -0.4569,  0.8614],\n",
      "        [-0.8086, -0.3712, -0.9822,  1.2867,  0.4466]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6530],\n",
      "        [-0.7048],\n",
      "        [-0.6258],\n",
      "        [-0.4348]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2543,  0.5211,  1.6363,  0.5765,  0.1339],\n",
      "        [ 1.1191,  0.6405,  0.9430, -1.5243,  1.8634],\n",
      "        [-1.1597,  1.6968, -1.3107, -1.6757,  0.8874],\n",
      "        [-0.2633,  0.6042,  1.2807,  0.5699, -0.8412]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7021, -0.9588, -0.7225, -0.9927, -1.5457],\n",
      "        [ 0.3431,  1.2160,  0.6187,  0.6408,  1.1027],\n",
      "        [ 0.5690,  0.0241, -0.2113,  0.3409, -0.2766],\n",
      "        [ 1.1289,  0.9443,  1.1037,  1.1023,  0.6719]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2543,  0.5211,  1.6363,  0.5765,  0.1339],\n",
      "        [ 1.1191,  0.6405,  0.9430, -1.5243,  1.8634],\n",
      "        [-1.1597,  1.6968, -1.3107, -1.6757,  0.8874],\n",
      "        [-0.2633,  0.6042,  1.2807,  0.5699, -0.8412]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.6396],\n",
      "        [ 2.8243],\n",
      "        [-1.1587],\n",
      "        [ 1.7497]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5336,  0.9597, -0.8961, -0.8352, -0.3862],\n",
      "        [ 0.3063, -1.3916,  1.1435,  0.1813,  0.5258],\n",
      "        [ 0.4396, -0.4374,  0.2169,  1.4894,  0.9792],\n",
      "        [ 0.9591,  0.6654,  2.1041,  0.4687,  0.6420]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5281,  0.4026,  0.4431,  0.7013,  1.4307],\n",
      "        [-0.6505, -1.3494, -0.5461, -0.6277, -2.2335],\n",
      "        [ 0.6899,  0.4571,  0.9420,  0.2174,  1.1435],\n",
      "        [ 0.5103, -0.0280,  0.2239, -0.7834,  0.2217]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5336,  0.9597, -0.8961, -0.8352, -0.3862],\n",
      "        [ 0.3063, -1.3916,  1.1435,  0.1813,  0.5258],\n",
      "        [ 0.4396, -0.4374,  0.2169,  1.4894,  0.9792],\n",
      "        [ 0.9591,  0.6654,  2.1041,  0.4687,  0.6420]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9590],\n",
      "        [-0.2341],\n",
      "        [ 1.7512],\n",
      "        [ 0.7169]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2867,  2.1785,  1.5333,  1.6260,  0.5393],\n",
      "        [-1.3617,  0.6419, -0.2815,  0.2535,  0.7077],\n",
      "        [ 1.1329, -0.9464,  0.2293, -1.8624,  0.3677],\n",
      "        [-0.4528, -1.3473,  0.1474, -0.1456,  0.7052]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2550,  0.9680,  0.8024,  0.5810,  0.9680],\n",
      "        [-0.2633, -1.1073, -0.4279, -0.9183, -1.1253],\n",
      "        [ 0.2585, -0.3813, -0.2155, -0.0504, -0.4367],\n",
      "        [-0.1293,  0.2295,  0.2929,  1.0746,  0.2409]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2867,  2.1785,  1.5333,  1.6260,  0.5393],\n",
      "        [-1.3617,  0.6419, -0.2815,  0.2535,  0.7077],\n",
      "        [ 1.1329, -0.9464,  0.2293, -1.8624,  0.3677],\n",
      "        [-0.4528, -1.3473,  0.1474, -0.1456,  0.7052]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.1908],\n",
      "        [-1.2611],\n",
      "        [ 0.5376],\n",
      "        [-0.1940]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4470, -0.7967,  1.3699, -0.0007,  0.1114],\n",
      "        [ 1.2917,  1.0729,  0.5137,  1.6110, -0.0789],\n",
      "        [-1.0529,  0.0301,  0.3774,  1.0872,  0.6185],\n",
      "        [ 0.2942, -0.9288,  0.2437, -0.4081, -1.7789]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2428, -0.7397, -0.2821, -1.1677, -0.4704],\n",
      "        [ 0.2392, -0.4084,  0.4075,  0.3786, -0.0294],\n",
      "        [-0.5764, -0.0149, -0.5012, -0.0334, -0.7527],\n",
      "        [-0.0840,  0.0126,  0.3746,  0.4958,  1.2120]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4470, -0.7967,  1.3699, -0.0007,  0.1114],\n",
      "        [ 1.2917,  1.0729,  0.5137,  1.6110, -0.0789],\n",
      "        [-1.0529,  0.0301,  0.3774,  1.0872,  0.6185],\n",
      "        [ 0.2942, -0.9288,  0.2437, -0.4081, -1.7789]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5026],\n",
      "        [ 0.6923],\n",
      "        [-0.0845],\n",
      "        [-2.3035]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0517, -0.5058,  2.0969,  0.5836,  0.6426],\n",
      "        [-0.0009,  0.5913,  0.0384, -0.3844,  1.6357],\n",
      "        [-0.7185, -0.0096,  0.3258,  1.4175,  0.4374],\n",
      "        [-0.8667,  0.1973,  0.1376, -0.7337,  0.9932]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1280, -0.8865, -0.2954, -1.3415, -1.4280],\n",
      "        [ 0.3386, -0.4954,  0.0816, -1.0784, -1.4550],\n",
      "        [ 0.1227, -0.1214, -1.1994,  0.2387,  0.2010],\n",
      "        [ 0.7615,  1.0874,  2.2551,  1.0422,  1.2847]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0517, -0.5058,  2.0969,  0.5836,  0.6426],\n",
      "        [-0.0009,  0.5913,  0.0384, -0.3844,  1.6357],\n",
      "        [-0.7185, -0.0096,  0.3258,  1.4175,  0.4374],\n",
      "        [-0.8667,  0.1973,  0.1376, -0.7337,  0.9932]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8649],\n",
      "        [-2.2555],\n",
      "        [-0.0514],\n",
      "        [ 0.3762]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7765,  1.5150, -0.8092,  2.3384,  1.0873],\n",
      "        [ 1.2830,  1.6053, -1.8651,  2.0989, -0.2465],\n",
      "        [ 1.2659,  1.0509, -1.4536, -0.7300, -0.7522],\n",
      "        [-0.2082,  0.8626, -0.9526,  0.0710,  0.6436]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2120,  0.9115,  0.0370, -0.5785,  0.3996],\n",
      "        [ 0.9060,  0.6813,  0.7445,  0.9899,  0.9296],\n",
      "        [ 0.1537,  0.0369,  0.4220, -0.0249,  0.7120],\n",
      "        [ 1.2195,  2.0892,  1.2495,  0.8151,  2.5555]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7765,  1.5150, -0.8092,  2.3384,  1.0873],\n",
      "        [ 1.2830,  1.6053, -1.8651,  2.0989, -0.2465],\n",
      "        [ 1.2659,  1.0509, -1.4536, -0.7300, -0.7522],\n",
      "        [-0.2082,  0.8626, -0.9526,  0.0710,  0.6436]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3739],\n",
      "        [ 2.7160],\n",
      "        [-0.8973],\n",
      "        [ 2.0605]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5009, -0.0385,  2.1684, -0.4568, -0.5396],\n",
      "        [-1.7652, -2.6654,  0.6856,  1.6825,  0.5305],\n",
      "        [-1.8969, -0.6757, -0.8430, -0.7589,  1.1326],\n",
      "        [ 0.9828,  2.3163,  2.3616,  0.9221, -0.6020]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3541,  0.2154, -0.5866, -0.3135, -0.8818],\n",
      "        [-0.3678, -0.2934, -0.0053, -1.4459, -0.8044],\n",
      "        [ 0.4498, -0.0618,  0.8084,  0.5077,  0.9065],\n",
      "        [-0.1028,  0.6285,  0.1371,  0.4119,  1.0841]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5009, -0.0385,  2.1684, -0.4568, -0.5396],\n",
      "        [-1.7652, -2.6654,  0.6856,  1.6825,  0.5305],\n",
      "        [-1.8969, -0.6757, -0.8430, -0.7589,  1.1326],\n",
      "        [ 0.9828,  2.3163,  2.3616,  0.9221, -0.6020]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8387],\n",
      "        [-1.4319],\n",
      "        [-0.8513],\n",
      "        [ 1.4058]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1075, -0.2395,  0.6949,  1.1451,  0.8779],\n",
      "        [-0.2174,  0.1958, -2.6501, -0.2337,  0.3255],\n",
      "        [ 0.6966, -1.3229, -0.2656, -0.3003, -0.7918],\n",
      "        [ 1.0197, -1.1949, -0.0291, -2.5181, -1.0392]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5627,  0.1346, -0.2534,  0.2502,  0.1267],\n",
      "        [ 0.1030, -0.0681, -0.2697, -0.0752, -0.3436],\n",
      "        [ 0.3989,  0.6827,  0.3155,  0.0031,  1.5153],\n",
      "        [-0.1910,  0.4218,  0.4295,  0.6679, -0.5172]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1075, -0.2395,  0.6949,  1.1451,  0.8779],\n",
      "        [-0.2174,  0.1958, -2.6501, -0.2337,  0.3255],\n",
      "        [ 0.6966, -1.3229, -0.2656, -0.3003, -0.7918],\n",
      "        [ 1.0197, -1.1949, -0.0291, -2.5181, -1.0392]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2499],\n",
      "        [ 0.5847],\n",
      "        [-1.9098],\n",
      "        [-1.8557]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2993, -0.9044,  0.3319,  1.6246,  0.9702],\n",
      "        [ 0.2251, -1.0949, -0.4986,  0.2235, -0.6121],\n",
      "        [-0.7818,  1.8901,  0.0770,  1.1812,  1.1300],\n",
      "        [-2.2593,  1.1542, -0.5128,  0.5586,  1.4743]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3760, -0.5129,  0.4586,  0.2971, -0.0913],\n",
      "        [ 0.2601,  0.1144,  0.2527, -0.1161, -0.5838],\n",
      "        [ 0.5752,  1.7407,  1.0032,  1.3026,  1.8713],\n",
      "        [ 0.6798,  1.1218,  0.9780,  1.4076,  0.8947]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2993, -0.9044,  0.3319,  1.6246,  0.9702],\n",
      "        [ 0.2251, -1.0949, -0.4986,  0.2235, -0.6121],\n",
      "        [-0.7818,  1.8901,  0.0770,  1.1812,  1.1300],\n",
      "        [-2.2593,  1.1542, -0.5128,  0.5586,  1.4743]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1228],\n",
      "        [ 0.1388],\n",
      "        [ 6.5709],\n",
      "        [ 1.3627]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0048,  0.3549,  0.2280,  0.3222, -0.5253],\n",
      "        [ 0.3414,  0.0479, -0.1827, -0.5045,  0.5554],\n",
      "        [-1.3487,  0.6233,  0.9197,  0.5111,  0.0932],\n",
      "        [-1.9794,  0.0776,  1.1385, -0.2184,  3.9134]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6479, -0.4930, -0.3024, -0.8221, -0.6274],\n",
      "        [ 0.4224, -0.6858,  0.7860, -0.1034, -0.4979],\n",
      "        [-1.2539, -1.2508, -1.5391, -1.9103, -3.2088],\n",
      "        [ 0.1154,  0.9364,  0.6735,  0.0651,  0.5439]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0048,  0.3549,  0.2280,  0.3222, -0.5253],\n",
      "        [ 0.3414,  0.0479, -0.1827, -0.5045,  0.5554],\n",
      "        [-1.3487,  0.6233,  0.9197,  0.5111,  0.0932],\n",
      "        [-1.9794,  0.0776,  1.1385, -0.2184,  3.9134]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4718],\n",
      "        [-0.2566],\n",
      "        [-1.7795],\n",
      "        [ 2.7255]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9846,  0.4554, -0.0570, -0.0234,  0.3817],\n",
      "        [ 0.4406,  1.3332, -1.3953, -0.5692,  0.1478],\n",
      "        [-1.9121,  0.6383,  0.7742,  0.4407,  0.5581],\n",
      "        [-0.3819, -0.7578, -1.3298, -0.1981, -1.6457]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3328, -0.8566, -0.8170, -0.5492, -0.6658],\n",
      "        [ 0.0452, -0.1412, -0.3794, -0.3232, -0.5312],\n",
      "        [-0.3965, -1.4507, -0.8499, -0.8533, -2.3016],\n",
      "        [-0.2590, -0.5102, -0.2349, -0.2428, -1.0356]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9846,  0.4554, -0.0570, -0.0234,  0.3817],\n",
      "        [ 0.4406,  1.3332, -1.3953, -0.5692,  0.1478],\n",
      "        [-1.9121,  0.6383,  0.7742,  0.4407,  0.5581],\n",
      "        [-0.3819, -0.7578, -1.3298, -0.1981, -1.6457]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9125],\n",
      "        [ 0.4664],\n",
      "        [-2.4862],\n",
      "        [ 2.5502]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5439,  1.1402,  0.0097,  1.1664, -0.4973],\n",
      "        [ 1.0579,  0.3947, -0.1929, -0.9434, -0.4838],\n",
      "        [-0.2188,  2.6150,  0.7677, -0.8369,  0.4163],\n",
      "        [ 0.1119,  0.5502, -0.3996, -1.2749, -0.1707]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1694,  0.4753, -0.1139, -0.1097,  0.1062],\n",
      "        [ 0.1167,  0.2547, -0.1749,  0.0882, -0.6928],\n",
      "        [ 0.6780, -0.1359, -0.4940, -0.2018, -0.1984],\n",
      "        [-1.4701, -2.0044, -1.5327, -1.2046, -3.1836]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5439,  1.1402,  0.0097,  1.1664, -0.4973],\n",
      "        [ 1.0579,  0.3947, -0.1929, -0.9434, -0.4838],\n",
      "        [-0.2188,  2.6150,  0.7677, -0.8369,  0.4163],\n",
      "        [ 0.1119,  0.5502, -0.3996, -1.2749, -0.1707]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4522],\n",
      "        [ 0.5097],\n",
      "        [-0.7967],\n",
      "        [ 1.4244]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1713,  0.7227, -1.3649,  0.2459, -0.6481],\n",
      "        [ 0.1846,  1.7817, -0.8163, -1.3841, -0.5206],\n",
      "        [-0.8029,  0.5431,  0.3129, -0.4477, -0.2903],\n",
      "        [-0.7353, -1.1107,  0.8374,  0.2218,  1.1935]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0421,  0.0003, -0.4710,  0.1506, -0.2243],\n",
      "        [-0.2187, -0.1754,  0.3134,  0.0788, -0.1864],\n",
      "        [ 0.4945,  0.3042, -0.8343, -0.5107, -0.2213],\n",
      "        [-1.9600, -1.5928, -1.5987, -1.9250, -4.1914]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1713,  0.7227, -1.3649,  0.2459, -0.6481],\n",
      "        [ 0.1846,  1.7817, -0.8163, -1.3841, -0.5206],\n",
      "        [-0.8029,  0.5431,  0.3129, -0.4477, -0.2903],\n",
      "        [-0.7353, -1.1107,  0.8374,  0.2218,  1.1935]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8182],\n",
      "        [-0.6208],\n",
      "        [-0.2000],\n",
      "        [-3.5577]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4315, -0.1849, -0.7059,  1.3012,  0.6234],\n",
      "        [-1.3780, -0.5066, -0.8030,  2.2163, -0.6337],\n",
      "        [ 0.0983, -1.4490, -1.0462,  0.1125, -0.2089],\n",
      "        [-0.8212,  0.7499, -0.7205, -0.3041,  0.1822]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2458, -0.2491,  0.0785, -0.4401,  0.2933],\n",
      "        [ 0.5053, -0.3884, -0.1404, -0.3619, -0.3805],\n",
      "        [-0.2505, -0.9572, -0.1583, -1.2246, -1.2362],\n",
      "        [-0.0742, -0.2133,  0.5163, -0.4403,  0.0344]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4315, -0.1849, -0.7059,  1.3012,  0.6234],\n",
      "        [-1.3780, -0.5066, -0.8030,  2.2163, -0.6337],\n",
      "        [ 0.0983, -1.4490, -1.0462,  0.1125, -0.2089],\n",
      "        [-0.8212,  0.7499, -0.7205, -0.3041,  0.1822]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7511],\n",
      "        [-0.9478],\n",
      "        [ 1.6484],\n",
      "        [-0.3308]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4076,  0.2347, -0.1607, -0.4679,  0.0810],\n",
      "        [-0.6335,  0.8133,  0.4220,  1.5558, -0.2407],\n",
      "        [ 0.2012,  0.0780,  0.0465, -0.7723, -0.4816],\n",
      "        [ 0.8248,  0.9104, -1.0685,  1.6783,  1.0861]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2585, -0.6109, -0.7479, -0.8453, -0.6468],\n",
      "        [ 0.3271,  0.4252, -0.5588,  0.3367,  1.1613],\n",
      "        [-0.2930, -1.5834, -1.1420, -0.9542, -1.5800],\n",
      "        [ 0.3935, -0.7540, -0.3981, -0.1190, -0.3544]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4076,  0.2347, -0.1607, -0.4679,  0.0810],\n",
      "        [-0.6335,  0.8133,  0.4220,  1.5558, -0.2407],\n",
      "        [ 0.2012,  0.0780,  0.0465, -0.7723, -0.4816],\n",
      "        [ 0.8248,  0.9104, -1.0685,  1.6783,  1.0861]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4252],\n",
      "        [ 0.1471],\n",
      "        [ 1.2623],\n",
      "        [-0.5211]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5378, -0.9180,  0.1629,  0.4155, -1.3623],\n",
      "        [-0.9214,  0.8065,  0.3868,  1.2030, -0.7743],\n",
      "        [ 0.1057,  1.2324,  1.1345,  0.7541,  0.3939],\n",
      "        [ 0.3926,  1.0457, -0.7961,  0.2933, -1.1193]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2210,  0.8132, -0.8113,  0.1034, -0.1508],\n",
      "        [-0.0010, -0.0877,  0.2169, -0.0661,  0.1018],\n",
      "        [-0.4965, -1.4237, -1.0809, -1.0236, -1.9717],\n",
      "        [ 0.3495,  0.1306, -0.0346,  0.3174, -0.0791]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5378, -0.9180,  0.1629,  0.4155, -1.3623],\n",
      "        [-0.9214,  0.8065,  0.3868,  1.2030, -0.7743],\n",
      "        [ 0.1057,  1.2324,  1.1345,  0.7541,  0.3939],\n",
      "        [ 0.3926,  1.0457, -0.7961,  0.2933, -1.1193]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9701],\n",
      "        [-0.1442],\n",
      "        [-4.5818],\n",
      "        [ 0.4830]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9855,  0.3341,  0.9094,  1.5371,  1.0656],\n",
      "        [ 0.0536, -1.1752, -0.4905,  1.3017,  1.4257],\n",
      "        [ 0.3500,  0.5585, -0.0033,  2.1666, -1.2289],\n",
      "        [ 0.3707, -0.3508, -0.8837,  0.1184,  0.8672]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6804,  0.3523,  1.0820,  0.5608,  0.2533],\n",
      "        [ 0.2047, -0.0314, -0.7121, -0.6487,  0.2548],\n",
      "        [ 0.9358,  0.8393,  0.8159,  0.1001,  0.8263],\n",
      "        [ 0.2262, -0.0979,  0.3411, -0.1058, -0.2524]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9855,  0.3341,  0.9094,  1.5371,  1.0656],\n",
      "        [ 0.0536, -1.1752, -0.4905,  1.3017,  1.4257],\n",
      "        [ 0.3500,  0.5585, -0.0033,  2.1666, -1.2289],\n",
      "        [ 0.3707, -0.3508, -0.8837,  0.1184,  0.8672]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5630],\n",
      "        [-0.0839],\n",
      "        [-0.0049],\n",
      "        [-0.4146]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7130,  1.5687, -1.4028, -0.5055, -0.4045],\n",
      "        [ 0.9460, -1.3224,  0.6678,  0.4361,  1.1187],\n",
      "        [ 0.5725, -0.6752, -1.3159, -0.8801, -0.0812],\n",
      "        [-0.5715, -0.2964,  1.1268, -0.1040, -0.2800]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0326, -0.2194, -0.7566,  0.3129, -0.8586],\n",
      "        [ 0.2933, -0.1202,  0.0098,  0.0438, -0.1005],\n",
      "        [ 0.4253,  0.4148, -0.6019, -0.2358,  0.6315],\n",
      "        [ 0.4849, -0.3513, -0.1972, -0.6786, -0.1411]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7130,  1.5687, -1.4028, -0.5055, -0.4045],\n",
      "        [ 0.9460, -1.3224,  0.6678,  0.4361,  1.1187],\n",
      "        [ 0.5725, -0.6752, -1.3159, -0.8801, -0.0812],\n",
      "        [-0.5715, -0.2964,  1.1268, -0.1040, -0.2800]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8831],\n",
      "        [ 0.3496],\n",
      "        [ 0.9117],\n",
      "        [-0.2851]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9571, -0.0137, -0.1860,  2.0657, -0.8137],\n",
      "        [-0.1730,  1.1800,  0.3449,  1.1795,  1.5887],\n",
      "        [ 1.1126,  0.4843, -0.2694,  0.5510,  2.0362],\n",
      "        [-0.1326,  1.3433,  1.9560, -0.0637,  0.7457]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7083, -1.2001, -0.7066, -0.4510, -1.8135],\n",
      "        [ 0.3325, -0.4777, -0.2798, -0.3170, -0.5130],\n",
      "        [-0.3629, -0.5419, -1.0087, -1.0622, -1.6309],\n",
      "        [ 0.2266, -0.3370,  0.0304, -0.2964, -0.2230]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9571, -0.0137, -0.1860,  2.0657, -0.8137],\n",
      "        [-0.1730,  1.1800,  0.3449,  1.1795,  1.5887],\n",
      "        [ 1.1126,  0.4843, -0.2694,  0.5510,  2.0362],\n",
      "        [-0.1326,  1.3433,  1.9560, -0.0637,  0.7457]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3697],\n",
      "        [-1.9066],\n",
      "        [-4.3005],\n",
      "        [-0.5707]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0602,  2.3054, -0.9420, -0.2679,  1.9892],\n",
      "        [-0.0677, -0.3654,  1.1524,  0.4731, -1.3710],\n",
      "        [-1.6929, -0.9383,  2.0168, -0.1288,  0.5040],\n",
      "        [-1.6652,  1.1360,  1.7402,  1.4974,  0.0413]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6857, -0.8873, -1.0404, -1.1208, -2.1556],\n",
      "        [ 1.1863,  0.8616,  0.7165,  1.3195,  0.8707],\n",
      "        [ 1.4969,  1.3634,  1.3769,  0.9644,  1.8784],\n",
      "        [ 0.4041, -0.5657, -0.5020,  0.0706, -0.1065]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0602,  2.3054, -0.9420, -0.2679,  1.9892],\n",
      "        [-0.0677, -0.3654,  1.1524,  0.4731, -1.3710],\n",
      "        [-1.6929, -0.9383,  2.0168, -0.1288,  0.5040],\n",
      "        [-1.6652,  1.1360,  1.7402,  1.4974,  0.0413]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-5.0121],\n",
      "        [-0.1389],\n",
      "        [-0.2139],\n",
      "        [-2.0877]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.5382,  0.9204, -0.1138,  0.3322,  0.1355],\n",
      "        [ 0.4816,  1.4522, -0.2594,  1.0107, -1.4930],\n",
      "        [ 1.1963,  0.5132,  1.2804,  0.6571,  1.5274],\n",
      "        [-0.0393, -0.4809,  1.2487,  0.4339, -0.6913]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9126,  1.8003,  1.2992,  1.0330,  1.3638],\n",
      "        [ 1.2132,  1.8599,  0.9251,  0.6547,  0.8828],\n",
      "        [ 1.6505,  1.6748,  1.5805,  0.8920,  1.9126],\n",
      "        [ 0.5431,  0.9979,  1.0326,  0.8625,  1.6487]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.5382,  0.9204, -0.1138,  0.3322,  0.1355],\n",
      "        [ 0.4816,  1.4522, -0.2594,  1.0107, -1.4930],\n",
      "        [ 1.1963,  0.5132,  1.2804,  0.6571,  1.5274],\n",
      "        [-0.0393, -0.4809,  1.2487,  0.4339, -0.6913]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2793],\n",
      "        [ 2.3890],\n",
      "        [ 8.3651],\n",
      "        [ 0.0227]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4460,  0.4592, -2.1440, -0.1913, -1.9468],\n",
      "        [ 0.8867,  2.4328,  1.8144,  0.2524, -1.3202],\n",
      "        [-1.2304,  0.4734,  1.4643,  1.0910, -0.6585],\n",
      "        [-0.0072, -0.0639, -0.0414, -0.2795, -0.7854]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1302,  1.3777,  1.3166,  1.0213,  2.0977],\n",
      "        [ 0.3015, -0.3766, -0.3232, -0.4517, -0.0687],\n",
      "        [-1.3536, -1.5225, -1.6252, -2.2117, -3.8454],\n",
      "        [ 0.6346,  0.7958,  1.2739,  0.3989,  0.7906]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4460,  0.4592, -2.1440, -0.1913, -1.9468],\n",
      "        [ 0.8867,  2.4328,  1.8144,  0.2524, -1.3202],\n",
      "        [-1.2304,  0.4734,  1.4643,  1.0910, -0.6585],\n",
      "        [-0.0072, -0.0639, -0.0414, -0.2795, -0.7854]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-5.9652],\n",
      "        [-1.2585],\n",
      "        [-1.3159],\n",
      "        [-0.8406]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2171,  0.6725,  1.7054, -2.0140,  2.0635],\n",
      "        [-0.0795, -0.8583, -0.4006, -0.9570,  0.4761],\n",
      "        [ 0.1259,  0.4413, -0.2172,  0.9122,  0.9401],\n",
      "        [ 0.8963,  0.3676,  1.1979, -0.1358,  0.4530]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 3.1072,  2.8145,  3.0149,  2.5096,  4.3811],\n",
      "        [ 0.7409,  0.0629, -0.3138,  0.2260,  0.6126],\n",
      "        [-0.5715, -1.4636, -1.5104, -1.5083, -2.2622],\n",
      "        [ 0.9844,  0.8232,  0.9324,  1.1725,  0.5019]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2171,  0.6725,  1.7054, -2.0140,  2.0635],\n",
      "        [-0.0795, -0.8583, -0.4006, -0.9570,  0.4761],\n",
      "        [ 0.1259,  0.4413, -0.2172,  0.9122,  0.9401],\n",
      "        [ 0.8963,  0.3676,  1.1979, -0.1358,  0.4530]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 11.6954],\n",
      "        [  0.0883],\n",
      "        [ -3.8925],\n",
      "        [  2.3701]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2219,  0.1394,  0.1178, -1.2159, -0.2654],\n",
      "        [ 0.4955, -0.3651, -0.2417, -1.8131, -0.4190],\n",
      "        [ 0.4995,  0.0391, -0.0903,  0.6343,  0.5405],\n",
      "        [-1.5072, -1.0879, -0.3983,  0.2678, -0.2415]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0905,  1.0907,  0.3715, -0.1984, -0.2946],\n",
      "        [ 0.2103,  0.0941,  0.5155, -0.1989,  0.4537],\n",
      "        [ 0.6585,  0.4573,  0.3562, -0.1930,  0.6757],\n",
      "        [-0.0850, -0.3096, -0.4450, -0.2270, -1.0676]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2219,  0.1394,  0.1178, -1.2159, -0.2654],\n",
      "        [ 0.4955, -0.3651, -0.2417, -1.8131, -0.4190],\n",
      "        [ 0.4995,  0.0391, -0.0903,  0.6343,  0.5405],\n",
      "        [-1.5072, -1.0879, -0.3983,  0.2678, -0.2415]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4047],\n",
      "        [ 0.1159],\n",
      "        [ 0.5575],\n",
      "        [ 0.8393]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.0057,  0.4877,  1.6699,  0.4805, -1.7104],\n",
      "        [ 0.6052, -1.0445,  0.7249, -1.1278,  0.2972],\n",
      "        [ 1.1394, -0.8863,  0.5758, -1.4245,  0.8152],\n",
      "        [-0.7440, -0.2940,  2.1893,  1.0096,  0.2795]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0834, -0.1420,  0.6552,  0.6392,  0.2484],\n",
      "        [ 0.2341,  0.7019,  0.2879, -0.0652, -0.0516],\n",
      "        [ 0.0225, -0.5937, -0.3146, -0.3715, -0.9046],\n",
      "        [ 0.2545, -0.6790, -0.8220, -0.9594, -0.2180]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.0057,  0.4877,  1.6699,  0.4805, -1.7104],\n",
      "        [ 0.6052, -1.0445,  0.7249, -1.1278,  0.2972],\n",
      "        [ 1.1394, -0.8863,  0.5758, -1.4245,  0.8152],\n",
      "        [-0.7440, -0.2940,  2.1893,  1.0096,  0.2795]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0745],\n",
      "        [-0.3246],\n",
      "        [ 0.1626],\n",
      "        [-2.8190]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4660,  1.4171, -1.4456,  1.1945,  0.4045],\n",
      "        [ 1.0356, -2.1630, -0.4984,  0.5555, -0.0641],\n",
      "        [ 0.9850, -0.3755,  1.1841,  1.4265,  0.2690],\n",
      "        [ 0.3897,  0.3393, -0.5216,  0.7316,  1.6410]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7831, -0.0406, -0.1320, -0.2247, -0.8479],\n",
      "        [ 0.1323,  0.3603,  0.0380,  0.2380,  0.4330],\n",
      "        [ 0.5166, -0.5177, -1.2329, -0.6423, -0.6299],\n",
      "        [ 1.5810,  1.0896,  0.9272,  1.1986,  1.6451]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4660,  1.4171, -1.4456,  1.1945,  0.4045],\n",
      "        [ 1.0356, -2.1630, -0.4984,  0.5555, -0.0641],\n",
      "        [ 0.9850, -0.3755,  1.1841,  1.4265,  0.2690],\n",
      "        [ 0.3897,  0.3393, -0.5216,  0.7316,  1.6410]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1132],\n",
      "        [-0.5569],\n",
      "        [-1.8425],\n",
      "        [ 4.0785]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9020, -0.3775,  0.1346,  0.0195,  0.6467],\n",
      "        [-2.0095, -0.2519,  1.5144, -1.1509,  0.5746],\n",
      "        [ 0.1046,  0.4865, -0.5335,  1.8947, -0.2154],\n",
      "        [-0.9622, -0.6361,  0.4712, -1.6794, -0.0448]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2141,  0.1980, -0.4365, -0.2044,  0.2334],\n",
      "        [ 0.1136,  0.2873, -0.2418,  0.2611,  0.0847],\n",
      "        [ 0.9159,  1.0420,  0.0734,  0.3956,  0.8638],\n",
      "        [-0.4151, -1.4527,  0.1457, -1.0908, -1.3560]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9020, -0.3775,  0.1346,  0.0195,  0.6467],\n",
      "        [-2.0095, -0.2519,  1.5144, -1.1509,  0.5746],\n",
      "        [ 0.1046,  0.4865, -0.5335,  1.8947, -0.2154],\n",
      "        [-0.9622, -0.6361,  0.4712, -1.6794, -0.0448]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2066],\n",
      "        [-0.9187],\n",
      "        [ 1.1270],\n",
      "        [ 3.2847]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2839, -1.2783,  0.0433,  2.3285,  0.1854],\n",
      "        [ 0.3735,  0.1553, -1.2850, -0.4529, -0.9573],\n",
      "        [ 0.7279, -0.0705, -0.5571, -1.5011,  0.2506],\n",
      "        [-0.4803, -1.5646,  1.7377, -0.4447,  0.0906]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3289, -0.0537,  0.2001,  0.2953,  0.3284],\n",
      "        [ 0.8187,  0.4437,  1.0082,  0.4454,  0.6502],\n",
      "        [-0.0611, -0.6065,  0.0422, -0.4247, -0.5575],\n",
      "        [-1.2263, -1.0351, -1.6542, -2.0686, -3.2421]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2839, -1.2783,  0.0433,  2.3285,  0.1854],\n",
      "        [ 0.3735,  0.1553, -1.2850, -0.4529, -0.9573],\n",
      "        [ 0.7279, -0.0705, -0.5571, -1.5011,  0.2506],\n",
      "        [-0.4803, -1.5646,  1.7377, -0.4447,  0.0906]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7324],\n",
      "        [-1.7451],\n",
      "        [ 0.4726],\n",
      "        [-0.0400]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4682,  0.5666,  1.8587, -1.6250,  2.0788],\n",
      "        [ 0.1864,  0.1557, -1.3362,  1.1518,  2.4460],\n",
      "        [-0.8310, -0.7389,  0.9661, -0.2383, -0.3608],\n",
      "        [-1.9105,  0.2602, -0.9540, -0.0384, -1.2901]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2922,  0.0264, -0.2669,  0.0821,  0.3189],\n",
      "        [ 1.1029,  0.4359,  1.7233,  0.7610,  1.2135],\n",
      "        [ 0.3974, -0.7435, -0.2819, -1.0324, -0.4988],\n",
      "        [-1.3063, -1.3264, -2.0772, -2.1295, -2.9883]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4682,  0.5666,  1.8587, -1.6250,  2.0788],\n",
      "        [ 0.1864,  0.1557, -1.3362,  1.1518,  2.4460],\n",
      "        [-0.8310, -0.7389,  0.9661, -0.2383, -0.3608],\n",
      "        [-1.9105,  0.2602, -0.9540, -0.0384, -1.2901]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0885],\n",
      "        [ 1.8154],\n",
      "        [ 0.3727],\n",
      "        [ 8.0695]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7816,  1.3937,  1.1456, -0.4936, -0.1719],\n",
      "        [ 0.1651,  0.0403, -2.2582, -1.4488,  0.5949],\n",
      "        [ 0.3564,  0.8507,  0.9084,  0.2614,  0.4221],\n",
      "        [ 0.1012,  0.0710,  1.6537, -0.3900,  1.2603]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2544,  0.4023,  0.5529, -0.0335,  0.0518],\n",
      "        [ 0.2738, -0.0650, -0.0165, -0.1844,  0.7630],\n",
      "        [ 0.2990, -1.0143, -0.1854, -0.4839, -0.9734],\n",
      "        [ 0.1976,  0.4975, -0.2004,  0.2271,  0.0971]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7816,  1.3937,  1.1456, -0.4936, -0.1719],\n",
      "        [ 0.1651,  0.0403, -2.2582, -1.4488,  0.5949],\n",
      "        [ 0.3564,  0.8507,  0.9084,  0.2614,  0.4221],\n",
      "        [ 0.1012,  0.0710,  1.6537, -0.3900,  1.2603]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0029],\n",
      "        [ 0.8009],\n",
      "        [-1.4621],\n",
      "        [-0.2422]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0290,  0.4407,  0.4479, -0.2118,  0.5153],\n",
      "        [-0.3807,  0.6960, -0.7643,  1.0885, -1.7250],\n",
      "        [-0.8839,  0.9471,  1.1332, -1.4773,  0.7211],\n",
      "        [-0.1632, -1.0262, -0.2335,  0.0363, -0.2607]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1760, -0.5593, -0.3994,  0.1255, -0.7370],\n",
      "        [ 0.7484,  0.7460,  0.1692,  0.0557,  0.2313],\n",
      "        [ 0.4390,  0.0924,  0.1662, -0.2554,  0.2469],\n",
      "        [-0.3781,  0.3858, -0.1073, -0.1492, -0.1820]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0290,  0.4407,  0.4479, -0.2118,  0.5153],\n",
      "        [-0.3807,  0.6960, -0.7643,  1.0885, -1.7250],\n",
      "        [-0.8839,  0.9471,  1.1332, -1.4773,  0.7211],\n",
      "        [-0.1632, -1.0262, -0.2335,  0.0363, -0.2607]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6506],\n",
      "        [-0.2334],\n",
      "        [ 0.4431],\n",
      "        [-0.2671]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9954, -1.2801,  1.2961,  0.8807, -0.4923],\n",
      "        [ 1.4444, -0.1010, -0.1835,  0.6410, -1.4348],\n",
      "        [-0.3851, -1.7384, -0.0977,  1.0541,  0.7873],\n",
      "        [-0.6514,  0.8653,  0.8680, -0.6020,  0.1127]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1897,  0.5334, -0.3630, -0.0941,  0.6149],\n",
      "        [ 0.2747,  0.2199,  0.6306,  0.4952,  0.2031],\n",
      "        [ 0.7634, -1.0093, -0.1731, -0.4490, -0.0591],\n",
      "        [ 0.0718, -0.3140,  0.6227, -0.7122,  0.3028]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9954, -1.2801,  1.2961,  0.8807, -0.4923],\n",
      "        [ 1.4444, -0.1010, -0.1835,  0.6410, -1.4348],\n",
      "        [-0.3851, -1.7384, -0.0977,  1.0541,  0.7873],\n",
      "        [-0.6514,  0.8653,  0.8680, -0.6020,  0.1127]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7277],\n",
      "        [ 0.2848],\n",
      "        [ 0.9577],\n",
      "        [ 0.6849]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5356, -0.9140,  0.3186, -0.5240, -0.4013],\n",
      "        [-1.2143, -0.4932,  1.3970, -1.1783,  0.6158],\n",
      "        [-1.4327, -0.1078, -0.7950, -0.4448, -0.5587],\n",
      "        [ 1.0241, -0.3804,  0.7421, -0.9341,  1.7062]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4549,  1.1462,  0.1158,  0.6087,  0.4214],\n",
      "        [ 0.4802,  1.2486,  0.4088,  0.2865,  0.6655],\n",
      "        [ 0.7348, -0.5816, -1.2472, -0.1316, -1.0734],\n",
      "        [ 0.6525, -0.3461,  0.0393, -0.0455, -0.0639]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5356, -0.9140,  0.3186, -0.5240, -0.4013],\n",
      "        [-1.2143, -0.4932,  1.3970, -1.1783,  0.6158],\n",
      "        [-1.4327, -0.1078, -0.7950, -0.4448, -0.5587],\n",
      "        [ 1.0241, -0.3804,  0.7421, -0.9341,  1.7062]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8003],\n",
      "        [-0.5557],\n",
      "        [ 0.6598],\n",
      "        [ 0.7625]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1416,  0.0701, -0.6243,  0.2195,  0.1710],\n",
      "        [-0.9921, -1.4548,  0.9043,  0.5205,  1.7864],\n",
      "        [ 0.2373, -0.7755, -0.0833, -0.7241,  0.7758],\n",
      "        [-1.8406, -0.2067,  0.0363,  0.0991, -1.2492]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2287,  0.5615,  1.5215,  0.9942,  1.7548],\n",
      "        [ 0.3445,  0.5450,  0.7193,  1.5230,  0.7505],\n",
      "        [-0.6302,  0.3678, -1.0679, -0.6266, -1.1291],\n",
      "        [ 0.2685, -0.0539, -0.1391, -0.0480, -0.3477]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1416,  0.0701, -0.6243,  0.2195,  0.1710],\n",
      "        [-0.9921, -1.4548,  0.9043,  0.5205,  1.7864],\n",
      "        [ 0.2373, -0.7755, -0.0833, -0.7241,  0.7758],\n",
      "        [-1.8406, -0.2067,  0.0363,  0.0991, -1.2492]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.0237],\n",
      "        [ 1.6494],\n",
      "        [-0.7681],\n",
      "        [-0.0586]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6751, -0.5048, -0.6262, -0.9666, -0.1122],\n",
      "        [ 1.2589, -0.5263,  0.0039, -0.5934, -1.2951],\n",
      "        [-0.7966,  1.6717, -0.2480, -0.2956, -0.1031],\n",
      "        [-0.8345, -0.5989, -0.0274,  0.7669, -2.0549]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.7070,  2.8419,  2.0302,  2.1004,  2.4550],\n",
      "        [-0.5373,  0.0682, -0.5510,  0.2705, -1.1735],\n",
      "        [-0.0628, -0.8707, -0.0855, -0.8275, -0.3756],\n",
      "        [ 0.4263,  0.4213,  0.5058,  0.3248, -0.0120]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6751, -0.5048, -0.6262, -0.9666, -0.1122],\n",
      "        [ 1.2589, -0.5263,  0.0039, -0.5934, -1.2951],\n",
      "        [-0.7966,  1.6717, -0.2480, -0.2956, -0.1031],\n",
      "        [-0.8345, -0.5989, -0.0274,  0.7669, -2.0549]], device='cuda:0') torch.Size([4, 5])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:\n",
      "tensor([[-2.1522],\n",
      "        [ 0.6448],\n",
      "        [-1.1011],\n",
      "        [-0.3481]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5062, -1.7255, -0.7981,  0.4883,  0.1280],\n",
      "        [ 0.2789,  0.1946,  1.1488,  0.7087,  0.6793],\n",
      "        [ 1.0487,  1.9057,  0.4622, -0.8229,  0.0203],\n",
      "        [-0.9020,  0.9917, -0.1048,  0.9431,  0.7765]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1443,  0.1595,  0.3653,  0.1768, -0.3015],\n",
      "        [-0.6501,  0.0237,  0.1467,  0.0274, -0.8145],\n",
      "        [ 0.7562, -0.4667,  0.1205,  0.0868,  0.1944],\n",
      "        [ 0.2320, -0.2394, -0.3304, -0.0008, -0.1056]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5062, -1.7255, -0.7981,  0.4883,  0.1280],\n",
      "        [ 0.2789,  0.1946,  1.1488,  0.7087,  0.6793],\n",
      "        [ 1.0487,  1.9057,  0.4622, -0.8229,  0.0203],\n",
      "        [-0.9020,  0.9917, -0.1048,  0.9431,  0.7765]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5921],\n",
      "        [-0.5420],\n",
      "        [-0.1081],\n",
      "        [-0.4949]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5709, -0.7635, -0.1538, -0.6365,  1.1551],\n",
      "        [ 1.1146, -0.7369, -0.4269,  0.1832, -0.7677],\n",
      "        [-1.0458, -0.4878,  1.5185, -0.1804, -0.6646],\n",
      "        [-0.1692,  0.3349, -2.1284, -0.1002,  0.8446]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6948, -0.2739, -0.8554, -0.2744, -0.1471],\n",
      "        [ 0.1720, -0.0700,  0.4879,  0.1468,  0.7047],\n",
      "        [ 0.2319, -0.1552, -0.7197, -0.1695,  0.0548],\n",
      "        [ 0.2238, -0.3029, -0.6001, -0.5890, -0.8270]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5709, -0.7635, -0.1538, -0.6365,  1.1551],\n",
      "        [ 1.1146, -0.7369, -0.4269,  0.1832, -0.7677],\n",
      "        [-1.0458, -0.4878,  1.5185, -0.1804, -0.6646],\n",
      "        [-0.1692,  0.3349, -2.1284, -0.1002,  0.8446]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7421],\n",
      "        [-0.4791],\n",
      "        [-1.2654],\n",
      "        [ 0.4985]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8608,  0.7095,  1.1417,  1.6930,  0.3895],\n",
      "        [ 1.2935, -0.5554,  1.9973,  0.1247, -0.2691],\n",
      "        [-0.0544,  0.3211, -0.6107,  2.1804, -0.0377],\n",
      "        [ 0.5427,  0.8965, -1.1274, -1.3433,  0.9935]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3125, -0.6856,  0.0038,  0.1147, -0.6304],\n",
      "        [ 0.7975, -0.2245,  1.2641,  0.9354,  0.9635],\n",
      "        [ 0.7839,  0.5620,  0.1217,  0.1964,  0.2810],\n",
      "        [ 0.1855, -0.4905, -0.1258, -0.6508, -0.4068]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8608,  0.7095,  1.1417,  1.6930,  0.3895],\n",
      "        [ 1.2935, -0.5554,  1.9973,  0.1247, -0.2691],\n",
      "        [-0.0544,  0.3211, -0.6107,  2.1804, -0.0377],\n",
      "        [ 0.5427,  0.8965, -1.1274, -1.3433,  0.9935]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2644],\n",
      "        [ 3.5384],\n",
      "        [ 0.4811],\n",
      "        [ 0.2728]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6077, -0.2719,  0.7175, -0.4801,  0.5826],\n",
      "        [ 0.6275,  0.0162,  0.0817, -1.6236, -0.5792],\n",
      "        [-0.4171, -0.4231,  0.2532, -0.5814,  1.3117],\n",
      "        [-0.4381, -0.7821,  0.8181, -0.9358,  1.7078]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2608, -0.2981, -0.0313, -0.3262, -0.1294],\n",
      "        [-0.9161, -1.1691, -1.3078, -1.8102, -2.3206],\n",
      "        [ 0.3087, -0.4041, -1.1646, -0.0442, -0.7150],\n",
      "        [ 0.1654,  0.1350, -0.2475,  0.2597, -0.5315]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6077, -0.2719,  0.7175, -0.4801,  0.5826],\n",
      "        [ 0.6275,  0.0162,  0.0817, -1.6236, -0.5792],\n",
      "        [-0.4171, -0.4231,  0.2532, -0.5814,  1.3117],\n",
      "        [-0.4381, -0.7821,  0.8181, -0.9358,  1.7078]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0187],\n",
      "        [ 3.5826],\n",
      "        [-1.1647],\n",
      "        [-1.5312]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3654,  0.6485,  1.6894,  1.0871,  0.9171],\n",
      "        [ 0.3124, -0.4650,  0.2396,  1.7297,  0.9601],\n",
      "        [-1.9990, -0.7565,  0.4011, -0.9569,  0.2922],\n",
      "        [-0.4163, -0.0704, -0.1559,  0.3810, -0.1385]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0489,  0.1042, -0.3965,  0.4599,  0.5918],\n",
      "        [-1.7862, -2.6046, -2.3100, -2.8417, -3.9908],\n",
      "        [ 0.7520,  0.9413, -0.2214, -0.2901,  0.9093],\n",
      "        [ 0.5985,  0.8737,  1.3088,  0.5097,  0.9500]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3654,  0.6485,  1.6894,  1.0871,  0.9171],\n",
      "        [ 0.3124, -0.4650,  0.2396,  1.7297,  0.9601],\n",
      "        [-1.9990, -0.7565,  0.4011, -0.9569,  0.2922],\n",
      "        [-0.4163, -0.0704, -0.1559,  0.3810, -0.1385]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4226],\n",
      "        [-8.6472],\n",
      "        [-1.7609],\n",
      "        [-0.4521]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4336, -0.2520, -1.3448,  0.3773, -0.9575],\n",
      "        [ 0.4023, -0.5275, -1.0033,  0.3087, -0.2358],\n",
      "        [-0.8134,  0.4163,  1.6760,  1.4656, -0.6950],\n",
      "        [-0.3936, -0.1583, -1.2622,  1.0834, -0.1983]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3242,  0.4796,  0.2393,  0.0022,  0.0443],\n",
      "        [ 0.8911,  0.5972,  1.1894,  0.7760,  1.4584],\n",
      "        [ 1.5356,  0.8291,  0.4764, -0.0146,  1.6186],\n",
      "        [ 0.9722,  0.9906,  0.1785,  1.2097,  1.4780]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4336, -0.2520, -1.3448,  0.3773, -0.9575],\n",
      "        [ 0.4023, -0.5275, -1.0033,  0.3087, -0.2358],\n",
      "        [-0.8134,  0.4163,  1.6760,  1.4656, -0.6950],\n",
      "        [-0.3936, -0.1583, -1.2622,  1.0834, -0.1983]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3437],\n",
      "        [-1.2543],\n",
      "        [-1.2520],\n",
      "        [ 0.2528]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2934, -0.7738,  0.7296, -1.3803, -0.6551],\n",
      "        [ 0.2342,  0.3416,  1.8935, -1.6148,  1.5322],\n",
      "        [-0.1192, -0.3341,  0.9613,  0.8212,  1.1096],\n",
      "        [ 0.7113,  0.5970,  1.7942, -0.7378,  1.0144]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3615, -0.2610, -0.1818,  0.2232, -0.4132],\n",
      "        [ 0.7356,  1.4953,  0.9895,  0.6822,  1.7546],\n",
      "        [ 1.5031,  0.8813,  1.8219,  1.2829,  1.4105],\n",
      "        [ 0.0305, -0.2465,  0.7961, -0.4447,  0.1335]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2934, -0.7738,  0.7296, -1.3803, -0.6551],\n",
      "        [ 0.2342,  0.3416,  1.8935, -1.6148,  1.5322],\n",
      "        [-0.1192, -0.3341,  0.9613,  0.8212,  1.1096],\n",
      "        [ 0.7113,  0.5970,  1.7942, -0.7378,  1.0144]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1381],\n",
      "        [ 4.1434],\n",
      "        [ 3.8963],\n",
      "        [ 1.7663]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0369,  0.2747,  1.9407, -1.0634, -0.6282],\n",
      "        [-1.6000,  0.3484,  0.9557,  1.9493, -0.3463],\n",
      "        [-0.6213, -1.2546,  0.2181,  0.0678, -1.7321],\n",
      "        [ 0.4484,  0.2250,  1.3774, -0.9287, -0.6633]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0526,  0.2011, -0.5495, -0.6035,  0.2806],\n",
      "        [-0.1634, -0.5496, -0.7449, -0.5401, -1.5950],\n",
      "        [ 0.1108, -0.4183, -0.2911, -0.2072, -0.6204],\n",
      "        [-0.7343, -0.9762, -0.8461, -0.7568, -1.0354]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0369,  0.2747,  1.9407, -1.0634, -0.6282],\n",
      "        [-1.6000,  0.3484,  0.9557,  1.9493, -0.3463],\n",
      "        [-0.6213, -1.2546,  0.2181,  0.0678, -1.7321],\n",
      "        [ 0.4484,  0.2250,  1.3774, -0.9287, -0.6633]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5477],\n",
      "        [-1.1423],\n",
      "        [ 1.4529],\n",
      "        [-0.3247]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3830,  0.6893,  0.7691,  0.5277,  0.4945],\n",
      "        [-0.1874,  0.3614, -0.0475, -0.4488, -0.9763],\n",
      "        [-0.5943, -1.1465,  0.4008,  0.3156,  0.1132],\n",
      "        [-0.7229, -0.8099, -1.1819, -0.2173,  1.2513]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3273, -0.2968, -0.1868, -0.2404, -0.3254],\n",
      "        [ 0.5207, -1.1727, -0.0168, -0.2836, -0.2516],\n",
      "        [-0.4976, -0.9491, -0.7668, -0.8956, -1.3452],\n",
      "        [ 0.0332,  0.2825, -0.1467, -0.2563,  0.0945]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3830,  0.6893,  0.7691,  0.5277,  0.4945],\n",
      "        [-0.1874,  0.3614, -0.0475, -0.4488, -0.9763],\n",
      "        [-0.5943, -1.1465,  0.4008,  0.3156,  0.1132],\n",
      "        [-0.7229, -0.8099, -1.1819, -0.2173,  1.2513]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1833],\n",
      "        [-0.1477],\n",
      "        [ 0.6415],\n",
      "        [ 0.0945]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1540,  1.2547,  0.7467, -0.1481, -0.2458],\n",
      "        [-0.4731,  0.3810,  1.8894, -1.1016, -1.0630],\n",
      "        [ 0.2927, -0.8304, -0.2649,  0.8140,  1.8967],\n",
      "        [-0.4250, -0.8320,  0.1665, -0.0550, -0.3037]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0325,  0.2145, -0.3973,  0.0425, -0.1390],\n",
      "        [ 0.4135, -0.4017, -0.7626, -0.8112, -1.1751],\n",
      "        [-0.1415, -0.9778, -0.7225, -0.5328, -1.3579],\n",
      "        [-0.2006, -0.2348,  0.1189,  0.1398, -0.1766]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1540,  1.2547,  0.7467, -0.1481, -0.2458],\n",
      "        [-0.4731,  0.3810,  1.8894, -1.1016, -1.0630],\n",
      "        [ 0.2927, -0.8304, -0.2649,  0.8140,  1.8967],\n",
      "        [-0.4250, -0.8320,  0.1665, -0.0550, -0.3037]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0053],\n",
      "        [ 0.3532],\n",
      "        [-2.0472],\n",
      "        [ 0.3464]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5292, -0.5315,  0.9636, -1.5676, -0.8626],\n",
      "        [-0.5644, -0.6057,  0.9227, -0.3913,  1.6814],\n",
      "        [-1.1478,  1.2143,  2.1249, -0.4945,  0.4963],\n",
      "        [-0.1357,  1.0873,  0.6052,  0.2432,  2.5818]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4550, -0.2257,  0.1434, -0.2131, -0.0976],\n",
      "        [-0.0161, -0.2428,  0.4497, -0.3491, -0.1991],\n",
      "        [ 1.0043,  0.4840,  0.9103,  0.5656,  0.4272],\n",
      "        [ 0.3729, -0.1491,  0.0392,  0.1607,  0.3753]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5292, -0.5315,  0.9636, -1.5676, -0.8626],\n",
      "        [-0.5644, -0.6057,  0.9227, -0.3913,  1.6814],\n",
      "        [-1.1478,  1.2143,  2.1249, -0.4945,  0.4963],\n",
      "        [-0.1357,  1.0873,  0.6052,  0.2432,  2.5818]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0194],\n",
      "        [ 0.3729],\n",
      "        [ 1.3016],\n",
      "        [ 0.8190]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5492, -0.7953,  0.2695,  1.8096,  0.1342],\n",
      "        [-1.0768, -0.2978, -0.4056,  0.3409,  0.2304],\n",
      "        [ 0.1649,  2.3000,  1.0384,  0.6962,  0.0024],\n",
      "        [ 0.2664, -0.1888,  0.5048, -0.2257,  1.0733]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3739,  0.3568,  0.0632,  0.0971,  0.2910],\n",
      "        [ 0.2686, -0.9491, -0.0391, -0.0512, -0.8748],\n",
      "        [ 0.6323,  0.0787, -0.2332,  0.0049, -0.1093],\n",
      "        [-0.0099,  0.8553,  0.6183, -0.0029,  0.0674]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5492, -0.7953,  0.2695,  1.8096,  0.1342],\n",
      "        [-1.0768, -0.2978, -0.4056,  0.3409,  0.2304],\n",
      "        [ 0.1649,  2.3000,  1.0384,  0.6962,  0.0024],\n",
      "        [ 0.2664, -0.1888,  0.5048, -0.2257,  1.0733]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2573],\n",
      "        [-0.2097],\n",
      "        [ 0.0464],\n",
      "        [ 0.2210]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2298,  0.4398, -0.8215, -0.4246,  0.8293],\n",
      "        [-0.7989,  1.5414, -0.9863, -0.9246,  1.0431],\n",
      "        [-0.6979,  0.5886,  0.1442,  0.6887,  1.5407],\n",
      "        [-0.4584,  0.9601,  0.8112, -1.2403, -0.6443]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3514,  0.4434, -0.2919,  0.0721, -0.0053],\n",
      "        [ 0.7492, -0.1318, -0.9658,  0.1892, -0.4520],\n",
      "        [ 0.7602,  0.0988,  0.2058,  0.7487,  0.4154],\n",
      "        [-0.3027, -0.0017,  0.1223,  0.2990,  0.2506]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2298,  0.4398, -0.8215, -0.4246,  0.8293],\n",
      "        [-0.7989,  1.5414, -0.9863, -0.9246,  1.0431],\n",
      "        [-0.6979,  0.5886,  0.1442,  0.6887,  1.5407],\n",
      "        [-0.4584,  0.9601,  0.8112, -1.2403, -0.6443]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3190],\n",
      "        [-0.4954],\n",
      "        [ 0.7130],\n",
      "        [-0.2961]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3537, -0.2616,  0.6488,  0.6684, -0.5924],\n",
      "        [ 1.3007, -0.9998,  1.5093, -0.0621,  0.6251],\n",
      "        [ 0.4697,  2.3481,  0.0427,  0.7790,  0.1441],\n",
      "        [ 0.3996,  0.6113,  0.2492,  0.7952, -0.1122]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3168,  0.1896,  0.6500, -0.7192, -0.6198],\n",
      "        [-0.1668, -0.4103, -0.8563, -0.5352, -0.4583],\n",
      "        [ 0.3918,  1.4012,  0.4063,  0.4418,  0.0584],\n",
      "        [ 0.2860, -0.1534, -0.4650,  0.4089,  0.4321]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3537, -0.2616,  0.6488,  0.6684, -0.5924],\n",
      "        [ 1.3007, -0.9998,  1.5093, -0.0621,  0.6251],\n",
      "        [ 0.4697,  2.3481,  0.0427,  0.7790,  0.1441],\n",
      "        [ 0.3996,  0.6113,  0.2492,  0.7952, -0.1122]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1466],\n",
      "        [-1.3524],\n",
      "        [ 3.8441],\n",
      "        [ 0.1814]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3752,  1.2678, -0.8236,  0.4097, -0.9742],\n",
      "        [-1.2266,  1.2245,  0.8706,  0.2573,  1.9642],\n",
      "        [ 1.9431,  0.0894,  0.2188, -0.1256,  0.1397],\n",
      "        [-1.1242,  0.6740,  0.9913,  0.1213,  0.1977]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3686,  0.1111,  0.0528, -0.6899, -0.1674],\n",
      "        [ 0.1116, -0.3551, -0.4750,  0.3587,  0.4033],\n",
      "        [-1.0314, -0.6959, -1.0184, -1.7661, -2.7711],\n",
      "        [ 0.3637, -0.0708,  0.2762,  0.0484, -0.1125]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3752,  1.2678, -0.8236,  0.4097, -0.9742],\n",
      "        [-1.2266,  1.2245,  0.8706,  0.2573,  1.9642],\n",
      "        [ 1.9431,  0.0894,  0.2188, -0.1256,  0.1397],\n",
      "        [-1.1242,  0.6740,  0.9913,  0.1213,  0.1977]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1160],\n",
      "        [-0.1009],\n",
      "        [-2.4545],\n",
      "        [-0.1990]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4617, -0.2897, -0.2227,  0.7137, -0.9967],\n",
      "        [ 0.0530, -0.7476,  0.9305, -0.1716,  1.4948],\n",
      "        [-0.8893,  0.0898,  1.1749, -0.6751,  0.4360],\n",
      "        [-0.0313, -0.3595,  1.4819, -1.0619, -1.5712]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1319, -0.3726,  0.4900,  0.2037, -0.5831],\n",
      "        [ 0.9101, -0.3643, -0.5415, -0.6087, -0.5357],\n",
      "        [-0.3003,  0.3463, -0.3281, -0.2334, -0.7342],\n",
      "        [ 0.0443,  0.4143,  0.1206, -0.4357, -0.2164]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4617, -0.2897, -0.2227,  0.7137, -0.9967],\n",
      "        [ 0.0530, -0.7476,  0.9305, -0.1716,  1.4948],\n",
      "        [-0.8893,  0.0898,  1.1749, -0.6751,  0.4360],\n",
      "        [-0.0313, -0.3595,  1.4819, -1.0619, -1.5712]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5327],\n",
      "        [-0.8796],\n",
      "        [-0.2499],\n",
      "        [ 0.8310]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2979, -0.4349,  0.4874, -0.4434, -0.1182],\n",
      "        [-0.6227, -0.4457, -0.7864,  1.5566,  0.9059],\n",
      "        [-1.0792, -1.4367,  1.2790,  0.9959,  0.6981],\n",
      "        [ 0.2579,  0.8956,  0.2093,  0.7921,  0.2598]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0311, -0.1757,  0.3937, -0.3573,  0.6019],\n",
      "        [ 0.1896, -1.1429, -0.7391, -0.9284, -0.4846],\n",
      "        [ 0.1436,  0.6020, -0.8457,  0.0645,  0.4655],\n",
      "        [-0.2690,  0.0592,  0.4845,  0.1281,  0.6593]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2979, -0.4349,  0.4874, -0.4434, -0.1182],\n",
      "        [-0.6227, -0.4457, -0.7864,  1.5566,  0.9059],\n",
      "        [-1.0792, -1.4367,  1.2790,  0.9959,  0.6981],\n",
      "        [ 0.2579,  0.8956,  0.2093,  0.7921,  0.2598]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3152],\n",
      "        [-0.9115],\n",
      "        [-1.7124],\n",
      "        [ 0.3578]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4356, -0.7755,  0.6780,  0.4859, -0.8777],\n",
      "        [-0.4257, -1.1682, -2.1076,  1.1240, -0.4201],\n",
      "        [ 0.1346, -1.3753,  0.3151,  2.9267,  0.8513],\n",
      "        [-0.8147, -0.7416,  0.5954, -0.3388,  1.1684]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2336,  0.6946, -0.5872,  0.3505,  0.6285],\n",
      "        [ 0.2981, -0.4638, -0.3183, -0.3699, -0.1179],\n",
      "        [ 0.6820,  0.4840,  0.5683,  0.5074,  0.7267],\n",
      "        [-0.1688,  0.3734, -1.0620,  0.3158,  0.5074]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4356, -0.7755,  0.6780,  0.4859, -0.8777],\n",
      "        [-0.4257, -1.1682, -2.1076,  1.1240, -0.4201],\n",
      "        [ 0.1346, -1.3753,  0.3151,  2.9267,  0.8513],\n",
      "        [-0.8147, -0.7416,  0.5954, -0.3388,  1.1684]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9828],\n",
      "        [ 0.7195],\n",
      "        [ 1.7089],\n",
      "        [-0.2859]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8110,  0.6968,  0.6776,  0.0838,  0.2431],\n",
      "        [-0.2503, -0.2161,  0.6143,  1.0212, -1.0984],\n",
      "        [-0.3188,  0.0832,  0.6258, -0.4727,  2.9591],\n",
      "        [ 0.3700, -0.7723,  0.1631, -0.9108,  0.1061]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9118, -0.0388,  0.5555,  0.1125,  0.6786],\n",
      "        [ 0.0737, -0.9808, -0.7211, -0.6364, -0.5863],\n",
      "        [ 0.0547,  0.2742, -0.9438, -0.0835, -0.9838],\n",
      "        [-0.2471, -0.2178, -0.4107, -0.0153,  0.1694]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8110,  0.6968,  0.6776,  0.0838,  0.2431],\n",
      "        [-0.2503, -0.2161,  0.6143,  1.0212, -1.0984],\n",
      "        [-0.3188,  0.0832,  0.6258, -0.4727,  2.9591],\n",
      "        [ 0.3700, -0.7723,  0.1631, -0.9108,  0.1061]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1749],\n",
      "        [-0.2553],\n",
      "        [-3.4570],\n",
      "        [ 0.0418]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1217, -1.1673,  1.3656, -2.4467,  0.0160],\n",
      "        [-0.9998, -0.5113,  0.0391,  1.5388,  1.7632],\n",
      "        [-0.5951,  0.3111, -0.0884, -0.6068,  0.3687],\n",
      "        [-0.9296, -0.5910,  1.1529, -1.2418,  1.6136]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8025, -0.7830,  0.0623, -0.4399, -2.1953],\n",
      "        [ 0.5668, -0.6641, -0.5558, -0.6394, -0.6897],\n",
      "        [ 1.0685,  1.6418,  0.8691,  1.2163,  2.2144],\n",
      "        [-0.1531,  0.4923,  0.6184,  0.3841, -0.1175]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1217, -1.1673,  1.3656, -2.4467,  0.0160],\n",
      "        [-0.9998, -0.5113,  0.0391,  1.5388,  1.7632],\n",
      "        [-0.5951,  0.3111, -0.0884, -0.6068,  0.3687],\n",
      "        [-0.9296, -0.5910,  1.1529, -1.2418,  1.6136]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.9404],\n",
      "        [-2.4489],\n",
      "        [-0.1235],\n",
      "        [-0.1023]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1689, -1.2870, -0.8061, -0.7191,  0.3817],\n",
      "        [ 0.5903,  0.5373,  1.0534, -0.5850,  1.1202],\n",
      "        [ 0.8910, -0.7086, -0.2623, -0.3399, -0.5078],\n",
      "        [-1.0127, -0.0426,  0.0526, -0.2837, -0.8598]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8474, -2.0729, -1.5587, -1.9473, -2.7192],\n",
      "        [ 0.7527,  0.1435,  0.7823, -0.5073,  1.0649],\n",
      "        [ 0.9756,  0.8921,  2.2192,  1.2884,  2.0844],\n",
      "        [ 0.7487,  0.2662,  0.3574,  0.3558, -0.0683]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1689, -1.2870, -0.8061, -0.7191,  0.3817],\n",
      "        [ 0.5903,  0.5373,  1.0534, -0.5850,  1.1202],\n",
      "        [ 0.8910, -0.7086, -0.2623, -0.3399, -0.5078],\n",
      "        [-1.0127, -0.0426,  0.0526, -0.2837, -0.8598]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.2773],\n",
      "        [ 2.8352],\n",
      "        [-1.8412],\n",
      "        [-0.7930]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3182, -0.2609, -0.1174,  0.1536, -1.2998],\n",
      "        [-0.1782,  1.2686, -1.4274, -0.0178, -1.4150],\n",
      "        [-0.5302,  0.1129, -0.2798,  0.1868, -0.2394],\n",
      "        [-0.8799,  0.3165,  0.7835, -0.2699, -0.2019]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6268, -0.1750, -0.0084, -0.2565,  0.3303],\n",
      "        [-0.2700, -0.1879, -0.8225, -0.8297, -1.1504],\n",
      "        [ 1.6060,  2.1888,  1.9862,  1.5546,  2.7113],\n",
      "        [ 0.1962, -0.2795, -0.4092, -0.4102,  0.4044]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3182, -0.2609, -0.1174,  0.1536, -1.2998],\n",
      "        [-0.1782,  1.2686, -1.4274, -0.0178, -1.4150],\n",
      "        [-0.5302,  0.1129, -0.2798,  0.1868, -0.2394],\n",
      "        [-0.8799,  0.3165,  0.7835, -0.2699, -0.2019]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2226],\n",
      "        [ 2.6263],\n",
      "        [-1.5186],\n",
      "        [-0.5526]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7703,  0.5636, -0.6676,  0.5038,  1.5537],\n",
      "        [-1.7885, -0.3001,  0.5033,  1.4607,  0.3198],\n",
      "        [ 0.7464,  0.7030, -1.2459,  0.8682,  0.6991],\n",
      "        [-0.2418, -0.6622, -0.4810, -0.2652, -0.4240]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1628,  0.5191, -0.1302, -0.7612, -0.2225],\n",
      "        [-0.7756, -1.3706, -2.2897, -1.7814, -3.1425],\n",
      "        [-0.2648,  0.0638, -0.5569,  0.3426, -0.2502],\n",
      "        [ 0.6012,  0.2386, -0.2726, -0.2554,  0.2636]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7703,  0.5636, -0.6676,  0.5038,  1.5537],\n",
      "        [-1.7885, -0.3001,  0.5033,  1.4607,  0.3198],\n",
      "        [ 0.7464,  0.7030, -1.2459,  0.8682,  0.6991],\n",
      "        [-0.2418, -0.6622, -0.4810, -0.2652, -0.4240]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4751],\n",
      "        [-2.9613],\n",
      "        [ 0.6636],\n",
      "        [-0.2163]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9821,  1.1953, -0.6147, -1.7895,  1.2308],\n",
      "        [-0.8123,  0.4459, -0.7707, -1.1540,  0.6382],\n",
      "        [-1.0722,  0.5567,  0.3260,  0.3189, -0.6537],\n",
      "        [-0.9652, -0.4251, -0.4793, -0.2749, -0.8402]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0079, -0.2541,  0.3219, -0.2687, -0.0370],\n",
      "        [ 0.4045, -1.0762, -0.7137, -1.1289, -1.5920],\n",
      "        [ 0.0235,  0.6144, -0.0500,  0.2179, -0.5117],\n",
      "        [ 0.6737, -0.2335, -0.7312,  0.0723, -0.1746]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9821,  1.1953, -0.6147, -1.7895,  1.2308],\n",
      "        [-0.8123,  0.4459, -0.7707, -1.1540,  0.6382],\n",
      "        [-1.0722,  0.5567,  0.3260,  0.3189, -0.6537],\n",
      "        [-0.9652, -0.4251, -0.4793, -0.2749, -0.8402]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0741],\n",
      "        [ 0.0283],\n",
      "        [ 0.7044],\n",
      "        [-0.0738]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3109,  0.8910,  0.2343,  2.2981,  0.9887],\n",
      "        [-1.6173, -0.4813,  0.4875, -0.2018,  1.4893],\n",
      "        [ 0.3383,  1.7844, -1.1084,  0.5452, -1.3983],\n",
      "        [-0.7300, -0.1237,  0.5830, -0.1587, -1.2961]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3909, -0.4745,  0.5289, -0.7565,  0.6206],\n",
      "        [ 0.4223, -0.3910, -0.8319, -0.9324, -0.6896],\n",
      "        [ 0.2316,  0.8044,  0.1592,  0.0483, -0.3217],\n",
      "        [ 0.0865, -0.5515,  0.0661, -0.4910, -0.5331]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3109,  0.8910,  0.2343,  2.2981,  0.9887],\n",
      "        [-1.6173, -0.4813,  0.4875, -0.2018,  1.4893],\n",
      "        [ 0.3383,  1.7844, -1.1084,  0.5452, -1.3983],\n",
      "        [-0.7300, -0.1237,  0.5830, -0.1587, -1.2961]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3024],\n",
      "        [-1.7391],\n",
      "        [ 1.8134],\n",
      "        [ 0.8125]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8421,  0.7647,  0.4932, -0.5199, -0.2882],\n",
      "        [ 0.4010,  1.1868, -0.2540, -1.0888, -0.3734],\n",
      "        [ 0.5536,  0.0443,  3.2562, -0.9578, -0.1314],\n",
      "        [ 0.1426,  1.6047, -0.4206,  0.2057,  0.6436]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3566,  0.7373,  0.7348,  0.4614,  0.2688],\n",
      "        [ 1.1471, -0.5873, -0.1241,  0.1406,  0.5567],\n",
      "        [-0.2529, -0.3223, -1.7722, -1.1446, -1.7872],\n",
      "        [ 0.1197, -1.2617,  0.2260,  0.0858, -1.2386]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8421,  0.7647,  0.4932, -0.5199, -0.2882],\n",
      "        [ 0.4010,  1.1868, -0.2540, -1.0888, -0.3734],\n",
      "        [ 0.5536,  0.0443,  3.2562, -0.9578, -0.1314],\n",
      "        [ 0.1426,  1.6047, -0.4206,  0.2057,  0.6436]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3087],\n",
      "        [-0.5664],\n",
      "        [-4.5937],\n",
      "        [-2.8821]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1386, -0.9498,  0.4410, -0.2653, -0.8587],\n",
      "        [-0.4503, -0.2882,  0.0252, -0.8151, -0.9020],\n",
      "        [-0.8339,  1.6531,  0.5287, -0.4807,  0.8453],\n",
      "        [-1.3811,  1.2358, -0.5424,  1.3816,  1.0017]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2460, -0.2733, -0.0861, -0.1068, -0.0575],\n",
      "        [ 0.5234,  0.1025, -0.0641,  0.1489,  0.3364],\n",
      "        [ 1.2194,  1.3762,  1.2028,  1.0514,  1.8257],\n",
      "        [ 0.6881,  0.6851,  1.7746,  1.2205,  1.3519]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1386, -0.9498,  0.4410, -0.2653, -0.8587],\n",
      "        [-0.4503, -0.2882,  0.0252, -0.8151, -0.9020],\n",
      "        [-0.8339,  1.6531,  0.5287, -0.4807,  0.8453],\n",
      "        [-1.3811,  1.2358, -0.5424,  1.3816,  1.0017]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5794],\n",
      "        [-0.6917],\n",
      "        [ 2.9317],\n",
      "        [ 1.9742]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3383,  0.2743,  0.7532, -0.4434, -0.7323],\n",
      "        [-0.2020,  0.5468,  0.6907, -1.0775, -0.1808],\n",
      "        [ 0.4209, -0.2528, -1.1619,  1.0406,  1.1566],\n",
      "        [-1.8344, -0.5787,  0.5615, -0.8188,  0.7373]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0032, -0.2797, -0.1654, -0.1111, -0.0228],\n",
      "        [ 0.6078, -0.6102,  0.2825, -0.0112,  0.6261],\n",
      "        [ 0.2414,  0.7714,  0.8429,  0.1052,  0.4524],\n",
      "        [ 0.6043,  0.7540,  0.0332, -0.3869, -0.2005]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3383,  0.2743,  0.7532, -0.4434, -0.7323],\n",
      "        [-0.2020,  0.5468,  0.6907, -1.0775, -0.1808],\n",
      "        [ 0.4209, -0.2528, -1.1619,  1.0406,  1.1566],\n",
      "        [-1.8344, -0.5787,  0.5615, -0.8188,  0.7373]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1343],\n",
      "        [-0.3626],\n",
      "        [-0.4400],\n",
      "        [-1.3573]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6118, -0.7549, -1.3967,  0.0791,  2.5814],\n",
      "        [-0.6135,  0.9996,  1.5074,  0.2331,  0.5229],\n",
      "        [ 0.0312, -0.0056, -0.5992, -0.1294, -0.1031],\n",
      "        [ 0.4596,  0.5418, -1.1990,  0.0442,  1.2677]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1892,  0.4312,  0.0690, -0.2312, -0.0274],\n",
      "        [ 0.9749, -0.3618, -0.0608, -0.3949,  0.1749],\n",
      "        [-0.2726, -0.0509,  1.0490,  0.3910,  0.9395],\n",
      "        [ 0.7700,  0.7804,  0.3985,  0.4516,  1.2855]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6118, -0.7549, -1.3967,  0.0791,  2.5814],\n",
      "        [-0.6135,  0.9996,  1.5074,  0.2331,  0.5229],\n",
      "        [ 0.0312, -0.0056, -0.5992, -0.1294, -0.1031],\n",
      "        [ 0.4596,  0.5418, -1.1990,  0.0442,  1.2677]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6267],\n",
      "        [-1.0521],\n",
      "        [-0.7842],\n",
      "        [ 1.9486]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1643,  1.4295, -0.2718,  2.3334,  0.8341],\n",
      "        [-2.3001, -1.9579,  2.1684,  0.6728,  1.4590],\n",
      "        [-0.1642,  0.8151,  3.7580,  1.6559, -0.1033],\n",
      "        [ 1.3323,  1.8709,  2.1848, -1.4073,  0.2191]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5814, -0.3956,  0.0835, -0.1706,  0.0373],\n",
      "        [ 0.9728, -0.2252,  0.1452,  0.5468,  0.5102],\n",
      "        [ 0.2247,  0.2828,  0.2328,  0.5389,  0.7668],\n",
      "        [-0.2782, -0.3479, -0.7196, -0.0542, -0.7091]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1643,  1.4295, -0.2718,  2.3334,  0.8341],\n",
      "        [-2.3001, -1.9579,  2.1684,  0.6728,  1.4590],\n",
      "        [-0.1642,  0.8151,  3.7580,  1.6559, -0.1033],\n",
      "        [ 1.3323,  1.8709,  2.1848, -1.4073,  0.2191]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8595],\n",
      "        [-0.3695],\n",
      "        [ 1.8815],\n",
      "        [-2.6727]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7746,  0.4588,  0.6149, -0.0284, -0.8658],\n",
      "        [ 1.2231,  0.6898, -0.0945,  0.6424,  0.6591],\n",
      "        [ 0.4021, -0.4884,  1.4857,  1.8655,  1.1463],\n",
      "        [ 0.2342,  0.6399, -2.4322, -1.0025, -0.0102]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0036, -0.0673,  1.0078,  0.5380,  0.8540],\n",
      "        [ 1.8015, -0.2263, -0.0645, -0.0327,  0.3960],\n",
      "        [-0.0118, -0.4952, -0.9521, -0.1775, -1.1574],\n",
      "        [ 0.6229,  0.6768,  1.3326,  1.0548,  1.2880]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7746,  0.4588,  0.6149, -0.0284, -0.8658],\n",
      "        [ 1.2231,  0.6898, -0.0945,  0.6424,  0.6591],\n",
      "        [ 0.4021, -0.4884,  1.4857,  1.8655,  1.1463],\n",
      "        [ 0.2342,  0.6399, -2.4322, -1.0025, -0.0102]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1595],\n",
      "        [ 2.2933],\n",
      "        [-2.8354],\n",
      "        [-3.7327]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4720,  0.7947,  0.1731,  1.7900,  0.4165],\n",
      "        [ 2.1170,  0.9564,  0.4233, -0.0107,  0.1617],\n",
      "        [ 0.5984,  1.1218,  0.6988,  0.6515, -1.5774],\n",
      "        [-1.4640,  0.7453,  0.1981, -0.2681, -0.7950]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4815, -0.2144, -0.0138, -0.4206,  0.0844],\n",
      "        [-0.4170, -1.1356, -0.9026, -0.4258, -1.6344],\n",
      "        [ 0.7143,  1.4029,  1.3922,  1.0003,  1.4023],\n",
      "        [ 1.8926,  2.1682,  2.4178,  1.6121,  3.1534]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4720,  0.7947,  0.1731,  1.7900,  0.4165],\n",
      "        [ 2.1170,  0.9564,  0.4233, -0.0107,  0.1617],\n",
      "        [ 0.5984,  1.1218,  0.6988,  0.6515, -1.5774],\n",
      "        [-1.4640,  0.7453,  0.1981, -0.2681, -0.7950]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1178],\n",
      "        [-2.6107],\n",
      "        [ 1.4139],\n",
      "        [-3.6150]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1034,  1.4716,  1.2514, -1.1743,  1.3348],\n",
      "        [ 0.5751,  0.8807,  0.1219, -0.2539,  0.3542],\n",
      "        [ 0.4406,  0.4707, -0.2739, -0.9038, -0.1308],\n",
      "        [ 0.6870, -0.4726,  1.1334,  1.5703, -0.6766]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2485,  0.0421, -0.0913,  0.4651,  0.4028],\n",
      "        [ 0.4062, -0.4622, -0.3815,  0.1714,  0.5850],\n",
      "        [ 0.7763,  1.0131,  0.7940,  0.6605,  0.7413],\n",
      "        [-0.0374,  0.2672,  0.3624, -0.6713, -0.1090]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1034,  1.4716,  1.2514, -1.1743,  1.3348],\n",
      "        [ 0.5751,  0.8807,  0.1219, -0.2539,  0.3542],\n",
      "        [ 0.4406,  0.4707, -0.2739, -0.9038, -0.1308],\n",
      "        [ 0.6870, -0.4726,  1.1334,  1.5703, -0.6766]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3350],\n",
      "        [-0.0562],\n",
      "        [-0.0924],\n",
      "        [-0.7216]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7386, -0.5724, -0.6890,  0.7317, -0.7584],\n",
      "        [ 1.7105,  0.6322,  0.3575, -0.0756,  0.4282],\n",
      "        [-0.5009, -1.9177,  1.9977,  1.3987,  1.5915],\n",
      "        [-0.4935,  0.4891,  0.4177,  1.1836, -1.9137]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7501,  0.5800,  0.3336,  0.1777,  0.6587],\n",
      "        [ 1.0696, -0.3443,  0.2387,  0.2176,  0.1542],\n",
      "        [-0.2561,  0.0172,  0.3636,  0.1965,  0.8872],\n",
      "        [ 0.3335,  0.6894, -0.8314, -0.0322,  0.3772]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7386, -0.5724, -0.6890,  0.7317, -0.7584],\n",
      "        [ 1.7105,  0.6322,  0.3575, -0.0756,  0.4282],\n",
      "        [-0.5009, -1.9177,  1.9977,  1.3987,  1.5915],\n",
      "        [-0.4935,  0.4891,  0.4177,  1.1836, -1.9137]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3773],\n",
      "        [ 1.7469],\n",
      "        [ 2.5086],\n",
      "        [-0.9346]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5745,  0.2152,  0.1841,  0.4711,  0.8171],\n",
      "        [-2.0725,  1.2637,  0.2295,  0.9112,  0.6766],\n",
      "        [-0.3490, -0.3531,  1.5871, -1.3114,  0.3861],\n",
      "        [ 1.0953,  2.0933, -0.1630,  0.1908,  0.5699]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7377,  0.2559,  0.7619,  0.1419,  0.5857],\n",
      "        [ 0.0291, -1.2488, -0.8597, -0.8428, -0.9089],\n",
      "        [-0.5425, -0.5068, -0.8338, -1.0776, -1.9291],\n",
      "        [ 0.6328, -0.5148,  0.2054,  0.2612,  1.0637]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5745,  0.2152,  0.1841,  0.4711,  0.8171],\n",
      "        [-2.0725,  1.2637,  0.2295,  0.9112,  0.6766],\n",
      "        [-0.3490, -0.3531,  1.5871, -1.3114,  0.3861],\n",
      "        [ 1.0953,  2.0933, -0.1630,  0.1908,  0.5699]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1646],\n",
      "        [-3.2187],\n",
      "        [-0.2867],\n",
      "        [ 0.2381]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0945,  1.4370, -0.3198,  0.3268, -0.1445],\n",
      "        [ 1.5129,  2.1693, -1.3280,  0.9872, -0.6850],\n",
      "        [-0.3149,  0.6491,  0.6790, -0.5605, -1.1979],\n",
      "        [ 0.0904,  0.4585,  0.7912, -0.3199,  0.3871]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4003, -0.0460, -0.2409, -0.4079, -0.8945],\n",
      "        [ 1.1051,  0.5447,  1.6797,  0.4597,  1.6978],\n",
      "        [-0.1620, -0.1964,  0.2079, -1.1224, -1.1983],\n",
      "        [ 0.6298, -0.5122,  0.5753, -0.6760, -0.1806]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0945,  1.4370, -0.3198,  0.3268, -0.1445],\n",
      "        [ 1.5129,  2.1693, -1.3280,  0.9872, -0.6850],\n",
      "        [-0.3149,  0.6491,  0.6790, -0.5605, -1.1979],\n",
      "        [ 0.0904,  0.4585,  0.7912, -0.3199,  0.3871]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0447],\n",
      "        [-0.0863],\n",
      "        [ 2.1292],\n",
      "        [ 0.4237]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1587, -1.5231,  0.7667,  1.0965,  0.4119],\n",
      "        [ 1.2220, -0.8310, -0.2660, -1.6813,  0.9389],\n",
      "        [ 0.3477,  0.4219, -1.0955, -1.1016,  0.5681],\n",
      "        [ 0.5951, -0.4201,  0.0998, -0.5128,  0.3540]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0131,  0.4162,  0.3170, -0.0768, -0.7628],\n",
      "        [ 1.3926,  0.8348,  1.0039,  0.0523,  1.0803],\n",
      "        [-0.6688, -0.9178, -1.5034, -1.3165, -2.6178],\n",
      "        [ 0.3417, -0.4875, -0.2459,  0.2417, -0.5536]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1587, -1.5231,  0.7667,  1.0965,  0.4119],\n",
      "        [ 1.2220, -0.8310, -0.2660, -1.6813,  0.9389],\n",
      "        [ 0.3477,  0.4219, -1.0955, -1.1016,  0.5681],\n",
      "        [ 0.5951, -0.4201,  0.0998, -0.5128,  0.3540]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7914],\n",
      "        [ 1.6673],\n",
      "        [ 0.9904],\n",
      "        [ 0.0637]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8365, -0.0453,  0.0638,  0.8489,  0.5489],\n",
      "        [-2.1214, -0.3569, -1.5902, -0.9111,  1.5046],\n",
      "        [-1.3506,  0.0221,  0.7230, -0.0961,  1.8456],\n",
      "        [ 0.9910, -0.2269, -0.0377,  0.4238,  1.0826]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4417, -0.5107, -0.1948, -0.4608,  0.4644],\n",
      "        [ 0.0101, -0.0153,  0.4214, -0.0951,  0.1986],\n",
      "        [-0.5070, -1.0712, -1.9510, -1.9444, -2.7190],\n",
      "        [ 0.2279,  0.3179,  0.0212, -0.3793, -0.2301]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8365, -0.0453,  0.0638,  0.8489,  0.5489],\n",
      "        [-2.1214, -0.3569, -1.5902, -0.9111,  1.5046],\n",
      "        [-1.3506,  0.0221,  0.7230, -0.0961,  1.8456],\n",
      "        [ 0.9910, -0.2269, -0.0377,  0.4238,  1.0826]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4950],\n",
      "        [-0.3008],\n",
      "        [-5.5806],\n",
      "        [-0.2570]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3904, -0.2693, -1.0334,  2.2382,  1.3504],\n",
      "        [-0.2586,  0.3484,  1.9982,  1.8810,  1.4172],\n",
      "        [-0.4477,  0.3984, -0.4767,  1.8336,  0.0771],\n",
      "        [-2.0957, -1.1573, -0.2557,  0.2841,  2.1605]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5439, -0.0121,  0.1348, -0.2565,  0.9818],\n",
      "        [ 1.5205,  0.4941, -0.4647,  0.0741, -0.4254],\n",
      "        [ 1.2159,  0.6108,  1.6896,  0.0442,  1.0949],\n",
      "        [ 0.1894,  0.1263, -0.5163, -0.5694, -0.5081]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3904, -0.2693, -1.0334,  2.2382,  1.3504],\n",
      "        [-0.2586,  0.3484,  1.9982,  1.8810,  1.4172],\n",
      "        [-0.4477,  0.3984, -0.4767,  1.8336,  0.0771],\n",
      "        [-2.0957, -1.1573, -0.2557,  0.2841,  2.1605]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4034],\n",
      "        [-1.6131],\n",
      "        [-0.9410],\n",
      "        [-1.6704]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1719, -1.2852,  1.2823, -0.5038,  0.0016],\n",
      "        [ 0.2796,  1.0868,  0.3611,  1.1126,  0.5206],\n",
      "        [-0.7046, -0.0554, -0.9133, -0.2198, -0.0745],\n",
      "        [ 0.5588,  0.9727,  0.8602,  0.7499,  0.6789]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0977,  0.8211, -0.7227,  0.7232, -0.9139],\n",
      "        [ 0.9447,  0.3127,  0.9526,  0.4130,  1.4206],\n",
      "        [ 0.4328, -0.0307,  0.8583,  0.8796,  1.3090],\n",
      "        [ 1.0544,  0.8442,  1.2582,  0.3595,  1.0672]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1719, -1.2852,  1.2823, -0.5038,  0.0016],\n",
      "        [ 0.2796,  1.0868,  0.3611,  1.1126,  0.5206],\n",
      "        [-0.7046, -0.0554, -0.9133, -0.2198, -0.0745],\n",
      "        [ 0.5588,  0.9727,  0.8602,  0.7499,  0.6789]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.3646],\n",
      "        [ 2.1470],\n",
      "        [-1.3780],\n",
      "        [ 3.4867]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0680,  0.4446, -0.7988,  1.4460,  0.9211],\n",
      "        [ 1.2295, -0.5074, -0.9951, -0.4413,  0.4323],\n",
      "        [-0.7792, -0.2844, -1.1085, -0.1833, -0.2686],\n",
      "        [-0.4549, -0.2403,  1.2465,  0.6034, -1.4264]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.8365,  1.1351,  1.3024,  1.7355,  1.5203],\n",
      "        [ 1.1276,  0.1552, -0.4029,  0.1095,  0.6700],\n",
      "        [ 0.6008,  1.0249,  0.6639,  0.5909,  1.8752],\n",
      "        [-0.6369, -0.9665, -0.7903, -1.0259, -1.8802]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0680,  0.4446, -0.7988,  1.4460,  0.9211],\n",
      "        [ 1.2295, -0.5074, -0.9951, -0.4413,  0.4323],\n",
      "        [-0.7792, -0.2844, -1.1085, -0.1833, -0.2686],\n",
      "        [-0.4549, -0.2403,  1.2465,  0.6034, -1.4264]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4128],\n",
      "        [ 1.9498],\n",
      "        [-2.1077],\n",
      "        [ 1.5996]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6679, -1.2318,  0.3123,  0.7837, -0.0073],\n",
      "        [-0.6899,  0.8862,  1.1940, -0.7278,  0.2445],\n",
      "        [-0.7652,  0.9455, -1.0434, -0.8264,  0.9334],\n",
      "        [ 1.5127, -1.0306,  0.0270, -0.8753,  0.8394]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1820,  1.0440,  0.5677,  0.3620,  0.3990],\n",
      "        [-0.8455, -0.6920, -0.6029, -0.9684, -1.6786],\n",
      "        [ 1.4251,  2.2613,  2.7585,  1.0103,  2.1191],\n",
      "        [-0.9939, -1.6302, -1.2496, -1.3981, -2.6521]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6679, -1.2318,  0.3123,  0.7837, -0.0073],\n",
      "        [-0.6899,  0.8862,  1.1940, -0.7278,  0.2445],\n",
      "        [-0.7652,  0.9455, -1.0434, -0.8264,  0.9334],\n",
      "        [ 1.5127, -1.0306,  0.0270, -0.8753,  0.8394]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7064],\n",
      "        [-0.4554],\n",
      "        [-0.6877],\n",
      "        [-0.8593]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5238, -0.2243,  1.3818, -0.2625, -0.4033],\n",
      "        [-2.0230, -0.6667, -0.3220,  0.5532,  0.7287],\n",
      "        [ 0.6577,  0.0785, -0.0024,  0.1328, -0.3104],\n",
      "        [ 0.1148, -0.8410,  0.9356, -1.0428, -0.2564]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0617,  0.5294,  0.2243,  0.8054,  1.5740],\n",
      "        [-0.5180, -0.5116, -0.0334, -0.8864, -0.9126],\n",
      "        [ 0.3888,  0.5567, -0.2132, -0.1053,  0.5639],\n",
      "        [-0.2249, -1.2320, -0.9749, -1.3393, -1.8503]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5238, -0.2243,  1.3818, -0.2625, -0.4033],\n",
      "        [-2.0230, -0.6667, -0.3220,  0.5532,  0.7287],\n",
      "        [ 0.6577,  0.0785, -0.0024,  0.1328, -0.3104],\n",
      "        [ 0.1148, -0.8410,  0.9356, -1.0428, -0.2564]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0989],\n",
      "        [ 0.2443],\n",
      "        [ 0.1109],\n",
      "        [ 1.9692]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4979,  0.6661,  1.1510, -0.9788, -1.5039],\n",
      "        [ 0.2678, -0.7035, -0.2214, -1.0225,  0.9012],\n",
      "        [ 0.0301,  0.8885,  0.3582, -1.3250,  0.4400],\n",
      "        [ 1.2458,  0.0034,  0.8113,  1.5344,  0.1498]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5127,  0.8373,  1.0093,  0.9856,  0.5869],\n",
      "        [-0.3371, -0.3234, -0.2298, -0.4182, -0.2206],\n",
      "        [ 0.0627,  0.1457, -0.0512, -0.5251,  0.0220],\n",
      "        [-1.2196, -1.4655, -1.6253, -1.8121, -2.6858]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4979,  0.6661,  1.1510, -0.9788, -1.5039],\n",
      "        [ 0.2678, -0.7035, -0.2214, -1.0225,  0.9012],\n",
      "        [ 0.0301,  0.8885,  0.3582, -1.3250,  0.4400],\n",
      "        [ 1.2458,  0.0034,  0.8113,  1.5344,  0.1498]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3832],\n",
      "        [ 0.4170],\n",
      "        [ 0.8185],\n",
      "        [-6.0255]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7575, -0.6603,  1.3857,  0.9230, -0.5948],\n",
      "        [ 0.5388,  1.7185, -1.1807, -0.1870,  0.1681],\n",
      "        [-0.8275,  0.5822,  0.6327, -1.1627,  1.4907],\n",
      "        [ 0.9925,  2.3552,  2.1681, -0.1452, -0.0591]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9867,  0.3929,  0.6746,  1.2678,  0.8640],\n",
      "        [ 0.1582, -0.1984,  0.1747, -0.0220,  0.0094],\n",
      "        [-0.4389, -0.3515, -0.5199,  0.3638, -0.5621],\n",
      "        [ 0.1356,  0.1164,  0.4644, -0.6883, -0.5256]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7575, -0.6603,  1.3857,  0.9230, -0.5948],\n",
      "        [ 0.5388,  1.7185, -1.1807, -0.1870,  0.1681],\n",
      "        [-0.8275,  0.5822,  0.6327, -1.1627,  1.4907],\n",
      "        [ 0.9925,  2.3552,  2.1681, -0.1452, -0.0591]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0790],\n",
      "        [-0.4563],\n",
      "        [-1.4313],\n",
      "        [ 1.5466]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1700, -1.1571, -0.2034, -2.1062,  0.6603],\n",
      "        [-0.1560, -0.8718, -0.1610,  1.3439,  0.3377],\n",
      "        [ 0.5477, -1.5357,  0.1152, -0.0668,  0.1864],\n",
      "        [-0.7596, -1.1071,  0.3523, -1.0005,  0.8191]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1315, -0.0402,  0.4488,  0.3632, -0.8109],\n",
      "        [ 0.1545, -0.2667,  0.0909, -0.5662, -0.6054],\n",
      "        [ 0.8475,  0.7607,  0.9747,  0.8015,  0.4791],\n",
      "        [-0.1794, -1.3375, -0.4490, -1.7424, -1.4632]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1700, -1.1571, -0.2034, -2.1062,  0.6603],\n",
      "        [-0.1560, -0.8718, -0.1610,  1.3439,  0.3377],\n",
      "        [ 0.5477, -1.5357,  0.1152, -0.0668,  0.1864],\n",
      "        [-0.7596, -1.1071,  0.3523, -1.0005,  0.8191]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3675],\n",
      "        [-0.7716],\n",
      "        [-0.5560],\n",
      "        [ 2.0035]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4546,  0.5538,  0.2377,  1.7899, -1.6922],\n",
      "        [-0.8499,  3.1047, -0.1826, -1.3533, -0.4772],\n",
      "        [ 0.4285, -1.1090,  0.1711,  0.5864, -0.9645],\n",
      "        [-0.3942,  1.3426, -1.8380, -2.1412,  1.4040]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8180,  1.3093,  1.3457,  0.9054,  0.5771],\n",
      "        [ 0.0003, -0.4608,  0.4293, -0.1623, -0.0514],\n",
      "        [ 0.2187,  0.0268, -0.0618,  0.7810,  0.5854],\n",
      "        [-0.4677, -1.1289, -1.1837, -0.9650, -2.5494]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4546,  0.5538,  0.2377,  1.7899, -1.6922],\n",
      "        [-0.8499,  3.1047, -0.1826, -1.3533, -0.4772],\n",
      "        [ 0.4285, -1.1090,  0.1711,  0.5864, -0.9645],\n",
      "        [-0.3942,  1.3426, -1.8380, -2.1412,  1.4040]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3172],\n",
      "        [-1.2652],\n",
      "        [-0.0533],\n",
      "        [-0.6690]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0176, -1.0193,  0.3175, -0.0143, -1.8092],\n",
      "        [ 0.0092,  0.7835, -1.1533, -2.3771,  1.9515],\n",
      "        [ 1.1979, -0.1470,  1.3016, -1.5774,  0.5826],\n",
      "        [-0.5935,  0.0776, -1.6614, -0.4649,  0.3051]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4165, -0.2020,  1.0052,  0.6814,  0.6497],\n",
      "        [ 0.6927,  0.6125,  0.3075,  0.4960,  0.7920],\n",
      "        [ 0.5691,  0.3846,  0.4584, -0.2438,  0.2200],\n",
      "        [-0.2770, -0.6228, -0.6571, -1.5884, -1.8334]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0176, -1.0193,  0.3175, -0.0143, -1.8092],\n",
      "        [ 0.0092,  0.7835, -1.1533, -2.3771,  1.9515],\n",
      "        [ 1.1979, -0.1470,  1.3016, -1.5774,  0.5826],\n",
      "        [-0.5935,  0.0776, -1.6614, -0.4649,  0.3051]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2363],\n",
      "        [ 0.4981],\n",
      "        [ 1.7346],\n",
      "        [ 1.3868]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.8427,  0.7642,  0.1502,  1.1894, -2.0782],\n",
      "        [-1.9599, -0.1275,  0.1433, -1.0839,  0.1475],\n",
      "        [-0.5030,  0.0990,  1.6548,  1.1464, -1.2249],\n",
      "        [-0.1209,  0.9673, -0.0973,  0.2111,  0.5196]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1883,  0.5275,  0.8969,  0.5461,  1.0005],\n",
      "        [-0.3163, -0.0883, -0.7082, -0.0283, -0.3849],\n",
      "        [-0.0353, -0.5752, -0.5365, -0.5246, -0.8600],\n",
      "        [-0.8288, -1.6784, -1.2686, -1.1638, -2.6369]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.8427,  0.7642,  0.1502,  1.1894, -2.0782],\n",
      "        [-1.9599, -0.1275,  0.1433, -1.0839,  0.1475],\n",
      "        [-0.5030,  0.0990,  1.6548,  1.1464, -1.2249],\n",
      "        [-0.1209,  0.9673, -0.0973,  0.2111,  0.5196]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3564],\n",
      "        [ 0.5036],\n",
      "        [-0.4751],\n",
      "        [-3.0157]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7117,  0.2610,  2.4362,  1.1119,  0.1776],\n",
      "        [ 0.4083, -0.5578,  2.1072, -0.8110, -1.7784],\n",
      "        [-0.2903,  0.0851,  1.0744,  0.3245, -1.6095],\n",
      "        [-1.0850,  0.2788,  1.8257, -1.2577, -0.1596]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5632,  1.3549,  1.2132,  0.6854,  0.7223],\n",
      "        [-0.1534,  0.2965, -0.2698, -0.1983, -0.4080],\n",
      "        [-0.3490, -0.9581,  0.0847, -0.1479, -0.1172],\n",
      "        [ 0.2518, -0.3166,  0.0493, -0.0560,  0.1671]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7117,  0.2610,  2.4362,  1.1119,  0.1776],\n",
      "        [ 0.4083, -0.5578,  2.1072, -0.8110, -1.7784],\n",
      "        [-0.2903,  0.0851,  1.0744,  0.3245, -1.6095],\n",
      "        [-1.0850,  0.2788,  1.8257, -1.2577, -0.1596]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.7986],\n",
      "        [ 0.0899],\n",
      "        [ 0.2515],\n",
      "        [-0.2278]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9409,  1.1953, -2.2212, -0.5516, -1.0565],\n",
      "        [ 0.3329, -0.3214, -0.0003,  0.8825,  0.8152],\n",
      "        [-2.1948,  0.6733, -1.0346,  0.8157,  1.1604],\n",
      "        [-0.1354, -0.8904, -0.6172,  1.0266, -0.1322]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6675, -0.7225, -1.2464, -0.7046, -1.7435],\n",
      "        [ 0.4953, -0.2315, -0.7347, -0.2391, -0.0020],\n",
      "        [ 0.4695,  0.6281,  0.5578,  0.2310,  0.2856],\n",
      "        [ 0.2241, -0.1134, -0.1091, -0.0968,  0.3258]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9409,  1.1953, -2.2212, -0.5516, -1.0565],\n",
      "        [ 0.3329, -0.3214, -0.0003,  0.8825,  0.8152],\n",
      "        [-2.1948,  0.6733, -1.0346,  0.8157,  1.1604],\n",
      "        [-0.1354, -0.8904, -0.6172,  1.0266, -0.1322]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.4311],\n",
      "        [ 0.0268],\n",
      "        [-0.6646],\n",
      "        [-0.0045]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6963, -0.5462, -0.8669, -0.8162, -1.2697],\n",
      "        [-0.3795, -2.4473,  0.2400, -0.8949,  0.3114],\n",
      "        [-0.3571,  2.1398, -0.3544,  0.8898, -2.3337],\n",
      "        [ 0.1246, -0.1468,  0.0826, -0.6238, -0.6445]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-2.0622, -3.2952, -2.1121, -2.5310, -5.7309],\n",
      "        [-0.2425, -0.0678, -0.5394, -0.5105,  0.0138],\n",
      "        [-0.7587,  0.5033,  0.6739, -0.5412,  0.1187],\n",
      "        [ 0.3396, -0.7100, -0.0655, -0.0083,  0.3462]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6963, -0.5462, -0.8669, -0.8162, -1.2697],\n",
      "        [-0.3795, -2.4473,  0.2400, -0.8949,  0.3114],\n",
      "        [-0.3571,  2.1398, -0.3544,  0.8898, -2.3337],\n",
      "        [ 0.1246, -0.1468,  0.0826, -0.6238, -0.6445]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 16.4714],\n",
      "        [  0.5897],\n",
      "        [  0.3505],\n",
      "        [ -0.0768]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3412, -0.5440,  0.8290,  0.3819, -0.0761],\n",
      "        [-0.0574,  1.1468,  1.5993, -0.3221, -0.3042],\n",
      "        [ 0.8774, -0.2237, -0.1010,  0.1777, -1.6786],\n",
      "        [-1.4972, -1.3475,  0.9694, -1.1427,  0.7495]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1855,  0.2740, -0.3525,  0.2367, -0.4200],\n",
      "        [ 0.0556, -0.1617, -0.8224, -0.4932, -0.1622],\n",
      "        [ 0.5200, -0.2972,  0.2217, -0.3717, -0.1725],\n",
      "        [ 0.0634,  0.3092, -0.5270, -0.2180,  0.1731]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3412, -0.5440,  0.8290,  0.3819, -0.0761],\n",
      "        [-0.0574,  1.1468,  1.5993, -0.3221, -0.3042],\n",
      "        [ 0.8774, -0.2237, -0.1010,  0.1777, -1.6786],\n",
      "        [-1.4972, -1.3475,  0.9694, -1.1427,  0.7495]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5677],\n",
      "        [-1.2957],\n",
      "        [ 0.7237],\n",
      "        [-0.6436]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4517,  0.1184, -0.5189,  0.1073,  0.0902],\n",
      "        [ 0.4844,  0.0436,  0.3353, -0.5187,  1.8439],\n",
      "        [-0.6774,  0.5247,  0.5386, -0.1464,  1.2093],\n",
      "        [-1.6037,  0.9378,  0.7979, -1.1313,  0.2888]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0359, -0.3602, -0.7942,  0.1452, -0.1801],\n",
      "        [ 0.8228,  0.7151,  0.1144,  0.6555,  0.5212],\n",
      "        [ 0.1102,  0.1914,  0.0795, -0.2628, -0.4337],\n",
      "        [-0.3037, -0.2680, -0.3194, -0.7474, -0.2622]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4517,  0.1184, -0.5189,  0.1073,  0.0902],\n",
      "        [ 0.4844,  0.0436,  0.3353, -0.5187,  1.8439],\n",
      "        [-0.6774,  0.5247,  0.5386, -0.1464,  1.2093],\n",
      "        [-1.6037,  0.9378,  0.7979, -1.1313,  0.2888]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4209],\n",
      "        [ 1.0891],\n",
      "        [-0.4174],\n",
      "        [ 0.7507]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4598, -0.9239,  1.3797, -1.0428, -0.7449],\n",
      "        [-0.8255,  1.8510,  0.4694, -2.3278, -1.2906],\n",
      "        [-0.5262,  0.2052, -0.0315,  0.0708,  0.0499],\n",
      "        [ 0.9411,  0.5936,  1.6444, -0.3153,  1.8898]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2736,  0.2973,  0.1526, -0.0194,  0.1346],\n",
      "        [-0.1504, -0.2815, -0.3299, -0.6295, -0.7040],\n",
      "        [ 0.1581,  0.4763, -0.5441,  0.1672,  0.3994],\n",
      "        [ 0.0629, -0.5294, -0.3145, -0.9575, -0.4568]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4598, -0.9239,  1.3797, -1.0428, -0.7449],\n",
      "        [-0.8255,  1.8510,  0.4694, -2.3278, -1.2906],\n",
      "        [-0.5262,  0.2052, -0.0315,  0.0708,  0.0499],\n",
      "        [ 0.9411,  0.5936,  1.6444, -0.3153,  1.8898]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0182],\n",
      "        [ 1.8220],\n",
      "        [ 0.0634],\n",
      "        [-1.3337]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0585, -1.7313,  0.0289,  1.6331,  0.2794],\n",
      "        [-2.3369, -1.8536,  1.9244,  0.0215, -0.3616],\n",
      "        [ 0.0848, -0.8814, -0.8451,  0.9810,  1.0035],\n",
      "        [-1.4349,  0.6950, -0.4302,  2.0939,  0.3183]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1263, -0.3696,  0.1968,  0.0621,  0.3967],\n",
      "        [-0.7323, -0.9297, -0.8488, -0.6298, -1.9808],\n",
      "        [ 0.0147,  0.4028,  0.4842, -0.0635,  0.0189],\n",
      "        [ 0.6736,  1.0691,  0.7144,  0.1160,  0.6328]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0585, -1.7313,  0.0289,  1.6331,  0.2794],\n",
      "        [-2.3369, -1.8536,  1.9244,  0.0215, -0.3616],\n",
      "        [ 0.0848, -0.8814, -0.8451,  0.9810,  1.0035],\n",
      "        [-1.4349,  0.6950, -0.4302,  2.0939,  0.3183]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8504],\n",
      "        [ 2.5037],\n",
      "        [-0.8063],\n",
      "        [-0.0865]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2614,  0.6885, -1.4088,  0.1632,  1.2224],\n",
      "        [-0.4517,  0.4308,  1.4996, -0.9054, -1.3460],\n",
      "        [-0.0748, -1.2672, -0.3437,  0.1359,  0.7017],\n",
      "        [ 0.5514,  1.1522,  2.3859,  1.8304, -1.0849]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0067, -0.2557, -0.2879,  0.3893, -0.2272],\n",
      "        [-1.0597, -1.3321, -1.8347, -1.9256, -3.7925],\n",
      "        [ 0.2768,  0.3565,  0.0478,  0.1924,  0.6469],\n",
      "        [ 0.4652, -0.0020,  0.0340, -0.1034, -0.0039]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2614,  0.6885, -1.4088,  0.1632,  1.2224],\n",
      "        [-0.4517,  0.4308,  1.4996, -0.9054, -1.3460],\n",
      "        [-0.0748, -1.2672, -0.3437,  0.1359,  0.7017],\n",
      "        [ 0.5514,  1.1522,  2.3859,  1.8304, -1.0849]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0067],\n",
      "        [ 4.0015],\n",
      "        [-0.0088],\n",
      "        [ 0.1501]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1121,  0.0685,  3.0302, -2.0063, -0.3088],\n",
      "        [-1.5412,  1.5486, -1.3946, -0.5006,  1.5791],\n",
      "        [-0.6349, -0.7172, -0.3581, -0.4284,  1.4381],\n",
      "        [-1.8922, -0.4769,  0.7188, -0.6613,  1.4989]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1066, -0.0282, -0.0659, -0.2711,  0.1327],\n",
      "        [ 0.1493,  0.0155, -0.2250, -0.2821, -0.4485],\n",
      "        [-0.4376,  0.4527,  0.1499,  0.3130,  0.0193],\n",
      "        [ 0.4206,  0.0928,  0.1936,  0.3857,  0.0994]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1121,  0.0685,  3.0302, -2.0063, -0.3088],\n",
      "        [-1.5412,  1.5486, -1.3946, -0.5006,  1.5791],\n",
      "        [-0.6349, -0.7172, -0.3581, -0.4284,  1.4381],\n",
      "        [-1.8922, -0.4769,  0.7188, -0.6613,  1.4989]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3132],\n",
      "        [-0.4593],\n",
      "        [-0.2069],\n",
      "        [-0.8072]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1868, -0.0371,  0.7365,  0.2684,  0.5687],\n",
      "        [-2.4740, -0.1415, -0.1543, -0.0932, -0.3080],\n",
      "        [ 0.6428, -0.9540,  1.3602, -0.5576,  0.5362],\n",
      "        [-0.2866,  0.2334,  0.7270,  1.5270,  1.5678]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1825, -0.0597, -0.0164, -0.1261,  0.5796],\n",
      "        [ 0.2277,  0.0713, -0.2870,  0.0863,  0.0531],\n",
      "        [ 0.2862,  0.5171, -0.5801,  0.0628, -0.5187],\n",
      "        [ 0.2467,  0.3704,  0.3563, -0.3590, -0.4118]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1868, -0.0371,  0.7365,  0.2684,  0.5687],\n",
      "        [-2.4740, -0.1415, -0.1543, -0.0932, -0.3080],\n",
      "        [ 0.6428, -0.9540,  1.3602, -0.5576,  0.5362],\n",
      "        [-0.2866,  0.2334,  0.7270,  1.5270,  1.5678]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2519],\n",
      "        [-0.5535],\n",
      "        [-1.4116],\n",
      "        [-0.9190]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1476,  0.8771,  1.0311, -0.6298, -0.6536],\n",
      "        [-0.1589, -1.0554,  0.8837,  0.2197, -0.6394],\n",
      "        [-0.9038,  0.9228, -0.5345,  1.0351,  1.3819],\n",
      "        [ 0.9407,  1.5248,  0.7720, -1.4264, -1.2873]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2876,  0.2466,  0.4575,  0.9889,  0.1640],\n",
      "        [ 0.2645, -0.3259, -0.1442, -0.4559, -0.3715],\n",
      "        [ 0.2718,  1.3002,  0.5739,  0.4633,  0.6207],\n",
      "        [ 0.4546, -0.0655, -0.4222,  0.3276,  0.8971]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1476,  0.8771,  1.0311, -0.6298, -0.6536],\n",
      "        [-0.1589, -1.0554,  0.8837,  0.2197, -0.6394],\n",
      "        [-0.9038,  0.9228, -0.5345,  1.0351,  1.3819],\n",
      "        [ 0.9407,  1.5248,  0.7720, -1.4264, -1.2873]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0843],\n",
      "        [ 0.3118],\n",
      "        [ 1.9848],\n",
      "        [-1.6203]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2237, -0.0222,  0.7742,  1.4436,  0.9090],\n",
      "        [ 0.9247,  1.3269,  0.6194,  0.1111,  1.8100],\n",
      "        [-0.6156,  0.0590,  0.4616, -0.2694,  1.7197],\n",
      "        [-1.6017, -0.7613,  1.1085,  0.0926, -0.2042]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1765,  0.1221, -0.0699, -0.0570, -0.4019],\n",
      "        [ 0.1214,  0.3227, -0.6302, -0.4667, -0.5981],\n",
      "        [-0.0273,  0.4945, -0.7846,  0.0427, -0.3354],\n",
      "        [ 1.1589,  1.0019,  0.9767,  0.3610,  1.6925]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2237, -0.0222,  0.7742,  1.4436,  0.9090],\n",
      "        [ 0.9247,  1.3269,  0.6194,  0.1111,  1.8100],\n",
      "        [-0.6156,  0.0590,  0.4616, -0.2694,  1.7197],\n",
      "        [-1.6017, -0.7613,  1.1085,  0.0926, -0.2042]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4650],\n",
      "        [-0.9842],\n",
      "        [-0.9045],\n",
      "        [-1.8484]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4466, -0.0366, -0.3799,  1.3185, -2.0891],\n",
      "        [-1.6048, -1.8366,  0.3935, -0.7104, -0.1641],\n",
      "        [ 0.0140,  1.2870, -0.3554, -1.8424, -1.7508],\n",
      "        [-2.8197,  0.2701,  0.8855,  1.4730,  0.9236]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3599, -0.0500,  0.0234,  0.6516, -0.0131],\n",
      "        [ 0.6829,  0.2540,  0.0340, -0.5805,  0.7981],\n",
      "        [ 0.3795,  0.3674,  0.7344, -0.5107,  0.1685],\n",
      "        [ 1.3595,  1.6472,  1.5197,  1.0087,  2.0151]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4466, -0.0366, -0.3799,  1.3185, -2.0891],\n",
      "        [-1.6048, -1.8366,  0.3935, -0.7104, -0.1641],\n",
      "        [ 0.0140,  1.2870, -0.3554, -1.8424, -1.7508],\n",
      "        [-2.8197,  0.2701,  0.8855,  1.4730,  0.9236]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3587],\n",
      "        [-1.2675],\n",
      "        [ 0.8630],\n",
      "        [ 1.3042]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5328, -1.0479, -1.7447,  0.8772,  0.3340],\n",
      "        [-0.1464,  0.3558, -0.5505,  1.7516,  0.7337],\n",
      "        [-0.6697,  0.2927, -0.1522,  1.3750,  0.2244],\n",
      "        [-0.1146,  1.6207,  0.9119, -0.8411,  0.3949]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1479,  0.3093,  0.1749,  0.1037, -0.2244],\n",
      "        [ 0.6680,  1.2779,  0.8371,  0.0408,  0.7558],\n",
      "        [ 0.2493,  0.0840, -0.4380,  0.1180, -0.1750],\n",
      "        [-0.0656, -0.0383,  0.5740, -0.2540,  0.2099]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5328, -1.0479, -1.7447,  0.8772,  0.3340],\n",
      "        [-0.1464,  0.3558, -0.5505,  1.7516,  0.7337],\n",
      "        [-0.6697,  0.2927, -0.1522,  1.3750,  0.2244],\n",
      "        [-0.1146,  1.6207,  0.9119, -0.8411,  0.3949]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8400],\n",
      "        [ 0.5220],\n",
      "        [ 0.0473],\n",
      "        [ 0.7655]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0545,  0.0667, -1.9974, -0.8321, -0.2130],\n",
      "        [ 0.7609,  1.1796,  0.9953, -1.0616,  1.2337],\n",
      "        [ 0.9387,  0.0611,  0.1433,  0.0089, -0.6128],\n",
      "        [-0.3961,  0.5523,  1.7596,  0.5148,  0.8297]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7141, -0.3236,  0.7691,  0.1244,  0.6770],\n",
      "        [ 1.3279,  0.6179, -0.4883,  0.0879, -0.2450],\n",
      "        [-0.2241,  0.0683,  1.0117, -0.1770,  0.3870],\n",
      "        [-0.0556, -0.3301,  0.1714, -0.2266,  0.1248]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0545,  0.0667, -1.9974, -0.8321, -0.2130],\n",
      "        [ 0.7609,  1.1796,  0.9953, -1.0616,  1.2337],\n",
      "        [ 0.9387,  0.0611,  0.1433,  0.0089, -0.6128],\n",
      "        [-0.3961,  0.5523,  1.7596,  0.5148,  0.8297]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0525],\n",
      "        [ 0.8577],\n",
      "        [-0.2999],\n",
      "        [ 0.1283]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1663,  0.4279, -0.4786, -0.0419,  0.4869],\n",
      "        [-0.4120,  1.9492, -1.5734,  0.6861, -0.8917],\n",
      "        [-0.1581,  0.4051,  0.9089, -1.2770,  1.3721],\n",
      "        [-1.3657,  0.4481,  1.1427, -1.7181,  0.3814]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5295,  0.8907,  0.9493,  0.9053,  1.2913],\n",
      "        [ 0.2866, -0.6688, -0.5721, -0.1880,  0.0502],\n",
      "        [ 0.4429,  0.1499, -0.1264, -0.0413,  0.6139],\n",
      "        [-0.3088, -0.2870,  0.4939,  0.0655, -0.1798]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1663,  0.4279, -0.4786, -0.0419,  0.4869],\n",
      "        [-0.4120,  1.9492, -1.5734,  0.6861, -0.8917],\n",
      "        [-0.1581,  0.4051,  0.9089, -1.2770,  1.3721],\n",
      "        [-1.3657,  0.4481,  1.1427, -1.7181,  0.3814]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4294],\n",
      "        [-0.6952],\n",
      "        [ 0.7709],\n",
      "        [ 0.6764]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4646,  1.9734,  0.6589,  1.4315,  1.8122],\n",
      "        [ 0.7748,  1.1005, -0.2263, -0.5862, -0.0099],\n",
      "        [-0.4572,  0.8979, -0.8905,  0.8642, -0.2439],\n",
      "        [-2.6430, -0.3254, -1.1821,  1.1101, -0.2888]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1519, -0.1601,  0.0539,  0.0823,  0.1025],\n",
      "        [ 0.3898,  0.1547, -0.6114, -0.5466,  0.3857],\n",
      "        [-0.0238, -0.2291, -0.2267,  0.3341,  0.0841],\n",
      "        [ 0.1262,  0.0112,  0.2513, -0.0088, -0.3035]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4646,  1.9734,  0.6589,  1.4315,  1.8122],\n",
      "        [ 0.7748,  1.1005, -0.2263, -0.5862, -0.0099],\n",
      "        [-0.4572,  0.8979, -0.8905,  0.8642, -0.2439],\n",
      "        [-2.6430, -0.3254, -1.1821,  1.1101, -0.2888]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0473],\n",
      "        [ 0.9272],\n",
      "        [ 0.2752],\n",
      "        [-0.5563]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0380, -0.6957,  0.7016, -0.2495, -0.3019],\n",
      "        [ 0.2630, -0.1488, -2.3891, -0.7550, -0.5701],\n",
      "        [ 0.5491,  0.8540, -2.0417, -0.3348, -0.6494],\n",
      "        [ 0.7050,  1.1366,  0.5245,  0.1786, -0.2377]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0897, -0.1797,  1.1956, -0.5367, -0.2594],\n",
      "        [-0.2256, -0.2474, -0.1340, -0.2339, -0.5127],\n",
      "        [-0.1194,  0.0685,  0.6771,  0.2614,  1.0636],\n",
      "        [ 0.0767, -0.2168, -0.3305,  0.0707,  0.7581]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0380, -0.6957,  0.7016, -0.2495, -0.3019],\n",
      "        [ 0.2630, -0.1488, -2.3891, -0.7550, -0.5701],\n",
      "        [ 0.5491,  0.8540, -2.0417, -0.3348, -0.6494],\n",
      "        [ 0.7050,  1.1366,  0.5245,  0.1786, -0.2377]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2692],\n",
      "        [ 0.7665],\n",
      "        [-2.1675],\n",
      "        [-0.5332]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[ 0.0387,  0.1244,  0.4806, -1.2001,  1.4329],\n",
      "        [ 0.3983,  0.1362,  1.9454,  0.7416,  0.8665],\n",
      "        [-0.3268,  0.2800,  1.4237, -0.5404,  0.2392],\n",
      "        [-1.0153, -1.1448,  0.1178, -0.5962,  0.6299]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7323, -0.8214, -0.4638,  0.0040, -0.5746],\n",
      "        [-0.3956, -0.2627, -0.7792, -0.2467, -1.3874],\n",
      "        [ 0.8287,  1.3039,  0.7199,  0.6979,  1.3332],\n",
      "        [ 0.0631, -0.2778, -0.1668,  0.3083, -0.0976]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0387,  0.1244,  0.4806, -1.2001,  1.4329],\n",
      "        [ 0.3983,  0.1362,  1.9454,  0.7416,  0.8665],\n",
      "        [-0.3268,  0.2800,  1.4237, -0.5404,  0.2392],\n",
      "        [-1.0153, -1.1448,  0.1178, -0.5962,  0.6299]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1816],\n",
      "        [-3.0943],\n",
      "        [ 1.0609],\n",
      "        [-0.0110]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3226, -0.8160,  1.7877,  0.7910,  0.9313],\n",
      "        [-0.6399,  0.8805,  1.0827, -0.2900,  1.1315],\n",
      "        [-1.8146,  0.2446,  0.5605,  1.2151,  1.3996],\n",
      "        [-0.1201, -0.5486,  0.7697, -0.5472, -0.0140]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0350, -0.2413,  0.7066,  0.2145,  0.2613],\n",
      "        [ 0.8684,  1.1603,  1.9561,  1.6792,  2.0987],\n",
      "        [ 0.6809,  0.4748,  0.7274,  0.4699,  1.3266],\n",
      "        [ 0.5934, -0.2648,  0.2089, -0.4345, -0.0921]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3226, -0.8160,  1.7877,  0.7910,  0.9313],\n",
      "        [-0.6399,  0.8805,  1.0827, -0.2900,  1.1315],\n",
      "        [-1.8146,  0.2446,  0.5605,  1.2151,  1.3996],\n",
      "        [-0.1201, -0.5486,  0.7697, -0.5472, -0.0140]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8269],\n",
      "        [ 4.4716],\n",
      "        [ 1.7160],\n",
      "        [ 0.4739]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4353, -1.2514,  2.1120,  0.3547, -1.0527],\n",
      "        [-0.5039,  0.0970, -0.4877, -1.0085,  0.5759],\n",
      "        [-1.6873,  1.7975, -0.1485, -0.9945,  2.2535],\n",
      "        [-0.5910,  1.1849, -1.9895,  0.6223,  0.3882]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6531, -0.5644, -0.7247, -1.1167, -1.7054],\n",
      "        [-0.1577, -0.6901, -0.9372, -1.0521, -1.6931],\n",
      "        [-0.5049, -0.3892, -0.3445, -0.0820, -0.6650],\n",
      "        [ 0.1019, -0.3167,  0.2655, -0.0428,  0.0133]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4353, -1.2514,  2.1120,  0.3547, -1.0527],\n",
      "        [-0.5039,  0.0970, -0.4877, -1.0085,  0.5759],\n",
      "        [-1.6873,  1.7975, -0.1485, -0.9945,  2.2535],\n",
      "        [-0.5910,  1.1849, -1.9895,  0.6223,  0.3882]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8590],\n",
      "        [ 0.5556],\n",
      "        [-1.2135],\n",
      "        [-0.9852]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4354, -1.0183, -0.1625, -1.4776,  0.1291],\n",
      "        [-0.2846, -1.5039, -0.9355, -1.2543,  1.0452],\n",
      "        [-1.3691,  0.0084, -0.5130,  1.0705, -0.5321],\n",
      "        [ 0.3972, -0.2203, -0.3476,  0.0178, -0.9973]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5471, -0.6240, -1.1974, -1.2820, -2.2054],\n",
      "        [-0.9198, -0.7318, -0.7325, -0.9816, -1.5378],\n",
      "        [ 0.2175,  0.3985, -0.0578,  1.0923,  0.8807],\n",
      "        [ 0.6497,  0.3017,  0.6100, -0.1926,  0.2686]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4354, -1.0183, -0.1625, -1.4776,  0.1291],\n",
      "        [-0.2846, -1.5039, -0.9355, -1.2543,  1.0452],\n",
      "        [-1.3691,  0.0084, -0.5130,  1.0705, -0.5321],\n",
      "        [ 0.3972, -0.2203, -0.3476,  0.0178, -0.9973]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2014],\n",
      "        [ 1.6713],\n",
      "        [ 0.4361],\n",
      "        [-0.2918]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1464, -1.6510, -0.2685, -0.5653, -0.5613],\n",
      "        [-1.3468, -0.6372,  0.1131, -1.0905, -1.2933],\n",
      "        [ 0.0726, -0.3335, -0.2077, -0.5448,  0.6808],\n",
      "        [-0.2141,  0.7928,  1.7511, -1.6736, -0.4654]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.3842, -1.4166, -1.1824, -1.6000, -3.1447],\n",
      "        [-1.2294, -1.0883, -1.3875, -1.9331, -2.9452],\n",
      "        [ 0.3126,  0.6006,  0.0175,  0.5850,  1.2691],\n",
      "        [ 0.3065,  0.0031,  0.3521,  0.4183, -0.3961]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1464, -1.6510, -0.2685, -0.5653, -0.5613],\n",
      "        [-1.3468, -0.6372,  0.1131, -1.0905, -1.2933],\n",
      "        [ 0.0726, -0.3335, -0.2077, -0.5448,  0.6808],\n",
      "        [-0.2141,  0.7928,  1.7511, -1.6736, -0.4654]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.5287],\n",
      "        [ 8.1092],\n",
      "        [ 0.3641],\n",
      "        [ 0.0378]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4418,  0.5226, -0.2823, -0.8827,  2.1901],\n",
      "        [-1.3038, -0.7019,  0.2163, -0.5287,  1.4306],\n",
      "        [ 0.3523, -0.7415,  0.5491, -1.5799,  0.8768],\n",
      "        [ 0.3786, -0.1115,  1.1812, -1.0299, -0.4850]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3321,  0.0570, -0.6846, -0.3922,  0.2501],\n",
      "        [ 0.2558, -1.1501,  0.1687, -0.7555,  0.1912],\n",
      "        [ 0.1517,  0.2416,  0.3158,  0.6957,  1.4140],\n",
      "        [ 0.2114, -0.4497,  0.2362,  0.2258,  0.2312]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4418,  0.5226, -0.2823, -0.8827,  2.1901],\n",
      "        [-1.3038, -0.7019,  0.2163, -0.5287,  1.4306],\n",
      "        [ 0.3523, -0.7415,  0.5491, -1.5799,  0.8768],\n",
      "        [ 0.3786, -0.1115,  1.1812, -1.0299, -0.4850]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9704],\n",
      "        [ 1.1833],\n",
      "        [ 0.1884],\n",
      "        [ 0.0646]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9827, -0.4505, -0.0976, -0.2252,  0.0334],\n",
      "        [ 0.0547, -0.9905, -0.8830, -1.2846,  0.1779],\n",
      "        [-0.0107, -0.7421,  0.0614,  0.1431,  0.2554],\n",
      "        [-1.2593,  0.5538,  0.6815, -0.7523,  1.0360]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5507,  0.1620, -1.0521,  0.2716, -0.6149],\n",
      "        [-0.1455, -0.6676, -0.3023, -0.5309, -0.8307],\n",
      "        [ 0.3138,  1.0300,  1.1435, -0.0060,  0.5624],\n",
      "        [ 0.3453,  0.6339, -0.0373, -0.2904, -0.0934]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9827, -0.4505, -0.0976, -0.2252,  0.0334],\n",
      "        [ 0.0547, -0.9905, -0.8830, -1.2846,  0.1779],\n",
      "        [-0.0107, -0.7421,  0.0614,  0.1431,  0.2554],\n",
      "        [-1.2593,  0.5538,  0.6815, -0.7523,  1.0360]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0399],\n",
      "        [ 1.4545],\n",
      "        [-0.5546],\n",
      "        [ 0.0125]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5913, -0.2654, -0.1498, -0.3178, -0.3998],\n",
      "        [-0.9493,  0.3625, -0.5277,  0.8359,  1.0577],\n",
      "        [ 1.7647,  0.6212,  0.5243,  0.6973, -0.3903],\n",
      "        [ 0.3448,  0.9154, -0.5382,  1.5482, -0.3097]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2219, -0.5106, -0.2016, -0.7478, -1.8629],\n",
      "        [-0.2591, -0.9212, -0.9550, -1.2970, -1.7956],\n",
      "        [ 0.9244,  0.7614,  0.2978,  0.5701,  1.2736],\n",
      "        [ 0.0747, -0.1527, -0.0110, -0.0496, -0.5152]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5913, -0.2654, -0.1498, -0.3178, -0.3998],\n",
      "        [-0.9493,  0.3625, -0.5277,  0.8359,  1.0577],\n",
      "        [ 1.7647,  0.6212,  0.5243,  0.6973, -0.3903],\n",
      "        [ 0.3448,  0.9154, -0.5382,  1.5482, -0.3097]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7950],\n",
      "        [-2.5674],\n",
      "        [ 2.1608],\n",
      "        [-0.0253]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2607, -0.2452,  0.8360,  1.5364, -0.8282],\n",
      "        [-0.2047, -0.5822, -0.2125, -0.5577,  0.3384],\n",
      "        [ 0.2391,  1.2005, -0.2429,  0.4513, -1.0410],\n",
      "        [-0.5218, -0.4113,  1.5021, -1.6607, -0.3976]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8361, -0.7576, -0.7326, -0.0030, -1.7366],\n",
      "        [-0.0770,  1.0299,  0.5107,  1.1630,  0.3018],\n",
      "        [-0.7495, -0.8521, -0.1326, -0.7773, -1.4482],\n",
      "        [ 0.4021, -0.3922, -0.3910,  0.0604,  0.4392]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2607, -0.2452,  0.8360,  1.5364, -0.8282],\n",
      "        [-0.2047, -0.5822, -0.2125, -0.5577,  0.3384],\n",
      "        [ 0.2391,  1.2005, -0.2429,  0.4513, -1.0410],\n",
      "        [-0.5218, -0.4113,  1.5021, -1.6607, -0.3976]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0471],\n",
      "        [-1.2389],\n",
      "        [-0.0132],\n",
      "        [-0.9107]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3757,  0.5780, -0.9584,  0.1896,  1.8471],\n",
      "        [-1.8936,  0.3162, -0.5627, -1.0260,  1.6754],\n",
      "        [-1.7762,  0.3113,  0.5517,  0.4391, -0.7352],\n",
      "        [-1.2512,  2.2787,  0.7336,  0.2713,  0.2174]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2121, -0.5154, -1.2976, -0.1593, -0.5727],\n",
      "        [ 1.1671,  0.6908,  0.8568,  0.0073,  1.3731],\n",
      "        [-0.1685, -0.1928,  0.0900, -0.3845, -1.1110],\n",
      "        [ 0.3346,  0.7218,  0.3966,  0.0368,  0.3459]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3757,  0.5780, -0.9584,  0.1896,  1.8471],\n",
      "        [-1.8936,  0.3162, -0.5627, -1.0260,  1.6754],\n",
      "        [-1.7762,  0.3113,  0.5517,  0.4391, -0.7352],\n",
      "        [-1.2512,  2.2787,  0.7336,  0.2713,  0.2174]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2220],\n",
      "        [-0.1807],\n",
      "        [ 0.9370],\n",
      "        [ 1.6023]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0234, -0.4475,  0.8966,  0.5719,  1.6487],\n",
      "        [ 0.3651, -0.2179,  0.8648, -0.6504,  2.4445],\n",
      "        [-0.3536, -0.2356,  1.6343, -1.3428,  1.0937],\n",
      "        [ 0.8221, -0.6723,  0.6634,  1.5198, -0.8931]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3857,  0.5129,  0.0479, -0.4944, -0.3881],\n",
      "        [ 0.6851, -0.4738,  0.0451,  0.2511,  1.4199],\n",
      "        [-0.1892, -0.5007,  0.4202, -0.1058, -0.2281],\n",
      "        [-0.0702, -0.1786, -1.4716, -0.3266, -0.7777]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0234, -0.4475,  0.8966,  0.5719,  1.6487],\n",
      "        [ 0.3651, -0.2179,  0.8648, -0.6504,  2.4445],\n",
      "        [-0.3536, -0.2356,  1.6343, -1.3428,  1.0937],\n",
      "        [ 0.8221, -0.6723,  0.6634,  1.5198, -0.8931]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5038],\n",
      "        [ 3.6999],\n",
      "        [ 0.7642],\n",
      "        [-0.7158]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5907,  0.3656, -0.0141,  1.7176, -0.9244],\n",
      "        [-0.2959,  0.5801,  0.2756,  0.0352, -0.1866],\n",
      "        [ 0.2203, -0.2506,  0.7773,  0.4637,  0.7518],\n",
      "        [-0.6395,  0.5060,  0.5604, -0.3709, -0.4676]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0785,  1.0303,  0.5105, -0.0844,  0.2719],\n",
      "        [-0.6942, -1.3650, -0.7715, -1.2405, -2.0127],\n",
      "        [-0.5473, -0.1207, -1.0248, -0.0229, -0.4795],\n",
      "        [ 0.0269, -0.2931,  0.2431, -0.0279, -0.2325]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5907,  0.3656, -0.0141,  1.7176, -0.9244],\n",
      "        [-0.2959,  0.5801,  0.2756,  0.0352, -0.1866],\n",
      "        [ 0.2203, -0.2506,  0.7773,  0.4637,  0.7518],\n",
      "        [-0.6395,  0.5060,  0.5604, -0.3709, -0.4676]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0195],\n",
      "        [-0.4671],\n",
      "        [-1.2580],\n",
      "        [ 0.0898]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0591, -1.8049, -0.4425, -1.3967, -1.0332],\n",
      "        [ 1.8139,  0.2727, -1.1464, -0.2994, -0.1006],\n",
      "        [-2.2609,  0.9995,  1.2262,  0.2609, -1.5381],\n",
      "        [ 0.6211,  1.4636,  0.9353,  0.5459,  0.5110]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7201,  0.1179, -0.5129,  0.0428,  0.3038],\n",
      "        [-0.6353, -1.0570, -1.7602, -1.6374, -1.7580],\n",
      "        [-0.8237,  0.6215,  0.5025,  0.4311,  0.9875],\n",
      "        [ 0.3139,  0.0650, -0.0753,  0.2636,  0.3137]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0591, -1.8049, -0.4425, -1.3967, -1.0332],\n",
      "        [ 1.8139,  0.2727, -1.1464, -0.2994, -0.1006],\n",
      "        [-2.2609,  0.9995,  1.2262,  0.2609, -1.5381],\n",
      "        [ 0.6211,  1.4636,  0.9353,  0.5459,  0.5110]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3170],\n",
      "        [ 1.2443],\n",
      "        [ 1.6931],\n",
      "        [ 0.5238]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4404, -0.4558,  0.1916, -1.1344,  0.0185],\n",
      "        [-0.8973,  0.2107, -0.0563,  0.5790,  1.2220],\n",
      "        [-2.0637, -0.1377,  1.4948, -0.2300,  0.4182],\n",
      "        [-0.4065,  1.2411,  0.8467, -0.9283, -0.1589]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2095, -0.5525,  0.3456, -0.4302, -0.9677],\n",
      "        [-0.4447, -1.2999, -1.2410, -1.5365, -2.6692],\n",
      "        [-0.5959,  0.3937,  0.0564, -0.8142, -0.4101],\n",
      "        [-0.0129,  0.2396,  0.4154, -0.1420, -0.0153]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4404, -0.4558,  0.1916, -1.1344,  0.0185],\n",
      "        [-0.8973,  0.2107, -0.0563,  0.5790,  1.2220],\n",
      "        [-2.0637, -0.1377,  1.4948, -0.2300,  0.4182],\n",
      "        [-0.4065,  1.2411,  0.8467, -0.9283, -0.1589]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8804],\n",
      "        [-3.9564],\n",
      "        [ 1.2755],\n",
      "        [ 0.7887]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3348,  1.6974, -1.9898, -0.0239,  0.1340],\n",
      "        [-1.2204,  1.5306,  1.0545,  0.2209,  0.5335],\n",
      "        [-0.9620,  1.1875,  0.2477, -0.7788, -2.2981],\n",
      "        [ 0.5105, -0.3659, -0.4087, -0.5174,  0.1915]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2495, -0.3661, -0.0912, -0.5350, -0.8854],\n",
      "        [ 0.1553, -0.1142, -0.9587, -0.3436,  0.7866],\n",
      "        [-0.2664, -0.8261, -1.2454, -1.3585, -1.5519],\n",
      "        [-0.5378, -0.1570,  0.6317, -0.4035,  0.2825]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3348,  1.6974, -1.9898, -0.0239,  0.1340],\n",
      "        [-1.2204,  1.5306,  1.0545,  0.2209,  0.5335],\n",
      "        [-0.9620,  1.1875,  0.2477, -0.7788, -2.2981],\n",
      "        [ 0.5105, -0.3659, -0.4087, -0.5174,  0.1915]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2128],\n",
      "        [-1.0317],\n",
      "        [ 3.5912],\n",
      "        [-0.2125]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2794,  0.9998, -0.2503, -0.3466,  1.5161],\n",
      "        [ 0.2931,  0.7726, -0.8137, -0.8888,  1.0500],\n",
      "        [ 0.1739, -2.3043,  0.1347,  0.2873, -0.1286],\n",
      "        [ 1.0241,  1.0524, -1.0451, -0.6391,  0.6286]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1661, -0.3897, -0.6242, -0.5632, -0.6087],\n",
      "        [ 1.0259, -0.0751,  0.1897, -0.0160, -0.1119],\n",
      "        [ 0.3102, -0.9266,  0.0202, -0.4345, -0.0096],\n",
      "        [-0.2476, -0.4663,  0.6694,  0.0563,  0.3318]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2794,  0.9998, -0.2503, -0.3466,  1.5161],\n",
      "        [ 0.2931,  0.7726, -0.8137, -0.8888,  1.0500],\n",
      "        [ 0.1739, -2.3043,  0.1347,  0.2873, -0.1286],\n",
      "        [ 1.0241,  1.0524, -1.0451, -0.6391,  0.6286]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9146],\n",
      "        [-0.0149],\n",
      "        [ 2.0682],\n",
      "        [-1.2713]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7952,  0.1308, -0.5756, -0.2183,  0.7203],\n",
      "        [-0.6813,  0.5428,  0.8807,  1.4729, -0.7995],\n",
      "        [-1.2806, -0.2868,  0.1356,  0.9810,  0.4299],\n",
      "        [-0.3929, -2.3922, -1.4108,  0.2633,  1.8529]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5751,  0.0665, -0.4948, -0.4900, -0.2773],\n",
      "        [ 0.3349, -0.6519, -0.9046, -1.0332, -1.0171],\n",
      "        [-0.5644, -1.1318, -0.5076, -0.9472, -0.7802],\n",
      "        [ 0.3056,  0.1938,  0.6992,  0.6010,  0.3301]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7952,  0.1308, -0.5756, -0.2183,  0.7203],\n",
      "        [-0.6813,  0.5428,  0.8807,  1.4729, -0.7995],\n",
      "        [-1.2806, -0.2868,  0.1356,  0.9810,  0.4299],\n",
      "        [-0.3929, -2.3922, -1.4108,  0.2633,  1.8529]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2566],\n",
      "        [-2.0873],\n",
      "        [-0.2860],\n",
      "        [-0.8004]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0825,  1.1936,  0.9934,  1.4706,  0.2357],\n",
      "        [-0.5766,  0.2072,  0.1400, -1.4105, -0.5195],\n",
      "        [ 0.4537,  1.8033, -0.5256, -0.9946,  0.9769],\n",
      "        [-0.0093, -0.4884,  0.0622,  0.6663, -0.1125]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1013, -0.8671, -0.0948, -0.6333, -1.3073],\n",
      "        [ 0.6863,  0.3905,  0.0770,  0.4382,  0.6813],\n",
      "        [-0.6750, -0.9701, -0.6509, -0.1134, -0.7299],\n",
      "        [ 0.1205,  0.7433,  1.1590,  1.1291,  1.2621]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0825,  1.1936,  0.9934,  1.4706,  0.2357],\n",
      "        [-0.5766,  0.2072,  0.1400, -1.4105, -0.5195],\n",
      "        [ 0.4537,  1.8033, -0.5256, -0.9946,  0.9769],\n",
      "        [-0.0093, -0.4884,  0.0622,  0.6663, -0.1125]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4782],\n",
      "        [-1.2759],\n",
      "        [-2.3138],\n",
      "        [ 0.3183]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7459,  0.0970,  1.0349,  0.9658, -0.2216],\n",
      "        [-2.1642, -1.0586, -0.9147, -1.8540,  1.4815],\n",
      "        [-1.1187, -0.9280,  1.2406,  0.2541, -0.2607],\n",
      "        [-0.3726, -0.1185, -1.9248,  2.7975,  0.3705]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1549,  0.6053, -0.0148,  0.1133,  0.4794],\n",
      "        [ 1.1205,  0.5208,  0.3981,  0.7844,  1.5166],\n",
      "        [ 0.3199,  0.6700,  0.5048,  0.6250,  1.5263],\n",
      "        [ 0.4943,  0.3161,  0.6137, -0.0523,  0.1938]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7459,  0.0970,  1.0349,  0.9658, -0.2216],\n",
      "        [-2.1642, -1.0586, -0.9147, -1.8540,  1.4815],\n",
      "        [-1.1187, -0.9280,  1.2406,  0.2541, -0.2607],\n",
      "        [-0.3726, -0.1185, -1.9248,  2.7975,  0.3705]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8149],\n",
      "        [-2.5478],\n",
      "        [-0.5924],\n",
      "        [-1.4773]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8488, -0.1210,  0.2955,  1.2879,  0.0197],\n",
      "        [-0.7129,  1.5473,  1.4701,  0.4843,  1.2909],\n",
      "        [-0.9436, -0.6161,  0.0763,  1.3063,  0.0707],\n",
      "        [-0.5880,  0.2852,  0.0079,  1.3450, -0.7423]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2867,  0.9530,  1.1971,  0.6505,  1.8552],\n",
      "        [ 1.2976,  0.9616,  1.5670,  0.9450,  1.9466],\n",
      "        [ 0.7943,  0.3390,  0.8853,  0.1385,  1.3019],\n",
      "        [ 1.1964,  0.7673,  1.2270,  0.5594,  1.5846]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8488, -0.1210,  0.2955,  1.2879,  0.0197],\n",
      "        [-0.7129,  1.5473,  1.4701,  0.4843,  1.2909],\n",
      "        [-0.9436, -0.6161,  0.0763,  1.3063,  0.0707],\n",
      "        [-0.5880,  0.2852,  0.0079,  1.3450, -0.7423]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2049],\n",
      "        [ 5.8369],\n",
      "        [-0.6178],\n",
      "        [-0.8989]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3614, -1.1659,  1.9872,  0.3436,  0.3143],\n",
      "        [ 1.1223,  1.3368, -0.8623, -0.4206,  0.0712],\n",
      "        [-1.0183, -0.7892, -2.3606, -1.0183, -0.6153],\n",
      "        [ 0.7032, -0.5161,  0.1652,  1.0127,  0.8137]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0460, -0.2969, -0.3023, -0.2732, -1.1196],\n",
      "        [ 0.1159, -1.7449, -1.0024, -1.7164, -1.7307],\n",
      "        [ 0.4205,  0.3770,  0.1775,  0.7332,  0.4342],\n",
      "        [ 1.4332,  1.1476,  1.3085,  0.9264,  1.2861]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3614, -1.1659,  1.9872,  0.3436,  0.3143],\n",
      "        [ 1.1223,  1.3368, -0.8623, -0.4206,  0.0712],\n",
      "        [-1.0183, -0.7892, -2.3606, -1.0183, -0.6153],\n",
      "        [ 0.7032, -0.5161,  0.1652,  1.0127,  0.8137]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7169],\n",
      "        [-0.7396],\n",
      "        [-2.1585],\n",
      "        [ 2.6164]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1949,  1.0019,  0.7428,  0.4928, -1.3024],\n",
      "        [-0.1199,  0.5397, -0.3359, -2.4930, -0.6765],\n",
      "        [ 0.3627,  1.7795,  0.0205,  0.4279,  1.0700],\n",
      "        [ 1.3106, -0.1359,  0.8063,  0.5442,  1.7743]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5474, -0.1979, -0.7051, -0.1900, -0.4315],\n",
      "        [ 0.4100, -0.7198, -0.6517, -0.1811, -0.3125],\n",
      "        [ 1.2081,  1.6357,  0.8696,  1.9876,  2.2741],\n",
      "        [ 0.4265, -0.0538, -0.4883, -0.5258, -0.9345]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1949,  1.0019,  0.7428,  0.4928, -1.3024],\n",
      "        [-0.1199,  0.5397, -0.3359, -2.4930, -0.6765],\n",
      "        [ 0.3627,  1.7795,  0.0205,  0.4279,  1.0700],\n",
      "        [ 1.3106, -0.1359,  0.8063,  0.5442,  1.7743]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1470],\n",
      "        [ 0.4442],\n",
      "        [ 6.6505],\n",
      "        [-1.7717]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8593, -0.5235,  1.1968,  0.7328,  1.1328],\n",
      "        [ 0.2455,  1.6452,  0.6693, -0.8930,  1.6420],\n",
      "        [-1.1650,  0.1059, -0.1318, -1.1373,  0.5807],\n",
      "        [ 0.6693, -0.3997, -0.2160, -0.5155, -0.5479]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2629, -0.3738,  0.0461, -0.7078, -0.8053],\n",
      "        [-0.1703, -0.2794, -0.3893, -0.5488, -1.0453],\n",
      "        [-1.1807, -1.3946, -1.8898, -1.8016, -2.8117],\n",
      "        [ 0.4824,  0.8308,  1.0280,  0.9048,  0.7364]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8593, -0.5235,  1.1968,  0.7328,  1.1328],\n",
      "        [ 0.2455,  1.6452,  0.6693, -0.8930,  1.6420],\n",
      "        [-1.1650,  0.1059, -0.1318, -1.1373,  0.5807],\n",
      "        [ 0.6693, -0.3997, -0.2160, -0.5155, -0.5479]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4061],\n",
      "        [-1.9883],\n",
      "        [ 1.8932],\n",
      "        [-1.1011]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3572, -1.0129,  0.9154,  2.3119,  0.3181],\n",
      "        [ 0.3840,  0.3636, -1.7107,  0.5799,  2.1497],\n",
      "        [-1.6230, -1.4832, -0.3544, -0.2478,  0.7347],\n",
      "        [ 1.7437,  0.3269,  0.5104,  1.3085,  0.0593]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5726,  0.8341,  0.2040, -0.0446,  1.1393],\n",
      "        [ 0.5106,  0.3019,  0.5982,  0.3522,  0.8521],\n",
      "        [-1.6241, -1.9441, -1.7276, -1.8681, -3.9903],\n",
      "        [ 0.7068,  1.4574,  1.9650,  0.8353,  1.6093]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3572, -1.0129,  0.9154,  2.3119,  0.3181],\n",
      "        [ 0.3840,  0.3636, -1.7107,  0.5799,  2.1497],\n",
      "        [-1.6230, -1.4832, -0.3544, -0.2478,  0.7347],\n",
      "        [ 1.7437,  0.3269,  0.5104,  1.3085,  0.0593]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1944],\n",
      "        [ 1.3187],\n",
      "        [ 3.6629],\n",
      "        [ 3.9003]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0692,  1.2756,  1.9133, -1.7823, -0.8498],\n",
      "        [-0.3058, -0.3004,  0.3145,  0.2579,  2.2945],\n",
      "        [-0.7917,  0.2285, -0.2428,  0.5494,  0.1077],\n",
      "        [-1.6802, -0.9539,  1.0033,  0.9547, -0.6802]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3949,  0.3963, -0.1448,  0.1344,  1.0992],\n",
      "        [ 0.1418,  0.1345,  0.2197, -0.1813,  0.1208],\n",
      "        [ 0.3138,  0.1801,  0.2457,  0.3314, -0.6611],\n",
      "        [-0.3236, -0.8170, -0.6167, -1.2713, -0.9244]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0692,  1.2756,  1.9133, -1.7823, -0.8498],\n",
      "        [-0.3058, -0.3004,  0.3145,  0.2579,  2.2945],\n",
      "        [-0.7917,  0.2285, -0.2428,  0.5494,  0.1077],\n",
      "        [-1.6802, -0.9539,  1.0033,  0.9547, -0.6802]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9180],\n",
      "        [ 0.2158],\n",
      "        [-0.1561],\n",
      "        [ 0.1194]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1610,  0.0883, -1.1829, -0.4195, -0.7650],\n",
      "        [ 0.2551, -2.1722, -0.3919, -0.0859,  0.2270],\n",
      "        [-0.4345,  0.4494, -0.1271, -0.3323, -0.8833],\n",
      "        [-0.1312,  1.2067, -1.0923, -0.4547,  0.4684]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0728,  0.5460,  0.3856,  0.7133,  0.7699],\n",
      "        [ 0.7385,  0.2749, -0.6483,  0.2388,  0.4038],\n",
      "        [-0.3194, -0.2091, -1.1055,  0.0310, -0.7369],\n",
      "        [-0.6834, -0.9148, -0.6976, -1.0803, -0.9563]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1610,  0.0883, -1.1829, -0.4195, -0.7650],\n",
      "        [ 0.2551, -2.1722, -0.3919, -0.0859,  0.2270],\n",
      "        [-0.4345,  0.4494, -0.1271, -0.3323, -0.8833],\n",
      "        [-0.1312,  1.2067, -1.0923, -0.4547,  0.4684]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1233],\n",
      "        [-0.0835],\n",
      "        [ 0.8259],\n",
      "        [-0.2090]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7616, -0.5517,  0.6775, -0.7762,  1.0968],\n",
      "        [ 1.4150, -0.9171,  0.8273, -0.1587, -0.0771],\n",
      "        [-0.7099,  1.6956,  0.8574,  1.0086,  0.3334],\n",
      "        [-2.6028,  0.4032,  0.3014,  1.3004, -2.6200]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8769,  0.4867,  1.5141,  0.2295,  1.1915],\n",
      "        [ 0.4145,  0.5414,  0.0264,  0.5499,  0.5247],\n",
      "        [-0.0193,  0.1506,  0.8698, -0.0505, -0.2486],\n",
      "        [-0.0444, -0.0431, -0.2153,  0.1811,  0.4266]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7616, -0.5517,  0.6775, -0.7762,  1.0968],\n",
      "        [ 1.4150, -0.9171,  0.8273, -0.1587, -0.0771],\n",
      "        [-0.7099,  1.6956,  0.8574,  1.0086,  0.3334],\n",
      "        [-2.6028,  0.4032,  0.3014,  1.3004, -2.6200]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5538],\n",
      "        [-0.0158],\n",
      "        [ 0.8809],\n",
      "        [-0.8488]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1429,  0.1130,  0.2721, -1.4326,  1.8342],\n",
      "        [-0.5742,  2.4338, -0.8561,  0.4553,  1.6747],\n",
      "        [ 1.1506, -0.1460, -0.6836, -0.9900,  1.7593],\n",
      "        [-0.1312, -1.3356,  0.0595,  0.5103,  0.3508]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1962,  0.2341, -0.0954,  0.0292, -1.5094],\n",
      "        [ 0.2389, -0.5001,  0.3113,  0.8866,  0.2171],\n",
      "        [-0.3157, -0.1702,  0.4743,  0.0393, -0.2280],\n",
      "        [ 0.2336,  0.7365,  0.7594,  1.2957,  1.1449]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1429,  0.1130,  0.2721, -1.4326,  1.8342],\n",
      "        [-0.5742,  2.4338, -0.8561,  0.4553,  1.6747],\n",
      "        [ 1.1506, -0.1460, -0.6836, -0.9900,  1.7593],\n",
      "        [-0.1312, -1.3356,  0.0595,  0.5103,  0.3508]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.7817],\n",
      "        [-0.8535],\n",
      "        [-1.1026],\n",
      "        [ 0.0937]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9449, -1.2500,  0.8158,  0.7336,  1.8426],\n",
      "        [-2.1496,  0.1967, -1.2336, -0.8364,  0.6853],\n",
      "        [-1.4275, -0.0079,  1.7877,  0.3615,  0.5001],\n",
      "        [ 0.9240, -2.3373, -0.9082, -0.3264, -2.1688]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2195,  0.8536,  0.7945,  0.7286,  1.6191],\n",
      "        [ 0.9438,  0.7943,  0.4716,  0.4093,  0.7317],\n",
      "        [ 0.2032,  0.3514,  0.0843, -0.7437,  0.3767],\n",
      "        [-0.2400, -0.0527,  0.7680,  0.7569,  0.2892]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9449, -1.2500,  0.8158,  0.7336,  1.8426],\n",
      "        [-2.1496,  0.1967, -1.2336, -0.8364,  0.6853],\n",
      "        [-1.4275, -0.0079,  1.7877,  0.3615,  0.5001],\n",
      "        [ 0.9240, -2.3373, -0.9082, -0.3264, -2.1688]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.2513],\n",
      "        [-2.2953],\n",
      "        [-0.2226],\n",
      "        [-1.6704]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2071, -0.8597,  0.8939, -1.4612,  0.1261],\n",
      "        [-0.6262,  0.0032, -0.3683, -0.4194, -0.9504],\n",
      "        [-0.4528,  1.2886, -1.1957,  0.7479, -1.0028],\n",
      "        [-2.3227,  0.3479, -0.3382, -1.4247,  1.3173]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8421, -0.3449, -1.0395, -0.6785, -1.2279],\n",
      "        [ 1.5885,  1.6433,  1.7996,  1.3516,  1.8097],\n",
      "        [ 0.2769,  0.2356,  0.0909,  0.2420, -0.0495],\n",
      "        [ 0.3928,  1.1967,  0.7909,  1.3973,  1.4341]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2071, -0.8597,  0.8939, -1.4612,  0.1261],\n",
      "        [-0.6262,  0.0032, -0.3683, -0.4194, -0.9504],\n",
      "        [-0.4528,  1.2886, -1.1957,  0.7479, -1.0028],\n",
      "        [-2.3227,  0.3479, -0.3382, -1.4247,  1.3173]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8127],\n",
      "        [-3.9391],\n",
      "        [ 0.3002],\n",
      "        [-0.8653]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9867, -0.2901, -1.0690,  0.2656,  1.4314],\n",
      "        [-1.3354,  1.9130,  1.9894,  1.6750, -0.2837],\n",
      "        [ 0.0602,  0.1043, -0.3661, -0.4027,  0.8316],\n",
      "        [-1.0791, -2.3256, -1.1600,  0.7536, -0.3871]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2859, -0.0502, -0.0292, -0.4664, -0.3325],\n",
      "        [-0.1779, -0.2478,  0.2360,  0.5038,  0.9476],\n",
      "        [ 0.6281, -0.5984, -0.1661,  0.0671,  0.4956],\n",
      "        [ 1.1014,  1.0847,  1.3885,  1.1604,  1.8350]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9867, -0.2901, -1.0690,  0.2656,  1.4314],\n",
      "        [-1.3354,  1.9130,  1.9894,  1.6750, -0.2837],\n",
      "        [ 0.0602,  0.1043, -0.3661, -0.4027,  0.8316],\n",
      "        [-1.0791, -2.3256, -1.1600,  0.7536, -0.3871]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0140],\n",
      "        [ 0.8081],\n",
      "        [ 0.4213],\n",
      "        [-5.1576]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7584,  1.4749, -0.3912,  1.1004,  0.7030],\n",
      "        [-0.6307,  0.3170,  0.0512,  1.8558,  1.6608],\n",
      "        [ 0.6305, -0.9204,  0.2873,  1.7483, -0.2177],\n",
      "        [-1.3349,  0.5207,  0.2054,  1.9161,  0.5225]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5698,  0.6783,  1.4838,  0.4427,  0.7413],\n",
      "        [-0.0765,  0.4971,  0.7849,  0.3158, -0.7557],\n",
      "        [ 0.6362, -0.2188, -0.1157,  0.2758,  0.4435],\n",
      "        [ 0.2871, -0.1759, -0.2657, -0.4355, -0.2753]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7584,  1.4749, -0.3912,  1.1004,  0.7030],\n",
      "        [-0.6307,  0.3170,  0.0512,  1.8558,  1.6608],\n",
      "        [ 0.6305, -0.9204,  0.2873,  1.7483, -0.2177],\n",
      "        [-1.3349,  0.5207,  0.2054,  1.9161,  0.5225]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9960],\n",
      "        [-0.4228],\n",
      "        [ 0.9549],\n",
      "        [-1.5076]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6116,  0.2213,  0.0900, -0.6341,  0.4352],\n",
      "        [ 0.1974, -1.4140,  0.1159,  1.8718, -0.0134],\n",
      "        [ 0.1792,  0.4179, -0.1958,  1.8306, -0.6562],\n",
      "        [ 0.1128,  1.2639, -0.1123, -0.5095, -0.4831]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0426,  1.3936,  1.1345,  1.1014,  0.9948],\n",
      "        [ 0.3176,  0.0179, -0.1960, -0.2952, -0.2386],\n",
      "        [-0.6981, -0.1122, -0.3028, -0.1772, -0.7982],\n",
      "        [ 0.1246,  1.0082,  0.7545,  0.5242,  1.0849]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6116,  0.2213,  0.0900, -0.6341,  0.4352],\n",
      "        [ 0.1974, -1.4140,  0.1159,  1.8718, -0.0134],\n",
      "        [ 0.1792,  0.4179, -0.1958,  1.8306, -0.6562],\n",
      "        [ 0.1128,  1.2639, -0.1123, -0.5095, -0.4831]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1189],\n",
      "        [-0.5348],\n",
      "        [ 0.0868],\n",
      "        [ 0.4124]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5901, -0.4714,  1.1644,  0.4333, -0.0370],\n",
      "        [-1.6733,  0.5987,  0.5014, -0.3002, -0.3360],\n",
      "        [-0.9695,  0.5961,  0.8341, -0.0238,  1.1509],\n",
      "        [ 0.6045,  0.0874, -2.0883,  0.2973,  0.5489]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2446,  0.5869,  1.3426,  0.4899,  1.2654],\n",
      "        [ 0.3630,  0.2395,  0.1204, -0.4861, -0.0380],\n",
      "        [-0.7216,  0.5929,  0.3556,  0.2301,  0.3715],\n",
      "        [ 0.2223, -0.0051, -0.3623, -0.2447,  0.4846]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5901, -0.4714,  1.1644,  0.4333, -0.0370],\n",
      "        [-1.6733,  0.5987,  0.5014, -0.3002, -0.3360],\n",
      "        [-0.9695,  0.5961,  0.8341, -0.0238,  1.1509],\n",
      "        [ 0.6045,  0.0874, -2.0883,  0.2973,  0.5489]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3079],\n",
      "        [-0.2450],\n",
      "        [ 1.7716],\n",
      "        [ 1.0837]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5356, -0.5486, -0.8649,  0.7174, -0.4274],\n",
      "        [-0.7508, -1.0906, -1.0090, -0.3053,  1.6498],\n",
      "        [-0.6302, -2.0195,  1.0969,  2.5088,  2.1104],\n",
      "        [-0.5720, -1.1525,  1.2953, -1.1096,  0.8935]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0101,  0.4534, -0.3267,  0.2367,  0.9890],\n",
      "        [ 0.2381, -0.4360, -1.1365, -0.0127,  0.5583],\n",
      "        [-0.7483, -0.5167, -0.9865, -0.1959, -0.3570],\n",
      "        [ 0.0565, -0.3362, -0.3720, -0.1869, -0.3667]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5356, -0.5486, -0.8649,  0.7174, -0.4274],\n",
      "        [-0.7508, -1.0906, -1.0090, -0.3053,  1.6498],\n",
      "        [-0.6302, -2.0195,  1.0969,  2.5088,  2.1104],\n",
      "        [-0.5720, -1.1525,  1.2953, -1.1096,  0.8935]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2137],\n",
      "        [ 2.3684],\n",
      "        [-0.8121],\n",
      "        [-0.2469]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4814,  0.9091, -1.9523, -1.9777,  0.7198],\n",
      "        [-1.4253, -0.2410, -1.1335, -0.9861,  2.7643],\n",
      "        [-0.9237, -0.7971,  0.5793,  0.3382,  0.5253],\n",
      "        [ 0.1817, -0.4150, -1.5474, -0.0340, -0.8820]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2838,  1.6550,  0.7808,  1.1341,  1.0371],\n",
      "        [-0.4222, -0.1783, -1.8513, -1.5505, -2.2040],\n",
      "        [-0.2150,  0.2961,  0.2176,  0.1921, -0.0852],\n",
      "        [-0.0901,  0.1492,  0.7767, -0.3583, -0.4228]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4814,  0.9091, -1.9523, -1.9777,  0.7198],\n",
      "        [-1.4253, -0.2410, -1.1335, -0.9861,  2.7643],\n",
      "        [-0.9237, -0.7971,  0.5793,  0.3382,  0.5253],\n",
      "        [ 0.1817, -0.4150, -1.5474, -0.0340, -0.8820]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3797],\n",
      "        [-1.8205],\n",
      "        [ 0.1089],\n",
      "        [-0.8951]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1360,  0.5382,  0.9442,  0.4933, -0.5658],\n",
      "        [-1.3021, -1.3811,  0.3161, -0.0302, -0.7791],\n",
      "        [-0.1820, -0.8718, -0.2325, -0.4781, -0.4305],\n",
      "        [-0.6163,  1.1004, -0.5787,  0.5784,  0.4660]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5475,  1.3251,  1.1757,  1.2764,  1.4750],\n",
      "        [ 0.1447, -0.5280,  0.0083, -0.0883, -0.6824],\n",
      "        [-0.1494,  0.1701, -0.4313,  0.4039,  0.2053],\n",
      "        [ 0.3231, -0.1141,  0.1039,  0.2909,  0.4812]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1360,  0.5382,  0.9442,  0.4933, -0.5658],\n",
      "        [-1.3021, -1.3811,  0.3161, -0.0302, -0.7791],\n",
      "        [-0.1820, -0.8718, -0.2325, -0.4781, -0.4305],\n",
      "        [-0.6163,  1.1004, -0.5787,  0.5784,  0.4660]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2404],\n",
      "        [ 1.0778],\n",
      "        [-0.3024],\n",
      "        [ 0.0077]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4415, -1.7158,  0.6984, -1.0311,  0.9072],\n",
      "        [-0.8605, -0.6310, -1.6490,  0.6737, -0.3703],\n",
      "        [ 0.9831,  0.3147,  0.2101, -0.3753,  1.0618],\n",
      "        [ 1.3773,  1.4787,  0.9036, -0.4200, -1.1696]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0711,  1.2457,  0.3516,  0.4388,  0.1626],\n",
      "        [-0.2366, -0.3768, -0.7263, -1.1071, -1.3616],\n",
      "        [ 0.0704, -0.1631,  0.1148, -0.3218, -0.4704],\n",
      "        [ 0.1012,  0.2378,  0.0944, -0.4956, -0.0158]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4415, -1.7158,  0.6984, -1.0311,  0.9072],\n",
      "        [-0.8605, -0.6310, -1.6490,  0.6737, -0.3703],\n",
      "        [ 0.9831,  0.3147,  0.2101, -0.3753,  1.0618],\n",
      "        [ 1.3773,  1.4787,  0.9036, -0.4200, -1.1696]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1653],\n",
      "        [ 1.3974],\n",
      "        [-0.3366],\n",
      "        [ 0.8031]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4585,  0.9521,  1.6105, -0.7961,  0.3635],\n",
      "        [-1.5306, -0.5004, -0.1312,  0.9066, -0.4594],\n",
      "        [-0.6656, -0.3016,  0.5971, -0.3262, -0.5233],\n",
      "        [-0.6617,  1.0895, -0.7560,  0.5342,  0.8036]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0318,  1.3161,  1.8540,  0.8105,  1.7407],\n",
      "        [-0.7671, -1.1963, -0.3878, -1.1354, -2.0228],\n",
      "        [-0.2732, -0.3607, -0.2008, -0.0942, -0.4967],\n",
      "        [ 0.0060, -0.6182,  0.0759,  0.6675, -0.2910]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4585,  0.9521,  1.6105, -0.7961,  0.3635],\n",
      "        [-1.5306, -0.5004, -0.1312,  0.9066, -0.4594],\n",
      "        [-0.6656, -0.3016,  0.5971, -0.3262, -0.5233],\n",
      "        [-0.6617,  1.0895, -0.7560,  0.5342,  0.8036]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.7534],\n",
      "        [ 1.7236],\n",
      "        [ 0.4614],\n",
      "        [-0.6122]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5589,  1.0934,  0.0557,  0.4224,  0.4751],\n",
      "        [ 1.2358, -0.1330, -0.0069,  1.3685,  0.3993],\n",
      "        [ 0.0676, -0.9369,  1.0705, -0.1743,  0.1474],\n",
      "        [-0.6973, -2.2867, -1.3041,  1.7064,  0.3674]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1418, -0.7770,  0.5779, -0.3047, -0.5679],\n",
      "        [-0.4700, -1.2794, -2.1677, -2.0966, -3.1958],\n",
      "        [ 0.1091, -0.0426,  0.0919, -0.3317, -1.1950],\n",
      "        [ 0.2319,  0.8295,  0.3514, -0.4987,  0.5554]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5589,  1.0934,  0.0557,  0.4224,  0.4751],\n",
      "        [ 1.2358, -0.1330, -0.0069,  1.3685,  0.3993],\n",
      "        [ 0.0676, -0.9369,  1.0705, -0.1743,  0.1474],\n",
      "        [-0.6973, -2.2867, -1.3041,  1.7064,  0.3674]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4370],\n",
      "        [-4.5408],\n",
      "        [ 0.0273],\n",
      "        [-3.1638]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5418, -0.1698, -1.7974, -0.0076, -0.2895],\n",
      "        [ 0.2287,  1.1766,  0.6609,  0.6862, -0.1414],\n",
      "        [-1.0905,  0.5868, -0.1350,  1.7834,  0.4436],\n",
      "        [-1.0682, -0.5896, -1.6469,  1.5269, -0.0861]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2404,  0.7760,  1.5927,  1.1399,  1.0733],\n",
      "        [ 0.6882,  0.1023, -0.2131, -0.8413,  0.0794],\n",
      "        [-0.2181,  0.3438,  0.7564,  0.1985,  0.1838],\n",
      "        [ 1.0995,  1.3541,  1.8190,  1.1321,  2.3122]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5418, -0.1698, -1.7974, -0.0076, -0.2895],\n",
      "        [ 0.2287,  1.1766,  0.6609,  0.6862, -0.1414],\n",
      "        [-1.0905,  0.5868, -0.1350,  1.7834,  0.4436],\n",
      "        [-1.0682, -0.5896, -1.6469,  1.5269, -0.0861]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.4441],\n",
      "        [-0.4516],\n",
      "        [ 0.7729],\n",
      "        [-3.4391]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5388, -0.0287,  0.0410,  0.2266, -0.6241],\n",
      "        [ 0.4003, -0.7639, -0.2634,  0.3757,  0.2496],\n",
      "        [-0.3163,  2.1400,  0.9146, -0.4842,  0.1000],\n",
      "        [-1.3627,  0.1803, -0.1688,  1.7510,  1.6711]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0878,  2.0845,  1.6471,  1.9223,  2.6354],\n",
      "        [ 0.3606, -0.7210, -0.5269, -1.0911, -0.4426],\n",
      "        [-0.2513, -0.4226, -0.0564, -0.1620, -1.3775],\n",
      "        [ 2.4565,  2.2486,  2.6011,  1.8402,  2.8169]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5388, -0.0287,  0.0410,  0.2266, -0.6241],\n",
      "        [ 0.4003, -0.7639, -0.2634,  0.3757,  0.2496],\n",
      "        [-0.3163,  2.1400,  0.9146, -0.4842,  0.1000],\n",
      "        [-1.3627,  0.1803, -0.1688,  1.7510,  1.6711]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6155],\n",
      "        [ 0.3135],\n",
      "        [-0.9357],\n",
      "        [ 4.5482]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1260, -0.3012,  0.0040,  0.4760,  0.7217],\n",
      "        [ 1.0243,  2.4530,  1.6842,  1.0322,  0.4830],\n",
      "        [-0.0742, -1.8444, -0.8859, -0.2798,  0.1737],\n",
      "        [ 1.1794,  0.6398,  0.3897,  0.3824, -0.7123]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2397, -0.1989,  0.5012, -0.0442,  0.3201],\n",
      "        [-0.0050, -1.4204, -1.0469, -1.1646, -1.1317],\n",
      "        [ 0.5188, -0.3718, -0.3006, -0.4929,  0.3397],\n",
      "        [ 0.0874, -0.2962,  0.5809, -0.2744,  0.4006]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1260, -0.3012,  0.0040,  0.4760,  0.7217],\n",
      "        [ 1.0243,  2.4530,  1.6842,  1.0322,  0.4830],\n",
      "        [-0.0742, -1.8444, -0.8859, -0.2798,  0.1737],\n",
      "        [ 1.1794,  0.6398,  0.3897,  0.3824, -0.7123]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3021],\n",
      "        [-7.0011],\n",
      "        [ 1.1104],\n",
      "        [-0.2503]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0415, -0.6720, -1.4883, -0.1728, -2.3408],\n",
      "        [ 0.9917,  1.7075,  0.9752,  0.9781,  0.0339],\n",
      "        [ 0.0572,  0.0120,  0.6188, -2.5831,  0.4968],\n",
      "        [ 0.0927,  1.1123,  0.8078,  0.2142, -0.3064]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2372,  0.3527,  1.0208,  0.5056,  0.0250],\n",
      "        [ 2.3641,  2.3933,  1.8564,  1.3895,  3.0842],\n",
      "        [-0.0008, -0.0118, -0.6242, -0.2415, -1.4231],\n",
      "        [ 0.7582, -0.2720,  0.3994, -0.3023,  0.0020]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0415, -0.6720, -1.4883, -0.1728, -2.3408],\n",
      "        [ 0.9917,  1.7075,  0.9752,  0.9781,  0.0339],\n",
      "        [ 0.0572,  0.0120,  0.6188, -2.5831,  0.4968],\n",
      "        [ 0.0927,  1.1123,  0.8078,  0.2142, -0.3064]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6552],\n",
      "        [ 9.7050],\n",
      "        [-0.4696],\n",
      "        [ 0.0251]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4952, -0.4160,  1.3442,  1.3686,  1.1644],\n",
      "        [ 0.2079,  0.5995,  1.4674,  0.3570, -0.2433],\n",
      "        [-0.5113,  0.1086,  0.3132, -0.2067,  0.1050],\n",
      "        [ 1.8737, -0.7691, -0.3703,  0.0158,  0.5533]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5342,  1.0529,  0.8184,  0.8010,  0.8986],\n",
      "        [-1.1644, -0.9611, -1.7234, -1.4927, -2.9341],\n",
      "        [-0.0640, -0.4319, -0.1395, -0.9641, -1.0309],\n",
      "        [ 0.0652, -0.2465, -0.2618,  0.3383,  0.5341]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4952, -0.4160,  1.3442,  1.3686,  1.1644],\n",
      "        [ 0.2079,  0.5995,  1.4674,  0.3570, -0.2433],\n",
      "        [-0.5113,  0.1086,  0.3132, -0.2067,  0.1050],\n",
      "        [ 1.8737, -0.7691, -0.3703,  0.0158,  0.5533]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0691],\n",
      "        [-3.1663],\n",
      "        [ 0.0332],\n",
      "        [ 0.7096]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0364, -0.5693, -1.3084, -0.9448, -0.6613],\n",
      "        [ 0.7495, -0.4341, -0.3838,  1.6457, -0.9451],\n",
      "        [-0.1576, -1.7885,  0.2617, -0.5360,  1.1035],\n",
      "        [-0.0282,  0.0909,  0.6520, -0.4422,  2.6605]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2461, -1.0499, -0.3544, -0.6752, -1.6385],\n",
      "        [-0.5013, -1.3898, -0.1628, -1.0846, -0.8957],\n",
      "        [-0.8587, -0.1911, -0.3053, -0.1346, -0.0984],\n",
      "        [-0.1792,  0.4070,  0.2818,  0.1542,  0.2438]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0364, -0.5693, -1.3084, -0.9448, -0.6613],\n",
      "        [ 0.7495, -0.4341, -0.3838,  1.6457, -0.9451],\n",
      "        [-0.1576, -1.7885,  0.2617, -0.5360,  1.1035],\n",
      "        [-0.0282,  0.0909,  0.6520, -0.4422,  2.6605]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.7917],\n",
      "        [-0.6485],\n",
      "        [ 0.3607],\n",
      "        [ 0.8064]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4163, -0.3396,  0.6047, -1.3691,  1.3636],\n",
      "        [ 0.0570,  0.6782, -0.2953, -0.9492,  0.4712],\n",
      "        [ 0.2322, -1.4304, -0.9464,  1.2165,  0.6237],\n",
      "        [ 0.9882, -0.3142,  0.4878,  0.6768,  2.8155]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.4269, -1.5720, -1.7503, -1.2913, -3.5877],\n",
      "        [ 0.3068, -1.0333, -1.0714, -0.8862, -1.2493],\n",
      "        [-0.3732, -0.1826, -0.5602, -0.0224, -0.4651],\n",
      "        [-0.4177, -0.1152,  0.5980,  0.7607, -0.1726]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4163, -0.3396,  0.6047, -1.3691,  1.3636],\n",
      "        [ 0.0570,  0.6782, -0.2953, -0.9492,  0.4712],\n",
      "        [ 0.2322, -1.4304, -0.9464,  1.2165,  0.6237],\n",
      "        [ 0.9882, -0.3142,  0.4878,  0.6768,  2.8155]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.2426],\n",
      "        [-0.1143],\n",
      "        [ 0.3875],\n",
      "        [-0.0560]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3712, -1.5587,  0.2422, -0.5068, -0.5346],\n",
      "        [ 0.5894,  0.1983,  0.1350,  2.5302,  2.1649],\n",
      "        [-1.9754,  1.6995,  2.2809,  1.1788, -0.2265],\n",
      "        [-1.5174, -1.0789, -1.4434, -0.9041, -0.3371]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5476, -0.4122, -0.2588, -0.4217,  0.0315],\n",
      "        [ 0.2738, -0.8192, -0.4170, -1.1341, -0.9746],\n",
      "        [ 0.0687, -0.7580, -0.3827,  0.0893, -0.1422],\n",
      "        [-0.6805,  0.7528,  0.5036,  0.0992,  0.5457]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3712, -1.5587,  0.2422, -0.5068, -0.5346],\n",
      "        [ 0.5894,  0.1983,  0.1350,  2.5302,  2.1649],\n",
      "        [-1.9754,  1.6995,  2.2809,  1.1788, -0.2265],\n",
      "        [-1.5174, -1.0789, -1.4434, -0.9041, -0.3371]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5735],\n",
      "        [-5.0367],\n",
      "        [-2.1594],\n",
      "        [-0.7800]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0050,  1.7738,  0.6256,  0.4025, -0.2721],\n",
      "        [ 0.6564, -1.0875, -0.9628, -0.5971, -0.1119],\n",
      "        [-0.3849, -0.7331,  0.0273,  0.0247,  0.2966],\n",
      "        [ 1.0022, -0.8146, -0.5485, -1.0771,  0.4226]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4134, -0.4244, -0.4501, -0.5471, -0.9031],\n",
      "        [ 1.0817,  1.9709,  2.1281,  1.1544,  2.2709],\n",
      "        [ 0.5675,  1.1307,  0.7312,  0.4978,  0.7891],\n",
      "        [ 0.3403,  0.4345, -0.0674,  0.4554,  0.7986]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0050,  1.7738,  0.6256,  0.4025, -0.2721],\n",
      "        [ 0.6564, -1.0875, -0.9628, -0.5971, -0.1119],\n",
      "        [-0.3849, -0.7331,  0.0273,  0.0247,  0.2966],\n",
      "        [ 1.0022, -0.8146, -0.5485, -1.0771,  0.4226]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5933],\n",
      "        [-4.4257],\n",
      "        [-0.7811],\n",
      "        [-0.1290]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1604,  0.9513,  1.5715,  0.3568,  0.1562],\n",
      "        [ 0.6411,  0.3738, -0.4487,  0.5930,  1.7700],\n",
      "        [-0.5527, -0.0207,  0.5248,  0.5342,  1.1491],\n",
      "        [ 0.5920, -1.1777, -1.4764,  0.2616,  1.1066]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2179, -0.8094, -1.0803, -0.3577, -0.0644],\n",
      "        [ 0.6884,  0.0223,  0.1211, -0.6431,  0.4635],\n",
      "        [ 0.6348,  0.8577,  0.5950,  0.2895,  0.9811],\n",
      "        [ 0.1881,  0.0351, -0.2013, -0.0731,  0.3338]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1604,  0.9513,  1.5715,  0.3568,  0.1562],\n",
      "        [ 0.6411,  0.3738, -0.4487,  0.5930,  1.7700],\n",
      "        [-0.5527, -0.0207,  0.5248,  0.5342,  1.1491],\n",
      "        [ 0.5920, -1.1777, -1.4764,  0.2616,  1.1066]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8583],\n",
      "        [ 0.8343],\n",
      "        [ 1.2257],\n",
      "        [ 0.7174]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2742, -0.2053,  0.5045,  0.4801,  0.3077],\n",
      "        [-2.0032,  0.3286,  2.4267, -0.9821, -1.7612],\n",
      "        [-1.1860, -1.0756,  0.1746,  0.6616,  0.5893],\n",
      "        [-0.2144,  0.5906,  0.4736,  1.2076, -0.0124]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1220,  1.0634,  1.7069,  0.6629,  1.9552],\n",
      "        [-0.6419, -0.5091, -0.6690,  0.0517, -1.0270],\n",
      "        [ 0.1422, -0.0831, -0.1764,  0.0122, -0.1896],\n",
      "        [ 0.2842,  0.9685, -0.1875, -0.1894,  1.0300]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2742, -0.2053,  0.5045,  0.4801,  0.3077],\n",
      "        [-2.0032,  0.3286,  2.4267, -0.9821, -1.7612],\n",
      "        [-1.1860, -1.0756,  0.1746,  0.6616,  0.5893],\n",
      "        [-0.2144,  0.5906,  0.4736,  1.2076, -0.0124]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8705],\n",
      "        [ 1.2532],\n",
      "        [-0.2137],\n",
      "        [ 0.1808]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2310,  1.2401, -0.5491,  0.5472,  0.8074],\n",
      "        [-0.9892,  0.4490,  0.8446,  0.4409,  1.0700],\n",
      "        [-0.0698, -0.0620,  0.8370,  0.6554, -0.3504],\n",
      "        [-0.5925, -1.0026, -0.4565,  1.1182, -1.1466]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3315,  0.2474, -0.2968, -0.1900, -0.7044],\n",
      "        [-0.1214, -0.3456, -0.3149, -0.0182, -1.0550],\n",
      "        [ 0.0380, -0.6371, -0.5781, -0.1666,  0.2183],\n",
      "        [ 0.0931, -0.4391, -0.0564,  0.3032,  0.3576]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2310,  1.2401, -0.5491,  0.5472,  0.8074],\n",
      "        [-0.9892,  0.4490,  0.8446,  0.4409,  1.0700],\n",
      "        [-0.0698, -0.0620,  0.8370,  0.6554, -0.3504],\n",
      "        [-0.5925, -1.0026, -0.4565,  1.1182, -1.1466]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1263],\n",
      "        [-1.4379],\n",
      "        [-0.6327],\n",
      "        [ 0.3399]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6469,  1.7900,  0.5383,  0.2519,  0.2189],\n",
      "        [-1.2679,  2.0109, -0.6814, -1.4634, -0.5751],\n",
      "        [-0.9119,  0.4948,  1.9663, -1.1090,  0.1020],\n",
      "        [-0.2081, -0.2705, -0.0235,  0.6441,  0.7828]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8611, -0.8050, -0.4143, -0.4627, -0.5095],\n",
      "        [ 0.5490,  0.3082,  0.1940,  0.7200,  0.1670],\n",
      "        [-0.1045,  0.5669,  0.0318, -0.2033, -0.4168],\n",
      "        [ 0.0440,  0.2535,  1.1353,  0.6736,  0.4321]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6469,  1.7900,  0.5383,  0.2519,  0.2189],\n",
      "        [-1.2679,  2.0109, -0.6814, -1.4634, -0.5751],\n",
      "        [-0.9119,  0.4948,  1.9663, -1.1090,  0.1020],\n",
      "        [-0.2081, -0.2705, -0.0235,  0.6441,  0.7828]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4491],\n",
      "        [-1.3581],\n",
      "        [ 0.6213],\n",
      "        [ 0.6677]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1347,  1.0022, -0.0507, -0.1877,  0.4583],\n",
      "        [ 2.1153,  1.0147,  1.6897,  1.3540,  0.8133],\n",
      "        [ 0.0155, -1.9589,  0.1536, -1.7109, -0.4979],\n",
      "        [ 0.8731,  0.7576,  0.6335, -0.6157, -0.2812]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9279,  1.2027,  1.2815,  0.7653,  1.9526],\n",
      "        [ 0.7161,  0.8578,  1.5451,  0.6051,  0.9220],\n",
      "        [-0.1828, -0.9454, -0.1054,  0.1654, -0.3069],\n",
      "        [-0.0516,  0.4675,  0.6757,  0.6967, -0.1014]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1347,  1.0022, -0.0507, -0.1877,  0.4583],\n",
      "        [ 2.1153,  1.0147,  1.6897,  1.3540,  0.8133],\n",
      "        [ 0.0155, -1.9589,  0.1536, -1.7109, -0.4979],\n",
      "        [ 0.8731,  0.7576,  0.6335, -0.6157, -0.2812]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7667],\n",
      "        [ 6.5653],\n",
      "        [ 1.7028],\n",
      "        [ 0.3368]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4834, -0.7562,  1.1525,  0.0773,  0.1432],\n",
      "        [-1.4099,  0.1607,  0.3892, -1.3666, -0.2149],\n",
      "        [-2.0649,  0.5850,  0.9861,  0.9614,  0.8579],\n",
      "        [-0.7080,  1.0367,  0.9098,  0.0022,  1.1519]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0122, -0.8120, -0.6197, -0.3615, -0.2232],\n",
      "        [-1.1488, -1.5766, -1.0628, -3.2025, -4.2313],\n",
      "        [-0.6766, -0.8138,  0.0435, -1.2766, -1.3671],\n",
      "        [-0.1334,  0.9902,  1.1336,  0.2811,  0.0914]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4834, -0.7562,  1.1525,  0.0773,  0.1432],\n",
      "        [-1.4099,  0.1607,  0.3892, -1.3666, -0.2149],\n",
      "        [-2.0649,  0.5850,  0.9861,  0.9614,  0.8579],\n",
      "        [-0.7080,  1.0367,  0.9098,  0.0022,  1.1519]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1660],\n",
      "        [ 6.2388],\n",
      "        [-1.4362],\n",
      "        [ 2.2582]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0788,  0.1987, -0.8572,  0.2381,  0.0045],\n",
      "        [-0.8086,  0.7417, -1.5422, -0.9978,  0.2852],\n",
      "        [ 2.0805,  0.5337, -1.1050, -0.7736,  0.4189],\n",
      "        [-2.4504, -0.7486,  0.1603,  0.0446,  1.6785]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3362, -0.4093,  0.0906, -0.2734, -0.6832],\n",
      "        [ 0.5620,  0.0790,  0.0136, -0.3627,  0.1649],\n",
      "        [ 0.1724, -0.3392, -0.5494, -0.3625, -0.9926],\n",
      "        [-0.3362,  0.1395, -0.6971, -1.1867, -1.4015]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0788,  0.1987, -0.8572,  0.2381,  0.0045],\n",
      "        [-0.8086,  0.7417, -1.5422, -0.9978,  0.2852],\n",
      "        [ 2.0805,  0.5337, -1.1050, -0.7736,  0.4189],\n",
      "        [-2.4504, -0.7486,  0.1603,  0.0446,  1.6785]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2536],\n",
      "        [-0.0077],\n",
      "        [ 0.6494],\n",
      "        [-1.7976]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4156, -1.4026,  0.4419, -0.0085,  0.9637],\n",
      "        [ 1.0336,  0.0103, -0.4287,  0.6177,  2.4026],\n",
      "        [ 1.3668, -0.5819,  0.1085, -0.8445, -0.3827],\n",
      "        [ 0.9747,  1.6463, -0.3910, -0.0305,  0.0475]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2721, -0.7864, -0.2187, -0.8123, -0.5707],\n",
      "        [-0.0629, -0.0196,  0.1124,  0.5372,  0.6126],\n",
      "        [-0.0387, -0.3650, -0.1438,  0.4606, -0.6400],\n",
      "        [-0.0129,  0.3489,  0.3148,  0.4713,  0.5745]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4156, -1.4026,  0.4419, -0.0085,  0.9637],\n",
      "        [ 1.0336,  0.0103, -0.4287,  0.6177,  2.4026],\n",
      "        [ 1.3668, -0.5819,  0.1085, -0.8445, -0.3827],\n",
      "        [ 0.9747,  1.6463, -0.3910, -0.0305,  0.0475]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 7.8034e-02],\n",
      "        [ 1.6902e+00],\n",
      "        [-8.9079e-05],\n",
      "        [ 4.5168e-01]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7154, -0.4587,  1.6407,  1.7494, -0.9384],\n",
      "        [ 0.3790, -0.9163,  3.0554, -1.1728,  0.2499],\n",
      "        [ 0.4259,  0.3724,  1.0040,  0.0224,  0.7137],\n",
      "        [-1.6553, -1.3084,  0.2716,  0.4216, -0.1936]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1562,  0.5593,  0.0476, -0.0528, -0.0554],\n",
      "        [-0.8566, -0.2687, -0.0746, -1.1522, -1.2172],\n",
      "        [-0.0387, -0.4666, -0.2690, -0.1788, -0.8187],\n",
      "        [-0.5599, -0.1356,  0.1668,  0.8789,  0.6871]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7154, -0.4587,  1.6407,  1.7494, -0.9384],\n",
      "        [ 0.3790, -0.9163,  3.0554, -1.1728,  0.2499],\n",
      "        [ 0.4259,  0.3724,  1.0040,  0.0224,  0.7137],\n",
      "        [-1.6553, -1.3084,  0.2716,  0.4216, -0.1936]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1071],\n",
      "        [ 0.7408],\n",
      "        [-1.0486],\n",
      "        [ 1.3871]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3721, -0.1397, -0.3368, -0.7438,  0.6768],\n",
      "        [-0.8785,  1.0104,  1.5126, -1.7370, -1.8591],\n",
      "        [-2.1180, -0.3318, -1.7723, -1.6935,  1.0242],\n",
      "        [ 1.1757, -0.1426,  0.3133, -1.4948, -0.4768]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3685, -0.1164, -0.7215,  0.0173,  0.1749],\n",
      "        [-0.6912, -0.5724, -0.6560, -0.6660, -1.8711],\n",
      "        [ 0.4321,  0.0249,  0.1029, -0.1644,  0.8948],\n",
      "        [-0.8410, -0.6194, -0.1262, -0.8913, -1.7806]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3721, -0.1397, -0.3368, -0.7438,  0.6768],\n",
      "        [-0.8785,  1.0104,  1.5126, -1.7370, -1.8591],\n",
      "        [-2.1180, -0.3318, -1.7723, -1.6935,  1.0242],\n",
      "        [ 1.1757, -0.1426,  0.3133, -1.4948, -0.4768]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5019],\n",
      "        [ 3.6718],\n",
      "        [ 0.0892],\n",
      "        [ 1.2413]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7008, -0.3101,  0.1896,  0.7079,  0.6262],\n",
      "        [ 0.6379, -0.0192,  0.5613, -0.8284, -0.8308],\n",
      "        [-0.7767, -0.9177,  1.2459,  0.0869, -0.9544],\n",
      "        [-0.6055,  0.4750, -0.1379, -0.7359, -0.3010]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2800, -0.3496, -0.8967, -0.1580, -0.6243],\n",
      "        [-1.1933, -1.8867, -1.3431, -2.4534, -4.1350],\n",
      "        [ 0.2795, -0.7784,  0.1325, -0.4868, -0.4862],\n",
      "        [-0.9206, -0.9331, -0.4426, -1.0554, -1.4614]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7008, -0.3101,  0.1896,  0.7079,  0.6262],\n",
      "        [ 0.6379, -0.0192,  0.5613, -0.8284, -0.8308],\n",
      "        [-0.7767, -0.9177,  1.2459,  0.0869, -0.9544],\n",
      "        [-0.6055,  0.4750, -0.1379, -0.7359, -0.3010]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3681],\n",
      "        [ 3.9888],\n",
      "        [ 1.0841],\n",
      "        [ 1.3919]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0650,  0.6042, -0.9105, -0.1390, -1.0018],\n",
      "        [-1.3446,  0.8056, -0.3243,  0.6187, -0.3708],\n",
      "        [-1.2635, -1.4178, -0.3256, -0.3358,  0.0105],\n",
      "        [ 0.0697,  2.4818, -0.6831, -0.7732,  1.0644]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2911, -0.2932,  0.1444, -0.4398,  0.0807],\n",
      "        [ 0.0381, -0.0338, -0.3028,  0.0155, -0.1245],\n",
      "        [-0.6604, -0.4588, -0.7908, -0.9541, -1.6602],\n",
      "        [-1.3789, -0.8013, -0.8820, -1.8223, -2.7453]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0650,  0.6042, -0.9105, -0.1390, -1.0018],\n",
      "        [-1.3446,  0.8056, -0.3243,  0.6187, -0.3708],\n",
      "        [-1.2635, -1.4178, -0.3256, -0.3358,  0.0105],\n",
      "        [ 0.0697,  2.4818, -0.6831, -0.7732,  1.0644]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3095],\n",
      "        [ 0.0754],\n",
      "        [ 2.0453],\n",
      "        [-2.9953]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4203, -1.3236,  0.2595,  0.4011, -0.8856],\n",
      "        [ 0.4171, -0.4265, -0.5498,  1.0778, -1.9222],\n",
      "        [ 0.9003, -0.5427,  0.5096,  1.3145,  0.8814],\n",
      "        [ 2.4037,  0.7714,  0.0956, -0.3146,  0.8258]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1950,  0.0581,  0.3359, -0.1254,  0.5699],\n",
      "        [-0.3378,  0.0861,  0.7864,  0.0937,  0.5397],\n",
      "        [-0.7028, -1.3161, -0.8703, -1.8602, -2.3540],\n",
      "        [-0.4069, -0.2240,  0.3959,  0.0617, -1.0546]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4203, -1.3236,  0.2595,  0.4011, -0.8856],\n",
      "        [ 0.4171, -0.4265, -0.5498,  1.0778, -1.9222],\n",
      "        [ 0.9003, -0.5427,  0.5096,  1.3145,  0.8814],\n",
      "        [ 2.4037,  0.7714,  0.0956, -0.3146,  0.8258]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2678],\n",
      "        [-1.5465],\n",
      "        [-4.8823],\n",
      "        [-2.0035]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4069, -0.2816,  0.2099,  0.5007,  0.4851],\n",
      "        [-1.5136, -1.4127, -0.3850, -0.7122, -0.6024],\n",
      "        [-0.7879,  1.0245,  1.0699,  1.2393,  0.3048],\n",
      "        [-0.3675,  0.3191, -0.1670, -0.4804,  0.0661]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3502, -0.3328, -0.4524, -0.0213, -0.3911],\n",
      "        [ 0.2107,  1.2641,  0.5821,  1.1266,  1.2778],\n",
      "        [ 1.3489,  0.9096,  1.5241,  1.0949,  1.3252],\n",
      "        [ 0.2260,  0.2311,  0.6738,  0.2689,  0.6714]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4069, -0.2816,  0.2099,  0.5007,  0.4851],\n",
      "        [-1.5136, -1.4127, -0.3850, -0.7122, -0.6024],\n",
      "        [-0.7879,  1.0245,  1.0699,  1.2393,  0.3048],\n",
      "        [-0.3675,  0.3191, -0.1670, -0.4804,  0.0661]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3441],\n",
      "        [-3.9009],\n",
      "        [ 3.2605],\n",
      "        [-0.2066]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0112,  0.3898,  0.2907,  1.1897,  2.0729],\n",
      "        [-0.3685, -1.4914,  0.8854, -1.9943,  1.4525],\n",
      "        [-0.5012, -0.5339, -2.1871, -1.0721, -1.0495],\n",
      "        [-0.0140,  0.0457, -0.2661,  0.7527, -0.4402]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5766,  0.2047,  0.3855, -0.9026, -0.1526],\n",
      "        [ 1.5924,  2.0718,  2.1106,  1.9014,  2.2666],\n",
      "        [-0.4544, -0.8043, -0.7959, -0.5031, -0.4599],\n",
      "        [ 0.3771,  0.0906,  0.2552,  0.0264,  0.9904]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0112,  0.3898,  0.2907,  1.1897,  2.0729],\n",
      "        [-0.3685, -1.4914,  0.8854, -1.9943,  1.4525],\n",
      "        [-0.5012, -0.5339, -2.1871, -1.0721, -1.0495],\n",
      "        [-0.0140,  0.0457, -0.2661,  0.7527, -0.4402]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1919],\n",
      "        [-2.3076],\n",
      "        [ 3.4200],\n",
      "        [-0.4852]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1893,  0.5878, -0.3241, -1.2864,  0.1821],\n",
      "        [-0.3056,  0.1335, -0.8837,  0.3611, -0.1269],\n",
      "        [-2.0246,  0.4329,  2.8204, -0.6111,  1.0838],\n",
      "        [ 0.0238, -0.1129,  0.0373,  0.3603,  0.8157]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3513, -0.0509,  0.6058,  0.2806,  0.6870],\n",
      "        [ 0.5217, -0.4159,  0.7521,  0.0111, -0.3165],\n",
      "        [-1.1282, -2.1483, -0.8989, -1.9299, -4.0086],\n",
      "        [-0.0151,  0.3203, -0.3088,  0.4444,  0.2773]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1893,  0.5878, -0.3241, -1.2864,  0.1821],\n",
      "        [-0.3056,  0.1335, -0.8837,  0.3611, -0.1269],\n",
      "        [-2.0246,  0.4329,  2.8204, -0.6111,  1.0838],\n",
      "        [ 0.0238, -0.1129,  0.0373,  0.3603,  0.8157]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3955],\n",
      "        [-0.8354],\n",
      "        [-4.3462],\n",
      "        [ 0.3383]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5880,  0.9459,  0.6600,  1.3914,  0.1012],\n",
      "        [-0.5207, -0.5683, -0.1265,  0.6365,  2.0550],\n",
      "        [ 0.7161,  0.1744,  0.6685,  0.7400,  1.2386],\n",
      "        [-0.2505,  1.1286,  1.2425, -0.0465,  1.3244]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4273,  0.2320,  0.1918, -0.0478,  0.5920],\n",
      "        [ 0.3906,  0.3305, -0.1585,  0.3992, -0.0837],\n",
      "        [ 0.5117, -1.1910, -0.4300, -0.5155, -0.8090],\n",
      "        [-0.1826, -0.3098, -0.6868, -0.7122, -1.0674]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5880,  0.9459,  0.6600,  1.3914,  0.1012],\n",
      "        [-0.5207, -0.5683, -0.1265,  0.6365,  2.0550],\n",
      "        [ 0.7161,  0.1744,  0.6685,  0.7400,  1.2386],\n",
      "        [-0.2505,  1.1286,  1.2425, -0.0465,  1.3244]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0882],\n",
      "        [-0.2890],\n",
      "        [-1.5123],\n",
      "        [-2.5377]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5635, -0.1965, -2.0418, -0.6115, -0.5519],\n",
      "        [ 1.1936, -0.5093, -0.3341,  0.0435,  0.7359],\n",
      "        [ 0.1072, -2.0176,  1.3398, -1.4101, -0.6536],\n",
      "        [-0.3756,  1.0119,  0.5004, -0.8346, -2.2261]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4801, -0.5734, -0.4570, -0.2389, -0.1178],\n",
      "        [ 0.0910, -0.5955, -0.3625, -0.4102, -0.0103],\n",
      "        [ 0.4596, -0.7902,  0.1785, -0.0689,  0.6246],\n",
      "        [ 0.8821,  1.7866,  0.7963, -0.0518,  1.4446]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5635, -0.1965, -2.0418, -0.6115, -0.5519],\n",
      "        [ 1.1936, -0.5093, -0.3341,  0.0435,  0.7359],\n",
      "        [ 0.1072, -2.0176,  1.3398, -1.4101, -0.6536],\n",
      "        [-0.3756,  1.0119,  0.5004, -0.8346, -2.2261]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5063],\n",
      "        [ 0.5075],\n",
      "        [ 1.5716],\n",
      "        [-1.2976]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7791,  0.1754, -0.6373, -0.2010,  1.4471],\n",
      "        [-1.4836, -0.2945,  1.2009, -1.3422,  0.1432],\n",
      "        [-1.3846,  0.6030,  0.6463, -1.0632,  0.0627],\n",
      "        [-0.3383,  0.4277,  2.0883,  0.3843,  0.3149]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5879, -0.3369, -0.4486, -0.3691,  0.1347],\n",
      "        [-0.4382,  0.3857, -0.7428, -0.3217, -0.6090],\n",
      "        [-0.5660, -0.4462, -0.5553, -0.3804, -1.4061],\n",
      "        [ 1.2541,  1.4516,  0.7226,  0.4737,  1.7741]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7791,  0.1754, -0.6373, -0.2010,  1.4471],\n",
      "        [-1.4836, -0.2945,  1.2009, -1.3422,  0.1432],\n",
      "        [-1.3846,  0.6030,  0.6463, -1.0632,  0.0627],\n",
      "        [-0.3383,  0.4277,  2.0883,  0.3843,  0.3149]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9540],\n",
      "        [-0.0109],\n",
      "        [ 0.4719],\n",
      "        [ 2.4462]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.3763,  1.1509, -0.3134, -1.2267,  1.3760],\n",
      "        [-2.1434,  0.8991,  0.3374,  0.1883,  0.7303],\n",
      "        [-0.9227,  0.5680, -0.8741,  0.4712, -0.7395],\n",
      "        [ 0.0271,  1.2440, -0.3505, -1.1743, -0.0420]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1857,  0.1633, -0.1960, -0.5783, -0.7081],\n",
      "        [-0.0156,  0.0051, -0.1367, -0.4633, -0.1298],\n",
      "        [-0.2311, -0.9897, -0.7021, -0.5310, -1.2445],\n",
      "        [ 0.1452, -0.3646,  0.5054,  0.4032, -0.0782]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3763,  1.1509, -0.3134, -1.2267,  1.3760],\n",
      "        [-2.1434,  0.8991,  0.3374,  0.1883,  0.7303],\n",
      "        [-0.9227,  0.5680, -0.8741,  0.4712, -0.7395],\n",
      "        [ 0.0271,  1.2440, -0.3505, -1.1743, -0.0420]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0543],\n",
      "        [-0.1900],\n",
      "        [ 0.9348],\n",
      "        [-1.0969]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5212, -1.3163,  2.2024, -0.3836,  0.9995],\n",
      "        [-0.9300,  0.1057, -0.4123,  0.3435,  0.4428],\n",
      "        [ 1.0827, -0.0575, -0.0145, -0.7403,  2.3340],\n",
      "        [-1.4957,  0.6478, -0.5313,  0.3390, -1.5143]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5253,  0.0224, -0.4351, -0.1898, -0.1955],\n",
      "        [ 0.3718,  0.6347, -0.0101, -0.1370, -0.1767],\n",
      "        [-0.6219, -1.9380, -0.8258, -0.9681, -2.0749],\n",
      "        [ 0.6456,  0.1978,  0.7630,  0.1532,  0.5700]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5212, -1.3163,  2.2024, -0.3836,  0.9995],\n",
      "        [-0.9300,  0.1057, -0.4123,  0.3435,  0.4428],\n",
      "        [ 1.0827, -0.0575, -0.0145, -0.7403,  2.3340],\n",
      "        [-1.4957,  0.6478, -0.5313,  0.3390, -1.5143]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9092],\n",
      "        [-0.3998],\n",
      "        [-4.6761],\n",
      "        [-2.0541]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0633, -0.2482, -0.5755,  0.8626, -0.2697],\n",
      "        [-0.9172, -0.3623, -0.8373, -0.4853,  0.0283],\n",
      "        [ 1.3128, -0.7728,  0.0329,  0.0421,  0.8453],\n",
      "        [-0.4832,  0.0548,  0.3855,  1.4909, -0.0795]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9843,  1.2321,  0.8158,  1.0099,  1.2141],\n",
      "        [ 0.2169, -0.0407,  0.0463, -0.0834, -0.4210],\n",
      "        [ 1.1432,  1.4056,  0.9250,  0.4296,  1.3358],\n",
      "        [ 1.1004,  2.7051,  1.3460,  0.4388,  1.3684]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0633, -0.2482, -0.5755,  0.8626, -0.2697],\n",
      "        [-0.9172, -0.3623, -0.8373, -0.4853,  0.0283],\n",
      "        [ 1.3128, -0.7728,  0.0329,  0.0421,  0.8453],\n",
      "        [-0.4832,  0.0548,  0.3855,  1.4909, -0.0795]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2940],\n",
      "        [-0.1944],\n",
      "        [ 1.5924],\n",
      "        [ 0.6807]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3338,  0.2628, -0.7684,  1.5230,  2.1288],\n",
      "        [ 1.0416,  0.5014,  0.2593, -0.1086,  0.7433],\n",
      "        [-1.2041, -0.3099, -0.3850,  0.7161,  1.3516],\n",
      "        [-0.5019, -0.1009,  0.4248, -1.6137,  1.4367]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9042,  0.2156,  1.7939,  0.3971,  1.4290],\n",
      "        [ 0.3075, -0.3138, -0.0757, -0.3488,  0.3132],\n",
      "        [ 0.7298, -0.5645, -0.7874, -1.4301, -0.0327],\n",
      "        [ 1.2327,  0.7995,  0.7200,  0.5296,  1.2460]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3338,  0.2628, -0.7684,  1.5230,  2.1288],\n",
      "        [ 1.0416,  0.5014,  0.2593, -0.1086,  0.7433],\n",
      "        [-1.2041, -0.3099, -0.3850,  0.7161,  1.3516],\n",
      "        [-0.5019, -0.1009,  0.4248, -1.6137,  1.4367]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0233],\n",
      "        [ 0.4140],\n",
      "        [-1.4691],\n",
      "        [ 0.5420]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8194, -0.0614, -0.2975,  0.2915, -0.5226],\n",
      "        [-2.0696, -1.9863,  1.6833, -0.5291, -1.2394],\n",
      "        [ 1.5847, -0.2951, -1.5807,  1.2814,  1.6983],\n",
      "        [ 0.4018, -0.9302,  0.7534,  0.4096,  0.6563]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4947,  0.2806, -0.1439, -0.1055, -0.9831],\n",
      "        [ 0.2548,  0.0780,  0.1327, -0.5085, -0.4153],\n",
      "        [ 1.1765, -0.0016,  0.0322,  0.1135,  0.6946],\n",
      "        [ 0.3551, -0.1042,  0.0861, -0.0562,  0.2118]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8194, -0.0614, -0.2975,  0.2915, -0.5226],\n",
      "        [-2.0696, -1.9863,  1.6833, -0.5291, -1.2394],\n",
      "        [ 1.5847, -0.2951, -1.5807,  1.2814,  1.6983],\n",
      "        [ 0.4018, -0.9302,  0.7534,  0.4096,  0.6563]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1032],\n",
      "        [ 0.3248],\n",
      "        [ 3.1390],\n",
      "        [ 0.4205]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8629, -0.2985, -1.4300,  2.0126,  0.6307],\n",
      "        [ 1.2206, -1.4169,  0.6263,  1.0350, -1.5439],\n",
      "        [-0.5127,  0.6705, -0.0041, -1.4924,  0.5378],\n",
      "        [ 1.1636,  2.3513, -0.7576,  0.9726, -0.5156]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5507,  0.8806,  0.6706, -0.0999,  0.9103],\n",
      "        [-0.2153,  0.0926, -0.1169, -0.5856, -0.0662],\n",
      "        [-0.0524, -0.3390, -1.1055, -1.5895, -2.8069],\n",
      "        [ 0.3432,  0.7722,  0.5581,  0.4629,  0.2912]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8629, -0.2985, -1.4300,  2.0126,  0.6307],\n",
      "        [ 1.2206, -1.4169,  0.6263,  1.0350, -1.5439],\n",
      "        [-0.5127,  0.6705, -0.0041, -1.4924,  0.5378],\n",
      "        [ 1.1636,  2.3513, -0.7576,  0.9726, -0.5156]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3240],\n",
      "        [-0.9711],\n",
      "        [ 0.6668],\n",
      "        [ 2.0923]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2358, -0.3640, -0.8095, -1.1043,  0.4838],\n",
      "        [-1.1133, -0.3404, -1.1323,  1.6025,  0.0722],\n",
      "        [-1.2138,  0.7521,  1.6479,  0.6404, -0.3381],\n",
      "        [ 0.0341, -0.4767,  0.6053, -0.8756,  1.1114]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3165,  1.5900,  1.2837,  1.3897,  1.1574],\n",
      "        [ 0.6777,  0.8633,  0.3099,  0.2036,  1.2160],\n",
      "        [-0.7144, -1.4292, -0.7656, -1.4041, -1.9661],\n",
      "        [-0.7686, -1.1737, -0.2804, -0.2085, -1.3667]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2358, -0.3640, -0.8095, -1.1043,  0.4838],\n",
      "        [-1.1133, -0.3404, -1.1323,  1.6025,  0.0722],\n",
      "        [-1.2138,  0.7521,  1.6479,  0.6404, -0.3381],\n",
      "        [ 0.0341, -0.4767,  0.6053, -0.8756,  1.1114]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2822],\n",
      "        [-0.9851],\n",
      "        [-1.7039],\n",
      "        [-0.9729]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8359,  0.3260, -0.9288, -0.5581,  0.7442],\n",
      "        [-1.1276,  1.2655, -0.4220, -0.0943, -0.7686],\n",
      "        [-1.0350,  1.3389, -0.4909, -0.3217,  1.6518],\n",
      "        [-1.0401,  1.0277, -0.0755, -1.0241,  0.6296]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4442,  1.6291,  1.9760,  1.5522,  2.5781],\n",
      "        [ 0.2269,  0.7967, -0.0014, -0.0480,  0.8063],\n",
      "        [ 0.1403, -1.0077, -1.5385, -0.9738, -1.6528],\n",
      "        [ 0.5892,  0.6443,  0.4966,  0.5537,  0.3345]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8359,  0.3260, -0.9288, -0.5581,  0.7442],\n",
      "        [-1.1276,  1.2655, -0.4220, -0.0943, -0.7686],\n",
      "        [-1.0350,  1.3389, -0.4909, -0.3217,  1.6518],\n",
      "        [-1.0401,  1.0277, -0.0755, -1.0241,  0.6296]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9032],\n",
      "        [ 0.1378],\n",
      "        [-3.1561],\n",
      "        [-0.3446]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3489,  1.7282,  0.1006,  0.8563, -0.0203],\n",
      "        [ 0.6967, -0.2740,  1.1801, -1.1828, -0.4316],\n",
      "        [ 0.8141, -1.1310,  0.2223,  0.8553, -0.8309],\n",
      "        [ 1.1985, -2.9300,  0.1490,  1.0350, -2.4153]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0931, -0.0264, -0.3628, -0.0034,  0.3986],\n",
      "        [ 0.4016,  0.0077, -0.4310, -0.7659,  0.2929],\n",
      "        [ 0.5006,  0.0021,  0.2443,  0.3438,  1.3462],\n",
      "        [ 0.2457,  0.4223,  0.4913,  0.6718,  0.6466]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3489,  1.7282,  0.1006,  0.8563, -0.0203],\n",
      "        [ 0.6967, -0.2740,  1.1801, -1.1828, -0.4316],\n",
      "        [ 0.8141, -1.1310,  0.2223,  0.8553, -0.8309],\n",
      "        [ 1.1985, -2.9300,  0.1490,  1.0350, -2.4153]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0323],\n",
      "        [ 0.5485],\n",
      "        [-0.3651],\n",
      "        [-1.7361]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0551,  1.8273,  1.0037,  0.4367,  0.0715],\n",
      "        [ 0.1119, -0.3516, -0.8422,  2.2123,  1.1090],\n",
      "        [ 0.1838,  2.1868,  1.2471, -1.3311, -1.4914],\n",
      "        [ 1.2992, -0.5796,  0.4954, -0.8659,  0.9067]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0981,  0.1344, -0.2014, -0.1415, -0.2559],\n",
      "        [ 0.1547, -0.2099,  0.0005, -0.3306,  0.2654],\n",
      "        [ 0.8067,  0.7496, -0.1471, -0.0577,  0.8610],\n",
      "        [ 0.8114,  1.0672,  1.7679,  1.5400,  1.5434]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0551,  1.8273,  1.0037,  0.4367,  0.0715],\n",
      "        [ 0.1119, -0.3516, -0.8422,  2.2123,  1.1090],\n",
      "        [ 0.1838,  2.1868,  1.2471, -1.3311, -1.4914],\n",
      "        [ 1.2992, -0.5796,  0.4954, -0.8659,  0.9067]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0313],\n",
      "        [-0.3464],\n",
      "        [ 0.3968],\n",
      "        [ 1.3773]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7773,  0.0300,  1.0539,  2.0467,  2.0466],\n",
      "        [ 0.6580,  0.0965, -1.3128,  1.2037, -0.0832],\n",
      "        [ 0.5168, -1.1104,  1.7451,  1.6823,  0.4044],\n",
      "        [-0.2594, -1.6575,  0.6613, -0.6379, -1.3473]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2297, -0.1050, -0.0541,  0.6696,  0.6358],\n",
      "        [ 0.2940, -0.5172,  0.0237, -0.3027, -0.2004],\n",
      "        [ 0.8374, -1.1581, -0.2499, -0.3094, -0.5806],\n",
      "        [-0.1158,  0.7122,  1.2296,  1.1951,  0.9028]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7773,  0.0300,  1.0539,  2.0467,  2.0466],\n",
      "        [ 0.6580,  0.0965, -1.3128,  1.2037, -0.0832],\n",
      "        [ 0.5168, -1.1104,  1.7451,  1.6823,  0.4044],\n",
      "        [-0.2594, -1.6575,  0.6613, -0.6379, -1.3473]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.7899],\n",
      "        [-0.2354],\n",
      "        [ 0.5272],\n",
      "        [-2.3159]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3130,  1.1014,  1.7457,  0.4568, -0.2179],\n",
      "        [-0.2583,  1.1551, -0.2785,  0.7189,  0.4071],\n",
      "        [-0.4030,  0.6887,  0.9452,  0.1584,  0.7405],\n",
      "        [ 0.0027,  1.1600, -1.1762,  1.9186, -0.3258]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5962, -1.7842, -1.1364, -1.5927, -2.1161],\n",
      "        [ 0.4537, -0.0617,  0.1453,  0.0825,  0.1795],\n",
      "        [-0.2036, -0.7672, -0.7746, -0.9337, -0.5586],\n",
      "        [ 1.0050,  1.6090,  0.9497,  1.3038,  1.8805]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3130,  1.1014,  1.7457,  0.4568, -0.2179],\n",
      "        [-0.2583,  1.1551, -0.2785,  0.7189,  0.4071],\n",
      "        [-0.4030,  0.6887,  0.9452,  0.1584,  0.7405],\n",
      "        [ 0.0027,  1.1600, -1.1762,  1.9186, -0.3258]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.4020],\n",
      "        [-0.0966],\n",
      "        [-1.7399],\n",
      "        [ 2.6410]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0901,  0.3136,  1.1949, -2.3586,  0.5715],\n",
      "        [ 1.5909, -0.5873, -1.3829, -0.5205,  1.2816],\n",
      "        [-1.8280,  0.2376,  0.5593,  2.0034,  2.4445],\n",
      "        [-0.7126, -0.1256, -0.7065,  2.4881,  0.8754]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8537,  1.0004, -0.1765,  0.7330,  0.6487],\n",
      "        [-0.1333,  0.2407,  1.2235, -0.1063, -0.3905],\n",
      "        [ 1.1234, -0.4822,  0.5109, -0.0824, -0.0167],\n",
      "        [ 0.1662,  0.4600,  0.7414, -0.5326, -0.2744]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0901,  0.3136,  1.1949, -2.3586,  0.5715],\n",
      "        [ 1.5909, -0.5873, -1.3829, -0.5205,  1.2816],\n",
      "        [-1.8280,  0.2376,  0.5593,  2.0034,  2.4445],\n",
      "        [-0.7126, -0.1256, -0.7065,  2.4881,  0.8754]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3323],\n",
      "        [-2.4905],\n",
      "        [-2.0883],\n",
      "        [-2.2653]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4701,  0.3107,  0.1497, -1.9640, -0.5090],\n",
      "        [ 0.2515, -0.2102, -0.3616,  1.0118,  1.1961],\n",
      "        [-0.5750,  0.4459,  0.3853, -0.7299,  0.3314],\n",
      "        [ 0.3675,  2.0186,  1.0369, -0.1529,  0.5404]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5376,  1.0377,  1.4970,  0.7810,  1.8868],\n",
      "        [ 1.2577,  1.5034,  1.2297,  1.0710,  1.8379],\n",
      "        [ 1.3957,  0.4837,  0.7032,  0.6571,  1.9869],\n",
      "        [ 0.7456,  0.6017,  1.1531,  0.7369,  1.8981]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4701,  0.3107,  0.1497, -1.9640, -0.5090],\n",
      "        [ 0.2515, -0.2102, -0.3616,  1.0118,  1.1961],\n",
      "        [-0.5750,  0.4459,  0.3853, -0.7299,  0.3314],\n",
      "        [ 0.3675,  2.0186,  1.0369, -0.1529,  0.5404]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6950],\n",
      "        [ 2.8377],\n",
      "        [-0.1371],\n",
      "        [ 3.5973]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5358, -0.7727, -0.4067,  1.3953,  0.5945],\n",
      "        [ 0.6530,  1.5038,  0.5923,  0.9490, -0.2597],\n",
      "        [-0.3214, -1.1444, -0.7996,  1.3250, -0.9201],\n",
      "        [-0.7778,  0.8408,  0.2132,  0.8204,  1.2618]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2424,  1.9014,  1.6471,  1.8516,  2.1507],\n",
      "        [ 0.3876, -0.1248, -0.3109,  0.3922,  0.0683],\n",
      "        [ 1.3736,  0.6835,  1.5782,  0.7595,  1.9500],\n",
      "        [-0.2953, -1.0082, -0.5922, -0.6813, -1.1342]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5358, -0.7727, -0.4067,  1.3953,  0.5945],\n",
      "        [ 0.6530,  1.5038,  0.5923,  0.9490, -0.2597],\n",
      "        [-0.3214, -1.1444, -0.7996,  1.3250, -0.9201],\n",
      "        [-0.7778,  0.8408,  0.2132,  0.8204,  1.2618]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0575],\n",
      "        [ 0.2357],\n",
      "        [-3.2735],\n",
      "        [-2.7343]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4928, -0.1944, -2.0214, -0.9355,  0.5242],\n",
      "        [ 2.6555,  1.4063, -0.4566, -1.8858,  0.0290],\n",
      "        [ 0.4860,  1.3011,  1.6523,  1.2693, -0.5977],\n",
      "        [ 0.5167, -0.3905,  1.1624,  0.7363, -0.1839]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7750,  1.5462,  0.9976,  0.9201,  1.7917],\n",
      "        [ 0.2591,  0.4749,  0.7903,  0.5557,  0.8764],\n",
      "        [ 0.0362,  0.5726,  0.2034,  0.1034,  0.4995],\n",
      "        [ 1.2755,  0.1682,  0.7456,  0.0433,  1.2318]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4928, -0.1944, -2.0214, -0.9355,  0.5242],\n",
      "        [ 2.6555,  1.4063, -0.4566, -1.8858,  0.0290],\n",
      "        [ 0.4860,  1.3011,  1.6523,  1.2693, -0.5977],\n",
      "        [ 0.5167, -0.3905,  1.1624,  0.7363, -0.1839]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.6206],\n",
      "        [-0.0274],\n",
      "        [ 0.9313],\n",
      "        [ 1.2655]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1737,  0.9872,  0.4017, -2.6092,  0.0419],\n",
      "        [ 2.2257, -2.1823,  1.4487,  0.3140, -0.2812],\n",
      "        [ 0.2471,  0.0286, -1.0435, -2.0945,  1.4352],\n",
      "        [-0.5917, -2.4932, -0.2798, -0.1624, -1.6876]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2430,  0.6052,  0.0208, -0.0399, -0.2610],\n",
      "        [ 0.1876,  0.6837,  0.0404, -0.1374,  0.5683],\n",
      "        [-0.1223,  0.4684, -0.5268,  0.0919, -0.2605],\n",
      "        [ 0.0587,  0.3152, -0.7024,  0.0014,  0.2738]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1737,  0.9872,  0.4017, -2.6092,  0.0419],\n",
      "        [ 2.2257, -2.1823,  1.4487,  0.3140, -0.2812],\n",
      "        [ 0.2471,  0.0286, -1.0435, -2.0945,  1.4352],\n",
      "        [-0.5917, -2.4932, -0.2798, -0.1624, -1.6876]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9843],\n",
      "        [-1.2188],\n",
      "        [-0.0336],\n",
      "        [-1.0863]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.4861, -0.2773, -1.5932,  0.5647,  1.1477],\n",
      "        [-0.3703, -0.7906, -0.2485, -0.7538, -0.3651],\n",
      "        [-1.1759, -0.9109, -1.5340,  0.1579,  1.4300],\n",
      "        [ 1.6125, -0.8144,  0.7589,  0.7797, -1.4778]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0706, -0.4516, -0.4483, -0.0400, -0.2557],\n",
      "        [ 0.8295,  1.7144,  1.2963,  0.5787,  1.4327],\n",
      "        [-0.0561,  0.2564, -0.1317,  0.5745,  0.2781],\n",
      "        [ 0.2117,  0.5880,  0.5417,  1.0488,  1.0577]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.4861, -0.2773, -1.5932,  0.5647,  1.1477],\n",
      "        [-0.3703, -0.7906, -0.2485, -0.7538, -0.3651],\n",
      "        [-1.1759, -0.9109, -1.5340,  0.1579,  1.4300],\n",
      "        [ 1.6125, -0.8144,  0.7589,  0.7797, -1.4778]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6989],\n",
      "        [-2.9439],\n",
      "        [ 0.5228],\n",
      "        [-0.4717]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3100, -0.0134,  1.2107,  0.6248,  1.1917],\n",
      "        [-0.0755,  1.4031,  0.9982, -0.8775,  0.1104],\n",
      "        [ 0.0503,  0.1168,  0.6245, -0.8096, -0.6050],\n",
      "        [-0.8075, -1.8832, -0.7419, -0.4209,  1.2295]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5517,  0.1085,  0.1277, -0.4412, -0.5172],\n",
      "        [ 1.5086,  1.6043,  2.3707,  1.2056,  2.3116],\n",
      "        [ 0.2784, -0.2445,  0.5364,  0.7242,  0.0259],\n",
      "        [-0.0631,  0.1379,  1.1908,  0.3419,  0.3669]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3100, -0.0134,  1.2107,  0.6248,  1.1917],\n",
      "        [-0.0755,  1.4031,  0.9982, -0.8775,  0.1104],\n",
      "        [ 0.0503,  0.1168,  0.6245, -0.8096, -0.6050],\n",
      "        [-0.8075, -1.8832, -0.7419, -0.4209,  1.2295]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5679],\n",
      "        [ 3.7009],\n",
      "        [-0.2816],\n",
      "        [-0.7850]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2694,  0.5369,  2.6696, -0.0658,  0.3022],\n",
      "        [-1.6212,  0.7632,  1.1602, -0.4608,  0.4335],\n",
      "        [-0.7075, -0.2617, -0.4073,  2.9366, -0.2621],\n",
      "        [ 1.0758, -0.8859,  0.1725,  1.2112, -0.2910]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3340, -0.0382,  0.1858,  0.2867, -0.1633],\n",
      "        [ 0.5990,  1.3024,  0.9167,  0.7774,  0.1324],\n",
      "        [-0.1651,  0.0899, -0.2165,  0.4388,  0.1689],\n",
      "        [ 0.9369,  0.4326,  1.1189,  1.3326,  0.7552]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2694,  0.5369,  2.6696, -0.0658,  0.3022],\n",
      "        [-1.6212,  0.7632,  1.1602, -0.4608,  0.4335],\n",
      "        [-0.7075, -0.2617, -0.4073,  2.9366, -0.2621],\n",
      "        [ 1.0758, -0.8859,  0.1725,  1.2112, -0.2910]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8313],\n",
      "        [ 0.7857],\n",
      "        [ 1.4259],\n",
      "        [ 2.2120]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5729,  0.6906,  1.7163, -0.5164,  0.8319],\n",
      "        [ 1.5002, -0.6619,  0.7637,  0.0578, -0.3061],\n",
      "        [ 0.3049,  0.5223, -0.2055,  0.0195,  0.0375],\n",
      "        [ 1.7869,  1.1056,  1.9170, -0.2815, -0.2611]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1962, -0.4547,  0.3815,  0.2998, -0.3698],\n",
      "        [ 0.5429,  0.7502,  1.3633,  1.4601,  0.9016],\n",
      "        [-0.3075, -0.7052, -0.7585, -0.2198, -1.7682],\n",
      "        [-0.1473, -0.3155, -0.4183, -0.2206, -0.8870]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5729,  0.6906,  1.7163, -0.5164,  0.8319],\n",
      "        [ 1.5002, -0.6619,  0.7637,  0.0578, -0.3061],\n",
      "        [ 0.3049,  0.5223, -0.2055,  0.0195,  0.0375],\n",
      "        [ 1.7869,  1.1056,  1.9170, -0.2815, -0.2611]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0093],\n",
      "        [ 1.1675],\n",
      "        [-0.3768],\n",
      "        [-1.1204]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6354, -0.1607, -0.1344,  0.2351,  0.3246],\n",
      "        [-0.7691, -1.1096, -0.2567, -0.3170,  0.9004],\n",
      "        [-2.0535, -0.6461,  0.1473,  0.8739, -0.2714],\n",
      "        [-0.8839,  1.7266, -0.1228,  1.1784, -0.1669]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0921,  0.2126,  0.0219,  0.0722,  0.5400],\n",
      "        [ 0.1888,  1.6808,  0.9947,  0.7643,  1.5708],\n",
      "        [-0.2389,  0.3702,  0.2291,  0.3648,  0.1385],\n",
      "        [-0.0398,  0.4510,  0.4382,  0.0078, -0.0811]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6354, -0.1607, -0.1344,  0.2351,  0.3246],\n",
      "        [-0.7691, -1.1096, -0.2567, -0.3170,  0.9004],\n",
      "        [-2.0535, -0.6461,  0.1473,  0.8739, -0.2714],\n",
      "        [-0.8839,  1.7266, -0.1228,  1.1784, -0.1669]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2136],\n",
      "        [-1.0936],\n",
      "        [ 0.5663],\n",
      "        [ 0.7828]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3350, -0.0412, -0.4974,  0.6159, -0.8245],\n",
      "        [-0.1908,  0.9264, -0.1121,  0.8463, -0.3358],\n",
      "        [-1.2456, -0.0456, -0.1621, -0.9520, -0.8617],\n",
      "        [-0.3458,  0.1083,  0.9961, -0.1815, -0.3715]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4681,  0.7950,  0.4775,  0.4283,  0.8511],\n",
      "        [ 1.1767,  1.7693,  1.1007,  1.2027,  1.7296],\n",
      "        [ 0.1944,  0.3167,  0.1399, -0.2239,  0.4028],\n",
      "        [-0.1414,  0.1941,  0.4022,  0.2811, -0.1718]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3350, -0.0412, -0.4974,  0.6159, -0.8245],\n",
      "        [-0.1908,  0.9264, -0.1121,  0.8463, -0.3358],\n",
      "        [-1.2456, -0.0456, -0.1621, -0.9520, -0.8617],\n",
      "        [-0.3458,  0.1083,  0.9961, -0.1815, -0.3715]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5513],\n",
      "        [ 1.7284],\n",
      "        [-0.4132],\n",
      "        [ 0.4834]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4301,  0.4342,  0.2176,  0.4115, -0.0276],\n",
      "        [-0.8967,  0.5994, -0.0003,  0.1490,  1.1229],\n",
      "        [ 1.3752, -0.5349,  1.0471,  0.0312, -0.5428],\n",
      "        [-0.2886, -1.4898,  1.2858,  1.9123,  1.2302]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0655,  0.1671,  0.6922,  0.2225,  0.5486],\n",
      "        [-0.2975,  1.4337,  1.4641,  1.7117,  1.1710],\n",
      "        [-0.1600,  0.0615, -0.2778,  0.3091, -0.1548],\n",
      "        [ 0.7072,  1.0581,  0.2589, -0.2043,  1.3241]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4301,  0.4342,  0.2176,  0.4115, -0.0276],\n",
      "        [-0.8967,  0.5994, -0.0003,  0.1490,  1.1229],\n",
      "        [ 1.3752, -0.5349,  1.0471,  0.0312, -0.5428],\n",
      "        [-0.2886, -1.4898,  1.2858,  1.9123,  1.2302]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2714],\n",
      "        [ 2.6956],\n",
      "        [-0.4501],\n",
      "        [-0.2093]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0956, -0.9559, -0.9010,  0.8733,  0.5013],\n",
      "        [-1.0142, -1.9413,  1.9156, -0.5092, -0.1508],\n",
      "        [-0.0407, -0.9853, -0.3220,  0.8080, -0.0851],\n",
      "        [ 0.7056,  0.6408,  0.3501,  0.3631, -0.1742]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4323,  0.1603,  0.0369,  0.1460, -0.0228],\n",
      "        [-1.1324,  0.2748,  0.4494, -0.1154, -1.2688],\n",
      "        [-0.0023,  0.1700,  0.1942, -0.0267,  0.5362],\n",
      "        [-0.3110,  0.1858,  0.3522,  0.1904,  0.4635]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0956, -0.9559, -0.9010,  0.8733,  0.5013],\n",
      "        [-1.0142, -1.9413,  1.9156, -0.5092, -0.1508],\n",
      "        [-0.0407, -0.9853, -0.3220,  0.8080, -0.0851],\n",
      "        [ 0.7056,  0.6408,  0.3501,  0.3631, -0.1742]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4033],\n",
      "        [ 1.7259],\n",
      "        [-0.2971],\n",
      "        [ 0.0112]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4916, -1.3141,  0.6286, -0.9794,  0.6474],\n",
      "        [-1.4978, -0.3089,  0.9895,  1.0964, -0.7219],\n",
      "        [-0.9287,  0.1720,  0.4750,  0.2982,  0.2791],\n",
      "        [ 0.6752,  0.0132,  0.9126,  1.3715, -0.5863]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0293,  0.0549, -0.5773,  0.2130,  0.1259],\n",
      "        [-0.7138, -0.0386, -0.2754, -1.0424, -2.1659],\n",
      "        [-0.1447, -0.3210, -0.2734, -0.7701, -0.3591],\n",
      "        [ 0.3961,  1.1148,  0.7870,  0.2003,  0.3917]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4916, -1.3141,  0.6286, -0.9794,  0.6474],\n",
      "        [-1.4978, -0.3089,  0.9895,  1.0964, -0.7219],\n",
      "        [-0.9287,  0.1720,  0.4750,  0.2982,  0.2791],\n",
      "        [ 0.6752,  0.0132,  0.9126,  1.3715, -0.5863]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5477],\n",
      "        [ 1.2291],\n",
      "        [-0.3806],\n",
      "        [ 1.0455]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4489,  0.1895,  1.4250, -0.2510, -0.1604],\n",
      "        [-1.2736,  1.2293,  0.9340, -0.3042,  1.0326],\n",
      "        [-1.1491,  0.1886, -0.5687,  0.3675, -1.7399],\n",
      "        [ 0.0040, -0.6544,  1.8790,  1.3205, -0.5129]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3163, -0.0718, -0.5699,  0.1902,  0.0127],\n",
      "        [-1.1338, -0.8076, -1.0082, -1.4195, -2.0897],\n",
      "        [ 0.1850,  0.6921, -0.3763, -0.2728, -0.8042],\n",
      "        [-0.4741, -0.2896,  0.5089,  0.4810,  0.2728]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4489,  0.1895,  1.4250, -0.2510, -0.1604],\n",
      "        [-1.2736,  1.2293,  0.9340, -0.3042,  1.0326],\n",
      "        [-1.1491,  0.1886, -0.5687,  0.3675, -1.7399],\n",
      "        [ 0.0040, -0.6544,  1.8790,  1.3205, -0.5129]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0175],\n",
      "        [-2.2164],\n",
      "        [ 1.4309],\n",
      "        [ 1.6389]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6498, -0.7100, -0.0629,  0.0316,  0.0531],\n",
      "        [-0.2165, -0.1948,  0.9632, -0.7901,  0.8303],\n",
      "        [-2.8641, -0.5024,  1.9577,  0.1048,  0.0048],\n",
      "        [-0.8470,  0.0796,  0.0924,  0.1727,  2.2166]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4384, -0.0898,  0.3277, -0.1190,  0.9556],\n",
      "        [-0.7157,  1.0831,  0.4373,  0.4897, -0.1273],\n",
      "        [-0.4758, -0.9934, -0.7034, -1.1695, -1.5791],\n",
      "        [-0.5936, -0.8004, -0.5749, -0.7182, -1.2605]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6498, -0.7100, -0.0629,  0.0316,  0.0531],\n",
      "        [-0.2165, -0.1948,  0.9632, -0.7901,  0.8303],\n",
      "        [-2.8641, -0.5024,  1.9577,  0.1048,  0.0048],\n",
      "        [-0.8470,  0.0796,  0.0924,  0.1727,  2.2166]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1948],\n",
      "        [-0.1274],\n",
      "        [ 0.3546],\n",
      "        [-2.5321]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7297,  1.9207,  1.8810,  0.9452, -1.3362],\n",
      "        [-1.4709, -2.1235,  0.9693, -0.5654,  0.8797],\n",
      "        [ 0.9458, -0.5781,  0.5040, -0.0062,  0.5746],\n",
      "        [ 1.1680,  1.1302, -0.8513,  0.5875, -0.5976]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2110, -0.1760,  0.3737, -0.5662,  0.0012],\n",
      "        [-0.1625,  0.6762,  0.1830,  0.1666,  0.1273],\n",
      "        [-0.3204, -0.5956, -0.6199, -0.3308, -1.4257],\n",
      "        [ 0.6839,  0.8147,  0.8627,  0.5711,  0.5888]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7297,  1.9207,  1.8810,  0.9452, -1.3362],\n",
      "        [-1.4709, -2.1235,  0.9693, -0.5654,  0.8797],\n",
      "        [ 0.9458, -0.5781,  0.5040, -0.0062,  0.5746],\n",
      "        [ 1.1680,  1.1302, -0.8513,  0.5875, -0.5976]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3258],\n",
      "        [-1.0017],\n",
      "        [-1.0883],\n",
      "        [ 0.9688]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0842, -2.3668, -0.0842, -0.5068, -0.7700],\n",
      "        [ 1.8173, -1.6014, -0.5363, -0.6413, -0.3127],\n",
      "        [-0.6431, -0.4518,  0.0294, -1.4156,  2.0771],\n",
      "        [ 0.7760, -1.4841,  1.1576, -0.2277, -0.8319]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4454, -0.3727, -1.0094,  0.9105, -1.1194],\n",
      "        [-0.4474,  1.1426,  0.2236,  0.1634,  1.3070],\n",
      "        [ 0.1246, -0.5724, -0.6777,  0.1012, -0.9292],\n",
      "        [ 0.0718,  0.5413,  0.5195,  1.0125,  0.3758]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0842, -2.3668, -0.0842, -0.5068, -0.7700],\n",
      "        [ 1.8173, -1.6014, -0.5363, -0.6413, -0.3127],\n",
      "        [-0.6431, -0.4518,  0.0294, -1.4156,  2.0771],\n",
      "        [ 0.7760, -1.4841,  1.1576, -0.2277, -0.8319]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4395],\n",
      "        [-3.2764],\n",
      "        [-1.9148],\n",
      "        [-0.6895]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2396,  0.2348,  1.0339,  0.5938,  0.2070],\n",
      "        [-0.1924,  0.2318,  0.5756, -0.7303,  0.8680],\n",
      "        [-0.1653,  0.0821, -0.6291, -0.5913,  2.1346],\n",
      "        [ 0.9002,  0.2674, -1.0160,  1.6153,  0.6656]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0670, -0.2221, -0.1476, -0.2739, -0.7503],\n",
      "        [ 0.5674,  1.3398,  1.8419,  1.5803,  2.2990],\n",
      "        [ 0.9430,  0.6557,  0.9522, -0.0869,  1.4328],\n",
      "        [ 0.1855,  0.8088, -0.4286,  0.8261,  0.5611]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2396,  0.2348,  1.0339,  0.5938,  0.2070],\n",
      "        [-0.1924,  0.2318,  0.5756, -0.7303,  0.8680],\n",
      "        [-0.1653,  0.0821, -0.6291, -0.5913,  2.1346],\n",
      "        [ 0.9002,  0.2674, -1.0160,  1.6153,  0.6656]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6059],\n",
      "        [ 2.1029],\n",
      "        [ 2.4088],\n",
      "        [ 2.5265]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1805, -0.5132, -2.3496,  0.1902,  0.3476],\n",
      "        [-1.0863,  1.9485, -0.4477,  1.7069,  0.6555],\n",
      "        [-0.0201, -0.0055,  0.0057, -0.3943, -0.5198],\n",
      "        [-1.0953, -0.6672, -0.2653, -0.9375, -1.3201]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4227,  0.2579, -0.2019, -0.6320, -0.3976],\n",
      "        [ 0.4262,  1.0803,  1.2601,  1.2304,  1.9618],\n",
      "        [-0.4960, -0.2255, -0.7655, -0.7641, -1.6572],\n",
      "        [-0.6834, -1.2077, -1.3664, -1.1490, -1.4553]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1805, -0.5132, -2.3496,  0.1902,  0.3476],\n",
      "        [-1.0863,  1.9485, -0.4477,  1.7069,  0.6555],\n",
      "        [-0.0201, -0.0055,  0.0057, -0.3943, -0.5198],\n",
      "        [-1.0953, -0.6672, -0.2653, -0.9375, -1.3201]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4153],\n",
      "        [ 4.4640],\n",
      "        [ 1.1696],\n",
      "        [ 4.9153]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6116, -0.5708,  0.1784,  0.9424, -1.6045],\n",
      "        [-0.8882, -0.5615,  0.1920,  0.2689,  0.2552],\n",
      "        [-1.1530,  0.2337, -0.2593, -0.4155,  0.7916],\n",
      "        [-0.7335, -0.6549,  0.4749, -1.9432, -0.3298]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4224,  0.4447, -0.7512, -0.5322, -0.3340],\n",
      "        [-0.7706, -0.5429, -1.1534, -1.0369, -2.3844],\n",
      "        [-0.4296, -0.8796, -1.1232, -0.9600, -2.2243],\n",
      "        [-1.4962, -1.9834, -1.8737, -3.1679, -5.2399]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6116, -0.5708,  0.1784,  0.9424, -1.6045],\n",
      "        [-0.8882, -0.5615,  0.1920,  0.2689,  0.2552],\n",
      "        [-1.1530,  0.2337, -0.2593, -0.4155,  0.7916],\n",
      "        [-0.7335, -0.6549,  0.4749, -1.9432, -0.3298]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0951],\n",
      "        [-0.1195],\n",
      "        [-0.7810],\n",
      "        [ 9.3907]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2914,  0.3543,  0.5413,  0.4399, -0.3974],\n",
      "        [-1.9303, -0.5354,  0.3192,  0.3277,  0.2616],\n",
      "        [-0.9017,  1.0965,  0.8807, -1.8048,  1.9189],\n",
      "        [-0.8344,  2.2488, -0.4303,  1.3876, -0.5903]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2060, -0.3440,  0.1541,  0.0369, -0.4500],\n",
      "        [-0.8081, -0.6976, -1.4299, -0.3529, -1.8759],\n",
      "        [-0.2143, -0.7036, -0.2911, -0.9119, -1.1702],\n",
      "        [ 0.6829, -0.2561, -0.1757,  0.2056, -0.3924]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2914,  0.3543,  0.5413,  0.4399, -0.3974],\n",
      "        [-1.9303, -0.5354,  0.3192,  0.3277,  0.2616],\n",
      "        [-0.9017,  1.0965,  0.8807, -1.8048,  1.9189],\n",
      "        [-0.8344,  2.2488, -0.4303,  1.3876, -0.5903]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4226],\n",
      "        [ 0.8706],\n",
      "        [-1.4342],\n",
      "        [-0.5531]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-3.0398, -0.2489,  2.3056,  0.1268, -0.7583],\n",
      "        [-1.3489, -0.5465,  0.6630, -0.4441, -0.0845],\n",
      "        [ 2.7113, -1.3886,  1.4327,  0.0175,  0.8504],\n",
      "        [-0.9776,  1.4204, -0.8126, -0.4918,  1.8406]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3463, -0.3355, -0.8959, -0.2892, -1.4795],\n",
      "        [-1.0345, -1.7987, -0.9963, -1.0788, -2.5107],\n",
      "        [ 0.1792, -0.3641, -0.5123, -1.3620, -0.5353],\n",
      "        [ 0.2664,  0.3548,  0.3108,  0.6793,  0.2548]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-3.0398, -0.2489,  2.3056,  0.1268, -0.7583],\n",
      "        [-1.3489, -0.5465,  0.6630, -0.4441, -0.0845],\n",
      "        [ 2.7113, -1.3886,  1.4327,  0.0175,  0.8504],\n",
      "        [-0.9776,  1.4204, -0.8126, -0.4918,  1.8406]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1558],\n",
      "        [ 2.4093],\n",
      "        [-0.2215],\n",
      "        [ 0.1259]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9117, -0.1449,  1.8510,  0.2417,  0.4974],\n",
      "        [ 0.9173, -0.5914,  0.9513, -0.2148, -1.0507],\n",
      "        [ 0.3691,  1.5492,  1.8973, -2.6631,  0.6487],\n",
      "        [-1.4859,  0.1935, -0.1067, -2.4752, -0.3427]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1387, -0.1488,  0.2377,  0.0859, -0.1675],\n",
      "        [ 0.0375, -0.0936,  0.7853, -0.5036,  0.2736],\n",
      "        [ 0.0187, -1.2418, -0.7134, -0.2781, -1.0238],\n",
      "        [-0.1624,  0.4708,  0.3624, -0.0115, -0.0457]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9117, -0.1449,  1.8510,  0.2417,  0.4974],\n",
      "        [ 0.9173, -0.5914,  0.9513, -0.2148, -1.0507],\n",
      "        [ 0.3691,  1.5492,  1.8973, -2.6631,  0.6487],\n",
      "        [-1.4859,  0.1935, -0.1067, -2.4752, -0.3427]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6641],\n",
      "        [ 0.6575],\n",
      "        [-3.1942],\n",
      "        [ 0.3380]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1992, -2.5354, -0.1715, -1.3228, -0.7423],\n",
      "        [ 0.6640,  0.3352, -0.5934,  0.5952, -0.4514],\n",
      "        [-1.0957, -0.4667,  0.0577,  1.3067,  0.3942],\n",
      "        [-3.2520, -0.4604, -0.4640, -0.7213, -1.0090]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1892, -0.6752, -0.1229,  0.0747, -0.7198],\n",
      "        [ 0.1867,  0.7121,  0.2489,  0.2993,  0.5207],\n",
      "        [ 1.3711,  0.6261,  1.1707,  0.8246,  1.0305],\n",
      "        [ 0.3817,  0.3057, -0.3101,  0.8606,  0.1305]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1992, -2.5354, -0.1715, -1.3228, -0.7423],\n",
      "        [ 0.6640,  0.3352, -0.5934,  0.5952, -0.4514],\n",
      "        [-1.0957, -0.4667,  0.0577,  1.3067,  0.3942],\n",
      "        [-3.2520, -0.4604, -0.4640, -0.7213, -1.0090]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.9415],\n",
      "        [ 0.1581],\n",
      "        [-0.2434],\n",
      "        [-1.9906]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5722,  0.3311, -0.8966,  1.1456,  0.1044],\n",
      "        [-0.2327, -0.4491, -0.0982, -1.5599,  0.2380],\n",
      "        [-1.2399,  1.3914,  0.2983,  0.1247,  0.3403],\n",
      "        [-0.2140,  0.9896, -1.3327, -0.1564, -0.1067]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7676, -0.2775, -0.3730, -1.2885, -1.2035],\n",
      "        [-0.0596,  0.3681, -0.3160,  0.0425,  0.2093],\n",
      "        [ 1.1467,  1.5028,  0.7972,  0.7056,  1.9366],\n",
      "        [ 0.8479,  1.8265,  1.1047,  1.3200,  1.4785]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5722,  0.3311, -0.8966,  1.1456,  0.1044],\n",
      "        [-0.2327, -0.4491, -0.0982, -1.5599,  0.2380],\n",
      "        [-1.2399,  1.3914,  0.2983,  0.1247,  0.3403],\n",
      "        [-0.2140,  0.9896, -1.3327, -0.1564, -0.1067]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7984],\n",
      "        [-0.1369],\n",
      "        [ 1.6539],\n",
      "        [-0.2106]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1496, -1.0900, -0.7061, -1.1373, -0.1348],\n",
      "        [-0.4185,  1.0542, -0.9978,  1.7573,  0.2113],\n",
      "        [ 0.3189,  0.7452, -1.0822, -0.4876,  1.4907],\n",
      "        [ 0.2964,  0.0532,  0.8154, -0.8250,  0.0815]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2536,  0.6548,  0.7504, -0.4711, -0.0896],\n",
      "        [ 0.1622, -0.0795, -0.4706,  0.3100,  0.1392],\n",
      "        [ 0.8804, -0.5515, -0.1814, -0.3283,  0.0371],\n",
      "        [ 0.3486,  0.8334,  0.9675,  1.1610,  1.1298]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1496, -1.0900, -0.7061, -1.1373, -0.1348],\n",
      "        [-0.4185,  1.0542, -0.9978,  1.7573,  0.2113],\n",
      "        [ 0.3189,  0.7452, -1.0822, -0.4876,  1.4907],\n",
      "        [ 0.2964,  0.0532,  0.8154, -0.8250,  0.0815]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9873],\n",
      "        [ 0.8920],\n",
      "        [ 0.2814],\n",
      "        [ 0.0709]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4718,  0.3057, -0.2538,  0.4279, -0.3509],\n",
      "        [ 2.0757, -0.0011,  1.2050, -1.3619, -0.3199],\n",
      "        [ 0.7098,  1.8878,  0.0541,  1.6745, -0.3934],\n",
      "        [ 0.1407, -1.7034,  0.0026,  1.1511,  1.6184]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4840,  0.4288,  0.0997,  0.2606,  0.8282],\n",
      "        [-0.5043, -0.1671,  0.9218,  0.0030, -1.0438],\n",
      "        [ 0.5917,  0.3151,  0.7579,  0.3678, -0.6261],\n",
      "        [ 0.3069,  1.4534,  1.2688,  0.8201,  0.8150]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4718,  0.3057, -0.2538,  0.4279, -0.3509],\n",
      "        [ 2.0757, -0.0011,  1.2050, -1.3619, -0.3199],\n",
      "        [ 0.7098,  1.8878,  0.0541,  1.6745, -0.3934],\n",
      "        [ 0.1407, -1.7034,  0.0026,  1.1511,  1.6184]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1551],\n",
      "        [ 0.3940],\n",
      "        [ 1.9181],\n",
      "        [-0.1662]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3929,  0.2993, -0.9224, -0.2473,  1.1207],\n",
      "        [-0.8018, -0.2374,  1.3675, -2.2244,  1.0732],\n",
      "        [-0.6361,  0.4631, -1.5512,  0.7203, -1.7435],\n",
      "        [ 0.1281,  0.8594, -0.0761,  0.0197,  0.5983]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3929,  0.0531,  0.0469, -0.1585, -0.3174],\n",
      "        [ 0.3889, -0.1630, -0.0299,  0.1953, -0.0182],\n",
      "        [-0.4444, -1.2252, -1.5544, -1.0290, -1.4099],\n",
      "        [ 0.4785,  0.5863,  0.3781,  1.2935,  0.4613]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3929,  0.2993, -0.9224, -0.2473,  1.1207],\n",
      "        [-0.8018, -0.2374,  1.3675, -2.2244,  1.0732],\n",
      "        [-0.6361,  0.4631, -1.5512,  0.7203, -1.7435],\n",
      "        [ 0.1281,  0.8594, -0.0761,  0.0197,  0.5983]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1895],\n",
      "        [-0.7680],\n",
      "        [ 3.8436],\n",
      "        [ 0.8379]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2728, -0.0348,  0.5951, -0.6539, -1.0987],\n",
      "        [-0.9527, -2.2722, -1.1176,  1.6757,  0.5200],\n",
      "        [-0.4181, -0.5008,  0.3087, -1.9530, -0.7125],\n",
      "        [-1.3726,  0.8650,  1.6860, -0.1065, -0.5652]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0395, -0.3490,  0.2127, -0.3115, -0.0284],\n",
      "        [ 0.2507,  0.6611,  0.9466,  0.3182,  0.9396],\n",
      "        [-1.1840, -2.1450, -2.5839, -2.0808, -4.2167],\n",
      "        [ 0.1530,  0.1223,  0.7457,  0.0952,  0.1965]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2728, -0.0348,  0.5951, -0.6539, -1.0987],\n",
      "        [-0.9527, -2.2722, -1.1176,  1.6757,  0.5200],\n",
      "        [-0.4181, -0.5008,  0.3087, -1.9530, -0.7125],\n",
      "        [-1.3726,  0.8650,  1.6860, -0.1065, -0.5652]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3844],\n",
      "        [-1.7772],\n",
      "        [ 7.8396],\n",
      "        [ 1.0318]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9859,  0.5239,  0.9547, -0.2046, -0.4834],\n",
      "        [-2.0799,  0.4960,  0.4642,  2.2334,  0.0590],\n",
      "        [-0.6396,  0.4920, -0.4504,  0.8386,  1.0436],\n",
      "        [-0.2630, -0.5576, -0.4882, -0.4509, -0.2212]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2097,  0.2798, -0.7369, -0.2600, -0.3207],\n",
      "        [ 1.1444,  1.0649,  1.5098,  1.3954,  2.0306],\n",
      "        [ 0.2379,  0.2296,  0.3639, -0.1706,  0.0427],\n",
      "        [-0.0195,  0.6090, -0.0316,  1.2177, -0.2834]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9859,  0.5239,  0.9547, -0.2046, -0.4834],\n",
      "        [-2.0799,  0.4960,  0.4642,  2.2334,  0.0590],\n",
      "        [-0.6396,  0.4920, -0.4504,  0.8386,  1.0436],\n",
      "        [-0.2630, -0.5576, -0.4882, -0.4509, -0.2212]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5554],\n",
      "        [ 2.0852],\n",
      "        [-0.3016],\n",
      "        [-0.8054]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5311,  2.0760, -0.6256, -1.9454, -2.7661],\n",
      "        [ 0.6847,  1.4228,  2.2989,  1.9107,  0.4178],\n",
      "        [-0.7469, -0.0559,  0.8265,  0.6981,  0.0207],\n",
      "        [-0.8999, -1.3422, -1.4812, -1.2044, -1.6733]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2317, -1.0016, -0.4337, -0.6720, -0.0580],\n",
      "        [ 0.2750,  0.6159, -0.1564,  0.3041,  0.2080],\n",
      "        [ 0.4049, -0.0203,  0.0885,  0.0135,  0.7296],\n",
      "        [ 0.3852,  0.6609,  1.0343,  0.8390,  1.4808]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5311,  2.0760, -0.6256, -1.9454, -2.7661],\n",
      "        [ 0.6847,  1.4228,  2.2989,  1.9107,  0.4178],\n",
      "        [-0.7469, -0.0559,  0.8265,  0.6981,  0.0207],\n",
      "        [-0.8999, -1.3422, -1.4812, -1.2044, -1.6733]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2171],\n",
      "        [ 1.3732],\n",
      "        [-0.2036],\n",
      "        [-6.2541]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2896, -0.8514, -0.7263, -0.7480,  0.8198],\n",
      "        [ 0.1770, -0.1577,  0.6611,  0.6417,  0.0318],\n",
      "        [ 0.1087, -0.5999,  0.2470, -0.3920, -0.5981],\n",
      "        [-1.2713,  0.2735,  0.7360, -0.7054,  0.3692]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0561, -0.6309, -0.0963, -0.1604,  0.3884],\n",
      "        [-0.5887, -0.0667, -0.3707, -0.6615, -0.7668],\n",
      "        [-0.1227,  0.0124, -0.0336,  0.3589, -0.5100],\n",
      "        [ 2.4630,  2.4540,  2.9275,  1.8125,  2.6333]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2896, -0.8514, -0.7263, -0.7480,  0.8198],\n",
      "        [ 0.1770, -0.1577,  0.6611,  0.6417,  0.0318],\n",
      "        [ 0.1087, -0.5999,  0.2470, -0.3920, -0.5981],\n",
      "        [-1.2713,  0.2735,  0.7360, -0.7054,  0.3692]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0293],\n",
      "        [-0.7876],\n",
      "        [ 0.1352],\n",
      "        [-0.6116]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2194,  0.3527, -0.1429,  3.5962,  0.2590],\n",
      "        [-1.5693, -0.0608, -0.0166, -1.6375,  0.4233],\n",
      "        [-1.6499,  0.3594, -0.6328, -1.3734,  0.1651],\n",
      "        [-2.1924, -1.3760, -1.0661, -0.1581,  0.2123]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1405, -1.0785, -0.8650, -0.8193, -1.5198],\n",
      "        [-0.1009,  0.2740, -0.0768,  0.5708, -0.1512],\n",
      "        [ 0.1022, -0.6642,  0.4923, -0.0481,  0.3807],\n",
      "        [-0.0515,  0.4071, -0.0669,  0.1259, -1.1095]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2194,  0.3527, -0.1429,  3.5962,  0.2590],\n",
      "        [-1.5693, -0.0608, -0.0166, -1.6375,  0.4233],\n",
      "        [-1.6499,  0.3594, -0.6328, -1.3734,  0.1651],\n",
      "        [-2.1924, -1.3760, -1.0661, -0.1581,  0.2123]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.7679],\n",
      "        [-0.8557],\n",
      "        [-0.5900],\n",
      "        [-0.6314]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2989, -0.9276, -1.2689, -0.3486,  1.9997],\n",
      "        [-2.4868, -2.4079,  2.2844, -1.2151, -0.2925],\n",
      "        [-0.4573,  1.4091, -1.5258,  0.8937,  0.0831],\n",
      "        [-0.9493,  1.0067,  0.8195,  1.2125,  0.3509]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4071,  1.7477,  2.0309,  0.8689,  2.7752],\n",
      "        [ 0.3316,  0.8654,  0.3639,  0.6686,  0.4399],\n",
      "        [ 0.0612, -0.4719,  0.2943, -0.7966,  0.3802],\n",
      "        [ 0.2630, -0.3054,  0.0498, -0.3310, -0.1354]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2989, -0.9276, -1.2689, -0.3486,  1.9997],\n",
      "        [-2.4868, -2.4079,  2.2844, -1.2151, -0.2925],\n",
      "        [-0.4573,  1.4091, -1.5258,  0.8937,  0.0831],\n",
      "        [-0.9493,  1.0067,  0.8195,  1.2125,  0.3509]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4692],\n",
      "        [-3.0182],\n",
      "        [-1.8225],\n",
      "        [-0.9651]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7542,  0.6284, -0.1900, -1.3841, -1.0539],\n",
      "        [-0.8909,  0.5533,  0.1583, -0.7002, -0.5811],\n",
      "        [-1.0100, -0.2245,  2.3222, -0.4898, -0.0067],\n",
      "        [ 0.5510,  2.1021, -0.3647,  0.4095,  0.9077]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3794,  0.5853, -0.0208,  0.4013,  1.0170],\n",
      "        [ 0.8929,  2.4073,  2.4474,  1.1648,  1.2723],\n",
      "        [ 0.9597,  0.4982,  0.5311,  0.4207,  0.7599],\n",
      "        [ 0.5573,  0.8187,  0.3508,  0.4687,  0.0699]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7542,  0.6284, -0.1900, -1.3841, -1.0539],\n",
      "        [-0.8909,  0.5533,  0.1583, -0.7002, -0.5811],\n",
      "        [-1.0100, -0.2245,  2.3222, -0.4898, -0.0067],\n",
      "        [ 0.5510,  2.1021, -0.3647,  0.4095,  0.9077]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5417],\n",
      "        [-0.6309],\n",
      "        [-0.0591],\n",
      "        [ 2.1555]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2457,  0.8484,  1.5091,  1.6143,  0.0095],\n",
      "        [-1.1615, -0.0994,  1.3944,  0.4405,  1.1354],\n",
      "        [ 1.0283,  0.1516,  1.5341,  0.6397, -0.0887],\n",
      "        [-0.6470,  1.1163, -0.0260, -1.3353, -1.0845]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8704,  1.4197,  1.0044,  1.3906,  1.3579],\n",
      "        [ 0.9223,  2.1480,  1.2496,  2.0812,  3.0108],\n",
      "        [ 1.0592,  0.1476,  0.3799,  0.3385,  0.7881],\n",
      "        [-0.0762, -0.7776, -0.6019, -0.8979, -0.7014]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2457,  0.8484,  1.5091,  1.6143,  0.0095],\n",
      "        [-1.1615, -0.0994,  1.3944,  0.4405,  1.1354],\n",
      "        [ 1.0283,  0.1516,  1.5341,  0.6397, -0.0887],\n",
      "        [-0.6470,  1.1163, -0.0260, -1.3353, -1.0845]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.7639],\n",
      "        [ 4.7929],\n",
      "        [ 1.8409],\n",
      "        [ 1.1564]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6664,  0.3627, -0.2851,  0.4251, -0.0638],\n",
      "        [ 1.0621,  0.8143,  2.1552, -0.7128, -1.1115],\n",
      "        [-0.0425,  1.7339, -0.3182, -1.5828,  1.0425],\n",
      "        [-2.0851,  0.5959, -0.4461, -1.5429,  0.5095]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9214, -1.3400, -0.6782, -1.2253, -2.5022],\n",
      "        [ 0.1652, -0.0447,  0.6856,  0.5833, -0.2566],\n",
      "        [-0.1913, -0.1924, -0.7014, -0.7821, -1.2526],\n",
      "        [-0.4643, -1.1060, -0.8070, -1.3788, -1.8127]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6664,  0.3627, -0.2851,  0.4251, -0.0638],\n",
      "        [ 1.0621,  0.8143,  2.1552, -0.7128, -1.1115],\n",
      "        [-0.0425,  1.7339, -0.3182, -1.5828,  1.0425],\n",
      "        [-2.0851,  0.5959, -0.4461, -1.5429,  0.5095]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1894],\n",
      "        [ 1.4860],\n",
      "        [-0.1702],\n",
      "        [ 1.8729]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4810,  0.5738,  1.1905, -1.8860,  1.0741],\n",
      "        [-0.1423,  1.4620,  0.8847,  1.2919,  0.4252],\n",
      "        [-2.5087,  0.1800, -0.2276,  0.3943,  0.2741],\n",
      "        [ 0.9009,  2.9845,  0.4848, -1.6852,  2.0166]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4590, -0.2041,  0.3512, -0.1111, -0.8845],\n",
      "        [ 0.3117, -0.3055, -0.3088, -0.4083, -0.6070],\n",
      "        [-0.0077,  0.3915, -0.4764, -0.2158, -0.1241],\n",
      "        [-1.0951, -0.9409, -0.8968, -1.7729, -2.7777]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4810,  0.5738,  1.1905, -1.8860,  1.0741],\n",
      "        [-0.1423,  1.4620,  0.8847,  1.2919,  0.4252],\n",
      "        [-2.5087,  0.1800, -0.2276,  0.3943,  0.2741],\n",
      "        [ 0.9009,  2.9845,  0.4848, -1.6852,  2.0166]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6603],\n",
      "        [-1.5498],\n",
      "        [ 0.0792],\n",
      "        [-6.8431]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3223,  0.4322, -0.2487,  1.8265,  0.5621],\n",
      "        [-2.0139, -0.3386,  0.9745,  1.1134, -1.1811],\n",
      "        [-1.5401,  0.1128,  0.5366, -0.0962,  1.3302],\n",
      "        [ 0.8931,  0.8956,  0.2918, -0.0081,  0.6493]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4454, -0.4693, -0.2133, -0.3335,  0.0067],\n",
      "        [ 0.3988,  0.7241,  0.9505,  0.6167,  0.7142],\n",
      "        [-0.3604,  0.3791, -0.3049,  0.4825,  0.1971],\n",
      "        [ 1.0934,  1.1144,  1.1141,  1.3069,  1.5059]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3223,  0.4322, -0.2487,  1.8265,  0.5621],\n",
      "        [-2.0139, -0.3386,  0.9745,  1.1134, -1.1811],\n",
      "        [-1.5401,  0.1128,  0.5366, -0.0962,  1.3302],\n",
      "        [ 0.8931,  0.8956,  0.2918, -0.0081,  0.6493]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1663],\n",
      "        [-0.2790],\n",
      "        [ 0.6501],\n",
      "        [ 3.2668]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0902,  0.0921,  0.6233,  0.8046,  0.9730],\n",
      "        [ 0.4914, -0.5505, -0.7694,  0.1615, -0.3664],\n",
      "        [-0.9073, -0.8524,  2.2489,  0.3733, -0.8327],\n",
      "        [ 0.8399,  0.4954,  1.2844, -1.1592,  0.3349]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5078, -0.6202, -0.0774,  0.3291, -0.4308],\n",
      "        [ 0.4925,  0.6447,  0.8103,  1.0601,  1.7555],\n",
      "        [ 0.3900,  0.6619,  0.4275,  0.2130,  0.8883],\n",
      "        [-0.5266, -1.6530, -0.6122, -1.4184, -1.4710]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0902,  0.0921,  0.6233,  0.8046,  0.9730],\n",
      "        [ 0.4914, -0.5505, -0.7694,  0.1615, -0.3664],\n",
      "        [-0.9073, -0.8524,  2.2489,  0.3733, -0.8327],\n",
      "        [ 0.8399,  0.4954,  1.2844, -1.1592,  0.3349]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2938],\n",
      "        [-1.2083],\n",
      "        [-0.6169],\n",
      "        [-0.8960]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7082, -0.5586, -0.8171,  0.0597,  0.4191],\n",
      "        [-0.7632, -1.5827, -2.0586,  0.4601,  0.5715],\n",
      "        [ 0.8660, -0.0950,  0.9542,  1.9083,  1.2363],\n",
      "        [ 0.2548, -0.2694,  1.8201,  0.5789, -1.7241]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5340, -0.6028, -0.1612, -0.1277, -0.0469],\n",
      "        [ 0.2284,  0.9713,  1.2425,  0.8134,  1.9323],\n",
      "        [ 0.5597,  0.4824,  1.1142,  0.0205,  0.5776],\n",
      "        [ 0.3142, -0.9022, -0.9491, -0.7514, -1.2172]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7082, -0.5586, -0.8171,  0.0597,  0.4191],\n",
      "        [-0.7632, -1.5827, -2.0586,  0.4601,  0.5715],\n",
      "        [ 0.8660, -0.0950,  0.9542,  1.9083,  1.2363],\n",
      "        [ 0.2548, -0.2694,  1.8201,  0.5789, -1.7241]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0630],\n",
      "        [-2.7909],\n",
      "        [ 2.2554],\n",
      "        [ 0.2592]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8413,  0.4303, -1.4504, -0.0339, -0.2992],\n",
      "        [ 0.2913, -0.9070,  0.8667,  0.3197, -0.8385],\n",
      "        [ 1.0927, -1.0515, -1.1895, -0.6804,  1.3249],\n",
      "        [-0.6412, -0.7258, -0.0097, -0.0453,  0.3157]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0444,  0.5093,  0.2200,  0.2412, -0.3065],\n",
      "        [ 0.2038, -0.6619, -0.1117, -0.2174,  0.1708],\n",
      "        [-0.2833, -0.9134, -0.9672, -0.8186, -2.0244],\n",
      "        [-0.2559, -1.0192, -0.5296, -0.5992, -1.0270]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8413,  0.4303, -1.4504, -0.0339, -0.2992],\n",
      "        [ 0.2913, -0.9070,  0.8667,  0.3197, -0.8385],\n",
      "        [ 1.0927, -1.0515, -1.1895, -0.6804,  1.3249],\n",
      "        [-0.6412, -0.7258, -0.0097, -0.0453,  0.3157]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0210],\n",
      "        [ 0.3501],\n",
      "        [-0.3239],\n",
      "        [ 0.6119]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8818,  0.6389, -0.9910,  0.1230, -2.2335],\n",
      "        [-0.3241, -1.7816, -0.6043, -0.4084,  0.4114],\n",
      "        [-0.1695, -1.0486,  0.0164,  1.7894, -1.1306],\n",
      "        [-1.0331, -0.5892,  0.2878, -1.1262,  2.2118]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0261,  0.1411, -0.5380,  0.7582, -0.2256],\n",
      "        [-0.1720,  0.0439, -0.1532, -0.0680,  0.5726],\n",
      "        [-0.2846,  0.1289, -0.2300, -0.2464, -0.9047],\n",
      "        [-0.1117, -0.9488, -0.3218, -0.5236, -1.2120]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8818,  0.6389, -0.9910,  0.1230, -2.2335],\n",
      "        [-0.3241, -1.7816, -0.6043, -0.4084,  0.4114],\n",
      "        [-0.1695, -1.0486,  0.0164,  1.7894, -1.1306],\n",
      "        [-1.0331, -0.5892,  0.2878, -1.1262,  2.2118]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1713],\n",
      "        [ 0.3335],\n",
      "        [ 0.4913],\n",
      "        [-1.5093]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0685, -0.2169, -1.1659, -2.7052, -0.2987],\n",
      "        [ 0.8620, -0.8100, -1.3008, -0.3343,  0.3042],\n",
      "        [ 0.4421,  0.1680, -0.0449, -1.1178,  0.1516],\n",
      "        [-0.4995,  2.1337, -0.1329,  0.1436,  1.1455]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4961, -1.1784,  0.2484, -0.1289, -1.2059],\n",
      "        [ 0.1275,  0.0812,  0.3843,  0.2558,  0.7721],\n",
      "        [-0.2583, -0.4920, -0.1343,  0.3057, -0.4290],\n",
      "        [ 0.8267, -0.9546, -0.1840, -0.9369, -0.1835]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0685, -0.2169, -1.1659, -2.7052, -0.2987],\n",
      "        [ 0.8620, -0.8100, -1.3008, -0.3343,  0.3042],\n",
      "        [ 0.4421,  0.1680, -0.0449, -1.1178,  0.1516],\n",
      "        [-0.4995,  2.1337, -0.1329,  0.1436,  1.1455]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7090],\n",
      "        [-0.3064],\n",
      "        [-0.5976],\n",
      "        [-2.7700]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3682,  0.1112,  0.8622, -0.3050, -1.4128],\n",
      "        [ 0.5474,  0.9631,  1.1142,  0.1281,  0.9354],\n",
      "        [-0.8344,  0.2117, -0.8719,  1.6308,  1.0461],\n",
      "        [ 0.9941, -0.4382,  1.0560, -1.5706, -0.0129]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1426, -0.9527, -0.4676, -0.0056, -0.5861],\n",
      "        [ 0.2678,  0.5193, -0.0831, -0.5050, -0.3186],\n",
      "        [-0.1841,  0.3828, -0.3844, -0.3876, -0.2958],\n",
      "        [ 1.5713,  0.7758,  0.8751,  0.6113,  1.5586]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3682,  0.1112,  0.8622, -0.3050, -1.4128],\n",
      "        [ 0.5474,  0.9631,  1.1142,  0.1281,  0.9354],\n",
      "        [-0.8344,  0.2117, -0.8719,  1.6308,  1.0461],\n",
      "        [ 0.9941, -0.4382,  1.0560, -1.5706, -0.0129]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1256],\n",
      "        [ 0.1915],\n",
      "        [-0.3717],\n",
      "        [ 1.1659]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8936,  0.1172,  0.3449, -1.0022, -1.3161],\n",
      "        [-1.1073,  0.1112,  1.2287,  0.1109, -0.1536],\n",
      "        [ 0.8188,  0.4724, -1.1334, -0.5127,  0.4915],\n",
      "        [-0.6577,  1.2143,  0.6720,  2.1646,  1.3608]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2147, -0.4269,  0.2459, -0.2762, -0.0004],\n",
      "        [-0.0906, -0.1691, -0.5938, -0.1903, -0.0679],\n",
      "        [-0.5794,  0.6112, -1.2167, -0.4693,  0.0524],\n",
      "        [ 0.7628,  0.6025, -0.0623, -0.7022,  0.5932]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8936,  0.1172,  0.3449, -1.0022, -1.3161],\n",
      "        [-1.1073,  0.1112,  1.2287,  0.1109, -0.1536],\n",
      "        [ 0.8188,  0.4724, -1.1334, -0.5127,  0.4915],\n",
      "        [-0.6577,  1.2143,  0.6720,  2.1646,  1.3608]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7186],\n",
      "        [-0.6588],\n",
      "        [ 1.4597],\n",
      "        [-0.5247]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8010,  0.8665,  1.0524,  1.0345, -1.2243],\n",
      "        [ 0.1463, -0.8997, -0.0789, -0.0282,  1.0739],\n",
      "        [-1.5135,  0.8915, -0.7920,  0.1750,  0.8886],\n",
      "        [ 0.4054, -1.4728, -0.8364, -0.1068, -0.6880]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2131, -0.1245, -0.1900,  0.5140, -0.1498],\n",
      "        [ 0.6453, -0.3400, -0.5943,  0.1258,  0.1987],\n",
      "        [-0.2450,  0.1416, -1.8836, -0.9664, -1.1592],\n",
      "        [ 0.9522, -0.1861,  0.2468, -0.3275,  0.3299]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8010,  0.8665,  1.0524,  1.0345, -1.2243],\n",
      "        [ 0.1463, -0.8997, -0.0789, -0.0282,  1.0739],\n",
      "        [-1.5135,  0.8915, -0.7920,  0.1750,  0.8886],\n",
      "        [ 0.4054, -1.4728, -0.8364, -0.1068, -0.6880]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7911],\n",
      "        [ 0.6570],\n",
      "        [ 0.7898],\n",
      "        [ 0.2618]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8225,  0.1558, -1.2221, -1.2861,  1.4287],\n",
      "        [ 0.0204, -0.3252, -1.3457,  0.8509,  0.5929],\n",
      "        [-0.5688, -0.1542,  1.0977,  0.9332,  1.0878],\n",
      "        [ 1.1955, -0.4119,  0.2052, -0.1476,  1.5966]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6295,  0.3448,  0.3458,  0.3671, -0.5310],\n",
      "        [ 0.2007, -0.1928,  0.5929, -0.2935, -0.7567],\n",
      "        [-0.6798, -1.3537, -0.7235, -0.9915, -1.7238],\n",
      "        [ 0.4074, -0.7531, -0.4566, -0.8456, -0.0531]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8225,  0.1558, -1.2221, -1.2861,  1.4287],\n",
      "        [ 0.0204, -0.3252, -1.3457,  0.8509,  0.5929],\n",
      "        [-0.5688, -0.1542,  1.0977,  0.9332,  1.0878],\n",
      "        [ 1.1955, -0.4119,  0.2052, -0.1476,  1.5966]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0819],\n",
      "        [-1.4295],\n",
      "        [-2.9992],\n",
      "        [ 0.7436]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4110,  1.8794,  0.8390,  0.9509,  0.4711],\n",
      "        [ 1.0933,  1.4184, -0.3070, -0.9081,  0.4274],\n",
      "        [ 0.8838,  1.6186, -0.2755, -0.4727,  1.1630],\n",
      "        [-1.2131, -0.0748, -1.2748,  2.0884, -0.0323]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0995,  0.7895,  0.4506,  0.2853,  0.3288],\n",
      "        [ 0.5653,  0.1366,  0.2917,  0.8768,  1.5748],\n",
      "        [ 0.5505,  0.6704,  0.6047,  0.6738,  0.4924],\n",
      "        [ 0.0035, -1.0067, -0.2865, -0.0622, -1.2209]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4110,  1.8794,  0.8390,  0.9509,  0.4711],\n",
      "        [ 1.0933,  1.4184, -0.3070, -0.9081,  0.4274],\n",
      "        [ 0.8838,  1.6186, -0.2755, -0.4727,  1.1630],\n",
      "        [-1.2131, -0.0748, -1.2748,  2.0884, -0.0323]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3288],\n",
      "        [ 0.5992],\n",
      "        [ 1.6592],\n",
      "        [ 0.3459]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0504,  1.1121, -0.2762,  0.6649, -0.3617],\n",
      "        [-0.0032,  0.0583, -0.4871, -0.6603, -0.1103],\n",
      "        [ 0.7102, -0.6402,  1.1110, -0.2666,  0.7036],\n",
      "        [-1.2799, -0.4024, -0.0123,  0.4622, -0.0665]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7699,  0.0817, -0.7483, -1.2776, -0.8901],\n",
      "        [-0.0526,  0.0296, -0.3754, -0.1771, -0.2488],\n",
      "        [-0.5890, -0.8392, -0.6760, -0.6404, -0.8594],\n",
      "        [-0.5748, -1.1472, -0.6319, -0.6665, -1.5746]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0504,  1.1121, -0.2762,  0.6649, -0.3617],\n",
      "        [-0.0032,  0.0583, -0.4871, -0.6603, -0.1103],\n",
      "        [ 0.7102, -0.6402,  1.1110, -0.2666,  0.7036],\n",
      "        [-1.2799, -0.4024, -0.0123,  0.4622, -0.0665]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1912],\n",
      "        [ 0.3291],\n",
      "        [-1.0660],\n",
      "        [ 1.0019]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3484, -1.1261,  0.1925, -0.1709,  0.8918],\n",
      "        [ 1.7266,  0.1704,  0.7081,  0.3389,  1.7288],\n",
      "        [ 1.9161,  1.7186,  0.2078,  0.7383, -0.7265],\n",
      "        [ 1.0730, -0.4398, -2.1541,  0.0542, -1.6896]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6091,  0.0378, -0.4285, -0.6696, -1.4832],\n",
      "        [-0.0133,  0.4775, -0.5525,  0.0618,  0.0575],\n",
      "        [-0.2230, -0.2344,  0.4553, -0.0980, -0.8991],\n",
      "        [-0.3570, -1.3238, -1.2499, -1.3484, -1.3633]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3484, -1.1261,  0.1925, -0.1709,  0.8918],\n",
      "        [ 1.7266,  0.1704,  0.7081,  0.3389,  1.7288],\n",
      "        [ 1.9161,  1.7186,  0.2078,  0.7383, -0.7265],\n",
      "        [ 1.0730, -0.4398, -2.1541,  0.0542, -1.6896]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1210],\n",
      "        [-0.2125],\n",
      "        [-0.1547],\n",
      "        [ 5.1220]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7409,  0.6601,  1.2403, -1.8292, -1.0894],\n",
      "        [-0.8352, -0.6761,  0.7966,  1.3183,  0.3305],\n",
      "        [-1.3406,  0.4983, -0.6538, -0.8098,  1.3819],\n",
      "        [-0.8750, -0.3225,  0.6768,  0.6707,  1.5742]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1304, -0.0965, -0.5669,  0.2868,  0.1387],\n",
      "        [ 0.0810,  0.0556,  0.3882, -0.4186,  0.3176],\n",
      "        [ 0.7389, -1.0411,  0.0010, -0.2402, -1.3135],\n",
      "        [-1.3956, -2.8320, -1.6757, -2.0310, -4.6633]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7409,  0.6601,  1.2403, -1.8292, -1.0894],\n",
      "        [-0.8352, -0.6761,  0.7966,  1.3183,  0.3305],\n",
      "        [-1.3406,  0.4983, -0.6538, -0.8098,  1.3819],\n",
      "        [-0.8750, -0.3225,  0.6768,  0.6707,  1.5742]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6696],\n",
      "        [-0.2429],\n",
      "        [-3.1307],\n",
      "        [-7.7027]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0413,  0.3820,  1.0872, -0.0195, -0.2053],\n",
      "        [-1.3433, -0.2296,  1.8595, -0.9957, -0.1207],\n",
      "        [-0.4854,  1.4961,  0.1389,  0.3138,  1.9886],\n",
      "        [-0.2235, -1.1804, -0.3893,  1.5620, -2.0194]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5127,  0.2582,  0.8098,  0.0871,  0.4514],\n",
      "        [-0.0462, -0.3133, -0.2678,  0.4524, -0.4014],\n",
      "        [ 1.3569,  1.0457,  0.9625,  0.8948,  1.9175],\n",
      "        [ 1.3508,  0.3021,  0.1715,  0.0070,  0.6365]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0413,  0.3820,  1.0872, -0.0195, -0.2053],\n",
      "        [-1.3433, -0.2296,  1.8595, -0.9957, -0.1207],\n",
      "        [-0.4854,  1.4961,  0.1389,  0.3138,  1.9886],\n",
      "        [-0.2235, -1.1804, -0.3893,  1.5620, -2.0194]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4185],\n",
      "        [-0.7659],\n",
      "        [ 5.1334],\n",
      "        [-1.9997]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1337,  1.6048,  1.0957, -1.2278,  0.4716],\n",
      "        [-3.1865,  0.0929,  0.3250, -0.2336,  1.7366],\n",
      "        [-0.4818, -0.4538, -0.6868,  3.2687, -0.4678],\n",
      "        [-2.0649,  0.4016,  1.3167,  0.2007,  0.5487]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2313, -0.5234,  0.0177, -0.7747, -0.8884],\n",
      "        [ 0.7381, -0.2511,  0.0300,  0.6349,  0.1414],\n",
      "        [-0.6013, -1.8716, -0.6884, -1.8160, -2.6726],\n",
      "        [ 1.1027,  1.4288,  1.2852,  0.2928,  1.4966]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1337,  1.6048,  1.0957, -1.2278,  0.4716],\n",
      "        [-3.1865,  0.0929,  0.3250, -0.2336,  1.7366],\n",
      "        [-0.4818, -0.4538, -0.6868,  3.2687, -0.4678],\n",
      "        [-2.0649,  0.4016,  1.3167,  0.2007,  0.5487]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3193],\n",
      "        [-2.2683],\n",
      "        [-3.0736],\n",
      "        [ 0.8691]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.2781,  0.5946,  0.2575, -0.8126, -1.4150],\n",
      "        [-0.6743,  1.2844,  0.2618,  0.7398,  0.0573],\n",
      "        [-0.3173, -0.4204,  0.7013, -2.8721,  1.2408],\n",
      "        [-0.6682,  0.6368, -0.8981, -0.4779, -1.0538]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2883, -0.6001, -0.4774, -0.4069, -0.7077],\n",
      "        [ 1.0556,  1.7112,  0.5364,  0.7146,  1.1847],\n",
      "        [ 0.7481, -0.3235, -0.1060, -0.1465,  1.0627],\n",
      "        [ 0.6232,  0.5308,  0.9299, -0.5465,  0.1306]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.2781,  0.5946,  0.2575, -0.8126, -1.4150],\n",
      "        [-0.6743,  1.2844,  0.2618,  0.7398,  0.0573],\n",
      "        [-0.3173, -0.4204,  0.7013, -2.8721,  1.2408],\n",
      "        [-0.6682,  0.6368, -0.8981, -0.4779, -1.0538]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1956],\n",
      "        [ 2.2231],\n",
      "        [ 1.5637],\n",
      "        [-0.7901]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5985,  0.7674, -0.3645,  1.5455,  0.3061],\n",
      "        [ 0.3722, -0.1417, -0.1094, -0.1669,  0.3835],\n",
      "        [-0.8816, -0.7687,  0.1152, -0.0342, -0.2901],\n",
      "        [-0.7657, -0.3357, -0.4897,  1.3062,  1.9037]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0849, -0.9819, -0.4211, -0.0879, -0.3894],\n",
      "        [ 0.1111,  0.9509,  0.0509, -0.4707,  0.0439],\n",
      "        [-0.2519, -1.0569, -1.1446, -0.8900, -1.7840],\n",
      "        [ 0.8537,  0.3097, -0.0661, -0.5059,  0.8522]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5985,  0.7674, -0.3645,  1.5455,  0.3061],\n",
      "        [ 0.3722, -0.1417, -0.1094, -0.1669,  0.3835],\n",
      "        [-0.8816, -0.7687,  0.1152, -0.0342, -0.2901],\n",
      "        [-0.7657, -0.3357, -0.4897,  1.3062,  1.9037]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9060],\n",
      "        [-0.0035],\n",
      "        [ 1.4507],\n",
      "        [ 0.2363]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.5703, -0.8133, -0.5009,  0.3441,  1.1545],\n",
      "        [ 1.2909, -0.3881, -0.2320,  0.6637,  0.6790],\n",
      "        [-2.2795,  0.2194,  0.0730,  0.2635, -1.4881],\n",
      "        [-0.5688, -0.9346,  0.1461, -2.1946,  1.0269]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1274, -0.4908, -0.6002,  0.0387,  0.1889],\n",
      "        [ 0.3861,  0.0024,  0.4305,  0.0836,  0.9816],\n",
      "        [-1.1332, -0.9544, -0.3468, -0.9018, -2.0829],\n",
      "        [ 0.6947, -0.3424,  0.8276, -0.2946, -0.0254]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.5703, -0.8133, -0.5009,  0.3441,  1.1545],\n",
      "        [ 1.2909, -0.3881, -0.2320,  0.6637,  0.6790],\n",
      "        [-2.2795,  0.2194,  0.0730,  0.2635, -1.4881],\n",
      "        [-0.5688, -0.9346,  0.1461, -2.1946,  1.0269]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2586],\n",
      "        [ 1.1196],\n",
      "        [ 5.2101],\n",
      "        [ 0.6663]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5516,  0.0408,  0.9304,  0.4467, -0.4239],\n",
      "        [ 0.2762,  1.3419,  0.0684,  0.2647, -0.1621],\n",
      "        [-1.3243, -3.1006, -0.9385,  0.4150, -1.4765],\n",
      "        [-1.3933,  0.1202, -0.1038,  0.2463, -0.2543]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6716, -0.9013, -0.7599, -0.5479, -1.3084],\n",
      "        [ 0.2676, -0.7281,  0.4108, -0.8410,  0.1558],\n",
      "        [-0.0020, -0.6121, -0.0030, -0.2215, -0.1809],\n",
      "        [ 0.9671, -0.3139, -0.2602, -0.4151, -0.6473]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5516,  0.0408,  0.9304,  0.4467, -0.4239],\n",
      "        [ 0.2762,  1.3419,  0.0684,  0.2647, -0.1621],\n",
      "        [-1.3243, -3.1006, -0.9385,  0.4150, -1.4765],\n",
      "        [-1.3933,  0.1202, -0.1038,  0.2463, -0.2543]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4759],\n",
      "        [-1.1230],\n",
      "        [ 2.0785],\n",
      "        [-1.2958]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4612, -0.0838,  0.1975,  0.0967,  0.8817],\n",
      "        [ 1.0760, -1.6160,  0.3670, -0.5410,  0.0103],\n",
      "        [-0.6132, -1.2018,  1.1488,  0.4256,  2.3928],\n",
      "        [ 1.4425, -0.4821,  1.6920,  0.2973, -1.1034]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3729, -0.4328, -0.2463, -0.9595, -1.1413],\n",
      "        [ 0.2555,  0.2409, -0.1564,  0.8908,  1.2456],\n",
      "        [-0.8024, -0.7401, -0.6392, -1.5559, -1.3511],\n",
      "        [ 0.5589, -0.6672, -0.6538,  0.3117,  0.6908]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4612, -0.0838,  0.1975,  0.0967,  0.8817],\n",
      "        [ 1.0760, -1.6160,  0.3670, -0.5410,  0.0103],\n",
      "        [-0.6132, -1.2018,  1.1488,  0.4256,  2.3928],\n",
      "        [ 1.4425, -0.4821,  1.6920,  0.2973, -1.1034]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2834],\n",
      "        [-0.6409],\n",
      "        [-3.2481],\n",
      "        [-0.6480]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7526,  0.7673,  0.6459, -1.3649,  0.0892],\n",
      "        [-1.6979, -0.0745,  1.0660, -0.5588,  1.7640],\n",
      "        [ 0.1164, -0.3995, -1.2234, -0.7477, -0.0948],\n",
      "        [-0.3917, -0.5014,  1.0187,  1.1180,  0.4669]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1152, -0.3858,  0.3509, -0.2978,  0.9711],\n",
      "        [ 0.5956,  0.4252,  1.0802,  0.2915,  0.7677],\n",
      "        [ 0.7976,  1.0557,  0.6394,  1.1461,  1.6743],\n",
      "        [ 0.3332,  0.1581, -0.2572,  0.1036,  0.9240]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7526,  0.7673,  0.6459, -1.3649,  0.0892],\n",
      "        [-1.6979, -0.0745,  1.0660, -0.5588,  1.7640],\n",
      "        [ 0.1164, -0.3995, -1.2234, -0.7477, -0.0948],\n",
      "        [-0.3917, -0.5014,  1.0187,  1.1180,  0.4669]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3371],\n",
      "        [ 1.2999],\n",
      "        [-2.1269],\n",
      "        [ 0.0755]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3109,  0.2479, -1.0642,  1.5679,  0.3824],\n",
      "        [ 0.3706,  0.0621,  1.6184, -0.4200,  1.7005],\n",
      "        [-0.2924, -0.3669,  0.5518,  1.2204, -1.2836],\n",
      "        [ 0.1348, -0.8689,  1.8608,  0.2983,  0.6227]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0492, -0.8452, -0.4158, -0.3737, -0.6574],\n",
      "        [ 0.3092,  0.6296,  0.3266,  0.3691,  0.3387],\n",
      "        [ 1.7571,  1.4618,  1.9272,  1.3526,  2.1094],\n",
      "        [ 0.2224, -1.1573, -0.2528, -0.4729, -0.6646]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3109,  0.2479, -1.0642,  1.5679,  0.3824],\n",
      "        [ 0.3706,  0.0621,  1.6184, -0.4200,  1.7005],\n",
      "        [-0.2924, -0.3669,  0.5518,  1.2204, -1.2836],\n",
      "        [ 0.1348, -0.8689,  1.8608,  0.2983,  0.6227]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6690],\n",
      "        [ 1.1032],\n",
      "        [-1.0436],\n",
      "        [ 0.0103]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4980,  0.2364, -0.0550, -0.0702,  0.3065],\n",
      "        [ 0.4271, -0.1038,  0.2680, -1.9224,  0.9100],\n",
      "        [ 0.3146, -0.5524, -0.9266,  0.7728, -0.7212],\n",
      "        [ 0.3271, -1.7317, -0.3006,  0.9722, -0.9067]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6915, -0.8003, -0.8065, -0.6365, -0.8452],\n",
      "        [-0.3686, -0.4422, -0.4197,  0.2902, -0.5768],\n",
      "        [ 1.0874,  1.4237,  2.4181,  1.5981,  2.2056],\n",
      "        [ 0.5115, -0.5869,  0.2665,  0.2656, -0.6697]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4980,  0.2364, -0.0550, -0.0702,  0.3065],\n",
      "        [ 0.4271, -0.1038,  0.2680, -1.9224,  0.9100],\n",
      "        [ 0.3146, -0.5524, -0.9266,  0.7728, -0.7212],\n",
      "        [ 0.3271, -1.7317, -0.3006,  0.9722, -0.9067]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7035],\n",
      "        [-1.3067],\n",
      "        [-3.0407],\n",
      "        [ 1.9689]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.3732, -1.2503, -0.7002,  1.3364, -1.1720],\n",
      "        [-0.3296,  0.7480, -1.0911,  0.7994,  1.1946],\n",
      "        [-0.4876, -0.0653, -0.6464,  0.1310,  0.2796],\n",
      "        [ 0.7589, -0.3099,  1.4780, -0.3609, -0.4984]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3377, -0.7481, -1.1155, -0.6577, -1.1998],\n",
      "        [ 0.4203,  0.7313, -0.1679,  0.8563,  1.1699],\n",
      "        [ 0.6190, -0.1192,  0.7871, -0.1974,  0.1967],\n",
      "        [-1.0128, -0.1727, -0.9465, -1.0398, -1.7714]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.3732, -1.2503, -0.7002,  1.3364, -1.1720],\n",
      "        [-0.3296,  0.7480, -1.0911,  0.7994,  1.1946],\n",
      "        [-0.4876, -0.0653, -0.6464,  0.1310,  0.2796],\n",
      "        [ 0.7589, -0.3099,  1.4780, -0.3609, -0.4984]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0451],\n",
      "        [ 2.6736],\n",
      "        [-0.7737],\n",
      "        [-0.8558]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4175,  1.4198,  0.8578,  1.2010,  0.5189],\n",
      "        [ 0.6696, -0.6812, -1.2072,  1.0357,  0.4579],\n",
      "        [-0.6367, -0.0823,  0.1721,  0.0349,  0.0589],\n",
      "        [ 1.1905,  0.5910,  0.1360, -1.5954,  0.8057]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7106, -1.0774, -1.9270, -1.2142, -2.8795],\n",
      "        [-0.2484, -0.9118, -0.8897, -0.5665, -1.4991],\n",
      "        [ 0.1668,  0.1769,  0.1730, -0.0970, -0.0964],\n",
      "        [ 0.1899, -1.3923, -1.3121, -0.5485, -0.3039]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4175,  1.4198,  0.8578,  1.2010,  0.5189],\n",
      "        [ 0.6696, -0.6812, -1.2072,  1.0357,  0.4579],\n",
      "        [-0.6367, -0.0823,  0.1721,  0.0349,  0.0589],\n",
      "        [ 1.1905,  0.5910,  0.1360, -1.5954,  0.8057]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-6.4318],\n",
      "        [ 0.2557],\n",
      "        [-0.1001],\n",
      "        [-0.1451]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1131,  1.0044,  1.2855, -0.4258, -0.4901],\n",
      "        [ 0.3102, -0.4225, -0.6400, -0.9793,  0.1103],\n",
      "        [-2.5544, -0.0157,  1.0753, -1.1268, -0.0522],\n",
      "        [-0.7566,  1.0409,  0.1308,  1.3627,  1.6341]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4132,  0.5851,  0.2872,  0.9487,  1.0465],\n",
      "        [-0.6617, -0.0933, -0.8834, -0.8504, -1.2876],\n",
      "        [ 0.2513, -0.2548,  0.1883, -0.5148, -0.6155],\n",
      "        [ 0.2629, -0.1655, -1.1353, -0.6458, -0.9611]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1131,  1.0044,  1.2855, -0.4258, -0.4901],\n",
      "        [ 0.3102, -0.4225, -0.6400, -0.9793,  0.1103],\n",
      "        [-2.5544, -0.0157,  1.0753, -1.1268, -0.0522],\n",
      "        [-0.7566,  1.0409,  0.1308,  1.3627,  1.6341]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5327],\n",
      "        [ 1.0904],\n",
      "        [ 0.1767],\n",
      "        [-2.9701]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0923,  1.4660, -0.0942,  3.0050,  1.0379],\n",
      "        [-0.0005,  0.6257, -0.6234, -0.0333,  0.4621],\n",
      "        [-1.1919, -0.7008,  0.1146,  2.5662, -0.8736],\n",
      "        [ 0.8846, -0.5831,  0.4211,  0.0246, -1.1997]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.8467,  1.7247,  1.3200,  1.1997,  2.5099],\n",
      "        [-0.1312, -1.1583, -1.4016, -0.9529, -1.3411],\n",
      "        [ 0.4005, -0.2103,  0.1533, -0.2328, -0.3014],\n",
      "        [ 1.4713,  0.8897,  1.1667,  1.0030,  1.3715]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0923,  1.4660, -0.0942,  3.0050,  1.0379],\n",
      "        [-0.0005,  0.6257, -0.6234, -0.0333,  0.4621],\n",
      "        [-1.1919, -0.7008,  0.1146,  2.5662, -0.8736],\n",
      "        [ 0.8846, -0.5831,  0.4211,  0.0246, -1.1997]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 8.7845],\n",
      "        [-0.4390],\n",
      "        [-0.6466],\n",
      "        [-0.3467]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8815,  1.4642, -1.0915, -0.2247,  0.3843],\n",
      "        [-1.6392,  2.4692,  0.3375, -0.2215,  0.5148],\n",
      "        [-0.3451,  0.0489,  0.0927, -1.1934, -0.6695],\n",
      "        [ 0.7027,  0.3179, -0.7774,  0.1007,  1.8488]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2023, -1.7074, -1.8862, -2.3934, -4.1432],\n",
      "        [-0.2333, -0.3250, -0.6520, -0.3700, -1.2500],\n",
      "        [ 0.5714,  0.2495, -0.4118, -0.0175,  0.1264],\n",
      "        [ 1.1558,  1.0727,  0.5190,  0.6789,  1.0147]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8815,  1.4642, -1.0915, -0.2247,  0.3843],\n",
      "        [-1.6392,  2.4692,  0.3375, -0.2215,  0.5148],\n",
      "        [-0.3451,  0.0489,  0.0927, -1.1934, -0.6695],\n",
      "        [ 0.7027,  0.3179, -0.7774,  0.1007,  1.8488]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.7579],\n",
      "        [-1.2015],\n",
      "        [-0.2869],\n",
      "        [ 2.6939]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6713, -2.2102, -0.6805,  0.2161,  0.0490],\n",
      "        [ 1.0619,  0.1669,  1.5316,  0.9976, -0.8175],\n",
      "        [-2.4477,  1.7855,  0.2393, -0.9853,  0.4624],\n",
      "        [ 0.1872,  1.7398,  0.0435,  1.1122, -0.1111]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0387, -1.4607, -1.4204, -1.2426, -1.3560],\n",
      "        [-0.1987,  0.2083,  0.6108, -0.4985, -0.2676],\n",
      "        [ 0.6139, -0.4008, -0.2470,  0.0608, -0.5772],\n",
      "        [ 0.2082,  0.0510,  0.0523, -0.1320, -1.0023]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6713, -2.2102, -0.6805,  0.2161,  0.0490],\n",
      "        [ 1.0619,  0.1669,  1.5316,  0.9976, -0.8175],\n",
      "        [-2.4477,  1.7855,  0.2393, -0.9853,  0.4624],\n",
      "        [ 0.1872,  1.7398,  0.0435,  1.1122, -0.1111]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.8860],\n",
      "        [ 0.4809],\n",
      "        [-2.6042],\n",
      "        [ 0.0945]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6078,  1.0226, -0.6668,  1.0830,  0.4399],\n",
      "        [ 0.9288,  0.4644, -0.2770,  0.9613, -0.3905],\n",
      "        [-0.9049, -0.2809, -1.2242,  0.5268, -0.5276],\n",
      "        [-0.7784, -0.5535, -0.3875, -1.5782, -0.5209]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.3829, -1.4661, -1.7662, -2.3553, -4.0526],\n",
      "        [-0.6053, -0.1607,  0.3158,  0.1110, -0.3403],\n",
      "        [ 1.1078,  0.8667,  1.4121,  0.8040,  1.0028],\n",
      "        [ 0.3855, -0.1322, -0.1282,  0.0669, -0.0613]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6078,  1.0226, -0.6668,  1.0830,  0.4399],\n",
      "        [ 0.9288,  0.4644, -0.2770,  0.9613, -0.3905],\n",
      "        [-0.9049, -0.2809, -1.2242,  0.5268, -0.5276],\n",
      "        [-0.7784, -0.5535, -0.3875, -1.5782, -0.5209]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-5.4956],\n",
      "        [-0.4847],\n",
      "        [-3.0802],\n",
      "        [-0.2509]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4588, -0.0909,  0.6633,  0.6832,  0.6817],\n",
      "        [-0.9718,  0.6002,  0.7985, -0.7582,  1.9301],\n",
      "        [-0.9879,  1.7344,  0.2298,  0.5594,  0.8542],\n",
      "        [ 0.1389, -0.3823,  0.5308,  0.0933,  1.1532]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6983, -0.9563, -0.6746, -0.1739,  0.1956],\n",
      "        [-0.3265, -0.3957,  0.3421, -0.4283, -0.0925],\n",
      "        [ 1.9025,  2.2919,  1.0758,  1.4482,  2.8296],\n",
      "        [ 0.2783,  0.1412, -0.0696,  0.0910,  0.7772]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4588, -0.0909,  0.6633,  0.6832,  0.6817],\n",
      "        [-0.9718,  0.6002,  0.7985, -0.7582,  1.9301],\n",
      "        [-0.9879,  1.7344,  0.2298,  0.5594,  0.8542],\n",
      "        [ 0.1389, -0.3823,  0.5308,  0.0933,  1.1532]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3647],\n",
      "        [ 0.4991],\n",
      "        [ 5.5696],\n",
      "        [ 0.8525]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6636, -0.9646,  0.6438,  1.9736,  0.2297],\n",
      "        [ 1.6356,  1.2106,  0.1361, -1.9887, -0.3679],\n",
      "        [-0.0093,  1.6886, -1.2899,  1.2677, -1.8499],\n",
      "        [ 0.6183,  1.0568, -0.0623, -1.3646,  0.6279]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6806, -0.4720, -0.9187,  0.0925,  1.1397],\n",
      "        [-0.4891, -0.1894,  0.2862, -0.4349, -0.0682],\n",
      "        [ 0.0291, -0.0118,  0.4928,  0.6908, -0.0460],\n",
      "        [ 0.8584, -0.6596, -1.1587,  0.2778, -0.2860]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6636, -0.9646,  0.6438,  1.9736,  0.2297],\n",
      "        [ 1.6356,  1.2106,  0.1361, -1.9887, -0.3679],\n",
      "        [-0.0093,  1.6886, -1.2899,  1.2677, -1.8499],\n",
      "        [ 0.6183,  1.0568, -0.0623, -1.3646,  0.6279]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7597],\n",
      "        [-0.1004],\n",
      "        [ 0.3050],\n",
      "        [-0.6527]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6396,  0.6415,  0.3086,  0.3733, -1.4395],\n",
      "        [-0.9108, -1.5302,  1.4574, -0.4180, -0.4824],\n",
      "        [-1.1406, -1.5720, -1.5483, -0.4813, -1.4267],\n",
      "        [-2.4430, -1.3549, -0.3770,  1.5703,  0.8576]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6810, -1.6538, -0.9267, -1.4715, -1.7275],\n",
      "        [-0.5647,  0.1013, -0.3262, -0.0641,  0.0234],\n",
      "        [ 0.3869,  0.3929,  0.6290,  1.4900,  0.8382],\n",
      "        [ 0.4949, -0.3650, -0.2549, -0.2958, -0.0312]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6396,  0.6415,  0.3086,  0.3733, -1.4395],\n",
      "        [-0.9108, -1.5302,  1.4574, -0.4180, -0.4824],\n",
      "        [-1.1406, -1.5720, -1.5483, -0.4813, -1.4267],\n",
      "        [-2.4430, -1.3549, -0.3770,  1.5703,  0.8576]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1547],\n",
      "        [-0.1006],\n",
      "        [-3.9458],\n",
      "        [-1.1097]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3899, -0.4847,  0.1982, -0.9350, -0.0080],\n",
      "        [ 0.3155,  0.1439, -0.3992, -0.8126,  1.2086],\n",
      "        [ 0.5992,  1.3265,  0.4787, -0.1133,  1.3723],\n",
      "        [-1.2998,  0.7090, -1.9530, -0.0564,  0.2106]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0275, -1.7205, -1.5548, -0.4240, -1.6485],\n",
      "        [-0.1048, -0.1349,  0.0203, -0.4034, -0.0571],\n",
      "        [ 0.1799, -0.2181,  1.0648, -0.0320,  0.2778],\n",
      "        [ 0.6007,  0.2125, -0.0571,  0.6340,  0.9651]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3899, -0.4847,  0.1982, -0.9350, -0.0080],\n",
      "        [ 0.3155,  0.1439, -0.3992, -0.8126,  1.2086],\n",
      "        [ 0.5992,  1.3265,  0.4787, -0.1133,  1.3723],\n",
      "        [-1.2998,  0.7090, -1.9530, -0.0564,  0.2106]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9247],\n",
      "        [ 0.1981],\n",
      "        [ 0.7130],\n",
      "        [-0.3509]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2740, -0.8538,  1.8012, -0.1488,  1.2399],\n",
      "        [-0.4224,  1.4067,  0.0142,  0.8946,  1.3870],\n",
      "        [-0.1723,  0.9794,  0.7599, -1.4523,  0.2158],\n",
      "        [-0.7276,  2.2847,  1.9936,  0.9054,  0.7016]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0301, -1.3741, -0.5316, -1.6441, -2.3973],\n",
      "        [-0.4881, -0.7150, -0.6480, -0.1204, -0.7318],\n",
      "        [-0.1966, -0.5256, -0.1419, -0.1847, -0.2220],\n",
      "        [ 0.7744, -0.2464,  0.3622,  0.5783,  0.1528]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2740, -0.8538,  1.8012, -0.1488,  1.2399],\n",
      "        [-0.4224,  1.4067,  0.0142,  0.8946,  1.3870],\n",
      "        [-0.1723,  0.9794,  0.7599, -1.4523,  0.2158],\n",
      "        [-0.7276,  2.2847,  1.9936,  0.9054,  0.7016]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4737],\n",
      "        [-1.9316],\n",
      "        [-0.3684],\n",
      "        [ 0.2264]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0644, -0.8044,  0.1741,  1.9246,  1.1969],\n",
      "        [ 0.1144, -0.0685,  0.2511, -0.2353,  0.7387],\n",
      "        [ 0.6999,  1.9581,  0.9121, -0.8401,  0.3717],\n",
      "        [-2.2421, -0.8794,  0.2992,  0.1470,  0.8617]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8707, -0.6495, -0.7989, -0.7502, -0.4480],\n",
      "        [ 0.6332,  0.2842,  0.5918,  0.4207,  0.6932],\n",
      "        [ 0.4630,  0.2329, -0.4988,  0.0611,  0.3792],\n",
      "        [ 0.7855,  0.2626,  0.0416,  0.0079,  0.0773]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0644, -0.8044,  0.1741,  1.9246,  1.1969],\n",
      "        [ 0.1144, -0.0685,  0.2511, -0.2353,  0.7387],\n",
      "        [ 0.6999,  1.9581,  0.9121, -0.8401,  0.3717],\n",
      "        [-2.2421, -0.8794,  0.2992,  0.1470,  0.8617]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5233],\n",
      "        [ 0.6147],\n",
      "        [ 0.4146],\n",
      "        [-1.9117]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8653,  0.5199, -0.6787, -1.1577,  1.7014],\n",
      "        [-0.9995,  1.2005, -0.4889, -0.9150, -1.0125],\n",
      "        [-1.6803, -1.4531, -0.9860, -0.5234,  1.0996],\n",
      "        [-1.4241,  0.6204, -0.9401, -0.5890,  0.2549]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9560,  0.3202, -0.0338, -0.2173,  1.1139],\n",
      "        [-0.1190, -0.5576, -0.3721, -0.4054,  0.2018],\n",
      "        [-0.0368,  0.0929, -0.3013,  0.2762,  0.5487],\n",
      "        [ 1.2562,  1.2901,  1.2084,  0.8720,  1.0045]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8653,  0.5199, -0.6787, -1.1577,  1.7014],\n",
      "        [-0.9995,  1.2005, -0.4889, -0.9150, -1.0125],\n",
      "        [-1.6803, -1.4531, -0.9860, -0.5234,  1.0996],\n",
      "        [-1.4241,  0.6204, -0.9401, -0.5890,  0.2549]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5089],\n",
      "        [-0.2020],\n",
      "        [ 0.6828],\n",
      "        [-2.3822]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5851,  0.7525,  0.7289, -0.6575,  3.0538],\n",
      "        [ 0.6831, -0.8581, -0.8661,  0.7913,  0.6980],\n",
      "        [-1.4382,  0.5260,  0.4165,  0.9032,  1.5658],\n",
      "        [-0.9608,  1.4360, -0.3929,  0.4258, -1.4901]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5095, -0.9179, -0.9748, -0.4421, -0.7857],\n",
      "        [-0.0227, -0.2987, -0.9750, -0.7005, -0.6478],\n",
      "        [-0.0096, -0.2913, -0.7954,  0.8656,  0.2814],\n",
      "        [ 0.0845, -0.3451,  0.2520,  0.4511, -0.4647]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5851,  0.7525,  0.7289, -0.6575,  3.0538],\n",
      "        [ 0.6831, -0.8581, -0.8661,  0.7913,  0.6980],\n",
      "        [-1.4382,  0.5260,  0.4165,  0.9032,  1.5658],\n",
      "        [-0.9608,  1.4360, -0.3929,  0.4258, -1.4901]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.8080],\n",
      "        [ 0.0788],\n",
      "        [ 0.7517],\n",
      "        [ 0.2088]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5799, -0.3513, -1.2256,  1.1627,  0.7419],\n",
      "        [-0.7377, -0.4180,  2.0657,  0.4598, -0.9077],\n",
      "        [-1.3201, -0.4435,  0.8624,  1.0195,  0.4846],\n",
      "        [ 0.8642, -0.6828,  0.0069,  0.1653, -0.2659]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6229,  0.7471,  1.1991,  0.4906,  1.7712],\n",
      "        [-0.2010, -0.6722, -0.2499, -0.4452, -0.4752],\n",
      "        [-0.3146,  0.3689,  0.3137,  0.2818, -0.5324],\n",
      "        [ 0.0426,  0.0466, -0.0540, -0.3910, -0.3432]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5799, -0.3513, -1.2256,  1.1627,  0.7419],\n",
      "        [-0.7377, -0.4180,  2.0657,  0.4598, -0.9077],\n",
      "        [-1.3201, -0.4435,  0.8624,  1.0195,  0.4846],\n",
      "        [ 0.8642, -0.6828,  0.0069,  0.1653, -0.2659]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7888],\n",
      "        [ 0.1395],\n",
      "        [ 0.5515],\n",
      "        [ 0.0312]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5980,  2.2515,  0.4325, -1.1813,  0.1979],\n",
      "        [-1.3098,  0.9981, -0.7539,  1.0358, -0.0752],\n",
      "        [ 0.3465,  1.3770, -0.6234, -0.7753,  1.3882],\n",
      "        [ 0.2240,  0.0647, -0.4136, -0.3945,  1.3028]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.2538,  0.9885,  0.9913,  0.7549,  2.7554],\n",
      "        [-0.2181, -1.2680, -0.0854, -0.2990, -0.6866],\n",
      "        [-0.3528,  0.5106,  0.5623,  0.0685, -0.2024],\n",
      "        [ 0.0057,  0.0390,  0.2858, -0.2565, -0.7915]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5980,  2.2515,  0.4325, -1.1813,  0.1979],\n",
      "        [-1.3098,  0.9981, -0.7539,  1.0358, -0.0752],\n",
      "        [ 0.3465,  1.3770, -0.6234, -0.7753,  1.3882],\n",
      "        [ 0.2240,  0.0647, -0.4136, -0.3945,  1.3028]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9601],\n",
      "        [-1.1735],\n",
      "        [-0.1037],\n",
      "        [-1.0444]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0547,  2.6803, -0.0512,  0.5555,  1.5997],\n",
      "        [-0.4949,  0.3568,  0.7062, -0.7506, -0.1213],\n",
      "        [ 1.5071, -0.9205,  2.0069, -0.0933, -0.7686],\n",
      "        [-0.0734, -0.3458, -0.2252,  0.7439,  0.8094]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5894,  0.5460,  1.5779,  1.0670,  1.4894],\n",
      "        [ 0.5226, -0.0713, -0.7512, -0.2276,  0.5488],\n",
      "        [ 0.2185,  0.2559, -0.0663,  0.8399,  0.0974],\n",
      "        [ 0.3736,  0.7099,  0.2045,  0.0325,  0.4656]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0547,  2.6803, -0.0512,  0.5555,  1.5997],\n",
      "        [-0.4949,  0.3568,  0.7062, -0.7506, -0.1213],\n",
      "        [ 1.5071, -0.9205,  2.0069, -0.0933, -0.7686],\n",
      "        [-0.0734, -0.3458, -0.2252,  0.7439,  0.8094]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 6.0341],\n",
      "        [-0.7103],\n",
      "        [-0.1925],\n",
      "        [ 0.0820]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0770,  0.8751,  1.9156,  0.2684,  0.9246],\n",
      "        [-0.0021,  0.4359,  0.5118,  0.1351, -0.1409],\n",
      "        [-0.7734, -0.9184, -0.1087,  0.3744, -0.3873],\n",
      "        [-0.1038,  1.6770,  0.0630, -0.5774, -1.1810]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8437, -1.5930, -1.3141, -0.7253, -2.0104],\n",
      "        [ 0.9214, -0.4330, -0.1727, -0.0104,  0.5674],\n",
      "        [ 0.1583,  0.5391, -0.1916, -0.4851,  0.3922],\n",
      "        [ 0.0515, -0.4213,  0.3603, -0.3742, -0.5862]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0770,  0.8751,  1.9156,  0.2684,  0.9246],\n",
      "        [-0.0021,  0.4359,  0.5118,  0.1351, -0.1409],\n",
      "        [-0.7734, -0.9184, -0.1087,  0.3744, -0.3873],\n",
      "        [-0.1038,  1.6770,  0.0630, -0.5774, -1.1810]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-6.0296],\n",
      "        [-0.3604],\n",
      "        [-0.9303],\n",
      "        [ 0.2191]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6626,  0.9939,  1.0737,  0.8902, -0.3316],\n",
      "        [-0.1327,  1.2740,  0.5822, -0.4455,  0.3491],\n",
      "        [-0.9113,  0.8132,  0.2392, -1.8864,  0.4327],\n",
      "        [ 0.0933, -1.5205,  1.4949,  1.8758,  0.5016]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2631,  1.1751,  1.5653,  0.4204,  1.9482],\n",
      "        [ 0.7176, -0.3431, -0.3093,  0.1857, -1.6004],\n",
      "        [ 0.3464,  0.5331,  0.8930,  0.6779,  1.0691],\n",
      "        [ 0.2300, -0.0518,  0.6305,  0.1557, -0.7558]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6626,  0.9939,  1.0737,  0.8902, -0.3316],\n",
      "        [-0.1327,  1.2740,  0.5822, -0.4455,  0.3491],\n",
      "        [-0.9113,  0.8132,  0.2392, -1.8864,  0.4327],\n",
      "        [ 0.0933, -1.5205,  1.4949,  1.8758,  0.5016]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7398],\n",
      "        [-1.3539],\n",
      "        [-0.4848],\n",
      "        [ 0.9557]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6530, -0.2012,  0.8165,  1.7557,  1.1173],\n",
      "        [ 0.6263,  1.8602, -1.5913,  0.5799,  1.8327],\n",
      "        [-1.5727,  0.1410, -1.2916, -0.4761, -1.5945],\n",
      "        [ 1.0712, -0.6388,  0.3522, -0.2747,  1.0920]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0142,  0.1954, -0.5370,  0.0426, -0.2023],\n",
      "        [ 0.6243,  0.1634,  0.6850,  0.0625,  0.3421],\n",
      "        [ 0.3812,  0.1825,  0.5959,  0.6038,  0.3512],\n",
      "        [-0.4394, -0.1142, -0.2499,  0.1190, -0.3119]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6530, -0.2012,  0.8165,  1.7557,  1.1173],\n",
      "        [ 0.6263,  1.8602, -1.5913,  0.5799,  1.8327],\n",
      "        [-1.5727,  0.1410, -1.2916, -0.4761, -1.5945],\n",
      "        [ 1.0712, -0.6388,  0.3522, -0.2747,  1.0920]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6525],\n",
      "        [ 0.2682],\n",
      "        [-2.1907],\n",
      "        [-0.8590]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2230,  0.1532,  0.0459,  0.4380,  0.7874],\n",
      "        [-0.5308,  0.3304, -0.0442,  1.0610, -0.6483],\n",
      "        [-0.0491,  1.9219,  0.8574, -0.4206,  1.0797],\n",
      "        [-1.1906,  0.4019,  1.0335, -1.1177,  2.3020]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3085,  0.0288,  0.1474,  0.2370,  0.2444],\n",
      "        [ 0.2101, -0.8188, -0.6913, -0.5711,  0.2017],\n",
      "        [ 1.1399,  1.0910,  1.4673,  1.0529,  2.0447],\n",
      "        [ 0.2193, -0.1859, -0.0188, -0.7678,  0.5229]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2230,  0.1532,  0.0459,  0.4380,  0.7874],\n",
      "        [-0.5308,  0.3304, -0.0442,  1.0610, -0.6483],\n",
      "        [-0.0491,  1.9219,  0.8574, -0.4206,  1.0797],\n",
      "        [-1.1906,  0.4019,  1.0335, -1.1177,  2.3020]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6847],\n",
      "        [-1.0882],\n",
      "        [ 5.0636],\n",
      "        [ 1.7065]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8977,  1.5804, -0.1814, -0.0651,  0.6562],\n",
      "        [ 0.3320,  0.1155,  0.2304,  0.1548, -0.1163],\n",
      "        [ 1.3415,  0.1095,  0.0419,  0.3095,  1.1955],\n",
      "        [-1.2281, -0.5112,  0.9256,  1.5552,  1.0914]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1257, -0.3417, -0.1586,  0.2260,  0.3456],\n",
      "        [ 0.7518,  0.1221,  0.5911,  0.4391,  0.3785],\n",
      "        [-1.2334, -1.4220, -1.0238, -1.6400, -1.9584],\n",
      "        [ 0.0981, -0.6303, -0.5773, -1.0040, -1.9776]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8977,  1.5804, -0.1814, -0.0651,  0.6562],\n",
      "        [ 0.3320,  0.1155,  0.2304,  0.1548, -0.1163],\n",
      "        [ 1.3415,  0.1095,  0.0419,  0.3095,  1.1955],\n",
      "        [-1.2281, -0.5112,  0.9256,  1.5552,  1.0914]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4121],\n",
      "        [ 0.4238],\n",
      "        [-4.7021],\n",
      "        [-4.0524]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0421,  0.3214,  0.6582,  0.4967,  2.8481],\n",
      "        [-1.4886, -0.2087, -0.9497,  0.6009,  0.9322],\n",
      "        [-0.6436,  0.7673,  0.0677, -2.1797,  1.5482],\n",
      "        [-1.3119,  1.0822, -1.6263,  1.0430, -0.3236]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0599, -0.0332, -0.5966, -0.2367,  0.0112],\n",
      "        [ 0.6399, -0.9090, -1.2264, -1.0619, -0.9339],\n",
      "        [ 1.6976,  1.4857,  1.2932,  1.0298,  0.8500],\n",
      "        [ 0.8317,  1.9768,  1.5927,  0.7077,  1.4457]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0421,  0.3214,  0.6582,  0.4967,  2.8481],\n",
      "        [-1.4886, -0.2087, -0.9497,  0.6009,  0.9322],\n",
      "        [-0.6436,  0.7673,  0.0677, -2.1797,  1.5482],\n",
      "        [-1.3119,  1.0822, -1.6263,  1.0430, -0.3236]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4266],\n",
      "        [-1.1069],\n",
      "        [-0.7937],\n",
      "        [-1.2716]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9911,  0.8086, -0.5505,  0.2838, -0.9561],\n",
      "        [ 0.0345,  0.0046,  1.7736, -0.2065, -0.1380],\n",
      "        [-1.8208,  0.9461, -0.6273,  1.1504,  0.5924],\n",
      "        [-1.1204,  1.7227, -0.6564, -0.7871,  0.6030]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3077, -0.2075, -0.0726,  0.3779,  0.0570],\n",
      "        [ 0.7548, -0.1389, -0.3715, -0.3043,  0.3521],\n",
      "        [ 0.8301,  0.8885,  1.3120,  0.9472,  1.4909],\n",
      "        [ 1.3192,  1.5814,  1.5801,  1.5126,  2.0917]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9911,  0.8086, -0.5505,  0.2838, -0.9561],\n",
      "        [ 0.0345,  0.0046,  1.7736, -0.2065, -0.1380],\n",
      "        [-1.8208,  0.9461, -0.6273,  1.1504,  0.5924],\n",
      "        [-1.1204,  1.7227, -0.6564, -0.7871,  0.6030]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3800],\n",
      "        [-0.6194],\n",
      "        [ 0.4790],\n",
      "        [ 0.2796]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.5026, -0.0143,  0.4080,  1.1880,  0.1721],\n",
      "        [ 0.0589,  1.1977, -0.3858,  0.5428,  0.6880],\n",
      "        [-1.3889,  1.1145, -0.5066, -0.4405, -0.1539],\n",
      "        [-0.2410,  0.8157, -0.9490, -0.2974,  1.6200]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1764,  0.1202, -0.1808, -0.2389, -0.6781],\n",
      "        [ 1.0420, -0.0766,  0.3639, -0.2944,  0.1421],\n",
      "        [ 0.6156,  0.2492,  1.2476,  1.3971,  1.1452],\n",
      "        [ 1.4683,  1.7691,  1.4613,  2.3153,  1.7499]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.5026, -0.0143,  0.4080,  1.1880,  0.1721],\n",
      "        [ 0.0589,  1.1977, -0.3858,  0.5428,  0.6880],\n",
      "        [-1.3889,  1.1145, -0.5066, -0.4405, -0.1539],\n",
      "        [-0.2410,  0.8157, -0.9490, -0.2974,  1.6200]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0345],\n",
      "        [-0.2328],\n",
      "        [-2.0008],\n",
      "        [ 1.8488]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2710,  0.1814,  0.5744,  0.5207,  1.0683],\n",
      "        [ 0.3687, -0.5358, -0.8979,  0.1189, -0.6148],\n",
      "        [ 0.2950, -0.3190,  0.3089, -1.2385, -1.9349],\n",
      "        [-0.2717, -0.0406,  1.2971, -0.9108,  1.3174]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0297,  0.5853, -0.9750, -0.7489, -0.4917],\n",
      "        [ 0.7795, -0.2382, -0.4739, -0.1162, -0.0195],\n",
      "        [ 1.3619,  2.2107,  2.1133,  1.5627,  1.9810],\n",
      "        [ 0.5732,  0.3224, -0.0676,  0.0771,  0.3688]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2710,  0.1814,  0.5744,  0.5207,  1.0683],\n",
      "        [ 0.3687, -0.5358, -0.8979,  0.1189, -0.6148],\n",
      "        [ 0.2950, -0.3190,  0.3089, -1.2385, -1.9349],\n",
      "        [-0.2717, -0.0406,  1.2971, -0.9108,  1.3174]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3771],\n",
      "        [ 0.8388],\n",
      "        [-5.4191],\n",
      "        [ 0.1592]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8123, -0.6462,  1.6516,  0.3370,  0.7208],\n",
      "        [-0.4003,  0.1339, -0.2277,  1.1648,  1.5878],\n",
      "        [-0.9577,  0.9004, -0.3218, -0.0422,  0.6221],\n",
      "        [ 1.1497, -1.2016, -0.3070, -1.0590, -0.1810]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7919, -0.1418,  0.9385,  0.8062,  0.3280],\n",
      "        [ 0.5089, -0.6710, -0.1000, -0.8602, -0.4352],\n",
      "        [-0.1325,  0.5030, -0.9811,  0.6540, -0.1237],\n",
      "        [ 0.7592, -0.4424, -0.3115, -0.1058,  0.0938]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8123, -0.6462,  1.6516,  0.3370,  0.7208],\n",
      "        [-0.4003,  0.1339, -0.2277,  1.1648,  1.5878],\n",
      "        [-0.9577,  0.9004, -0.3218, -0.0422,  0.6221],\n",
      "        [ 1.1497, -1.2016, -0.3070, -1.0590, -0.1810]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.5851],\n",
      "        [-1.9636],\n",
      "        [ 0.7909],\n",
      "        [ 1.5952]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5837,  0.1200,  0.0112,  0.6511, -1.4887],\n",
      "        [ 0.1350, -0.3029, -1.1460,  0.1215,  1.0570],\n",
      "        [ 0.8005, -0.2054,  0.0823,  0.3786, -1.1242],\n",
      "        [ 1.7087, -0.7201,  1.1313,  1.9375, -0.7100]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8152, -1.2722, -0.7385, -1.3392, -2.1639],\n",
      "        [ 1.3105,  0.8544,  0.4419,  0.3518,  0.9708],\n",
      "        [ 0.2442,  0.4076,  0.5076, -0.2871, -0.7152],\n",
      "        [-0.2732, -1.4612, -0.2259, -1.0641, -0.5956]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5837,  0.1200,  0.0112,  0.6511, -1.4887],\n",
      "        [ 0.1350, -0.3029, -1.1460,  0.1215,  1.0570],\n",
      "        [ 0.8005, -0.2054,  0.0823,  0.3786, -1.1242],\n",
      "        [ 1.7087, -0.7201,  1.1313,  1.9375, -0.7100]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.4797],\n",
      "        [ 0.4806],\n",
      "        [ 0.8488],\n",
      "        [-1.3090]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8128, -0.6059,  0.7659, -0.8654, -0.9121],\n",
      "        [-0.3121,  0.9626, -1.7639,  0.0490, -0.4949],\n",
      "        [ 0.8926, -1.5153, -0.0858,  0.4194,  0.0696],\n",
      "        [-0.3162,  1.0750,  1.7908, -0.9216, -1.1575]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0552, -2.2474, -2.3314, -2.9484, -4.1132],\n",
      "        [ 1.0478, -0.0577,  0.3001, -0.0796,  0.7708],\n",
      "        [-0.0783, -0.1693, -0.5657, -0.2970, -0.3127],\n",
      "        [-0.1078, -0.5388, -0.0499,  0.2542, -0.4375]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8128, -0.6059,  0.7659, -0.8654, -0.9121],\n",
      "        [-0.3121,  0.9626, -1.7639,  0.0490, -0.4949],\n",
      "        [ 0.8926, -1.5153, -0.0858,  0.4194,  0.0696],\n",
      "        [-0.3162,  1.0750,  1.7908, -0.9216, -1.1575]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.0217],\n",
      "        [-1.2973],\n",
      "        [ 0.0889],\n",
      "        [-0.3624]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2532,  0.4251, -0.6632,  0.8375, -0.0045],\n",
      "        [-0.3777,  0.4751, -0.6721,  2.0296, -0.0541],\n",
      "        [-0.7137,  0.8837,  0.6254,  2.2245, -0.9195],\n",
      "        [-0.4187,  0.9098,  1.2104,  0.0588,  1.9479]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4850,  0.2791, -0.0889, -0.0359,  0.0323],\n",
      "        [ 0.9616,  0.4467,  1.4698,  0.9782,  1.8126],\n",
      "        [-0.0986,  0.1382,  0.4019, -0.2329, -0.5555],\n",
      "        [ 0.3997, -0.2643, -0.4974, -0.9120, -0.5970]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2532,  0.4251, -0.6632,  0.8375, -0.0045],\n",
      "        [-0.3777,  0.4751, -0.6721,  2.0296, -0.0541],\n",
      "        [-0.7137,  0.8837,  0.6254,  2.2245, -0.9195],\n",
      "        [-0.4187,  0.9098,  1.2104,  0.0588,  1.9479]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0246],\n",
      "        [ 0.7484],\n",
      "        [ 0.4365],\n",
      "        [-2.2265]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-3.7405,  0.1557,  1.1258,  0.8994,  1.0495],\n",
      "        [-1.1703,  0.6723, -0.8633, -0.6827,  0.2834],\n",
      "        [-0.5345,  1.3315, -0.6264,  0.3017,  1.1441],\n",
      "        [ 1.3956, -0.6533,  0.9428,  0.1394, -0.2872]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1019,  0.7553, -0.2470,  0.2986,  0.4580],\n",
      "        [ 0.4725, -0.1006,  0.0524,  0.3056,  0.3599],\n",
      "        [-0.4102,  0.0407,  0.2879,  0.4272, -0.0526],\n",
      "        [ 1.0014,  0.8224,  0.7812,  0.9347,  1.3622]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-3.7405,  0.1557,  1.1258,  0.8994,  1.0495],\n",
      "        [-1.1703,  0.6723, -0.8633, -0.6827,  0.2834],\n",
      "        [-0.5345,  1.3315, -0.6264,  0.3017,  1.1441],\n",
      "        [ 1.3956, -0.6533,  0.9428,  0.1394, -0.2872]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2077],\n",
      "        [-0.7724],\n",
      "        [ 0.1618],\n",
      "        [ 1.3358]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1509,  0.6776,  0.2844,  0.1354, -0.0343],\n",
      "        [-1.0358,  0.8538, -0.8665,  0.3841,  1.3696],\n",
      "        [-1.6907,  1.9765,  0.4562, -1.6630,  0.2703],\n",
      "        [-1.0906, -0.5806,  1.9612, -0.1437,  1.5114]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1102, -0.5033, -0.2123, -0.0299, -0.3909],\n",
      "        [ 0.6106,  0.4172,  0.3448,  0.6560,  0.3937],\n",
      "        [ 0.2751,  0.7924,  0.2843,  1.1952, -0.2139],\n",
      "        [ 0.7591,  0.2261, -0.1889,  0.3585,  0.1764]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1509,  0.6776,  0.2844,  0.1354, -0.0343],\n",
      "        [-1.0358,  0.8538, -0.8665,  0.3841,  1.3696],\n",
      "        [-1.6907,  1.9765,  0.4562, -1.6630,  0.2703],\n",
      "        [-1.0906, -0.5806,  1.9612, -0.1437,  1.5114]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3754],\n",
      "        [ 0.2161],\n",
      "        [-0.8148],\n",
      "        [-1.1145]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1255, -1.5140, -1.5987, -1.3679, -1.7774],\n",
      "        [ 0.7680, -0.9137, -1.1956,  0.8965,  1.1496],\n",
      "        [-0.8542,  2.2195, -1.0490,  0.0268,  0.8178],\n",
      "        [-1.1217, -0.5433, -0.0083,  0.3682, -1.2526]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0120, -1.0239, -0.4535,  0.1968, -0.3211],\n",
      "        [ 0.3738, -0.6106, -0.0560, -0.4775,  0.4862],\n",
      "        [ 0.0111,  0.2389,  0.7941, -0.2028,  1.2735],\n",
      "        [ 0.0573,  1.1502,  0.9602,  0.4184,  0.4548]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1255, -1.5140, -1.5987, -1.3679, -1.7774],\n",
      "        [ 0.7680, -0.9137, -1.1956,  0.8965,  1.1496],\n",
      "        [-0.8542,  2.2195, -1.0490,  0.0268,  0.8178],\n",
      "        [-1.1217, -0.5433, -0.0083,  0.3682, -1.2526]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5904],\n",
      "        [ 1.0427],\n",
      "        [ 0.7237],\n",
      "        [-1.1128]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4648, -0.5641,  0.9051, -0.6191,  0.9335],\n",
      "        [-1.5903, -0.5988,  0.7531, -0.0500, -1.4989],\n",
      "        [ 1.5221,  0.4575, -0.7472, -0.1657, -1.3890],\n",
      "        [ 0.6801,  0.5653,  0.2187,  0.5874,  0.5350]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6266, -1.1431, -1.2131, -1.2158, -2.5078],\n",
      "        [-0.0435, -0.5926, -0.0253, -0.1646, -0.4665],\n",
      "        [-0.0890,  0.7396,  0.5917,  1.2328,  0.3616],\n",
      "        [ 0.7927,  1.0562,  0.8735,  1.0012,  0.6644]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4648, -0.5641,  0.9051, -0.6191,  0.9335],\n",
      "        [-1.5903, -0.5988,  0.7531, -0.0500, -1.4989],\n",
      "        [ 1.5221,  0.4575, -0.7472, -0.1657, -1.3890],\n",
      "        [ 0.6801,  0.5653,  0.2187,  0.5874,  0.5350]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7504],\n",
      "        [ 1.1125],\n",
      "        [-0.9457],\n",
      "        [ 2.2708]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0870, -0.5651, -0.1046, -1.3226, -0.2859],\n",
      "        [-1.3347,  0.1181, -1.9043, -1.7694,  2.2371],\n",
      "        [-0.1709,  0.5117,  0.4608, -0.1449, -1.0094],\n",
      "        [ 0.3373,  0.2607, -0.1500, -0.1128, -0.1721]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4903, -0.3901, -0.7677, -0.2162, -1.2192],\n",
      "        [-0.3700, -0.3950, -0.6627, -0.1296, -1.5937],\n",
      "        [ 0.4143,  0.4970,  1.1511,  0.5702,  1.2750],\n",
      "        [-0.3058, -0.8528, -0.1971, -0.1643, -1.0172]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0870, -0.5651, -0.1046, -1.3226, -0.2859],\n",
      "        [-1.3347,  0.1181, -1.9043, -1.7694,  2.2371],\n",
      "        [-0.1709,  0.5117,  0.4608, -0.1449, -1.0094],\n",
      "        [ 0.3373,  0.2607, -0.1500, -0.1128, -0.1721]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4682],\n",
      "        [-1.6269],\n",
      "        [-0.6557],\n",
      "        [-0.1022]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1253,  0.5490, -1.8691, -0.5228,  1.5640],\n",
      "        [ 0.3021,  2.7725, -1.0688,  0.0400, -1.5635],\n",
      "        [ 0.4148, -0.5657,  0.2985, -0.7204,  0.1919],\n",
      "        [-0.5287, -0.7275,  1.5902,  0.3179,  0.8510]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0999, -0.8921, -1.0920, -0.9128, -1.1937],\n",
      "        [ 0.3267, -0.1750, -0.5609, -0.5063,  0.5607],\n",
      "        [ 0.6835,  1.0051,  0.7515,  0.0577,  1.0908],\n",
      "        [ 0.4311, -0.5045,  0.6611,  0.4621,  0.0239]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1253,  0.5490, -1.8691, -0.5228,  1.5640],\n",
      "        [ 0.3021,  2.7725, -1.0688,  0.0400, -1.5635],\n",
      "        [ 0.4148, -0.5657,  0.2985, -0.7204,  0.1919],\n",
      "        [-0.5287, -0.7275,  1.5902,  0.3179,  0.8510]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0237],\n",
      "        [-0.6838],\n",
      "        [ 0.1070],\n",
      "        [ 1.3577]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1892, -0.6850, -0.3043,  0.1812,  0.2313],\n",
      "        [ 0.1597,  0.4925,  0.4675, -1.0042, -0.2915],\n",
      "        [ 0.7797, -0.6891, -0.4788,  0.2993, -0.1938],\n",
      "        [-0.2832,  0.4644, -1.4024,  2.4363, -0.8997]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6769, -1.7248, -0.9114, -0.6413, -0.9420],\n",
      "        [ 0.5836,  0.1521, -0.1136, -0.1911, -0.0631],\n",
      "        [ 0.4025, -0.3071, -0.0088,  0.6298,  0.8564],\n",
      "        [-0.2922,  0.2496, -0.3131, -0.0167, -0.9313]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1892, -0.6850, -0.3043,  0.1812,  0.2313],\n",
      "        [ 0.1597,  0.4925,  0.4675, -1.0042, -0.2915],\n",
      "        [ 0.7797, -0.6891, -0.4788,  0.2993, -0.1938],\n",
      "        [-0.2832,  0.4644, -1.4024,  2.4363, -0.8997]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.9296],\n",
      "        [ 0.3254],\n",
      "        [ 0.5521],\n",
      "        [ 1.4349]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5215, -0.2773, -0.0825, -0.3085,  1.4673],\n",
      "        [-0.6084, -1.5740, -0.0957,  0.3405,  0.1026],\n",
      "        [ 0.5941,  0.5222,  1.6825, -0.9927, -0.6841],\n",
      "        [ 0.2041,  0.7953,  0.1658, -1.3487, -1.1307]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2613, -1.1860, -1.2030, -1.6784, -2.7221],\n",
      "        [-0.0776, -0.4682, -1.5715, -0.4862, -0.4940],\n",
      "        [ 0.1165,  0.2015,  0.2364,  0.4133,  0.6084],\n",
      "        [-0.4434, -0.9539, -0.3941, -0.8453, -1.2339]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5215, -0.2773, -0.0825, -0.3085,  1.4673],\n",
      "        [-0.6084, -1.5740, -0.0957,  0.3405,  0.1026],\n",
      "        [ 0.5941,  0.5222,  1.6825, -0.9927, -0.6841],\n",
      "        [ 0.2041,  0.7953,  0.1658, -1.3487, -1.1307]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.3902],\n",
      "        [ 0.7184],\n",
      "        [-0.2542],\n",
      "        [ 1.6209]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6240, -0.2342,  0.0975, -1.2448,  0.3404],\n",
      "        [-1.8022, -0.7272,  1.0012, -0.5954, -0.1709],\n",
      "        [ 2.2079,  1.3179, -0.5827, -0.1363, -0.3627],\n",
      "        [ 0.2319, -0.8180,  0.0900, -1.7259,  0.1678]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2529, -0.9680, -1.4757, -0.8853, -1.3505],\n",
      "        [-0.4752, -0.3885, -0.0065, -0.1459, -0.6315],\n",
      "        [ 0.4227,  0.3359,  0.3040,  0.0567, -0.3350],\n",
      "        [-1.3495, -1.6678, -0.7509, -1.7765, -3.1351]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6240, -0.2342,  0.0975, -1.2448,  0.3404],\n",
      "        [-1.8022, -0.7272,  1.0012, -0.5954, -0.1709],\n",
      "        [ 2.2079,  1.3179, -0.5827, -0.1363, -0.3627],\n",
      "        [ 0.2319, -0.8180,  0.0900, -1.7259,  0.1678]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8830],\n",
      "        [ 1.3273],\n",
      "        [ 1.3126],\n",
      "        [ 3.5237]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4608, -0.3838,  0.3552,  0.9783, -1.1712],\n",
      "        [-0.1422,  0.0408,  0.6791, -1.2589, -0.6422],\n",
      "        [ 0.6399, -0.6754,  0.8787,  0.2524, -0.1710],\n",
      "        [ 0.7197, -0.3264, -0.5712,  0.7479, -1.4107]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0270,  0.1235,  0.0596,  0.1372, -0.6140],\n",
      "        [-0.3598, -0.2210, -0.6391, -0.6609, -0.4228],\n",
      "        [-0.5848, -0.0597,  0.3739, -0.7947, -0.6972],\n",
      "        [-1.5912, -1.8093, -2.1198, -2.0910, -4.0209]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4608, -0.3838,  0.3552,  0.9783, -1.1712],\n",
      "        [-0.1422,  0.0408,  0.6791, -1.2589, -0.6422],\n",
      "        [ 0.6399, -0.6754,  0.8787,  0.2524, -0.1710],\n",
      "        [ 0.7197, -0.3264, -0.5712,  0.7479, -1.4107]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8395],\n",
      "        [ 0.7116],\n",
      "        [-0.0867],\n",
      "        [ 4.7646]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5197,  0.6589, -0.3975,  0.3571,  0.3330],\n",
      "        [-0.2852,  0.9162,  0.2183,  0.1125,  1.7973],\n",
      "        [-0.5589,  0.8474,  1.2432,  1.3306,  0.0476],\n",
      "        [ 0.1361,  1.2496, -0.3308,  0.8660,  0.8907]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3726, -0.6200, -0.0128,  0.2562, -0.4694],\n",
      "        [-0.0986, -0.7622, -0.9857, -0.9509, -1.5226],\n",
      "        [ 0.4984,  0.1676,  0.2555,  0.5486, -0.0263],\n",
      "        [ 0.2424, -0.2253, -0.3017, -0.6649,  0.1508]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5197,  0.6589, -0.3975,  0.3571,  0.3330],\n",
      "        [-0.2852,  0.9162,  0.2183,  0.1125,  1.7973],\n",
      "        [-0.5589,  0.8474,  1.2432,  1.3306,  0.0476],\n",
      "        [ 0.1361,  1.2496, -0.3308,  0.8660,  0.8907]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2746],\n",
      "        [-3.7289],\n",
      "        [ 0.9099],\n",
      "        [-0.5903]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8178,  1.2908,  1.1290,  0.1667,  2.5234],\n",
      "        [ 0.4689, -1.0343, -0.7287,  1.1698,  0.4750],\n",
      "        [-0.0673, -0.8385,  0.3221,  1.9558,  1.1280],\n",
      "        [ 0.3420,  1.3130, -0.0122, -0.9382, -0.0790]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0616, -0.5926,  0.2347,  0.0276, -0.0820],\n",
      "        [ 1.2537,  1.2897,  1.0666,  0.5455,  1.6941],\n",
      "        [-0.2428, -0.0178,  0.7275, -0.2972, -0.5330],\n",
      "        [ 0.3668, -0.2117, -0.3861,  0.1477,  0.1239]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8178,  1.2908,  1.1290,  0.1667,  2.5234],\n",
      "        [ 0.4689, -1.0343, -0.7287,  1.1698,  0.4750],\n",
      "        [-0.0673, -0.8385,  0.3221,  1.9558,  1.1280],\n",
      "        [ 0.3420,  1.3130, -0.0122, -0.9382, -0.0790]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6520],\n",
      "        [-0.0806],\n",
      "        [-0.9170],\n",
      "        [-0.2962]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.8522,  0.6831, -0.3865,  0.7617,  0.0781],\n",
      "        [-0.3438,  0.0815, -2.0286, -0.7724, -0.9755],\n",
      "        [ 0.7628, -0.8269,  0.0275, -1.1746, -0.8494],\n",
      "        [ 0.1914, -0.4180,  0.2574,  1.4866,  0.6815]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1481,  0.2384, -1.1375, -0.1870,  0.1113],\n",
      "        [ 1.1882,  0.9405,  0.6731,  1.3079,  1.5401],\n",
      "        [ 0.4641,  0.1751, -0.1689,  0.4882,  0.1764],\n",
      "        [-0.0108, -0.1315,  0.2269,  0.0102, -0.0911]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.8522,  0.6831, -0.3865,  0.7617,  0.0781],\n",
      "        [-0.3438,  0.0815, -2.0286, -0.7724, -0.9755],\n",
      "        [ 0.7628, -0.8269,  0.0275, -1.1746, -0.8494],\n",
      "        [ 0.1914, -0.4180,  0.2574,  1.4866,  0.6815]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8911],\n",
      "        [-4.2100],\n",
      "        [-0.5187],\n",
      "        [ 0.0643]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2540, -1.0270, -0.8515,  1.7630,  0.1378],\n",
      "        [-1.4161,  2.0976, -1.1732,  0.0357,  0.4959],\n",
      "        [ 0.7981,  0.2085, -0.0804,  0.7047,  1.1861],\n",
      "        [-1.3502, -1.1764,  0.6511,  0.1314,  1.5359]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4203,  0.1487, -0.4505, -0.1509, -0.9119],\n",
      "        [ 1.7988,  2.3709,  2.2200,  1.4395,  2.9728],\n",
      "        [-0.1047,  0.6465,  0.7728,  0.1412,  0.0357],\n",
      "        [ 0.2031, -0.3276, -0.5510, -0.2053,  0.1874]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2540, -1.0270, -0.8515,  1.7630,  0.1378],\n",
      "        [-1.4161,  2.0976, -1.1732,  0.0357,  0.4959],\n",
      "        [ 0.7981,  0.2085, -0.0804,  0.7047,  1.1861],\n",
      "        [-1.3502, -1.1764,  0.6511,  0.1314,  1.5359]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3663],\n",
      "        [ 1.3475],\n",
      "        [ 0.1311],\n",
      "        [ 0.0133]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4103, -0.2953,  0.4745, -0.2825, -0.4100],\n",
      "        [-0.2260, -0.0106, -1.1431, -0.2257, -0.0099],\n",
      "        [-0.4810, -0.6537,  0.5878, -1.8507, -1.5377],\n",
      "        [-0.4026, -1.2237,  0.0697,  0.0283,  1.7840]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2484, -0.3169, -0.0944, -0.2270, -0.1646],\n",
      "        [ 0.0866, -0.0325, -0.1039,  0.5361,  0.2620],\n",
      "        [ 0.7913,  0.2648,  0.5347,  0.6029,  0.5361],\n",
      "        [ 0.0645, -0.1816,  0.1684, -0.3633, -0.7391]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4103, -0.2953,  0.4745, -0.2825, -0.4100],\n",
      "        [-0.2260, -0.0106, -1.1431, -0.2257, -0.0099],\n",
      "        [-0.4810, -0.6537,  0.5878, -1.8507, -1.5377],\n",
      "        [-0.4026, -1.2237,  0.0697,  0.0283,  1.7840]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0785],\n",
      "        [-0.0241],\n",
      "        [-2.1796],\n",
      "        [-1.1209]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4190,  0.6300,  0.1771,  0.1714,  1.1744],\n",
      "        [-0.6552,  0.9406, -0.0600, -0.4865,  0.7295],\n",
      "        [ 1.3944, -0.0135, -0.4334,  0.3429,  1.2932],\n",
      "        [-0.5345, -0.3983, -1.2276,  1.1261,  0.1842]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1818,  0.9318,  0.3915, -0.4027,  0.2493],\n",
      "        [-0.1180, -0.0090,  1.1030,  0.5210, -0.0161],\n",
      "        [ 0.7831,  1.4633,  0.6311,  0.6180,  1.0268],\n",
      "        [ 0.6080,  0.6883,  0.3501,  0.6212,  1.1033]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4190,  0.6300,  0.1771,  0.1714,  1.1744],\n",
      "        [-0.6552,  0.9406, -0.0600, -0.4865,  0.7295],\n",
      "        [ 1.3944, -0.0135, -0.4334,  0.3429,  1.2932],\n",
      "        [-0.5345, -0.3983, -1.2276,  1.1261,  0.1842]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1383],\n",
      "        [-0.2625],\n",
      "        [ 2.3385],\n",
      "        [-0.1262]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1390,  0.9861, -2.3500,  0.4507, -0.4666],\n",
      "        [-0.9993,  1.2712, -1.4505,  0.7507,  1.3057],\n",
      "        [-1.3268, -0.3955,  0.3898, -1.2341,  1.2664],\n",
      "        [-0.7482,  0.0449,  2.3644,  0.3685,  0.2880]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0167, -0.2134, -0.3922, -0.2834, -0.1933],\n",
      "        [ 0.5284, -0.1001, -0.3368,  0.0789, -0.5423],\n",
      "        [ 0.2133,  0.0468,  0.7610, -0.2891, -0.8242],\n",
      "        [-0.0081,  0.2365, -0.5210,  0.2213,  0.2145]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1390,  0.9861, -2.3500,  0.4507, -0.4666],\n",
      "        [-0.9993,  1.2712, -1.4505,  0.7507,  1.3057],\n",
      "        [-1.3268, -0.3955,  0.3898, -1.2341,  1.2664],\n",
      "        [-0.7482,  0.0449,  2.3644,  0.3685,  0.2880]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6714],\n",
      "        [-0.8156],\n",
      "        [-0.6919],\n",
      "        [-1.0720]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4574, -0.5064,  1.9699, -0.3647, -0.2181],\n",
      "        [ 0.3625, -0.8080,  0.9425,  1.0262, -0.1306],\n",
      "        [-1.9469, -1.7740, -0.8473,  1.0499,  0.4104],\n",
      "        [-0.5762,  0.1068,  0.0112,  0.9392, -0.0752]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4617, -0.8828,  0.0009, -0.9409, -0.9689],\n",
      "        [ 0.6101,  0.8564, -0.0456, -0.2567,  0.2895],\n",
      "        [-0.0902,  0.4396,  0.7765,  0.2507,  1.3251],\n",
      "        [ 0.5267,  0.7960,  0.1497,  0.1673,  0.4016]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4574, -0.5064,  1.9699, -0.3647, -0.2181],\n",
      "        [ 0.3625, -0.8080,  0.9425,  1.0262, -0.1306],\n",
      "        [-1.9469, -1.7740, -0.8473,  1.0499,  0.4104],\n",
      "        [-0.5762,  0.1068,  0.0112,  0.9392, -0.0752]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2146],\n",
      "        [-0.8151],\n",
      "        [-0.4553],\n",
      "        [-0.0899]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0761,  2.1333, -1.0289, -1.3863, -1.2077],\n",
      "        [-1.0547, -0.7483, -1.0211,  0.1624,  1.0115],\n",
      "        [-2.5841, -0.2328,  0.6989,  3.2919, -0.8431],\n",
      "        [-0.3064, -0.3949, -0.0889, -2.3933, -0.9357]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4424, -1.1411, -0.8476, -0.6683, -1.3746],\n",
      "        [ 0.3510,  0.6918, -0.2468, -0.2293, -0.1330],\n",
      "        [ 0.2721,  0.2655, -0.0520,  0.4111,  0.2143],\n",
      "        [-0.1326, -0.5614, -0.1221, -0.1571,  0.0594]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0761,  2.1333, -1.0289, -1.3863, -1.2077],\n",
      "        [-1.0547, -0.7483, -1.0211,  0.1624,  1.0115],\n",
      "        [-2.5841, -0.2328,  0.6989,  3.2919, -0.8431],\n",
      "        [-0.3064, -0.3949, -0.0889, -2.3933, -0.9357]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9906],\n",
      "        [-0.8076],\n",
      "        [ 0.3713],\n",
      "        [ 0.5936]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7384,  0.7283, -2.2286, -1.6463,  0.4064],\n",
      "        [-0.3631,  1.2282,  0.5040,  0.4204,  1.0202],\n",
      "        [ 0.0417,  2.2955, -0.1081, -2.2061, -1.3342],\n",
      "        [-0.2008,  1.1288,  0.9283, -0.6741,  0.2866]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8561, -1.0403, -1.4484, -1.5601, -1.8494],\n",
      "        [ 0.3950,  0.3920,  0.7209,  0.2282,  0.4319],\n",
      "        [ 0.3698,  0.1251,  0.6620,  0.4792,  0.0318],\n",
      "        [ 0.3046, -0.1964, -0.6342, -0.1934, -0.5262]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7384,  0.7283, -2.2286, -1.6463,  0.4064],\n",
      "        [-0.3631,  1.2282,  0.5040,  0.4204,  1.0202],\n",
      "        [ 0.0417,  2.2955, -0.1081, -2.2061, -1.3342],\n",
      "        [-0.2008,  1.1288,  0.9283, -0.6741,  0.2866]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.9192],\n",
      "        [ 1.2380],\n",
      "        [-0.8686],\n",
      "        [-0.8920]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0632, -0.5917,  0.0207, -0.6862, -0.7596],\n",
      "        [ 1.6030,  0.9231,  1.9150,  0.7646, -0.7196],\n",
      "        [-0.9390,  1.7258, -0.3266,  0.6180,  0.2894],\n",
      "        [-2.6724, -2.8228,  0.0766,  2.3914,  0.8152]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3002, -0.3367,  0.0066, -0.6531,  1.0152],\n",
      "        [-0.2658, -0.8536, -0.1722, -0.2671, -0.5135],\n",
      "        [ 0.4007,  1.0714,  0.8131,  0.9701,  0.3268],\n",
      "        [ 0.5428, -0.4280,  0.0430, -0.0725, -0.0314]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0632, -0.5917,  0.0207, -0.6862, -0.7596],\n",
      "        [ 1.6030,  0.9231,  1.9150,  0.7646, -0.7196],\n",
      "        [-0.9390,  1.7258, -0.3266,  0.6180,  0.2894],\n",
      "        [-2.6724, -2.8228,  0.0766,  2.3914,  0.8152]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1046],\n",
      "        [-1.3785],\n",
      "        [ 1.9013],\n",
      "        [-0.4379]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1720,  0.3096,  0.4049,  0.5979, -0.2223],\n",
      "        [ 1.0062, -0.2928,  0.6238,  0.3535, -0.4370],\n",
      "        [-0.3637, -0.0377, -0.2506,  1.4989,  0.6465],\n",
      "        [ 1.0287, -0.9642, -1.0869, -0.0065,  0.0002]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6402, -0.3671, -0.3941, -0.1647, -0.4436],\n",
      "        [ 1.1640,  0.2868,  1.4416,  1.1007,  1.3081],\n",
      "        [ 0.0389, -0.2020, -0.0950,  0.2027, -0.3651],\n",
      "        [ 0.4671, -0.4199, -0.5226, -0.3029,  0.3217]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1720,  0.3096,  0.4049,  0.5979, -0.2223],\n",
      "        [ 1.0062, -0.2928,  0.6238,  0.3535, -0.4370],\n",
      "        [-0.3637, -0.0377, -0.2506,  1.4989,  0.6465],\n",
      "        [ 1.0287, -0.9642, -1.0869, -0.0065,  0.0002]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3832],\n",
      "        [ 1.8040],\n",
      "        [ 0.0851],\n",
      "        [ 1.4554]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[ 1.0008, -1.0932,  0.3383,  0.4745,  0.3809],\n",
      "        [-0.5825,  3.5557,  0.5688,  1.6306, -0.8776],\n",
      "        [-0.0194, -1.1589,  0.3123, -0.8926, -1.4696],\n",
      "        [ 0.4256,  0.9270,  1.6858, -0.4045,  1.2247]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1341,  0.4038, -0.0182, -0.4763,  0.0080],\n",
      "        [ 0.0157, -0.8617, -0.5527, -0.3773, -1.6407],\n",
      "        [ 0.0140,  0.0430,  0.1478,  0.2790,  0.4760],\n",
      "        [-0.7023, -0.4274, -0.5116, -0.9079, -1.6297]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0008, -1.0932,  0.3383,  0.4745,  0.3809],\n",
      "        [-0.5825,  3.5557,  0.5688,  1.6306, -0.8776],\n",
      "        [-0.0194, -1.1589,  0.3123, -0.8926, -1.4696],\n",
      "        [ 0.4256,  0.9270,  1.6858, -0.4045,  1.2247]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8047],\n",
      "        [-2.5626],\n",
      "        [-0.9526],\n",
      "        [-3.1861]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0371,  0.2161, -0.2020,  1.1373, -0.5267],\n",
      "        [ 0.1521,  0.2146,  0.6275, -0.0506,  0.1857],\n",
      "        [-0.0415,  0.0273,  1.0050,  0.9670,  0.6226],\n",
      "        [-1.8052,  0.7396, -1.0964, -0.8296,  1.8270]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7599, -0.2297, -0.1630,  0.1401,  0.6983],\n",
      "        [ 0.8152,  0.8812,  0.4105,  0.2176,  0.8247],\n",
      "        [-0.1718,  0.4904,  0.8909,  0.6479,  1.0997],\n",
      "        [ 0.8303,  0.6966,  0.8819,  0.8185,  1.4066]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0371,  0.2161, -0.2020,  1.1373, -0.5267],\n",
      "        [ 0.1521,  0.2146,  0.6275, -0.0506,  0.1857],\n",
      "        [-0.0415,  0.0273,  1.0050,  0.9670,  0.6226],\n",
      "        [-1.8052,  0.7396, -1.0964, -0.8296,  1.8270]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2533],\n",
      "        [ 0.7129],\n",
      "        [ 2.2270],\n",
      "        [-0.0599]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2150, -0.3761, -1.7636, -1.7125,  1.3797],\n",
      "        [-0.1992,  0.7976,  0.1545, -0.4930,  2.5824],\n",
      "        [ 1.0934,  0.2197,  0.5552, -1.5523,  2.8070],\n",
      "        [-1.3813,  0.2361, -1.5478,  0.4462, -1.1478]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4878, -0.3160, -0.0789, -0.8045,  0.1805],\n",
      "        [ 0.6194, -0.2735,  0.2321,  0.2685,  0.7700],\n",
      "        [-0.6527, -0.1683,  0.1856, -0.4215, -1.3021],\n",
      "        [ 0.9659,  0.9426,  1.0876,  0.6947,  1.4727]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2150, -0.3761, -1.7636, -1.7125,  1.3797],\n",
      "        [-0.1992,  0.7976,  0.1545, -0.4930,  2.5824],\n",
      "        [ 1.0934,  0.2197,  0.5552, -1.5523,  2.8070],\n",
      "        [-1.3813,  0.2361, -1.5478,  0.4462, -1.1478]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4774],\n",
      "        [ 1.5504],\n",
      "        [-3.6484],\n",
      "        [-4.1753]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0193,  0.5176, -0.5658, -0.7656,  0.0426],\n",
      "        [-0.5592,  1.1141,  0.1243, -1.7297, -0.5584],\n",
      "        [-1.1456, -0.0436, -0.5790,  2.0210,  1.2628],\n",
      "        [ 0.2517,  0.0742,  0.0537, -0.0815,  0.9462]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6127, -1.1219, -0.4686, -1.3993, -2.1172],\n",
      "        [-0.1320, -0.5023, -0.7643, -1.1298, -1.0303],\n",
      "        [ 1.4111,  0.9853,  1.0645,  0.5846,  2.0563],\n",
      "        [ 1.6018,  1.7526,  1.5459,  1.2676,  3.0184]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0193,  0.5176, -0.5658, -0.7656,  0.0426],\n",
      "        [-0.5592,  1.1141,  0.1243, -1.7297, -0.5584],\n",
      "        [-1.1456, -0.0436, -0.5790,  2.0210,  1.2628],\n",
      "        [ 0.2517,  0.0742,  0.0537, -0.0815,  0.9462]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.9027],\n",
      "        [ 1.9488],\n",
      "        [ 1.5023],\n",
      "        [ 3.3687]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7696, -0.3618,  0.2339, -0.1726, -0.0084],\n",
      "        [-0.4306, -0.3976, -0.4391, -0.6596,  0.3400],\n",
      "        [-1.2034, -0.2910,  0.7345,  0.7380, -0.0211],\n",
      "        [-0.8149,  2.1068,  2.1215,  1.4575, -2.4894]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5860, -1.5563, -1.8270, -1.3157, -2.6600],\n",
      "        [-0.7174, -1.0589, -1.0572, -1.2097, -1.8069],\n",
      "        [ 0.0846,  1.2340,  0.7174,  0.3433,  1.3408],\n",
      "        [-0.1942, -0.3207, -0.5702, -0.1485, -0.0301]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7696, -0.3618,  0.2339, -0.1726, -0.0084],\n",
      "        [-0.4306, -0.3976, -0.4391, -0.6596,  0.3400],\n",
      "        [-1.2034, -0.2910,  0.7345,  0.7380, -0.0211],\n",
      "        [-0.8149,  2.1068,  2.1215,  1.4575, -2.4894]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0657],\n",
      "        [ 1.3777],\n",
      "        [ 0.2911],\n",
      "        [-1.8687]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6856,  1.2213, -0.5778,  0.1324, -0.6113],\n",
      "        [-0.5497, -0.1716, -1.4866, -1.0585,  0.2677],\n",
      "        [ 0.8641, -0.5748,  1.4452, -1.2763,  0.8150],\n",
      "        [-0.0853,  0.0207,  1.2936,  0.3317,  1.2598]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6857, -1.8502, -1.2473, -2.3347, -2.8114],\n",
      "        [-0.5818, -1.0297, -0.5266, -1.9660, -2.3944],\n",
      "        [ 0.2879,  0.2203, -0.1529,  0.5388,  0.4881],\n",
      "        [ 0.9757,  0.3128,  0.9687,  0.6888,  1.6546]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6856,  1.2213, -0.5778,  0.1324, -0.6113],\n",
      "        [-0.5497, -0.1716, -1.4866, -1.0585,  0.2677],\n",
      "        [ 0.8641, -0.5748,  1.4452, -1.2763,  0.8150],\n",
      "        [-0.0853,  0.0207,  1.2936,  0.3317,  1.2598]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5994],\n",
      "        [ 2.7196],\n",
      "        [-0.3886],\n",
      "        [ 3.4893]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5474,  0.4150, -0.7860,  1.1581,  2.6106],\n",
      "        [ 1.0263,  1.1677,  0.7515, -0.9358, -0.5156],\n",
      "        [-1.6841,  1.7041, -1.3186, -1.0111,  0.7370],\n",
      "        [-0.0775,  0.4434,  1.3950, -1.3922, -0.6818]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0100,  0.1134, -0.2250, -0.5268, -0.5027],\n",
      "        [-0.8578, -2.2000, -2.1638, -2.5574, -4.1490],\n",
      "        [ 0.2572,  0.7156,  0.5169,  0.4702,  0.9594],\n",
      "        [-0.5079, -0.8189, -1.4716, -1.0952, -0.7907]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5474,  0.4150, -0.7860,  1.1581,  2.6106],\n",
      "        [ 1.0263,  1.1677,  0.7515, -0.9358, -0.5156],\n",
      "        [-1.6841,  1.7041, -1.3186, -1.0111,  0.7370],\n",
      "        [-0.0775,  0.4434,  1.3950, -1.3922, -0.6818]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6930],\n",
      "        [-0.5429],\n",
      "        [ 0.3363],\n",
      "        [-0.3129]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7185, -0.8869,  0.2656,  1.3936,  1.8702],\n",
      "        [-2.0028, -1.0701,  1.6538,  0.3031, -1.1047],\n",
      "        [-0.0714,  0.6527, -0.0179, -1.1624, -0.1820],\n",
      "        [ 0.8778, -0.9463, -0.6185, -1.0903, -0.4686]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1049,  0.4170,  0.8537,  0.5472,  1.6185],\n",
      "        [ 0.4698,  0.0545, -0.3704, -0.2610, -0.0471],\n",
      "        [ 0.0343,  1.5033,  0.2857,  1.2057,  0.8475],\n",
      "        [ 0.0790, -1.2888,  0.5512, -0.7044, -1.0145]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7185, -0.8869,  0.2656,  1.3936,  1.8702],\n",
      "        [-2.0028, -1.0701,  1.6538,  0.3031, -1.1047],\n",
      "        [-0.0714,  0.6527, -0.0179, -1.1624, -0.1820],\n",
      "        [ 0.8778, -0.9463, -0.6185, -1.0903, -0.4686]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.4403],\n",
      "        [-1.6388],\n",
      "        [-0.5821],\n",
      "        [ 2.1914]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8635, -0.7208,  0.7870, -1.8099, -0.6482],\n",
      "        [-0.7402, -0.8288,  1.1182, -1.7783, -1.1777],\n",
      "        [ 0.9748,  0.0377,  0.4011, -0.8430,  1.1465],\n",
      "        [-1.2212, -0.0826, -1.1746, -0.3155, -0.4309]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7962, -1.6698, -1.1902, -1.2928, -3.3602],\n",
      "        [ 0.8437,  0.6308,  0.3048,  0.5870,  1.4111],\n",
      "        [ 0.5507,  0.6561,  0.6986,  0.3723,  0.5356],\n",
      "        [-0.7911, -1.0055, -0.7488, -1.0331, -2.3554]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8635, -0.7208,  0.7870, -1.8099, -0.6482],\n",
      "        [-0.7402, -0.8288,  1.1182, -1.7783, -1.1777],\n",
      "        [ 0.9748,  0.0377,  0.4011, -0.8430,  1.1465],\n",
      "        [-1.2212, -0.0826, -1.1746, -0.3155, -0.4309]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.3012],\n",
      "        [-3.5122],\n",
      "        [ 1.1419],\n",
      "        [ 3.2695]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3760,  0.1603, -0.4774,  0.2687,  0.5250],\n",
      "        [-1.8782,  2.3632, -0.3607,  0.4455,  0.5326],\n",
      "        [ 1.2374,  0.2826, -0.0688, -0.7711,  0.3568],\n",
      "        [ 1.0860, -0.8937, -0.0061, -0.7971,  0.7010]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.7498, -1.9889, -2.4717, -3.3853, -4.7392],\n",
      "        [ 1.4246,  2.3345,  2.4094,  1.7110,  3.1769],\n",
      "        [-0.1238,  0.1675,  0.1899,  0.4952,  0.5028],\n",
      "        [-1.3118, -1.6577, -2.3316, -2.4441, -5.0855]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3760,  0.1603, -0.4774,  0.2687,  0.5250],\n",
      "        [-1.8782,  2.3632, -0.3607,  0.4455,  0.5326],\n",
      "        [ 1.2374,  0.2826, -0.0688, -0.7711,  0.3568],\n",
      "        [ 1.0860, -0.8937, -0.0061, -0.7971,  0.7010]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.1942],\n",
      "        [ 4.4267],\n",
      "        [-0.3213],\n",
      "        [-1.5458]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.2093, -0.2829,  0.0121, -0.1838, -0.1971],\n",
      "        [ 1.7564,  1.1161, -0.3639,  1.4283,  1.3274],\n",
      "        [-0.3512,  1.3554, -0.2904,  1.2997, -0.7539],\n",
      "        [-0.6422,  1.6307, -1.2949,  1.6825, -0.9223]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1065, -0.0431,  0.3887, -0.1840, -0.1958],\n",
      "        [ 0.5120,  0.2737,  0.4359,  0.6071,  0.5466],\n",
      "        [-0.2764,  1.1606,  0.1087,  0.3912,  0.4536],\n",
      "        [ 0.0671, -0.1490, -0.2502, -0.3279,  0.8181]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.2093, -0.2829,  0.0121, -0.1838, -0.1971],\n",
      "        [ 1.7564,  1.1161, -0.3639,  1.4283,  1.3274],\n",
      "        [-0.3512,  1.3554, -0.2904,  1.2997, -0.7539],\n",
      "        [-0.6422,  1.6307, -1.2949,  1.6825, -0.9223]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1460],\n",
      "        [ 2.6387],\n",
      "        [ 1.8051],\n",
      "        [-1.2682]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2772, -0.9868,  0.4256,  0.9638, -0.2833],\n",
      "        [-0.9360,  1.1071,  0.4804,  0.9897,  0.3044],\n",
      "        [-0.8214,  1.0431,  0.8399, -0.6220,  1.1240],\n",
      "        [-0.7550,  1.7018,  0.6236, -2.5335, -0.4754]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6734, -0.1400, -0.8432, -0.3489, -0.1121],\n",
      "        [-0.6414, -1.2582, -1.0718, -0.6944, -1.1453],\n",
      "        [-0.6605,  0.0520, -0.2826, -0.0608, -0.8312],\n",
      "        [ 0.8283,  0.1356,  0.9661,  0.0305,  1.2338]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2772, -0.9868,  0.4256,  0.9638, -0.2833],\n",
      "        [-0.9360,  1.1071,  0.4804,  0.9897,  0.3044],\n",
      "        [-0.8214,  1.0431,  0.8399, -0.6220,  1.1240],\n",
      "        [-0.7550,  1.7018,  0.6236, -2.5335, -0.4754]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3385],\n",
      "        [-2.3435],\n",
      "        [-0.5371],\n",
      "        [-0.4560]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1139,  0.3970,  0.9039, -0.2764, -0.3323],\n",
      "        [-0.0074,  0.9480,  0.8297, -0.0509,  0.1640],\n",
      "        [ 0.6097, -2.0625, -1.3444, -0.3244, -0.2746],\n",
      "        [-2.2212,  1.2906,  0.2873, -0.7810,  1.3536]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1295, -0.0641,  0.5479, -0.6062,  0.3004],\n",
      "        [ 0.3583,  1.3526,  1.1010,  0.8736,  1.1843],\n",
      "        [ 0.2782,  0.5270,  0.4795,  0.0707, -0.4598],\n",
      "        [ 0.4443,  0.5265,  0.7134,  0.9540,  1.0879]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1139,  0.3970,  0.9039, -0.2764, -0.3323],\n",
      "        [-0.0074,  0.9480,  0.8297, -0.0509,  0.1640],\n",
      "        [ 0.6097, -2.0625, -1.3444, -0.3244, -0.2746],\n",
      "        [-2.2212,  1.2906,  0.2873, -0.7810,  1.3536]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6818],\n",
      "        [ 2.3429],\n",
      "        [-1.4587],\n",
      "        [ 0.6253]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7142,  0.7185, -0.3557, -1.2611, -0.7868],\n",
      "        [-0.3483, -0.9248,  0.1426,  0.4462, -1.0611],\n",
      "        [-0.6954,  0.1487,  0.9147,  0.0035, -0.4317],\n",
      "        [-0.1217, -0.0013,  0.5875,  0.9168, -0.7057]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2526, -0.0676,  0.1389,  0.0838, -0.6288],\n",
      "        [-0.4098, -0.4116, -0.3551, -0.1319, -1.7370],\n",
      "        [ 0.5507,  1.3699,  1.1098,  0.8440,  1.3835],\n",
      "        [ 0.1841,  0.0072,  0.3631,  0.0205,  0.6560]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7142,  0.7185, -0.3557, -1.2611, -0.7868],\n",
      "        [-0.3483, -0.9248,  0.1426,  0.4462, -1.0611],\n",
      "        [-0.6954,  0.1487,  0.9147,  0.0035, -0.4317],\n",
      "        [-0.1217, -0.0013,  0.5875,  0.9168, -0.7057]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1106],\n",
      "        [ 2.2570],\n",
      "        [ 0.2417],\n",
      "        [-0.2533]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8446,  0.4262,  0.0537,  0.5932,  0.5112],\n",
      "        [-0.0785, -1.4707,  0.9955,  1.2274,  0.2922],\n",
      "        [-0.2942, -0.2534, -0.8654,  1.6599, -0.3834],\n",
      "        [-1.4067,  1.0595,  0.5612,  0.8205, -0.0900]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4098,  0.5959,  0.3586, -0.1289,  0.0116],\n",
      "        [-0.9110, -1.1295, -1.1221, -2.0005, -2.7226],\n",
      "        [ 0.2841,  0.6758,  1.0361,  0.9939,  2.0273],\n",
      "        [ 0.0277,  0.3618,  0.5868,  0.2513,  0.3516]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8446,  0.4262,  0.0537,  0.5932,  0.5112],\n",
      "        [-0.0785, -1.4707,  0.9955,  1.2274,  0.2922],\n",
      "        [-0.2942, -0.2534, -0.8654,  1.6599, -0.3834],\n",
      "        [-1.4067,  1.0595,  0.5612,  0.8205, -0.0900]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1434],\n",
      "        [-2.6355],\n",
      "        [-0.2789],\n",
      "        [ 0.8482]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9157, -0.9898,  0.1006,  1.8734,  0.2465],\n",
      "        [-0.8216,  0.1527, -1.1213,  0.2904, -0.2677],\n",
      "        [-1.4491,  1.4269,  1.0534,  0.7199,  0.7128],\n",
      "        [-1.1746, -1.6135,  1.1892, -1.1563, -0.5520]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1392, -0.3912, -0.0848, -0.1317,  0.2048],\n",
      "        [-0.0632,  0.8430, -0.4082,  0.5198,  0.0446],\n",
      "        [-0.1183,  0.7054,  0.2297,  0.9729,  0.4604],\n",
      "        [ 0.0764, -0.1676,  0.4496, -0.0736,  0.3586]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9157, -0.9898,  0.1006,  1.8734,  0.2465],\n",
      "        [-0.8216,  0.1527, -1.1213,  0.2904, -0.2677],\n",
      "        [-1.4491,  1.4269,  1.0534,  0.7199,  0.7128],\n",
      "        [-1.1746, -1.6135,  1.1892, -1.1563, -0.5520]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0843],\n",
      "        [ 0.7774],\n",
      "        [ 2.4486],\n",
      "        [ 0.6025]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6125,  0.7750,  0.3419,  1.5788, -0.0293],\n",
      "        [-0.4208, -1.5083,  0.7748, -0.5737,  0.8641],\n",
      "        [-2.4762, -0.5136,  0.4647, -1.9242,  0.9916],\n",
      "        [ 0.5078, -0.8004,  1.0781,  0.2470, -0.0821]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2579, -0.6098,  0.3562,  0.0348,  0.0750],\n",
      "        [-0.4487,  0.3355,  0.3309, -0.0372, -0.0022],\n",
      "        [-0.5151, -0.7191, -0.9826, -1.1861, -1.9587],\n",
      "        [-0.1530, -0.0119,  0.2079,  0.3075, -0.2577]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6125,  0.7750,  0.3419,  1.5788, -0.0293],\n",
      "        [-0.4208, -1.5083,  0.7748, -0.5737,  0.8641],\n",
      "        [-2.4762, -0.5136,  0.4647, -1.9242,  0.9916],\n",
      "        [ 0.5078, -0.8004,  1.0781,  0.2470, -0.0821]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7139],\n",
      "        [-0.0414],\n",
      "        [ 1.5283],\n",
      "        [ 0.2531]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1733, -1.8233, -0.9363, -1.0215,  0.8415],\n",
      "        [ 0.8654,  1.0855, -0.5456,  0.9235,  0.0649],\n",
      "        [ 0.3791, -0.7771,  0.0920,  0.2044,  0.3501],\n",
      "        [ 1.7095,  1.0463,  1.1680,  1.5570, -0.8058]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2094, -0.5367, -0.6054, -0.2655, -0.2407],\n",
      "        [ 0.1292,  1.1876,  0.4418,  0.5744, -0.1071],\n",
      "        [-0.4867, -0.6024, -0.7545, -1.8642, -3.1491],\n",
      "        [ 0.0482,  0.4265,  0.2200,  0.3583,  0.1407]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1733, -1.8233, -0.9363, -1.0215,  0.8415],\n",
      "        [ 0.8654,  1.0855, -0.5456,  0.9235,  0.0649],\n",
      "        [ 0.3791, -0.7771,  0.0920,  0.2044,  0.3501],\n",
      "        [ 1.7095,  1.0463,  1.1680,  1.5570, -0.8058]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6504],\n",
      "        [ 1.6835],\n",
      "        [-1.2693],\n",
      "        [ 1.2301]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0489, -0.3789,  0.4828,  1.0645, -0.0487],\n",
      "        [-1.7050,  0.9091, -0.6158,  0.2143,  2.9732],\n",
      "        [ 1.4731,  1.4025, -0.1821, -1.5948, -0.2367],\n",
      "        [-1.1251, -0.2441,  0.8261,  2.1724,  2.3156]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5673, -0.2388, -0.5481, -0.6523, -1.5623],\n",
      "        [-0.9177, -0.5204, -0.4530,  0.0994, -0.7383],\n",
      "        [-0.4624, -0.3562, -0.3180, -1.0208, -1.6554],\n",
      "        [-0.3794,  0.3328,  0.0029, -0.3425, -0.5715]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0489, -0.3789,  0.4828,  1.0645, -0.0487],\n",
      "        [-1.7050,  0.9091, -0.6158,  0.2143,  2.9732],\n",
      "        [ 1.4731,  1.4025, -0.1821, -1.5948, -0.2367],\n",
      "        [-1.1251, -0.2441,  0.8261,  2.1724,  2.3156]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8201],\n",
      "        [-0.8032],\n",
      "        [ 0.8971],\n",
      "        [-1.7195]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4247, -0.8709,  0.7622, -0.8161,  0.8976],\n",
      "        [-1.6576, -2.2096, -0.7619,  2.3198, -0.3516],\n",
      "        [ 0.4624,  2.3932,  1.3114, -0.8941, -0.9718],\n",
      "        [-0.2717,  0.6117, -0.4495, -0.7092,  1.1675]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4366,  0.0029, -0.4210, -0.2294, -0.3148],\n",
      "        [-0.2503, -0.2798,  0.3559, -0.0221,  0.1358],\n",
      "        [-1.1253, -0.2655, -0.1496, -1.2157, -1.8047],\n",
      "        [ 0.7077,  0.7269,  1.1836,  1.2481,  1.0520]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4247, -0.8709,  0.7622, -0.8161,  0.8976],\n",
      "        [-1.6576, -2.2096, -0.7619,  2.3198, -0.3516],\n",
      "        [ 0.4624,  2.3932,  1.3114, -0.8941, -0.9718],\n",
      "        [-0.2717,  0.6117, -0.4495, -0.7092,  1.1675]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6043],\n",
      "        [ 0.6630],\n",
      "        [ 1.4886],\n",
      "        [ 0.0634]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1701, -0.6808,  0.8835,  1.7137,  0.7274],\n",
      "        [ 0.8990, -1.4404,  0.0014,  1.9708, -0.9127],\n",
      "        [-0.4843,  0.5328, -0.4752, -0.3139, -0.8414],\n",
      "        [ 1.0240, -0.5216, -0.6801,  1.1528,  0.2878]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1112, -0.7565, -0.5324, -0.3336, -0.9134],\n",
      "        [-0.2530,  0.1781, -0.1853,  0.0793, -0.2894],\n",
      "        [-0.2608, -0.5610, -1.1545, -2.1287, -2.1508],\n",
      "        [-0.0420,  0.7646,  0.8820,  0.4306,  1.0077]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1701, -0.6808,  0.8835,  1.7137,  0.7274],\n",
      "        [ 0.8990, -1.4404,  0.0014,  1.9708, -0.9127],\n",
      "        [-0.4843,  0.5328, -0.4752, -0.3139, -0.8414],\n",
      "        [ 1.0240, -0.5216, -0.6801,  1.1528,  0.2878]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1726],\n",
      "        [-0.0639],\n",
      "        [ 2.8538],\n",
      "        [-0.2554]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5420, -0.1195, -0.8851, -0.2274,  0.9548],\n",
      "        [-1.8462, -0.6866,  1.5311,  1.4130, -0.9005],\n",
      "        [ 0.3633, -0.5097,  1.2991,  1.6761, -0.0124],\n",
      "        [-0.1799, -0.4652,  0.8183, -1.4730,  0.3517]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0586,  0.4471, -0.3352, -0.1262,  0.3469],\n",
      "        [ 0.0375,  0.0592,  0.1058, -0.0741,  0.1241],\n",
      "        [-0.0188,  0.0485,  0.2614, -0.3025, -0.1558],\n",
      "        [ 0.3581, -0.2536, -0.1648,  0.5523,  0.4016]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5420, -0.1195, -0.8851, -0.2274,  0.9548],\n",
      "        [-1.8462, -0.6866,  1.5311,  1.4130, -0.9005],\n",
      "        [ 0.3633, -0.5097,  1.2991,  1.6761, -0.0124],\n",
      "        [-0.1799, -0.4652,  0.8183, -1.4730,  0.3517]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6349],\n",
      "        [-0.1644],\n",
      "        [-0.1970],\n",
      "        [-0.7536]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.8566,  0.6144, -0.8333,  0.5731,  1.1150],\n",
      "        [-1.6676, -0.5697,  1.5731,  0.3956, -1.0563],\n",
      "        [ 1.2616, -0.5566, -0.3506,  0.1759,  1.1272],\n",
      "        [ 0.7670, -0.8676,  1.7728, -0.1683,  1.2767]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7441, -0.1439, -0.4809, -0.3220,  0.0329],\n",
      "        [-0.4012,  0.7268,  0.5134, -0.1888,  0.2382],\n",
      "        [ 0.0245,  0.4386, -0.0570,  0.1042, -0.4024],\n",
      "        [ 0.4758, -0.0896,  1.4243,  0.7390,  1.0599]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.8566,  0.6144, -0.8333,  0.5731,  1.1150],\n",
      "        [-1.6676, -0.5697,  1.5731,  0.3956, -1.0563],\n",
      "        [ 1.2616, -0.5566, -0.3506,  0.1759,  1.1272],\n",
      "        [ 0.7670, -0.8676,  1.7728, -0.1683,  1.2767]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9612],\n",
      "        [ 0.7364],\n",
      "        [-0.6284],\n",
      "        [ 4.1966]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5249,  0.1995, -0.7395,  1.8220, -0.2230],\n",
      "        [ 0.2037,  0.8808,  0.0324, -0.0963, -0.4442],\n",
      "        [-0.2551,  1.1969,  1.2308, -2.3102,  1.3145],\n",
      "        [-0.6788,  0.7215,  2.4484, -1.4298, -0.4880]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8080,  1.1314,  0.4577,  0.2401,  1.4010],\n",
      "        [ 0.0691,  0.0434,  0.0144,  0.0342, -0.1008],\n",
      "        [ 0.1967, -0.3903, -0.5470, -0.7430, -0.3223],\n",
      "        [-0.7761, -1.8140, -2.0506, -1.6781, -3.3048]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5249,  0.1995, -0.7395,  1.8220, -0.2230],\n",
      "        [ 0.2037,  0.8808,  0.0324, -0.0963, -0.4442],\n",
      "        [-0.2551,  1.1969,  1.2308, -2.3102,  1.3145],\n",
      "        [-0.6788,  0.7215,  2.4484, -1.4298, -0.4880]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4119],\n",
      "        [ 0.0943],\n",
      "        [ 0.1023],\n",
      "        [-1.7906]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1993,  1.6957, -0.3010, -0.4122,  1.9504],\n",
      "        [ 0.5102,  1.1983,  2.7436,  0.3687, -0.2717],\n",
      "        [ 0.7049,  0.6656,  0.0007, -0.3695,  0.2166],\n",
      "        [-0.7791, -0.0025,  0.4587,  0.8567, -1.2742]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4703,  0.4884,  1.0849,  0.7889,  0.8722],\n",
      "        [-0.1830, -0.2542,  0.3009,  0.3544,  0.4496],\n",
      "        [ 0.2670,  0.4188,  0.1239, -0.4906, -0.7003],\n",
      "        [-0.4070, -0.7143, -0.6783, -0.9062, -1.4579]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1993,  1.6957, -0.3010, -0.4122,  1.9504],\n",
      "        [ 0.5102,  1.1983,  2.7436,  0.3687, -0.2717],\n",
      "        [ 0.7049,  0.6656,  0.0007, -0.3695,  0.2166],\n",
      "        [-0.7791, -0.0025,  0.4587,  0.8567, -1.2742]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7839],\n",
      "        [ 0.4361],\n",
      "        [ 0.4966],\n",
      "        [ 1.0891]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1547,  1.2336, -0.7223, -0.0477, -1.3445],\n",
      "        [-2.3199,  0.3812, -0.2262, -1.1642, -0.2704],\n",
      "        [ 0.5778,  1.2647, -0.4235, -0.8157, -0.8662],\n",
      "        [ 1.2751,  1.2077, -0.2485,  0.8401,  0.7161]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0560, -0.2556, -0.4041, -0.5088, -0.6080],\n",
      "        [-0.6798, -0.2862,  0.1994,  0.4974,  0.4513],\n",
      "        [ 0.1038, -0.5652,  0.2720, -0.3113, -0.2960],\n",
      "        [-0.8338, -0.6136, -1.4893, -0.8221, -2.5868]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1547,  1.2336, -0.7223, -0.0477, -1.3445],\n",
      "        [-2.3199,  0.3812, -0.2262, -1.1642, -0.2704],\n",
      "        [ 0.5778,  1.2647, -0.4235, -0.8157, -0.8662],\n",
      "        [ 1.2751,  1.2077, -0.2485,  0.8401,  0.7161]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7537],\n",
      "        [ 0.7218],\n",
      "        [-0.2598],\n",
      "        [-3.9772]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5906, -0.9786,  1.5708, -0.2685,  0.0942],\n",
      "        [-0.8656, -1.2293,  0.4591, -1.5684, -0.1500],\n",
      "        [-0.3285,  0.8152, -0.4755,  0.9208,  1.4116],\n",
      "        [ 0.5167,  1.4588, -0.1136,  0.7918, -0.7063]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0745, -0.6090, -0.7030, -0.4784, -0.7088],\n",
      "        [ 0.1549,  0.2187,  0.3793, -0.0971, -0.2272],\n",
      "        [-0.1727, -0.3099, -0.4053, -0.0670, -0.3643],\n",
      "        [ 0.7156, -0.0409,  0.6248,  0.0065,  0.8736]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5906, -0.9786,  1.5708, -0.2685,  0.0942],\n",
      "        [-0.8656, -1.2293,  0.4591, -1.5684, -0.1500],\n",
      "        [-0.3285,  0.8152, -0.4755,  0.9208,  1.4116],\n",
      "        [ 0.5167,  1.4588, -0.1136,  0.7918, -0.7063]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4027],\n",
      "        [-0.0423],\n",
      "        [-0.5790],\n",
      "        [-0.3727]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0425,  0.2704, -0.7078, -1.4963, -0.3204],\n",
      "        [ 0.8843,  0.7339,  0.1213, -0.0307, -0.2372],\n",
      "        [ 0.4525, -0.6871,  0.7017, -0.5669,  2.3989],\n",
      "        [-0.7935,  0.9183, -0.8277,  0.4040,  1.4465]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8294,  0.0058, -0.1087,  0.2483,  0.9304],\n",
      "        [-1.0267,  0.4596,  0.5532,  0.4834,  0.2887],\n",
      "        [ 0.2938,  0.0839, -0.0063, -0.6671, -0.5783],\n",
      "        [ 0.2913,  0.3753,  1.6735,  0.2883,  1.2024]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0425,  0.2704, -0.7078, -1.4963, -0.3204],\n",
      "        [ 0.8843,  0.7339,  0.1213, -0.0307, -0.2372],\n",
      "        [ 0.4525, -0.6871,  0.7017, -0.5669,  2.3989],\n",
      "        [-0.7935,  0.9183, -0.8277,  0.4040,  1.4465]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5558],\n",
      "        [-0.5867],\n",
      "        [-0.9381],\n",
      "        [ 0.5840]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2043,  0.7924, -0.5096, -0.5933, -0.3773],\n",
      "        [-0.9671,  0.0426,  1.8366,  0.2194, -0.3386],\n",
      "        [ 0.5885, -1.9390,  0.8926,  2.0864,  0.4086],\n",
      "        [ 1.3375,  0.7398, -0.3355,  0.1247, -1.0171]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6075,  0.1919, -0.2708,  0.1145, -0.4015],\n",
      "        [ 0.0541,  0.0115,  0.5094,  0.6220, -0.1952],\n",
      "        [ 0.6660, -0.7495, -0.0190, -0.5035,  0.9154],\n",
      "        [ 0.0774, -0.9838, -0.2830, -0.0234, -0.9301]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2043,  0.7924, -0.5096, -0.5933, -0.3773],\n",
      "        [-0.9671,  0.0426,  1.8366,  0.2194, -0.3386],\n",
      "        [ 0.5885, -1.9390,  0.8926,  2.0864,  0.4086],\n",
      "        [ 1.3375,  0.7398, -0.3355,  0.1247, -1.0171]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2496],\n",
      "        [ 1.0863],\n",
      "        [ 1.1519],\n",
      "        [ 0.4138]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0834, -0.1614,  1.3832,  0.9135, -0.7722],\n",
      "        [ 0.4725,  0.1462,  0.6105,  0.0503,  1.9464],\n",
      "        [-0.0549, -0.0619,  0.2686,  0.7321, -0.8337],\n",
      "        [-0.9653, -1.9076,  0.5945, -1.0474,  0.7371]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2293,  0.6679,  0.0581,  0.0193,  0.6178],\n",
      "        [-0.6802, -0.3479,  0.5878, -0.0280,  0.1112],\n",
      "        [ 0.2159, -0.2457, -0.6595, -0.5504, -1.4250],\n",
      "        [ 0.4040, -1.0374, -0.2934, -0.3606, -0.2794]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0834, -0.1614,  1.3832,  0.9135, -0.7722],\n",
      "        [ 0.4725,  0.1462,  0.6105,  0.0503,  1.9464],\n",
      "        [-0.0549, -0.0619,  0.2686,  0.7321, -0.8337],\n",
      "        [-0.9653, -1.9076,  0.5945, -1.0474,  0.7371]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5060],\n",
      "        [ 0.2017],\n",
      "        [ 0.6112],\n",
      "        [ 1.5864]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1589, -0.1573, -0.7994,  0.7631,  1.2600],\n",
      "        [ 0.3725,  0.1254,  0.8819,  0.8369,  0.7878],\n",
      "        [-1.0247, -0.5277,  0.3515,  1.0011, -1.5945],\n",
      "        [ 0.8603, -1.0235,  0.9952, -0.6910, -0.0109]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1953,  0.5345, -0.2479,  0.3381, -0.2195],\n",
      "        [-0.1869, -0.2901,  0.5131,  0.8283, -0.0447],\n",
      "        [-0.2104, -0.3735, -0.4749, -0.3475, -1.0997],\n",
      "        [-0.4062, -1.3023, -1.6692, -1.1450, -1.4867]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1589, -0.1573, -0.7994,  0.7631,  1.2600],\n",
      "        [ 0.3725,  0.1254,  0.8819,  0.8369,  0.7878],\n",
      "        [-1.0247, -0.5277,  0.3515,  1.0011, -1.5945],\n",
      "        [ 0.8603, -1.0235,  0.9952, -0.6910, -0.0109]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3219],\n",
      "        [ 1.0045],\n",
      "        [ 1.6513],\n",
      "        [ 0.1295]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5604,  0.2312,  2.1261, -0.8249, -0.3422],\n",
      "        [ 0.5325,  0.2449, -0.3827,  2.7738,  0.9502],\n",
      "        [-0.3929,  0.7191,  0.9321,  0.2593,  1.1806],\n",
      "        [-1.2598,  0.0888, -0.2580,  2.1790, -1.4606]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.2442,  0.5620, -0.0517, -0.1988,  0.6602],\n",
      "        [-0.0482, -0.5509, -0.3781, -0.1910, -0.6347],\n",
      "        [-0.9055, -1.0761, -0.9440, -0.8487, -1.4847],\n",
      "        [-0.4088, -2.0272, -0.4243, -0.4300, -1.2473]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5604,  0.2312,  2.1261, -0.8249, -0.3422],\n",
      "        [ 0.5325,  0.2449, -0.3827,  2.7738,  0.9502],\n",
      "        [-0.3929,  0.7191,  0.9321,  0.2593,  1.1806],\n",
      "        [-1.2598,  0.0888, -0.2580,  2.1790, -1.4606]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1789],\n",
      "        [-1.1489],\n",
      "        [-3.2709],\n",
      "        [ 1.3293]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5774, -1.1023, -0.7918,  0.3167,  0.7553],\n",
      "        [ 1.2176, -0.4734,  1.1269,  0.2795,  0.5087],\n",
      "        [-1.3126,  1.3168,  0.5540,  0.3616,  1.5597],\n",
      "        [ 0.4070, -0.3769,  0.2615,  0.2734,  0.8206]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5176,  0.0753,  0.3151,  0.1078,  0.6946],\n",
      "        [-0.4986, -0.0492, -0.7930,  0.7908,  0.1207],\n",
      "        [ 0.2986,  0.9405,  0.3651,  0.0073,  1.1846],\n",
      "        [-0.6241, -0.8905, -0.9802, -1.0529, -2.4266]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5774, -1.1023, -0.7918,  0.3167,  0.7553],\n",
      "        [ 1.2176, -0.4734,  1.1269,  0.2795,  0.5087],\n",
      "        [-1.3126,  1.3168,  0.5540,  0.3616,  1.5597],\n",
      "        [ 0.4070, -0.3769,  0.2615,  0.2734,  0.8206]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0727],\n",
      "        [-1.1950],\n",
      "        [ 2.8990],\n",
      "        [-2.4538]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1908,  1.4966,  1.8377, -0.3987, -0.8593],\n",
      "        [-0.0620,  0.0412, -0.4207,  1.4115,  2.0295],\n",
      "        [ 0.2296, -1.0296, -1.0971,  0.3552,  0.8658],\n",
      "        [ 0.7069,  1.7704, -0.3962, -0.4379, -1.0532]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0370,  0.2839, -0.1114,  0.5641,  0.2175],\n",
      "        [ 0.0397,  0.6245,  0.1274,  0.9029,  1.3179],\n",
      "        [-0.1286, -0.7930, -0.7318, -0.6213, -2.1521],\n",
      "        [ 0.1652, -0.9588, -1.0274, -0.8128, -0.7828]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1908,  1.4966,  1.8377, -0.3987, -0.8593],\n",
      "        [-0.0620,  0.0412, -0.4207,  1.4115,  2.0295],\n",
      "        [ 0.2296, -1.0296, -1.0971,  0.3552,  0.8658],\n",
      "        [ 0.7069,  1.7704, -0.3962, -0.4379, -1.0532]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2356],\n",
      "        [ 3.9188],\n",
      "        [-0.4941],\n",
      "        [ 0.0067]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2857,  0.4546,  1.2962,  2.1396, -0.5567],\n",
      "        [ 0.9647,  2.1396,  1.4914,  1.2515, -0.4329],\n",
      "        [-0.4124,  0.7319,  1.0671,  1.5789,  2.0842],\n",
      "        [ 0.8398,  1.6383,  0.3691, -0.8577,  1.0070]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4650,  0.3509,  0.0082, -0.3231,  0.1374],\n",
      "        [-1.1114, -1.1708, -1.0277, -1.0855, -2.9118],\n",
      "        [-0.2098, -0.5393, -0.1570, -0.6704, -1.1592],\n",
      "        [ 0.4338, -0.5185, -1.0926, -1.3163, -1.7034]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2857,  0.4546,  1.2962,  2.1396, -0.5567],\n",
      "        [ 0.9647,  2.1396,  1.4914,  1.2515, -0.4329],\n",
      "        [-0.4124,  0.7319,  1.0671,  1.5789,  2.0842],\n",
      "        [ 0.8398,  1.6383,  0.3691, -0.8577,  1.0070]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7305],\n",
      "        [-5.2078],\n",
      "        [-3.9502],\n",
      "        [-1.4746]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4678,  0.8979, -0.1312,  0.9853,  1.2875],\n",
      "        [-0.0822,  0.2850,  0.7420,  0.3116,  0.1406],\n",
      "        [-0.3630,  0.6261,  0.2696,  0.2952, -1.0884],\n",
      "        [-0.6952,  0.0571, -0.1517, -2.1578,  1.7091]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2513,  0.6057,  0.2755, -0.0409,  0.2142],\n",
      "        [ 0.8402,  1.8028,  0.4570,  0.5882,  1.4366],\n",
      "        [ 0.5670,  1.1368,  1.9314,  0.9123,  1.9027],\n",
      "        [ 0.4122, -0.9592, -1.0262, -0.8391, -0.2970]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4678,  0.8979, -0.1312,  0.9853,  1.2875],\n",
      "        [-0.0822,  0.2850,  0.7420,  0.3116,  0.1406],\n",
      "        [-0.3630,  0.6261,  0.2696,  0.2952, -1.0884],\n",
      "        [-0.6952,  0.0571, -0.1517, -2.1578,  1.7091]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6256],\n",
      "        [ 1.1690],\n",
      "        [-0.7750],\n",
      "        [ 1.1174]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3276,  0.3690, -0.9860,  1.6903, -0.4660],\n",
      "        [-0.7548,  2.3734,  0.5492, -0.2107, -0.2124],\n",
      "        [-1.5771, -0.0141,  0.2138,  0.6550, -0.6483],\n",
      "        [-0.5286, -0.7483, -0.6417,  1.3420,  0.3975]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2071,  0.0775,  0.3341,  0.2187,  0.5399],\n",
      "        [-0.0367, -0.1806, -0.1218,  0.2438, -0.4891],\n",
      "        [ 1.8528,  1.9160,  1.8059,  1.3020,  1.8904],\n",
      "        [ 0.0239, -1.0008, -0.3877, -1.5048, -1.3833]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3276,  0.3690, -0.9860,  1.6903, -0.4660],\n",
      "        [-0.7548,  2.3734,  0.5492, -0.2107, -0.2124],\n",
      "        [-1.5771, -0.0141,  0.2138,  0.6550, -0.6483],\n",
      "        [-0.5286, -0.7483, -0.6417,  1.3420,  0.3975]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1149],\n",
      "        [-0.4154],\n",
      "        [-2.9357],\n",
      "        [-1.5842]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6539, -0.6588,  1.3452, -0.6534, -1.5546],\n",
      "        [ 0.4797, -1.3615,  0.2184,  0.1646, -1.1609],\n",
      "        [-0.6519, -0.4234,  1.0070,  0.1107,  1.9237],\n",
      "        [ 1.1709, -1.2798,  1.1038,  0.3312, -0.0436]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3152,  0.1137,  0.9517, -0.1219, -0.0406],\n",
      "        [-0.2742,  0.0104,  0.6925, -0.3778,  0.5232],\n",
      "        [ 0.0844,  0.3752, -0.1472,  0.1648,  0.3638],\n",
      "        [-0.1289, -0.6503, -0.7880, -1.1088, -0.3116]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6539, -0.6588,  1.3452, -0.6534, -1.5546],\n",
      "        [ 0.4797, -1.3615,  0.2184,  0.1646, -1.1609],\n",
      "        [-0.6519, -0.4234,  1.0070,  0.1107,  1.9237],\n",
      "        [ 1.1709, -1.2798,  1.1038,  0.3312, -0.0436]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5542],\n",
      "        [-0.6640],\n",
      "        [ 0.3560],\n",
      "        [-0.5421]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0468,  0.5713,  0.9881, -0.0923,  0.6788],\n",
      "        [-0.2171, -2.1641,  0.6081,  1.2119,  0.0294],\n",
      "        [ 1.6427, -0.1774,  1.8795,  1.9827,  2.3375],\n",
      "        [-2.1045,  1.6660,  0.5133, -0.0171,  0.1568]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8795, -0.4574, -0.4272, -0.0182, -0.9496],\n",
      "        [ 0.1797, -0.4629,  0.2625,  0.0615, -1.0121],\n",
      "        [ 0.6629, -0.2965, -0.7391,  0.5622,  0.4467],\n",
      "        [ 0.5200, -0.2468, -1.0439, -1.5053, -0.9331]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0468,  0.5713,  0.9881, -0.0923,  0.6788],\n",
      "        [-0.2171, -2.1641,  0.6081,  1.2119,  0.0294],\n",
      "        [ 1.6427, -0.1774,  1.8795,  1.9827,  2.3375],\n",
      "        [-2.1045,  1.6660,  0.5133, -0.0171,  0.1568]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3675],\n",
      "        [ 1.1670],\n",
      "        [ 1.9114],\n",
      "        [-2.1619]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1982,  0.9837, -0.3686, -1.4396,  1.6852],\n",
      "        [-1.1169, -0.4672,  0.2881,  1.3207, -0.0655],\n",
      "        [ 0.5041, -0.6731, -0.6598,  0.6686,  0.3323],\n",
      "        [-2.0069, -0.1995,  0.4547, -0.9351, -0.6972]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3584,  0.1278, -0.0116,  0.0930,  0.4804],\n",
      "        [-0.1523, -0.8128, -1.1076, -0.5482, -0.8579],\n",
      "        [-0.7232, -1.4185, -0.5079, -0.8713, -1.8482],\n",
      "        [ 1.4455,  0.7183,  0.3657, -0.4196,  0.7227]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1982,  0.9837, -0.3686, -1.4396,  1.6852],\n",
      "        [-1.1169, -0.4672,  0.2881,  1.3207, -0.0655],\n",
      "        [ 0.5041, -0.6731, -0.6598,  0.6686,  0.3323],\n",
      "        [-2.0069, -0.1995,  0.4547, -0.9351, -0.6972]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7347],\n",
      "        [-0.4370],\n",
      "        [-0.2714],\n",
      "        [-2.9896]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1077,  0.7290,  1.6254,  2.2754, -0.2075],\n",
      "        [ 0.1731, -0.6913,  0.3182, -1.4709,  1.8073],\n",
      "        [ 1.7011, -1.1139,  0.8924, -0.2972,  0.7661],\n",
      "        [-0.8167,  0.8721,  0.6973,  1.6780,  0.6321]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2382,  0.8540,  0.4856, -0.1462,  0.0141],\n",
      "        [-0.5933, -1.2015,  0.3206, -0.6403, -0.8241],\n",
      "        [ 0.1983, -0.3851, -0.9253, -0.2965, -1.4325],\n",
      "        [ 0.8993,  1.2415,  1.9378,  0.5952,  2.1576]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1077,  0.7290,  1.6254,  2.2754, -0.2075],\n",
      "        [ 0.1731, -0.6913,  0.3182, -1.4709,  1.8073],\n",
      "        [ 1.7011, -1.1139,  0.8924, -0.2972,  0.7661],\n",
      "        [-0.8167,  0.8721,  0.6973,  1.6780,  0.6321]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5741],\n",
      "        [ 0.2823],\n",
      "        [-1.0689],\n",
      "        [ 4.0622]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3772, -1.0149, -0.7756,  0.9498, -0.7939],\n",
      "        [-1.0756, -0.7870,  1.6067, -1.2904,  1.8658],\n",
      "        [-0.5778,  0.3385,  2.1399,  0.8631, -0.3440],\n",
      "        [ 1.1018, -0.5124,  0.2753, -0.6292, -0.4188]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3612,  0.0685,  0.4476, -0.0563,  0.1393],\n",
      "        [-0.1542, -0.8433, -1.0230, -0.3424, -1.4880],\n",
      "        [-0.3727, -0.2919,  0.1996,  0.3494,  0.7986],\n",
      "        [ 0.0962, -0.8694, -1.5154, -0.7946, -1.8286]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3772, -1.0149, -0.7756,  0.9498, -0.7939],\n",
      "        [-1.0756, -0.7870,  1.6067, -1.2904,  1.8658],\n",
      "        [-0.5778,  0.3385,  2.1399,  0.8631, -0.3440],\n",
      "        [ 1.1018, -0.5124,  0.2753, -0.6292, -0.4188]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4445],\n",
      "        [-3.1487],\n",
      "        [ 0.5704],\n",
      "        [ 1.4001]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9406, -0.1708, -0.4629, -2.6000, -0.0373],\n",
      "        [ 0.1303,  1.7662,  0.1209, -0.1219, -1.6717],\n",
      "        [ 0.3956, -1.8336, -0.9151, -0.1182, -0.1008],\n",
      "        [ 0.2561,  1.5859, -0.1777,  0.2213,  0.0057]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3401,  0.4298,  0.4125,  0.7423,  1.1073],\n",
      "        [ 0.9098,  1.3397,  1.1488,  0.5651,  1.2699],\n",
      "        [ 0.1131,  0.1633, -0.0657, -0.3340,  0.5484],\n",
      "        [-0.8144, -1.4340, -1.5329, -1.4920, -2.7787]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9406, -0.1708, -0.4629, -2.6000, -0.0373],\n",
      "        [ 0.1303,  1.7662,  0.1209, -0.1219, -1.6717],\n",
      "        [ 0.3956, -1.8336, -0.9151, -0.1182, -0.1008],\n",
      "        [ 0.2561,  1.5859, -0.1777,  0.2213,  0.0057]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5756],\n",
      "        [ 0.4318],\n",
      "        [-0.2105],\n",
      "        [-2.5562]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5039,  0.0072, -0.3998,  2.2878, -0.1634],\n",
      "        [-0.7463,  0.0572,  1.5299, -0.1607,  1.3028],\n",
      "        [-1.5075,  0.6664, -0.0862,  0.4586,  1.4300],\n",
      "        [-0.5686, -1.0706,  1.4037, -0.1100, -1.0577]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8896,  1.4242,  0.6028,  1.0480,  1.6036],\n",
      "        [ 0.7024,  0.8436,  0.2043,  0.6303,  1.1504],\n",
      "        [-0.4305, -0.9192, -0.8224, -0.1474, -0.3681],\n",
      "        [ 0.7946, -0.4972, -0.2029,  0.0945,  0.2279]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5039,  0.0072, -0.3998,  2.2878, -0.1634],\n",
      "        [-0.7463,  0.0572,  1.5299, -0.1607,  1.3028],\n",
      "        [-1.5075,  0.6664, -0.0862,  0.4586,  1.4300],\n",
      "        [-0.5686, -1.0706,  1.4037, -0.1100, -1.0577]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.2425],\n",
      "        [ 1.2342],\n",
      "        [-0.4866],\n",
      "        [-0.4556]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0790,  0.8471, -0.1887,  0.9758,  0.7772],\n",
      "        [-0.4723,  0.0840, -0.5487,  1.5213,  0.5325],\n",
      "        [ 1.2899, -1.4128, -0.4628,  1.4998, -1.3966],\n",
      "        [ 1.3732, -0.3855, -1.1171,  0.5261,  2.2233]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9559, -0.6147, -0.8333, -1.0650, -1.2201],\n",
      "        [-0.5542, -0.1015,  0.1406,  0.1416,  0.0837],\n",
      "        [ 0.2409, -0.8839, -0.2233,  0.0046, -0.5441],\n",
      "        [ 0.8945, -0.0302,  0.2325,  0.1131, -0.3083]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0790,  0.8471, -0.1887,  0.9758,  0.7772],\n",
      "        [-0.4723,  0.0840, -0.5487,  1.5213,  0.5325],\n",
      "        [ 1.2899, -1.4128, -0.4628,  1.4998, -1.3966],\n",
      "        [ 1.3732, -0.3855, -1.1171,  0.5261,  2.2233]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2753],\n",
      "        [ 0.4361],\n",
      "        [ 2.4296],\n",
      "        [ 0.3545]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2592, -0.7264,  0.9208,  2.2308, -2.4694],\n",
      "        [ 0.3034, -0.2226, -0.5333,  0.1667,  1.5547],\n",
      "        [ 1.4111, -0.4250,  1.5720,  0.2006, -0.3870],\n",
      "        [-0.9992,  1.6909, -0.7338,  1.9596, -1.3468]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6276,  0.9106,  1.1652,  0.9946,  1.4894],\n",
      "        [-0.2941, -0.3482,  0.0223, -0.4105, -0.2323],\n",
      "        [-0.4316, -1.8698, -0.9502, -1.3476, -2.5089],\n",
      "        [ 0.2877, -0.3811, -0.7868, -0.3838, -0.3254]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2592, -0.7264,  0.9208,  2.2308, -2.4694],\n",
      "        [ 0.3034, -0.2226, -0.5333,  0.1667,  1.5547],\n",
      "        [ 1.4111, -0.4250,  1.5720,  0.2006, -0.3870],\n",
      "        [-0.9992,  1.6909, -0.7338,  1.9596, -1.3468]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2106],\n",
      "        [-0.4531],\n",
      "        [-0.6076],\n",
      "        [-0.6683]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3329, -0.1805,  0.1023, -0.8353,  0.7983],\n",
      "        [ 0.7855, -0.2349,  0.5652,  0.3537, -0.2079],\n",
      "        [-0.0276,  0.7691,  0.4298, -0.6505,  0.8252],\n",
      "        [-0.6912, -0.0974,  0.2700, -0.3679, -0.3690]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5879,  0.5674,  1.1494,  0.5929,  1.5504],\n",
      "        [-0.9486, -0.7007,  0.0522,  0.5298,  0.0487],\n",
      "        [-0.7878, -1.2380, -1.0541, -0.7967, -1.4601],\n",
      "        [ 0.7643, -0.5506, -0.2148, -0.3397, -0.2453]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3329, -0.1805,  0.1023, -0.8353,  0.7983],\n",
      "        [ 0.7855, -0.2349,  0.5652,  0.3537, -0.2079],\n",
      "        [-0.0276,  0.7691,  0.4298, -0.6505,  0.8252],\n",
      "        [-0.6912, -0.0974,  0.2700, -0.3679, -0.3690]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9534],\n",
      "        [-0.3737],\n",
      "        [-2.0700],\n",
      "        [-0.3172]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5115, -0.1042, -0.1110, -0.0311,  1.6340],\n",
      "        [-2.0101,  1.7648, -1.6046,  2.0299, -0.4625],\n",
      "        [-0.7753,  0.0888, -0.5550, -1.1027, -1.1274],\n",
      "        [-0.3234,  0.9133,  0.0316, -0.8269, -0.0564]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1086,  0.9175,  0.8770,  0.3003,  1.5155],\n",
      "        [ 0.0206, -0.4353, -0.0366, -0.0973, -0.5372],\n",
      "        [ 0.4025,  0.7101, -0.1875, -0.9003,  0.0972],\n",
      "        [ 0.1441, -0.7920, -0.5870, -0.7860, -0.1973]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5115, -0.1042, -0.1110, -0.0311,  1.6340],\n",
      "        [-2.0101,  1.7648, -1.6046,  2.0299, -0.4625],\n",
      "        [-0.7753,  0.0888, -0.5550, -1.1027, -1.1274],\n",
      "        [-0.3234,  0.9133,  0.0316, -0.8269, -0.0564]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5985],\n",
      "        [-0.6999],\n",
      "        [ 0.7381],\n",
      "        [-0.1274]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3629, -0.9483,  0.7748,  0.7176,  0.5059],\n",
      "        [-0.0265, -2.0733,  0.5329,  0.9317, -2.0213],\n",
      "        [-0.6765,  1.8996,  1.7660, -0.6123, -0.2380],\n",
      "        [ 1.0871, -1.4934, -0.5202,  0.2135,  0.0767]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0907,  1.2633,  0.5069,  1.1091,  0.9155],\n",
      "        [ 0.0945, -0.4614, -0.2762,  0.1140, -0.5172],\n",
      "        [ 0.0196, -0.9238, -0.2728, -0.6744, -1.6743],\n",
      "        [ 0.3191, -0.7585, -0.4560, -0.6887, -0.4662]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3629, -0.9483,  0.7748,  0.7176,  0.5059],\n",
      "        [-0.0265, -2.0733,  0.5329,  0.9317, -2.0213],\n",
      "        [-0.6765,  1.8996,  1.7660, -0.6123, -0.2380],\n",
      "        [ 1.0871, -1.4934, -0.5202,  0.2135,  0.0767]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4868],\n",
      "        [ 1.9585],\n",
      "        [-1.4385],\n",
      "        [ 1.5339]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9310,  0.0732,  1.1136, -0.3604, -0.7263],\n",
      "        [-0.3220, -1.4435, -0.4190,  1.0720,  1.4245],\n",
      "        [-1.3986, -2.0854, -0.3230,  2.1799,  2.7547],\n",
      "        [-0.4054, -0.9140, -0.4856, -0.6337,  0.3161]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5510,  1.6912,  0.4520,  0.4845,  0.6614],\n",
      "        [-0.3678, -0.9315, -0.8365, -0.9413, -1.4679],\n",
      "        [ 0.3977, -0.3241, -0.2211, -0.2142, -0.3759],\n",
      "        [-0.3112, -1.0238, -1.9654, -1.0169, -1.6832]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9310,  0.0732,  1.1136, -0.3604, -0.7263],\n",
      "        [-0.3220, -1.4435, -0.4190,  1.0720,  1.4245],\n",
      "        [-1.3986, -2.0854, -0.3230,  2.1799,  2.7547],\n",
      "        [-0.4054, -0.9140, -0.4856, -0.6337,  0.3161]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4852],\n",
      "        [-1.2867],\n",
      "        [-1.3113],\n",
      "        [ 2.1288]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2627, -0.4405,  1.2144, -0.7658, -0.6811],\n",
      "        [-0.4632, -0.5709, -0.2629,  2.5464,  0.5382],\n",
      "        [-0.2094, -1.8860,  2.3885,  0.3972, -1.8792],\n",
      "        [-0.2539, -0.6347, -0.1567, -1.3612,  0.7439]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1482,  1.1252,  1.2844, -0.2129,  0.3045],\n",
      "        [-0.0642, -1.0629,  0.2056,  0.1907, -0.8213],\n",
      "        [ 0.6376,  0.2413,  0.4851, -0.2319,  0.1903],\n",
      "        [-0.9177, -2.0151, -1.2672, -2.0593, -3.5520]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2627, -0.4405,  1.2144, -0.7658, -0.6811],\n",
      "        [-0.4632, -0.5709, -0.2629,  2.5464,  0.5382],\n",
      "        [-0.2094, -1.8860,  2.3885,  0.3972, -1.8792],\n",
      "        [-0.2539, -0.6347, -0.1567, -1.3612,  0.7439]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8326],\n",
      "        [ 0.6260],\n",
      "        [ 0.1202],\n",
      "        [ 1.8717]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.8157, -0.0913,  1.1508,  0.3386,  0.0796],\n",
      "        [-0.6373,  0.4831,  2.2111,  0.6385,  1.1252],\n",
      "        [ 1.6414, -1.1291,  1.5696,  2.6576, -1.2888],\n",
      "        [ 0.0795, -0.3863,  0.3204, -0.0071, -0.5539]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0170,  0.3647,  0.8391,  0.1725,  0.9310],\n",
      "        [ 0.0550,  0.1422, -0.2064,  0.2707, -0.7019],\n",
      "        [ 0.4623, -0.5570, -0.5320, -1.0173, -0.6940],\n",
      "        [-0.6755, -2.6927, -2.1735, -1.9007, -4.6068]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.8157, -0.0913,  1.1508,  0.3386,  0.0796],\n",
      "        [-0.6373,  0.4831,  2.2111,  0.6385,  1.1252],\n",
      "        [ 1.6414, -1.1291,  1.5696,  2.6576, -1.2888],\n",
      "        [ 0.0795, -0.3863,  0.3204, -0.0071, -0.5539]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0170],\n",
      "        [-1.0397],\n",
      "        [-1.2565],\n",
      "        [ 2.8555]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.4959, -1.8636,  0.8454,  0.4899,  2.7607],\n",
      "        [ 0.4242, -0.3774,  0.6117, -0.4131, -0.3389],\n",
      "        [-0.6396,  0.1657, -0.4237, -0.9766, -0.4987],\n",
      "        [-1.0125, -0.6344,  0.0048,  0.5817, -1.0667]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0261,  0.5910,  0.0755,  1.0154,  0.0872],\n",
      "        [ 0.0310, -0.3366,  0.1133, -0.0836,  0.3798],\n",
      "        [ 0.6942, -1.1449, -0.3013, -0.6836,  0.5927],\n",
      "        [ 0.0959,  0.0011,  0.0294, -0.3755,  0.2200]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.4959, -1.8636,  0.8454,  0.4899,  2.7607],\n",
      "        [ 0.4242, -0.3774,  0.6117, -0.4131, -0.3389],\n",
      "        [-0.6396,  0.1657, -0.4237, -0.9766, -0.4987],\n",
      "        [-1.0125, -0.6344,  0.0048,  0.5817, -1.0667]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3644],\n",
      "        [ 0.1153],\n",
      "        [-0.1341],\n",
      "        [-0.5507]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2475, -0.6649,  0.7317, -2.3595, -1.3275],\n",
      "        [ 0.7148, -1.8571,  2.3327, -0.2957,  0.7396],\n",
      "        [-0.4462,  0.0059,  1.5338,  1.3847,  0.4629],\n",
      "        [-0.9042, -1.5906,  0.9138,  0.3348, -0.7139]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1371,  1.4244,  0.7264,  0.4627,  0.6646],\n",
      "        [ 0.8657,  0.0986, -0.6689, -0.6315, -0.9504],\n",
      "        [ 0.2446, -1.0588, -1.0288, -0.9411, -1.0554],\n",
      "        [ 0.2954, -0.0111, -0.3079,  0.2703,  0.2539]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2475, -0.6649,  0.7317, -2.3595, -1.3275],\n",
      "        [ 0.7148, -1.8571,  2.3327, -0.2957,  0.7396],\n",
      "        [-0.4462,  0.0059,  1.5338,  1.3847,  0.4629],\n",
      "        [-0.9042, -1.5906,  0.9138,  0.3348, -0.7139]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5606],\n",
      "        [-1.6411],\n",
      "        [-3.4850],\n",
      "        [-0.6216]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6547, -0.6530,  1.6457, -0.2561, -0.1199],\n",
      "        [ 1.0401, -1.4556, -0.8858,  2.6625,  1.5167],\n",
      "        [ 0.2608,  0.1459,  0.6742,  0.3144,  1.0332],\n",
      "        [-1.3468, -0.8170, -0.4158, -0.0283, -0.5985]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2277,  2.3350,  2.1213,  0.8368,  2.1236],\n",
      "        [ 0.5200, -0.2757,  0.2733,  0.1325,  1.2633],\n",
      "        [ 1.5155,  0.4250,  1.1758,  0.8480,  1.9770],\n",
      "        [ 0.4308, -0.0437,  0.1412,  0.1126, -0.0173]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6547, -0.6530,  1.6457, -0.2561, -0.1199],\n",
      "        [ 1.0401, -1.4556, -0.8858,  2.6625,  1.5167],\n",
      "        [ 0.2608,  0.1459,  0.6742,  0.3144,  1.0332],\n",
      "        [-1.3468, -0.8170, -0.4158, -0.0283, -0.5985]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6934],\n",
      "        [ 2.9687],\n",
      "        [ 3.5590],\n",
      "        [-0.5960]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0630, -0.4761,  2.2130,  1.2209, -0.6489],\n",
      "        [-0.5322,  0.9891, -0.4940,  0.1661,  0.4110],\n",
      "        [-1.4204, -0.7754,  0.6898, -0.0556, -0.2772],\n",
      "        [ 0.5404,  0.6285,  1.2652,  0.9483,  2.3572]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5745,  1.7643,  0.6171,  2.4460,  1.6360],\n",
      "        [-0.4409, -0.7582, -1.0711, -1.4671, -2.3288],\n",
      "        [ 0.3523, -0.9750, -0.4353, -1.1759, -1.4242],\n",
      "        [ 0.4467, -0.0328,  0.1853, -0.3225, -0.1208]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0630, -0.4761,  2.2130,  1.2209, -0.6489],\n",
      "        [-0.5322,  0.9891, -0.4940,  0.1661,  0.4110],\n",
      "        [-1.4204, -0.7754,  0.6898, -0.0556, -0.2772],\n",
      "        [ 0.5404,  0.6285,  1.2652,  0.9483,  2.3572]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4142],\n",
      "        [-1.1870],\n",
      "        [ 0.4154],\n",
      "        [-0.1354]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2765, -0.0332, -0.3243,  0.4750, -0.4515],\n",
      "        [ 1.8711, -0.7829, -1.1777,  1.0479,  0.2545],\n",
      "        [ 0.8699, -0.4090, -0.0111,  0.0959,  0.0699],\n",
      "        [-2.0807, -1.3925,  1.8765,  0.7840,  0.2382]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0767,  0.6749,  0.1066,  1.2777,  0.4087],\n",
      "        [-0.7492, -1.0316, -0.5587, -1.0537, -1.0214],\n",
      "        [-0.4097, -0.6038, -0.8172, -1.0553, -1.5578],\n",
      "        [ 0.3622, -0.0196, -0.6302, -0.3161, -0.5433]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2765, -0.0332, -0.3243,  0.4750, -0.4515],\n",
      "        [ 1.8711, -0.7829, -1.1777,  1.0479,  0.2545],\n",
      "        [ 0.8699, -0.4090, -0.0111,  0.0959,  0.0699],\n",
      "        [-2.0807, -1.3925,  1.8765,  0.7840,  0.2382]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3865],\n",
      "        [-1.3005],\n",
      "        [-0.3104],\n",
      "        [-2.2863]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1238, -1.4915, -0.4782,  0.9927,  0.7831],\n",
      "        [-0.0220,  1.4103, -0.8292,  1.1127,  0.1604],\n",
      "        [-0.0576, -1.1993,  0.0617,  1.1820,  0.8108],\n",
      "        [ 1.4282,  0.8742,  0.4032, -0.2210,  1.4632]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0834,  1.0469,  1.2893,  1.0213,  0.6730],\n",
      "        [-0.3318, -0.6127, -0.4690, -0.1225, -0.1454],\n",
      "        [ 0.7747, -0.2745, -0.2427, -0.5822, -0.6121],\n",
      "        [ 0.4834,  0.5828,  1.1887,  0.7322,  1.4872]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1238, -1.4915, -0.4782,  0.9927,  0.7831],\n",
      "        [-0.0220,  1.4103, -0.8292,  1.1127,  0.1604],\n",
      "        [-0.0576, -1.1993,  0.0617,  1.1820,  0.8108],\n",
      "        [ 1.4282,  0.8742,  0.4032, -0.2210,  1.4632]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6475],\n",
      "        [-0.6276],\n",
      "        [-0.9148],\n",
      "        [ 3.6934]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.3334,  0.3360,  2.1687, -0.9819, -0.6690],\n",
      "        [ 0.1594, -0.8121, -0.3318,  0.2629,  0.7860],\n",
      "        [-0.9227,  0.6440,  0.5357, -1.2313, -0.3446],\n",
      "        [ 0.0404, -0.3958, -1.4379, -0.8461,  1.0168]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2155,  1.2736,  1.3704,  0.8782,  1.3950],\n",
      "        [ 0.6974, -0.4230, -1.0269, -0.3835,  0.2566],\n",
      "        [ 0.2145, -0.9163, -0.0110, -0.3492, -0.2329],\n",
      "        [-0.6010, -1.6869, -1.3721, -1.4729, -1.3214]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.3334,  0.3360,  2.1687, -0.9819, -0.6690],\n",
      "        [ 0.1594, -0.8121, -0.3318,  0.2629,  0.7860],\n",
      "        [-0.9227,  0.6440,  0.5357, -1.2313, -0.3446],\n",
      "        [ 0.0404, -0.3958, -1.4379, -0.8461,  1.0168]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1017],\n",
      "        [ 0.8963],\n",
      "        [-0.2836],\n",
      "        [ 2.5189]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.9401,  0.6292,  0.0620,  0.8139,  0.2226],\n",
      "        [-1.0137,  1.1652,  2.6363, -1.8963, -0.4933],\n",
      "        [ 0.4384, -0.0788,  1.1507, -1.3995, -1.1124],\n",
      "        [ 1.1494, -1.2949,  0.3733,  0.5117, -1.4844]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1346,  1.0934,  1.3233,  0.6672,  1.3891],\n",
      "        [-0.5396, -1.2510, -1.1749, -1.6529, -1.7819],\n",
      "        [ 0.3655, -1.0531, -0.3520, -1.1909, -0.3354],\n",
      "        [-0.4647, -1.8851, -1.5709, -1.1422, -3.6014]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.9401,  0.6292,  0.0620,  0.8139,  0.2226],\n",
      "        [-1.0137,  1.1652,  2.6363, -1.8963, -0.4933],\n",
      "        [ 0.4384, -0.0788,  1.1507, -1.3995, -1.1124],\n",
      "        [ 1.1494, -1.2949,  0.3733,  0.5117, -1.4844]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0181],\n",
      "        [ 0.0056],\n",
      "        [ 1.8780],\n",
      "        [ 6.0817]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7240,  2.6752, -0.5313, -0.4703, -0.1812],\n",
      "        [ 1.0982,  0.6208, -0.9747,  0.6723,  1.1810],\n",
      "        [-1.9231, -1.3368, -0.0596,  0.5497,  0.0737],\n",
      "        [ 0.1325, -0.2944, -0.4733,  1.8518,  1.1093]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6277,  0.3417,  0.2736,  0.1928, -0.1683],\n",
      "        [ 0.0676, -0.9096, -0.4256, -1.1464, -1.3289],\n",
      "        [-1.0671, -0.8848, -1.6798, -1.4350, -1.6041],\n",
      "        [-0.1709, -0.2218, -0.8514, -0.6860,  0.5476]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7240,  2.6752, -0.5313, -0.4703, -0.1812],\n",
      "        [ 1.0982,  0.6208, -0.9747,  0.6723,  1.1810],\n",
      "        [-1.9231, -1.3368, -0.0596,  0.5497,  0.0737],\n",
      "        [ 0.1325, -0.2944, -0.4733,  1.8518,  1.1093]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2543],\n",
      "        [-2.4159],\n",
      "        [ 2.4281],\n",
      "        [-0.2172]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6062,  1.1794, -1.0467, -2.4842, -0.7896],\n",
      "        [ 0.7277,  2.1279,  0.1836, -1.2915,  2.7961],\n",
      "        [ 1.0939, -0.4720,  0.7157,  2.3079, -1.2790],\n",
      "        [ 0.4219,  0.1084, -0.5653,  0.4158, -0.7007]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0542,  0.5027,  1.7587,  0.8550,  0.7768],\n",
      "        [ 0.4862,  1.3943,  0.8167,  0.0959,  1.6053],\n",
      "        [-0.7665, -1.6108, -2.2430, -2.1884, -3.4047],\n",
      "        [-0.0218, -0.0439, -0.1364,  0.1542, -0.1067]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6062,  1.1794, -1.0467, -2.4842, -0.7896],\n",
      "        [ 0.7277,  2.1279,  0.1836, -1.2915,  2.7961],\n",
      "        [ 1.0939, -0.4720,  0.7157,  2.3079, -1.2790],\n",
      "        [ 0.4219,  0.1084, -0.5653,  0.4158, -0.7007]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.0724],\n",
      "        [ 7.8354],\n",
      "        [-2.3793],\n",
      "        [ 0.2021]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4995,  1.0061,  0.1126,  0.1416,  0.0644],\n",
      "        [-0.8258,  1.2915, -1.6524, -0.0953, -0.5489],\n",
      "        [ 0.3765,  0.9531, -0.3784, -0.2857, -0.0100],\n",
      "        [-0.7050,  0.2610, -1.4060,  0.1072, -1.4418]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2115,  2.6522,  2.6120,  1.8274,  2.6364],\n",
      "        [-1.4839, -2.2004, -2.7411, -1.9397, -4.3204],\n",
      "        [-0.2557, -2.0126, -1.7599, -1.0285, -2.0350],\n",
      "        [ 0.1113,  0.2105,  0.0043, -0.1245, -0.4552]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4995,  1.0061,  0.1126,  0.1416,  0.0644],\n",
      "        [-0.8258,  1.2915, -1.6524, -0.0953, -0.5489],\n",
      "        [ 0.3765,  0.9531, -0.3784, -0.2857, -0.0100],\n",
      "        [-0.7050,  0.2610, -1.4060,  0.1072, -1.4418]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.7858],\n",
      "        [ 5.4691],\n",
      "        [-1.0344],\n",
      "        [ 0.6134]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0467,  0.5553, -1.4281,  0.7564, -0.0434],\n",
      "        [ 0.7749,  0.6964, -2.0639, -0.5422, -0.0767],\n",
      "        [ 1.3501, -1.0135, -0.1722,  0.2195,  0.5115],\n",
      "        [ 0.0908, -0.6016,  1.4328,  0.5649, -0.8054]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0528, -0.2066,  0.0086, -0.1022, -0.1856],\n",
      "        [-0.0904,  0.5400, -0.0768,  0.4440,  0.6095],\n",
      "        [ 0.1513, -0.7283, -1.1267,  0.0182, -1.1001],\n",
      "        [ 0.0611,  0.2879,  0.7043,  0.3043,  0.4745]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0467,  0.5553, -1.4281,  0.7564, -0.0434],\n",
      "        [ 0.7749,  0.6964, -2.0639, -0.5422, -0.0767],\n",
      "        [ 1.3501, -1.0135, -0.1722,  0.2195,  0.5115],\n",
      "        [ 0.0908, -0.6016,  1.4328,  0.5649, -0.8054]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1987],\n",
      "        [ 0.1770],\n",
      "        [ 0.5777],\n",
      "        [ 0.6312]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6248,  1.2386,  0.5749,  0.8901,  1.5062],\n",
      "        [-1.7979,  0.9984, -0.2559,  0.2218,  0.2143],\n",
      "        [ 0.0793, -0.1805,  0.8569,  0.3970,  0.6076],\n",
      "        [-0.1814, -0.7867,  0.6141, -1.1625, -0.8300]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5689,  0.7512,  0.5702,  0.4516, -0.2017],\n",
      "        [-0.2573,  0.2060, -0.4945, -0.4357, -0.6397],\n",
      "        [-0.0960, -1.3950, -1.4865, -1.4712, -2.1926],\n",
      "        [ 0.2016, -0.0226,  0.5079,  0.2358,  0.0084]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6248,  1.2386,  0.5749,  0.8901,  1.5062],\n",
      "        [-1.7979,  0.9984, -0.2559,  0.2218,  0.2143],\n",
      "        [ 0.0793, -0.1805,  0.8569,  0.3970,  0.6076],\n",
      "        [-0.1814, -0.7867,  0.6141, -1.1625, -0.8300]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2808],\n",
      "        [ 0.5610],\n",
      "        [-2.9460],\n",
      "        [ 0.0120]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1488,  0.5734, -1.4752,  0.3890,  0.6702],\n",
      "        [ 1.3537,  0.9838, -0.5628, -0.3389,  0.1121],\n",
      "        [ 1.0072, -1.2543,  0.8541, -0.0011,  2.0235],\n",
      "        [-0.8491, -0.2275,  0.9863, -0.5220,  1.6264]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0030, -0.8521, -0.7664, -0.9107, -1.6746],\n",
      "        [-0.1837,  0.0266,  0.4440,  0.0062, -0.3994],\n",
      "        [ 1.4026, -0.6607, -0.2504, -0.6871,  0.5604],\n",
      "        [-0.7668,  0.3001,  0.0192,  0.1551, -0.1690]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1488,  0.5734, -1.4752,  0.3890,  0.6702],\n",
      "        [ 1.3537,  0.9838, -0.5628, -0.3389,  0.1121],\n",
      "        [ 1.0072, -1.2543,  0.8541, -0.0011,  2.0235],\n",
      "        [-0.8491, -0.2275,  0.9863, -0.5220,  1.6264]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8382],\n",
      "        [-0.5192],\n",
      "        [ 3.1621],\n",
      "        [ 0.2460]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8494, -0.3141,  0.1710,  0.1890,  0.9913],\n",
      "        [ 0.0184,  1.0117, -1.4823,  0.0529,  0.0141],\n",
      "        [-0.7604,  1.0902,  1.3128, -0.6669,  0.5568],\n",
      "        [ 0.8466, -0.7100,  0.8008, -1.6626, -1.1553]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4106, -1.4738, -0.1728, -0.4472, -0.7965],\n",
      "        [-0.0178, -0.1783,  0.3421, -0.4595,  0.1190],\n",
      "        [-0.8119, -0.9927, -1.6254, -1.4943, -2.8225],\n",
      "        [-0.1530,  0.4769,  0.2696,  0.3200, -0.0990]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8494, -0.3141,  0.1710,  0.1890,  0.9913],\n",
      "        [ 0.0184,  1.0117, -1.4823,  0.0529,  0.0141],\n",
      "        [-0.7604,  1.0902,  1.3128, -0.6669,  0.5568],\n",
      "        [ 0.8466, -0.7100,  0.8008, -1.6626, -1.1553]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7894],\n",
      "        [-0.7105],\n",
      "        [-3.1738],\n",
      "        [-0.6699]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3618, -0.5753, -0.3656,  1.8921,  0.3283],\n",
      "        [-1.0411, -0.6599,  0.0373,  1.1369, -0.1302],\n",
      "        [ 0.4085, -0.5214,  0.5106,  0.8817,  0.1872],\n",
      "        [-1.6087,  1.9174, -0.3204,  1.8488, -0.6573]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0148, -1.0103, -0.2402, -0.5238,  0.2971],\n",
      "        [-0.0901,  0.0473,  0.3874, -0.1468,  0.3658],\n",
      "        [ 0.6725, -0.7767, -0.9806, -0.2453,  0.4573],\n",
      "        [ 0.1652,  0.5881,  0.6670,  0.1391,  0.9042]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3618, -0.5753, -0.3656,  1.8921,  0.3283],\n",
      "        [-1.0411, -0.6599,  0.0373,  1.1369, -0.1302],\n",
      "        [ 0.4085, -0.5214,  0.5106,  0.8817,  0.1872],\n",
      "        [-1.6087,  1.9174, -0.3204,  1.8488, -0.6573]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2448],\n",
      "        [-0.1375],\n",
      "        [ 0.0482],\n",
      "        [ 0.3111]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5486, -1.0823,  0.5557, -0.9290, -1.3979],\n",
      "        [ 0.3096, -0.5390,  1.4995,  0.7068,  1.3893],\n",
      "        [-1.0682, -0.3966,  0.7893,  0.1728,  0.4895],\n",
      "        [-0.1859,  0.4536, -0.4157,  0.7823,  0.2576]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0269, -0.7084,  0.1155,  0.1804, -0.4646],\n",
      "        [ 0.1773,  0.0130, -0.2380, -0.0762, -0.1293],\n",
      "        [ 0.4451, -1.9280, -1.7108, -1.3329, -1.1392],\n",
      "        [-0.1555, -0.0056, -0.0062, -0.1500,  0.0521]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5486, -1.0823,  0.5557, -0.9290, -1.3979],\n",
      "        [ 0.3096, -0.5390,  1.4995,  0.7068,  1.3893],\n",
      "        [-1.0682, -0.3966,  0.7893,  0.1728,  0.4895],\n",
      "        [-0.1859,  0.4536, -0.4157,  0.7823,  0.2576]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2979],\n",
      "        [-0.5425],\n",
      "        [-1.8490],\n",
      "        [-0.0750]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2989, -0.6106,  0.8928, -0.5171, -0.8141],\n",
      "        [-1.2134, -0.2137,  0.7318, -0.7881, -2.2436],\n",
      "        [-1.1301, -0.2258, -0.4562, -0.4753,  1.4380],\n",
      "        [ 0.4674,  1.6529,  1.6845, -0.9364,  0.0132]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3646, -0.5532, -0.1257, -0.5719, -1.5540],\n",
      "        [ 0.5255, -0.5924, -0.6482, -0.4991, -0.1975],\n",
      "        [ 1.1263, -1.0773, -0.7450, -0.5810,  0.6030],\n",
      "        [ 0.3101, -0.1022,  0.4606,  0.0893,  0.5707]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2989, -0.6106,  0.8928, -0.5171, -0.8141],\n",
      "        [-1.2134, -0.2137,  0.7318, -0.7881, -2.2436],\n",
      "        [-1.1301, -0.2258, -0.4562, -0.4753,  1.4380],\n",
      "        [ 0.4674,  1.6529,  1.6845, -0.9364,  0.0132]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6774],\n",
      "        [-0.1488],\n",
      "        [ 0.4536],\n",
      "        [ 0.6757]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4638, -0.6278, -0.2222, -0.8192, -0.9728],\n",
      "        [-0.7818,  0.0389,  0.7922,  0.1006,  0.3919],\n",
      "        [ 0.3114, -0.7364, -0.2574,  0.4944, -1.4337],\n",
      "        [-0.6336,  1.2980, -1.3707,  1.3735,  0.9930]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1633, -1.6778, -1.7960, -1.2047, -1.7735],\n",
      "        [ 0.1632, -0.3381,  0.4297, -0.5440, -0.2521],\n",
      "        [ 0.5412, -1.2849, -1.1249, -0.6496, -1.1927],\n",
      "        [ 0.0114,  0.6766,  0.7090,  0.3401,  0.4293]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4638, -0.6278, -0.2222, -0.8192, -0.9728],\n",
      "        [-0.7818,  0.0389,  0.7922,  0.1006,  0.3919],\n",
      "        [ 0.3114, -0.7364, -0.2574,  0.4944, -1.4337],\n",
      "        [-0.6336,  1.2980, -1.3707,  1.3735,  0.9930]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.6249],\n",
      "        [ 0.0462],\n",
      "        [ 2.7933],\n",
      "        [ 0.7926]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2184,  1.0014,  1.4748,  1.3686, -0.3706],\n",
      "        [ 0.1363,  1.9493, -0.3210, -0.6081,  0.8414],\n",
      "        [-0.2900,  1.7012,  0.3812,  1.0399,  0.1233],\n",
      "        [-1.2792,  1.5924,  0.1110,  0.1893,  0.7732]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1359, -0.3927,  0.4617,  0.2103,  0.5402],\n",
      "        [-0.1258, -0.3266,  0.1564, -0.3614, -0.4017],\n",
      "        [-1.0235, -1.5374, -1.9052, -1.6408, -3.8109],\n",
      "        [-0.2368, -0.0952,  0.1286, -0.3025,  0.1383]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2184,  1.0014,  1.4748,  1.3686, -0.3706],\n",
      "        [ 0.1363,  1.9493, -0.3210, -0.6081,  0.8414],\n",
      "        [-0.2900,  1.7012,  0.3812,  1.0399,  0.1233],\n",
      "        [-1.2792,  1.5924,  0.1110,  0.1893,  0.7732]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4049],\n",
      "        [-0.8223],\n",
      "        [-5.2208],\n",
      "        [ 0.2152]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4573,  1.6278,  1.6664, -1.4642,  0.2882],\n",
      "        [ 0.1489,  0.3395,  0.8701, -1.1487, -0.9936],\n",
      "        [ 1.1716, -0.8435,  0.6696,  1.9891, -0.9342],\n",
      "        [-0.7610, -0.5632,  2.0007, -1.0075,  0.5968]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0862,  0.8791,  0.2413, -0.4363, -0.1532],\n",
      "        [ 0.6026,  0.0618,  0.3405,  0.7330,  0.2634],\n",
      "        [ 0.6151, -0.3630, -0.0852, -0.2256,  0.7279],\n",
      "        [-0.1559, -0.0041,  0.7992,  0.6880,  0.5218]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4573,  1.6278,  1.6664, -1.4642,  0.2882],\n",
      "        [ 0.1489,  0.3395,  0.8701, -1.1487, -0.9936],\n",
      "        [ 1.1716, -0.8435,  0.6696,  1.9891, -0.9342],\n",
      "        [-0.7610, -0.5632,  2.0007, -1.0075,  0.5968]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5535],\n",
      "        [-0.6967],\n",
      "        [-0.1590],\n",
      "        [ 1.3382]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1537, -0.2880, -0.5634,  0.7148,  0.9734],\n",
      "        [-0.5412, -0.1646, -1.7839,  0.3672,  0.7205],\n",
      "        [-0.4670, -0.2692, -0.1418, -0.0323, -2.0830],\n",
      "        [-0.3025,  0.9027,  0.2082,  0.8753, -1.3476]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0848, -1.1812, -1.4180, -0.7372, -2.3934],\n",
      "        [ 0.5272,  0.0997, -0.4245,  0.1212, -0.4408],\n",
      "        [ 0.9244, -0.2049, -0.6513, -0.6337,  0.0425],\n",
      "        [-0.4189, -0.3039, -0.2326, -0.7863, -1.3146]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1537, -0.2880, -0.5634,  0.7148,  0.9734],\n",
      "        [-0.5412, -0.1646, -1.7839,  0.3672,  0.7205],\n",
      "        [-0.4670, -0.2692, -0.1418, -0.0323, -2.0830],\n",
      "        [-0.3025,  0.9027,  0.2082,  0.8753, -1.3476]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4659],\n",
      "        [ 0.1824],\n",
      "        [-0.3522],\n",
      "        [ 0.8872]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0222, -1.3876,  1.5508, -1.3576, -1.1607],\n",
      "        [ 1.2204, -0.4574,  1.7676, -1.9845,  0.8199],\n",
      "        [-0.3156,  1.1074, -0.6413,  0.2448, -0.1555],\n",
      "        [-1.4519,  0.1374,  1.4619, -0.0503,  0.5360]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4040, -0.2070, -1.2739, -0.8468, -1.4847],\n",
      "        [ 0.1051, -0.7941, -1.2826, -0.7362, -0.2485],\n",
      "        [ 0.7684, -1.0016, -1.4610, -0.8730, -0.8263],\n",
      "        [-0.4481, -0.1225,  0.1878, -0.0059, -0.8224]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0222, -1.3876,  1.5508, -1.3576, -1.1607],\n",
      "        [ 1.2204, -0.4574,  1.7676, -1.9845,  0.8199],\n",
      "        [-0.3156,  1.1074, -0.6413,  0.2448, -0.1555],\n",
      "        [-1.4519,  0.1374,  1.4619, -0.0503,  0.5360]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1755],\n",
      "        [-0.5183],\n",
      "        [-0.5000],\n",
      "        [ 0.4678]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0867, -0.0489,  1.2226,  0.3302,  0.3477],\n",
      "        [-0.9009, -0.1964,  0.6443, -2.1267,  0.5572],\n",
      "        [ 1.3549, -0.0135, -1.3010,  0.5339, -0.7218],\n",
      "        [-0.3658,  0.0454,  1.0684,  0.1543,  0.4380]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5143, -1.3810, -0.7899, -1.1406, -1.2333],\n",
      "        [ 0.2892, -0.1155, -0.3223, -0.3646, -0.7709],\n",
      "        [ 0.3025, -1.1719, -1.1268, -0.7176, -0.6093],\n",
      "        [-0.7159, -0.3405, -0.6660, -0.6839, -0.4197]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0867, -0.0489,  1.2226,  0.3302,  0.3477],\n",
      "        [-0.9009, -0.1964,  0.6443, -2.1267,  0.5572],\n",
      "        [ 1.3549, -0.0135, -1.3010,  0.5339, -0.7218],\n",
      "        [-0.3658,  0.0454,  1.0684,  0.1543,  0.4380]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1449],\n",
      "        [-0.0997],\n",
      "        [ 1.9482],\n",
      "        [-0.7546]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1820, -0.2469, -1.2207,  0.2773, -0.2940],\n",
      "        [-0.4497,  0.5924, -0.7112, -0.9614, -0.1226],\n",
      "        [ 1.4573,  2.0363, -0.0811, -1.2829,  0.4212],\n",
      "        [-0.1741,  1.6884,  0.9767, -0.4924, -1.6284]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1291, -1.4519, -0.6914, -0.9554, -1.1865],\n",
      "        [ 0.5108, -1.2473, -0.2478,  0.1409, -0.6348],\n",
      "        [-0.2192, -2.3628, -1.8420, -0.8872, -2.4038],\n",
      "        [-0.0245,  0.0419,  0.2848,  0.3372,  0.0763]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1820, -0.2469, -1.2207,  0.2773, -0.2940],\n",
      "        [-0.4497,  0.5924, -0.7112, -0.9614, -0.1226],\n",
      "        [ 1.4573,  2.0363, -0.0811, -1.2829,  0.4212],\n",
      "        [-0.1741,  1.6884,  0.9767, -0.4924, -1.6284]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3099],\n",
      "        [-0.8501],\n",
      "        [-4.8554],\n",
      "        [ 0.0630]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5320,  1.9737,  0.3841,  1.5790,  1.5045],\n",
      "        [-0.3997, -0.2677, -0.6421,  0.4371,  0.5674],\n",
      "        [-0.6664,  0.9363,  1.1298,  2.3824, -0.5325],\n",
      "        [-1.3476, -0.5645,  1.0312,  0.3931,  0.5450]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0549, -1.5372, -0.2830, -0.6971, -2.4832],\n",
      "        [ 0.5715, -0.4962,  0.2459, -0.2705, -0.0723],\n",
      "        [ 1.1981, -0.5639, -0.3187,  0.6240,  1.3854],\n",
      "        [-0.3130, -0.0702, -0.6798,  0.0400,  0.2075]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5320,  1.9737,  0.3841,  1.5790,  1.5045],\n",
      "        [-0.3997, -0.2677, -0.6421,  0.4371,  0.5674],\n",
      "        [-0.6664,  0.9363,  1.1298,  2.3824, -0.5325],\n",
      "        [-1.3476, -0.5645,  1.0312,  0.3931,  0.5450]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-6.3631],\n",
      "        [-0.4127],\n",
      "        [-0.9375],\n",
      "        [-0.1108]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3778, -0.6053,  0.6704,  0.2311,  1.6316],\n",
      "        [-0.3660,  0.6866, -2.0695, -1.4821, -0.4807],\n",
      "        [-2.9256, -0.2818, -0.3716, -1.3662,  2.2156],\n",
      "        [-0.7874, -0.4247, -0.1641,  0.5722,  0.3759]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3223,  1.8335,  1.1810,  0.7001,  2.5105],\n",
      "        [ 0.6302, -0.2913, -0.2361, -0.6537, -0.6688],\n",
      "        [ 1.6798,  0.3161,  0.4344, -0.0296,  1.7580],\n",
      "        [-0.3688, -0.1529,  0.2908,  0.3080, -0.0539]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3778, -0.6053,  0.6704,  0.2311,  1.6316],\n",
      "        [-0.3660,  0.6866, -2.0695, -1.4821, -0.4807],\n",
      "        [-2.9256, -0.2818, -0.3716, -1.3662,  2.2156],\n",
      "        [-0.7874, -0.4247, -0.1641,  0.5722,  0.3759]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.4402],\n",
      "        [ 1.3484],\n",
      "        [-1.2294],\n",
      "        [ 0.4636]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2488,  1.1974,  0.3431, -0.4753,  0.6200],\n",
      "        [-1.4606,  0.1012,  1.6574,  0.5788, -0.8378],\n",
      "        [ 0.4568,  0.0185, -0.0646,  0.4294,  1.4705],\n",
      "        [-1.2167,  0.4699, -0.9316,  0.2588,  2.0166]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0366,  0.0671,  0.0477,  0.1036,  1.1069],\n",
      "        [-0.5424, -0.8582, -0.7344, -0.7641, -2.1401],\n",
      "        [-0.5970, -2.2261, -2.4899, -2.4674, -4.2504],\n",
      "        [-0.1372,  0.3371, -0.6607,  0.2489, -0.3192]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2488,  1.1974,  0.3431, -0.4753,  0.6200],\n",
      "        [-1.4606,  0.1012,  1.6574,  0.5788, -0.8378],\n",
      "        [ 0.4568,  0.0185, -0.0646,  0.4294,  1.4705],\n",
      "        [-1.2167,  0.4699, -0.9316,  0.2588,  2.0166]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7246],\n",
      "        [ 0.8388],\n",
      "        [-7.4629],\n",
      "        [ 0.3615]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3025, -0.2316, -0.1603, -0.0470, -0.4528],\n",
      "        [-0.7570,  0.3172,  0.2229,  0.4193,  2.3568],\n",
      "        [ 1.8581,  0.9916, -0.6360, -0.9060, -0.7002],\n",
      "        [-0.5337, -0.1295, -0.6221, -0.0561, -1.2184]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4369, -0.0758,  0.5989, -0.4139, -0.9220],\n",
      "        [-0.3271, -1.3726, -1.3676, -1.4255, -1.2559],\n",
      "        [-1.6362, -3.0378, -2.6600, -2.9143, -5.5355],\n",
      "        [-0.3016,  0.2576, -0.2078, -0.1434,  0.0599]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3025, -0.2316, -0.1603, -0.0470, -0.4528],\n",
      "        [-0.7570,  0.3172,  0.2229,  0.4193,  2.3568],\n",
      "        [ 1.8581,  0.9916, -0.6360, -0.9060, -0.7002],\n",
      "        [-0.5337, -0.1295, -0.6221, -0.0561, -1.2184]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9276],\n",
      "        [-4.0501],\n",
      "        [ 2.1556],\n",
      "        [ 0.1919]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6426, -0.0613, -0.1536,  0.2041, -0.9556],\n",
      "        [-0.0171,  1.1002,  1.5068,  1.1527, -1.3014],\n",
      "        [ 0.2641,  0.6447,  1.3540,  2.4377,  1.1160],\n",
      "        [ 1.5323,  0.3812, -1.3881,  0.7318, -1.7722]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3539,  0.1186,  0.2492, -0.2447, -0.7711],\n",
      "        [ 1.3469,  0.9391,  0.6958, -0.0819,  1.4762],\n",
      "        [-0.1316, -0.2500, -0.6224,  0.5241,  0.0921],\n",
      "        [-0.2257, -0.2685,  0.1535, -0.3462,  0.8429]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6426, -0.0613, -0.1536,  0.2041, -0.9556],\n",
      "        [-0.0171,  1.1002,  1.5068,  1.1527, -1.3014],\n",
      "        [ 0.2641,  0.6447,  1.3540,  2.4377,  1.1160],\n",
      "        [ 1.5323,  0.3812, -1.3881,  0.7318, -1.7722]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4139],\n",
      "        [ 0.0430],\n",
      "        [ 0.3418],\n",
      "        [-2.4083]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1673,  0.0370, -0.2801, -1.1434,  1.2340],\n",
      "        [-0.8085, -1.1920,  0.8358,  0.5439,  0.4788],\n",
      "        [ 1.6986,  0.8718,  2.4293,  0.5817,  1.0839],\n",
      "        [ 0.6833, -0.1404, -0.1230, -0.3351,  1.0267]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2345, -0.6081,  0.1670, -0.9609, -1.0721],\n",
      "        [ 0.6707,  0.4078,  0.4950,  0.2938,  0.6455],\n",
      "        [ 0.3031, -0.0015, -0.1065,  0.0014,  0.2495],\n",
      "        [ 0.6078,  0.5202,  0.9219,  1.4678,  1.4714]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1673,  0.0370, -0.2801, -1.1434,  1.2340],\n",
      "        [-0.8085, -1.1920,  0.8358,  0.5439,  0.4788],\n",
      "        [ 1.6986,  0.8718,  2.4293,  0.5817,  1.0839],\n",
      "        [ 0.6833, -0.1404, -0.1230, -0.3351,  1.0267]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3328],\n",
      "        [-0.1458],\n",
      "        [ 0.5261],\n",
      "        [ 1.2478]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5267, -1.4806,  0.8509,  0.6855, -1.2012],\n",
      "        [-1.5672,  1.6144,  1.2133, -1.1471, -0.7362],\n",
      "        [-0.0301,  0.6047,  1.2923,  1.3581, -0.5097],\n",
      "        [-0.0911,  1.2319,  0.0305, -0.9403,  0.8260]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0485, -0.2427, -0.0608, -0.2077, -0.6782],\n",
      "        [ 0.8528,  1.0457,  1.1393,  0.4112,  1.3955],\n",
      "        [ 0.1038,  0.6260, -0.0792,  0.7292, -0.3807],\n",
      "        [-0.2432,  1.3607,  0.1664,  0.3320,  0.8897]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5267, -1.4806,  0.8509,  0.6855, -1.2012],\n",
      "        [-1.5672,  1.6144,  1.2133, -1.1471, -0.7362],\n",
      "        [-0.0301,  0.6047,  1.2923,  1.3581, -0.5097],\n",
      "        [-0.0911,  1.2319,  0.0305, -0.9403,  0.8260]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9543],\n",
      "        [ 0.2349],\n",
      "        [ 1.4575],\n",
      "        [ 2.1263]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0444, -1.1358,  0.6345, -0.0587, -0.1008],\n",
      "        [ 0.8573,  0.9163,  0.2183,  0.4991, -1.0984],\n",
      "        [ 0.3528,  0.0051,  0.4114, -1.8274, -1.7520],\n",
      "        [ 0.1112, -1.2498, -0.9407,  0.3214,  0.3425]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3849, -0.3913, -0.4123, -0.4695, -0.8892],\n",
      "        [ 1.3700,  1.1419,  0.1479,  0.1018,  0.0978],\n",
      "        [-1.0510, -0.1969, -0.8033, -0.8038, -1.2791],\n",
      "        [-0.7286, -0.2229, -0.6356, -0.1961, -1.1643]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0444, -1.1358,  0.6345, -0.0587, -0.1008],\n",
      "        [ 0.8573,  0.9163,  0.2183,  0.4991, -1.0984],\n",
      "        [ 0.3528,  0.0051,  0.4114, -1.8274, -1.7520],\n",
      "        [ 0.1112, -1.2498, -0.9407,  0.3214,  0.3425]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1020],\n",
      "        [ 2.1965],\n",
      "        [ 3.0075],\n",
      "        [ 0.3335]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7602,  0.1502, -0.0606, -0.7743,  1.0637],\n",
      "        [ 0.3848,  0.0597, -0.6495, -0.4290, -0.4169],\n",
      "        [-0.3947, -1.1047,  0.6504,  0.7862,  1.0672],\n",
      "        [ 0.5794, -1.4509, -2.3541, -1.0003, -1.2547]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4260, -0.4686, -0.8774, -0.4197, -0.3910],\n",
      "        [-0.4708, -0.4876, -0.5582, -1.1313, -1.4687],\n",
      "        [-1.0743, -0.6778, -2.3329, -1.5181, -2.8242],\n",
      "        [-0.6542, -0.5025,  0.2151, -0.7351, -0.8619]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7602,  0.1502, -0.0606, -0.7743,  1.0637],\n",
      "        [ 0.3848,  0.0597, -0.6495, -0.4290, -0.4169],\n",
      "        [-0.3947, -1.1047,  0.6504,  0.7862,  1.0672],\n",
      "        [ 0.5794, -1.4509, -2.3541, -1.0003, -1.2547]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4319],\n",
      "        [ 1.2500],\n",
      "        [-4.5518],\n",
      "        [ 1.6604]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5826,  0.6642,  2.9396, -1.0679,  1.0208],\n",
      "        [ 0.5963, -0.3706,  0.2259,  0.8554,  1.4869],\n",
      "        [-0.2507, -0.2754, -0.8229, -0.1002,  0.4199],\n",
      "        [-1.3438, -2.0139, -0.1090,  0.1937,  2.6971]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0387, -0.4148, -0.5774, -0.2409, -0.3841],\n",
      "        [-0.5852, -0.2421, -0.7241, -1.2509, -0.9523],\n",
      "        [ 0.6046,  0.2722,  0.4678,  0.0351,  0.6509],\n",
      "        [-0.9697, -0.0408, -0.2531, -1.2322, -2.2727]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5826,  0.6642,  2.9396, -1.0679,  1.0208],\n",
      "        [ 0.5963, -0.3706,  0.2259,  0.8554,  1.4869],\n",
      "        [-0.2507, -0.2754, -0.8229, -0.1002,  0.4199],\n",
      "        [-1.3438, -2.0139, -0.1090,  0.1937,  2.6971]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0852],\n",
      "        [-2.9087],\n",
      "        [-0.3417],\n",
      "        [-4.9558]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3320, -1.1320,  1.0778,  1.1597, -0.2108],\n",
      "        [-0.7389, -0.1847,  0.1069, -1.5303, -1.0787],\n",
      "        [-0.5792, -0.6646, -2.4905, -0.2320,  2.4731],\n",
      "        [ 0.2838,  0.6596,  0.7200, -0.8009, -0.0790]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9134,  0.6064,  0.7466,  0.2567,  1.4929],\n",
      "        [ 0.7600,  0.0194,  0.7725,  0.8047,  1.7045],\n",
      "        [ 0.3661, -0.7724, -0.7007, -0.6793, -0.0116],\n",
      "        [ 0.7724,  1.1734,  1.5766,  0.4294,  2.2430]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3320, -1.1320,  1.0778,  1.1597, -0.2108],\n",
      "        [-0.7389, -0.1847,  0.1069, -1.5303, -1.0787],\n",
      "        [-0.5792, -0.6646, -2.4905, -0.2320,  2.4731],\n",
      "        [ 0.2838,  0.6596,  0.7200, -0.8009, -0.0790]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4045],\n",
      "        [-3.5526],\n",
      "        [ 2.1755],\n",
      "        [ 1.6074]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4803,  1.8090,  0.8718,  0.5839, -0.2744],\n",
      "        [-1.0298,  0.8349,  0.6513, -0.4710, -2.8429],\n",
      "        [ 0.6154,  0.3174,  0.3361, -0.8721, -0.3161],\n",
      "        [ 1.1601,  1.1162, -0.0844,  0.1103,  0.6420]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2157, -0.1176, -0.5914, -0.3410, -0.1001],\n",
      "        [ 1.5350,  2.2089,  1.8076,  0.8327,  2.0632],\n",
      "        [-1.0112, -0.6776, -1.3107, -1.2487, -1.5157],\n",
      "        [-0.1144,  0.5770,  0.5377, -0.0607,  0.2867]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4803,  1.8090,  0.8718,  0.5839, -0.2744],\n",
      "        [-1.0298,  0.8349,  0.6513, -0.4710, -2.8429],\n",
      "        [ 0.6154,  0.3174,  0.3361, -0.8721, -0.3161],\n",
      "        [ 1.1601,  1.1162, -0.0844,  0.1103,  0.6420]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5807],\n",
      "        [-4.8168],\n",
      "        [ 0.2900],\n",
      "        [ 0.6434]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0172, -0.1922,  0.0595,  0.3735,  1.2730],\n",
      "        [-0.6439, -2.0929, -0.0561,  0.4434, -0.1010],\n",
      "        [ 0.5500, -1.0932,  0.3364,  1.7671,  0.7416],\n",
      "        [ 0.1669,  1.2603,  0.1051,  0.6357, -1.5172]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3534, -0.4907, -0.2728, -0.4620,  0.5420],\n",
      "        [ 0.5720,  0.1257, -0.3626,  0.0736, -0.6289],\n",
      "        [-0.8887, -1.3329, -1.0472, -1.1875, -1.8098],\n",
      "        [-0.4177,  0.1575, -0.3192,  0.3072,  0.0054]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0172, -0.1922,  0.0595,  0.3735,  1.2730],\n",
      "        [-0.6439, -2.0929, -0.0561,  0.4434, -0.1010],\n",
      "        [ 0.5500, -1.0932,  0.3364,  1.7671,  0.7416],\n",
      "        [ 0.1669,  1.2603,  0.1051,  0.6357, -1.5172]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5895],\n",
      "        [-0.5148],\n",
      "        [-2.8244],\n",
      "        [ 0.2824]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3383,  0.8739,  0.6338, -0.7107,  0.4069],\n",
      "        [-0.6746,  1.4828, -2.6254, -0.3568, -0.1696],\n",
      "        [ 1.1721, -2.0756, -0.0465,  2.0544, -1.5107],\n",
      "        [-0.0189,  1.0213, -0.8418,  0.8050, -1.0916]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2596, -0.5319,  0.2450, -0.1203, -0.4244],\n",
      "        [ 0.5419, -0.1139, -0.6549,  0.4058, -0.1153],\n",
      "        [ 0.8392,  0.0815,  0.4053, -0.6178,  0.6999],\n",
      "        [-0.2637, -0.2003, -0.0803, -0.2326, -0.3858]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3383,  0.8739,  0.6338, -0.7107,  0.4069],\n",
      "        [-0.6746,  1.4828, -2.6254, -0.3568, -0.1696],\n",
      "        [ 1.1721, -2.0756, -0.0465,  2.0544, -1.5107],\n",
      "        [-0.0189,  1.0213, -0.8418,  0.8050, -1.0916]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4846],\n",
      "        [ 1.0596],\n",
      "        [-1.5307],\n",
      "        [ 0.1019]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5331,  2.0405, -0.2129,  1.3726, -0.6009],\n",
      "        [-0.1161,  0.3174, -1.3791,  0.8164, -0.6659],\n",
      "        [ 0.7597, -0.6021, -0.2931, -0.2210,  1.0857],\n",
      "        [ 0.5397, -1.5362,  0.7908, -0.8530,  0.2821]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0731, -0.5330,  0.4339, -0.2858, -1.5555],\n",
      "        [-0.3021, -0.5174, -0.1604, -0.7546, -1.1336],\n",
      "        [ 0.6653,  1.1015,  0.3072,  0.3498,  1.1441],\n",
      "        [ 0.1112, -0.0215, -0.0214, -0.3384, -0.4121]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5331,  2.0405, -0.2129,  1.3726, -0.6009],\n",
      "        [-0.1161,  0.3174, -1.3791,  0.8164, -0.6659],\n",
      "        [ 0.7597, -0.6021, -0.2931, -0.2210,  1.0857],\n",
      "        [ 0.5397, -1.5362,  0.7908, -0.8530,  0.2821]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5986],\n",
      "        [ 0.2309],\n",
      "        [ 0.9168],\n",
      "        [ 0.2485]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0315,  2.1207,  0.2903,  1.5984, -1.3605],\n",
      "        [-0.1685,  1.8655, -0.5579, -0.5793,  1.1923],\n",
      "        [-0.5062, -0.8963,  0.4691,  1.2280, -0.3346],\n",
      "        [ 0.0258,  1.1054,  1.9011,  1.4111,  0.1499]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4174, -0.6158, -0.4302, -1.3529, -0.9513],\n",
      "        [ 0.0799, -0.0700, -0.2501, -0.4472, -0.4529],\n",
      "        [ 0.6019,  0.1412, -0.5748, -0.4721, -0.2870],\n",
      "        [-0.0557, -0.4540,  0.0748,  0.6033, -0.0447]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0315,  2.1207,  0.2903,  1.5984, -1.3605],\n",
      "        [-0.1685,  1.8655, -0.5579, -0.5793,  1.1923],\n",
      "        [-0.5062, -0.8963,  0.4691,  1.2280, -0.3346],\n",
      "        [ 0.0258,  1.1054,  1.9011,  1.4111,  0.1499]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2857],\n",
      "        [-0.2854],\n",
      "        [-1.1846],\n",
      "        [ 0.4834]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6145,  2.2631, -0.5200, -0.5158,  1.7930],\n",
      "        [-2.0044,  0.2792,  0.1751,  0.3009, -0.3950],\n",
      "        [ 0.3749,  0.5432,  0.3479,  0.8864, -0.3975],\n",
      "        [-1.0360,  0.7441, -0.4451,  0.0658,  0.1248]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2018,  0.8973,  0.7582, -0.1725,  0.8919],\n",
      "        [ 0.1309, -0.3378, -0.1824, -0.7941,  0.3503],\n",
      "        [ 0.5909, -0.2049, -0.7048, -0.4763,  0.0613],\n",
      "        [-0.7297,  0.2610,  0.4960,  0.4425, -0.0369]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6145,  2.2631, -0.5200, -0.5158,  1.7930],\n",
      "        [-2.0044,  0.2792,  0.1751,  0.3009, -0.3950],\n",
      "        [ 0.3749,  0.5432,  0.3479,  0.8864, -0.3975],\n",
      "        [-1.0360,  0.7441, -0.4451,  0.0658,  0.1248]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5863],\n",
      "        [-0.7659],\n",
      "        [-0.5815],\n",
      "        [ 0.7539]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8388, -1.2238,  0.0259, -0.6816,  0.9928],\n",
      "        [-1.0560, -2.2313,  0.3617,  1.9011,  0.7451],\n",
      "        [ 1.3521, -0.0426,  0.3653, -0.5148,  0.6416],\n",
      "        [ 0.0797, -0.1598,  1.7216, -0.3002, -0.0640]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0316, -1.6111, -1.3582, -1.0686, -1.4085],\n",
      "        [-0.1771,  0.3218,  0.2822, -0.1914, -0.8681],\n",
      "        [ 0.9115, -0.9493, -0.1796,  0.2305, -0.3789],\n",
      "        [ 0.3637, -0.6137, -0.0874, -0.2648, -0.8193]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8388, -1.2238,  0.0259, -0.6816,  0.9928],\n",
      "        [-1.0560, -2.2313,  0.3617,  1.9011,  0.7451],\n",
      "        [ 1.3521, -0.0426,  0.3653, -0.5148,  0.6416],\n",
      "        [ 0.0797, -0.1598,  1.7216, -0.3002, -0.0640]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2930],\n",
      "        [-1.4396],\n",
      "        [ 0.8456],\n",
      "        [ 0.1085]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.1018, -0.0395,  1.1628,  0.3412,  2.0273],\n",
      "        [-1.1426,  0.2652, -0.8746, -2.8782,  1.2320],\n",
      "        [-1.4067,  0.6297, -0.1248,  1.4063,  0.8930],\n",
      "        [ 1.0311,  1.2007, -1.0503,  0.1170,  2.0003]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0992, -1.3738, -0.6677, -1.5976, -2.7199],\n",
      "        [ 0.5028,  0.2018,  0.0735,  0.7141,  1.2328],\n",
      "        [ 0.3301, -0.4528, -0.7081, -0.6956, -0.5073],\n",
      "        [-0.0572,  0.2582, -0.3335, -0.0684, -0.1445]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1018, -0.0395,  1.1628,  0.3412,  2.0273],\n",
      "        [-1.1426,  0.2652, -0.8746, -2.8782,  1.2320],\n",
      "        [-1.4067,  0.6297, -0.1248,  1.4063,  0.8930],\n",
      "        [ 1.0311,  1.2007, -1.0503,  0.1170,  2.0003]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-6.6694],\n",
      "        [-1.1216],\n",
      "        [-2.0922],\n",
      "        [ 0.3043]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9413,  0.3844, -0.5800,  1.3350, -0.3654],\n",
      "        [ 0.2499, -0.0120,  0.4565,  0.5573,  0.6700],\n",
      "        [-0.2426,  0.2654,  0.1932, -0.1467, -0.1790],\n",
      "        [-0.3099, -1.8091, -0.5681,  0.5193,  2.1791]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.8596,  2.2933,  1.8050,  1.0989,  2.1171],\n",
      "        [ 0.7632,  0.7797,  1.0695,  0.6270,  1.6031],\n",
      "        [ 0.2197,  0.6587,  1.1775, -0.4644,  1.1835],\n",
      "        [ 0.0565, -0.0416, -0.1313,  0.1575, -0.1785]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9413,  0.3844, -0.5800,  1.3350, -0.3654],\n",
      "        [ 0.2499, -0.0120,  0.4565,  0.5573,  0.6700],\n",
      "        [-0.2426,  0.2654,  0.1932, -0.1467, -0.1790],\n",
      "        [-0.3099, -1.8091, -0.5681,  0.5193,  2.1791]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2224],\n",
      "        [ 2.0931],\n",
      "        [ 0.2053],\n",
      "        [-0.1748]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7256,  0.1789, -1.5908,  0.9002,  0.3984],\n",
      "        [-0.1035, -0.1842, -1.1712,  0.9192, -1.0390],\n",
      "        [-0.4838,  1.0910, -0.0146,  1.4136,  0.5395],\n",
      "        [-0.7572, -1.6717, -1.6147,  0.8527,  0.7169]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.8890,  2.3293,  2.3912,  1.8858,  3.1971],\n",
      "        [-0.1515, -0.4399, -0.6199, -1.1818, -0.0814],\n",
      "        [ 0.4163, -1.3580, -0.0918, -0.1818,  0.1414],\n",
      "        [-0.2969, -0.2839,  0.4064,  0.2828, -0.4585]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7256,  0.1789, -1.5908,  0.9002,  0.3984],\n",
      "        [-0.1035, -0.1842, -1.1712,  0.9192, -1.0390],\n",
      "        [-0.4838,  1.0910, -0.0146,  1.4136,  0.5395],\n",
      "        [-0.7572, -1.6717, -1.6147,  0.8527,  0.7169]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9548],\n",
      "        [-0.1791],\n",
      "        [-1.8623],\n",
      "        [-0.0445]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9778, -0.1004,  0.8396,  1.9292,  0.3764],\n",
      "        [ 1.4528,  1.9682,  0.9180, -0.1433, -1.1762],\n",
      "        [-1.8386,  1.5775,  0.7709,  1.3051,  0.9995],\n",
      "        [-1.1630,  0.3836,  0.7478,  1.1928,  0.0427]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0884, -0.4310, -0.7695,  0.1097, -0.4321],\n",
      "        [ 0.1285,  0.1400,  0.3124,  0.3847,  0.3533],\n",
      "        [ 0.8102, -0.2039,  0.6483,  0.2271,  0.9487],\n",
      "        [-0.6151, -0.2760, -0.1418,  0.0028,  0.3304]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9778, -0.1004,  0.8396,  1.9292,  0.3764],\n",
      "        [ 1.4528,  1.9682,  0.9180, -0.1433, -1.1762],\n",
      "        [-1.8386,  1.5775,  0.7709,  1.3051,  0.9995],\n",
      "        [-1.1630,  0.3836,  0.7478,  1.1928,  0.0427]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4672],\n",
      "        [ 0.2784],\n",
      "        [-0.0668],\n",
      "        [ 0.5209]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0061,  0.0462,  0.7565, -0.0843,  0.2233],\n",
      "        [-0.2116,  0.1031,  0.6625,  1.5790,  0.2828],\n",
      "        [-0.3818, -0.0851,  1.3851,  0.3177,  0.3786],\n",
      "        [-0.3639,  0.8753,  0.5527,  0.0739,  1.0272]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1073,  0.2703, -0.4754, -0.1367, -0.1106],\n",
      "        [ 0.5626,  0.1409,  0.0981,  0.4829, -0.0140],\n",
      "        [ 1.0159, -0.2825,  0.1558,  0.2525,  1.1822],\n",
      "        [-0.0971, -0.3340,  0.0886, -0.1050,  0.1088]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0061,  0.0462,  0.7565, -0.0843,  0.2233],\n",
      "        [-0.2116,  0.1031,  0.6625,  1.5790,  0.2828],\n",
      "        [-0.3818, -0.0851,  1.3851,  0.3177,  0.3786],\n",
      "        [-0.3639,  0.8753,  0.5527,  0.0739,  1.0272]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2524],\n",
      "        [ 0.7191],\n",
      "        [ 0.3798],\n",
      "        [-0.1041]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8726, -0.6775, -0.1461, -0.1871, -0.4416],\n",
      "        [ 0.4729, -0.0999,  1.8427, -0.0903, -1.3584],\n",
      "        [ 0.0543, -0.0571, -0.3015, -0.7614,  0.4709],\n",
      "        [-2.0703, -0.9692,  1.6117,  0.9395,  0.8308]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5543, -0.6055, -0.2835, -0.3339, -0.0135],\n",
      "        [ 0.0214,  0.2432,  0.7301,  0.3038,  0.5291],\n",
      "        [ 0.4492,  0.0753,  0.4726, -0.0456,  0.4225],\n",
      "        [-0.8357,  0.1876,  0.4893,  0.1821, -0.2208]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8726, -0.6775, -0.1461, -0.1871, -0.4416],\n",
      "        [ 0.4729, -0.0999,  1.8427, -0.0903, -1.3584],\n",
      "        [ 0.0543, -0.0571, -0.3015, -0.7614,  0.4709],\n",
      "        [-2.0703, -0.9692,  1.6117,  0.9395,  0.8308]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0038],\n",
      "        [ 0.5850],\n",
      "        [ 0.1113],\n",
      "        [ 2.3245]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1204, -0.8964,  0.0485,  0.3026,  0.0167],\n",
      "        [-0.4503, -0.7844,  1.8592, -0.2650, -0.4435],\n",
      "        [ 1.5506,  0.5445, -0.9345, -0.3200,  1.0381],\n",
      "        [-0.8123, -0.2387, -1.3187,  0.4999,  2.1365]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2305, -0.1753, -0.7853, -0.3038, -0.3561],\n",
      "        [ 0.0466,  1.3789,  0.3903,  0.2211, -0.0022],\n",
      "        [ 0.4830, -0.5794, -0.6666, -0.8767,  0.1136],\n",
      "        [-1.2103, -0.6483, -1.0768, -0.9646, -1.8061]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1204, -0.8964,  0.0485,  0.3026,  0.0167],\n",
      "        [-0.4503, -0.7844,  1.8592, -0.2650, -0.4435],\n",
      "        [ 1.5506,  0.5445, -0.9345, -0.3200,  1.0381],\n",
      "        [-0.8123, -0.2387, -1.3187,  0.4999,  2.1365]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0490],\n",
      "        [-0.4346],\n",
      "        [ 1.4547],\n",
      "        [-1.7830]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7959, -1.8787,  0.7177,  0.3862,  1.2389],\n",
      "        [-1.1953,  0.2156,  0.1162,  1.2271, -0.3656],\n",
      "        [-0.0196,  0.4793,  0.6186,  0.0512, -1.0482],\n",
      "        [ 0.2926,  1.1675, -0.3035,  0.7000,  0.1838]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4774,  0.0722,  0.1764, -0.0577, -0.5250],\n",
      "        [ 0.1363,  0.8321,  0.2399,  0.5791,  1.1168],\n",
      "        [ 0.0205, -1.0344, -0.3414, -1.1803, -1.7227],\n",
      "        [-0.3305, -0.0455,  0.0600,  0.2242, -0.9976]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7959, -1.8787,  0.7177,  0.3862,  1.2389],\n",
      "        [-1.1953,  0.2156,  0.1162,  1.2271, -0.3656],\n",
      "        [-0.0196,  0.4793,  0.6186,  0.0512, -1.0482],\n",
      "        [ 0.2926,  1.1675, -0.3035,  0.7000,  0.1838]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0618],\n",
      "        [ 0.3467],\n",
      "        [ 1.0380],\n",
      "        [-0.1944]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4274, -0.8126,  1.6962, -0.3503, -1.8786],\n",
      "        [-0.3169,  0.5774, -0.5648,  0.6198, -0.0563],\n",
      "        [-0.6041,  2.0053, -0.7291, -1.0685, -0.2130],\n",
      "        [ 0.8486,  1.3874,  1.0426,  0.9310, -0.4863]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2110,  0.1135, -0.1368,  0.1098,  0.5374],\n",
      "        [ 0.1500,  0.5969,  0.0566,  0.9427,  0.6325],\n",
      "        [-0.9514, -1.0873, -1.3102, -1.5267, -1.7608],\n",
      "        [ 0.3493,  0.1080,  0.1193, -0.3851, -0.8778]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4274, -0.8126,  1.6962, -0.3503, -1.8786],\n",
      "        [-0.3169,  0.5774, -0.5648,  0.6198, -0.0563],\n",
      "        [-0.6041,  2.0053, -0.7291, -1.0685, -0.2130],\n",
      "        [ 0.8486,  1.3874,  1.0426,  0.9310, -0.4863]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0711],\n",
      "        [ 0.8139],\n",
      "        [ 1.3560],\n",
      "        [ 0.6390]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1461,  0.3004,  1.4205,  0.1824, -0.2018],\n",
      "        [ 1.7361, -0.5552, -0.9596,  0.6406, -0.7907],\n",
      "        [ 1.1055,  0.0303,  1.6395, -0.2852,  0.5058],\n",
      "        [ 0.9945, -1.7971, -0.0770, -1.4897,  1.1172]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0196,  0.9889,  0.8670,  0.3731,  1.2909],\n",
      "        [-0.2333, -0.2955, -0.0147,  0.3327,  0.4599],\n",
      "        [-1.0035, -1.1795, -1.3249, -1.2849, -2.9569],\n",
      "        [-0.0292, -0.6393,  0.0260, -0.1490, -1.2914]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1461,  0.3004,  1.4205,  0.1824, -0.2018],\n",
      "        [ 1.7361, -0.5552, -0.9596,  0.6406, -0.7907],\n",
      "        [ 1.1055,  0.0303,  1.6395, -0.2852,  0.5058],\n",
      "        [ 0.9945, -1.7971, -0.0770, -1.4897,  1.1172]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3333],\n",
      "        [-0.3774],\n",
      "        [-4.4464],\n",
      "        [-0.1029]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4040,  1.6458,  1.2675,  0.9860,  1.2640],\n",
      "        [-0.2911,  0.4543, -0.0225,  0.4367,  0.3671],\n",
      "        [ 1.6266,  0.8603, -0.4558,  0.9435, -0.0884],\n",
      "        [ 0.1514,  0.4997,  0.9494, -1.0743,  0.2222]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1042, -0.6926, -0.2675,  0.0752,  0.0272],\n",
      "        [-0.1954,  0.5946,  0.7356,  0.3435,  0.4446],\n",
      "        [ 1.2781,  0.5312,  0.7360, -0.3195,  1.2871],\n",
      "        [-0.4457, -0.4393, -0.0262, -0.8857, -0.3325]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4040,  1.6458,  1.2675,  0.9860,  1.2640],\n",
      "        [-0.2911,  0.4543, -0.0225,  0.4367,  0.3671],\n",
      "        [ 1.6266,  0.8603, -0.4558,  0.9435, -0.0884],\n",
      "        [ 0.1514,  0.4997,  0.9494, -1.0743,  0.2222]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2241],\n",
      "        [ 0.6237],\n",
      "        [ 1.7853],\n",
      "        [ 0.5657]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9264,  0.0080,  0.9290,  0.2163, -1.2992],\n",
      "        [-0.2640, -2.2843,  0.2011,  0.2328,  1.7759],\n",
      "        [ 0.3749, -0.8311, -0.3266,  0.7693, -0.3535],\n",
      "        [ 0.2775,  0.9652,  0.2901,  0.0064,  1.6735]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5651,  0.6768,  0.6906,  0.3947,  1.1397],\n",
      "        [-0.0843,  0.8683,  0.3323,  1.3733,  1.4246],\n",
      "        [ 0.0689, -0.3635, -1.5010, -0.7791, -0.8159],\n",
      "        [-0.1705,  0.4920, -0.3548, -0.0502, -1.2999]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9264,  0.0080,  0.9290,  0.2163, -1.2992],\n",
      "        [-0.2640, -2.2843,  0.2011,  0.2328,  1.7759],\n",
      "        [ 0.3749, -0.8311, -0.3266,  0.7693, -0.3535],\n",
      "        [ 0.2775,  0.9652,  0.2901,  0.0064,  1.6735]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2718],\n",
      "        [ 0.9551],\n",
      "        [ 0.5072],\n",
      "        [-1.8511]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.2438, -0.3995,  0.2250,  1.3497,  0.5052],\n",
      "        [-1.1201,  1.3612,  1.3365,  1.3476,  1.6763],\n",
      "        [-0.3433, -0.2962,  1.0511,  1.0189,  1.4399],\n",
      "        [-2.0216,  1.6565,  0.6267, -0.0716, -0.6131]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1792,  0.8199,  0.2582,  1.3892,  1.8071],\n",
      "        [-0.2020,  0.7266,  0.3807,  0.2678,  0.6028],\n",
      "        [-0.3633, -1.0516, -1.5363, -1.0545, -1.4590],\n",
      "        [-0.4990,  0.2694, -0.7203, -0.1257, -0.2878]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.2438, -0.3995,  0.2250,  1.3497,  0.5052],\n",
      "        [-1.1201,  1.3612,  1.3365,  1.3476,  1.6763],\n",
      "        [-0.3433, -0.2962,  1.0511,  1.0189,  1.4399],\n",
      "        [-2.0216,  1.6565,  0.6267, -0.0716, -0.6131]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.1643],\n",
      "        [ 3.0955],\n",
      "        [-4.3539],\n",
      "        [ 1.1893]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2472,  0.3118,  0.4895,  1.3507,  0.9610],\n",
      "        [-0.4587, -0.2546, -1.8240,  1.0900,  1.5625],\n",
      "        [ 0.1564, -1.6757,  0.6475,  0.6535, -0.6580],\n",
      "        [-0.9417,  0.5818, -0.4764,  1.0005, -0.3470]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7079, -1.4062, -2.0576, -1.4312, -3.2857],\n",
      "        [-0.9804, -1.6873, -0.8860, -1.7855, -2.0495],\n",
      "        [ 0.9381,  1.3453,  1.5682,  0.8922,  1.8216],\n",
      "        [-0.4771,  0.2342, -0.3119, -0.3371, -0.5765]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2472,  0.3118,  0.4895,  1.3507,  0.9610],\n",
      "        [-0.4587, -0.2546, -1.8240,  1.0900,  1.5625],\n",
      "        [ 0.1564, -1.6757,  0.6475,  0.6535, -0.6580],\n",
      "        [-0.9417,  0.5818, -0.4764,  1.0005, -0.3470]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-7.4194],\n",
      "        [-2.6531],\n",
      "        [-1.7077],\n",
      "        [ 0.5969]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.0914,  1.8116, -0.3485, -1.8785, -0.5332],\n",
      "        [ 0.3457,  0.3018, -2.0592,  0.8677,  0.9533],\n",
      "        [-0.9134,  0.9426, -0.1447,  2.0287, -0.7691],\n",
      "        [ 0.0107,  0.4646,  0.5186,  0.8290,  1.9659]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3765,  1.6583,  1.3080,  0.6974,  2.0114],\n",
      "        [-0.3121,  0.0780,  0.5973,  0.3264,  0.1584],\n",
      "        [ 2.2246,  2.1469,  1.1369,  2.2249,  2.8078],\n",
      "        [-0.5077, -0.8750, -0.1905, -0.3496, -0.8400]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.0914,  1.8116, -0.3485, -1.8785, -0.5332],\n",
      "        [ 0.3457,  0.3018, -2.0592,  0.8677,  0.9533],\n",
      "        [-0.9134,  0.9426, -0.1447,  2.0287, -0.7691],\n",
      "        [ 0.0107,  0.4646,  0.5186,  0.8290,  1.9659]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0446],\n",
      "        [-0.8802],\n",
      "        [ 2.1811],\n",
      "        [-2.4518]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0201,  1.8697, -0.9281,  0.0699, -1.4559],\n",
      "        [ 1.1525, -1.0658, -2.0031,  0.1099,  1.1969],\n",
      "        [-1.4608,  0.0774, -0.2958,  2.2580, -0.6184],\n",
      "        [ 1.7351,  3.1668, -1.2388, -0.1403, -0.2818]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6028,  0.4972,  0.0530,  0.5590,  0.2942],\n",
      "        [ 0.2297,  0.8608,  1.0151,  0.9092,  0.8071],\n",
      "        [ 0.1966, -0.0044, -0.2135,  0.1499,  0.6262],\n",
      "        [ 0.1155, -0.2475, -0.0480,  0.1665,  0.7345]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0201,  1.8697, -0.9281,  0.0699, -1.4559],\n",
      "        [ 1.1525, -1.0658, -2.0031,  0.1099,  1.1969],\n",
      "        [-1.4608,  0.0774, -0.2958,  2.2580, -0.6184],\n",
      "        [ 1.7351,  3.1668, -1.2388, -0.1403, -0.2818]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4789],\n",
      "        [-1.6201],\n",
      "        [-0.2733],\n",
      "        [-0.7543]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6131, -0.0643,  0.1978,  0.5457,  0.3156],\n",
      "        [-0.6930,  0.3654, -0.3149, -0.1839, -0.1203],\n",
      "        [-0.4413,  0.6056,  1.7894, -0.2422,  0.1510],\n",
      "        [ 0.0102,  0.0141,  0.8914, -0.2883, -0.1419]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2904,  0.4234,  0.5023,  0.5262,  0.8542],\n",
      "        [ 0.9299,  2.1598,  1.4103,  0.4701,  1.1301],\n",
      "        [-0.1980,  0.1096, -1.0356, -0.6560, -0.7616],\n",
      "        [ 0.1869, -0.0728, -0.5450, -0.0498,  0.1541]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6131, -0.0643,  0.1978,  0.5457,  0.3156],\n",
      "        [-0.6930,  0.3654, -0.3149, -0.1839, -0.1203],\n",
      "        [-0.4413,  0.6056,  1.7894, -0.2422,  0.1510],\n",
      "        [ 0.0102,  0.0141,  0.8914, -0.2883, -0.1419]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8069],\n",
      "        [-0.5217],\n",
      "        [-1.6555],\n",
      "        [-0.4925]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6173,  0.1120,  0.0280,  0.0082, -0.4787],\n",
      "        [ 0.5416, -0.6219, -0.3910, -0.2357, -0.8516],\n",
      "        [-0.2401,  1.7035, -0.4006,  1.3785,  0.7826],\n",
      "        [ 0.3708,  0.4681, -0.9948, -2.6213, -0.3953]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4801,  0.4205, -0.2234,  0.5234,  0.1956],\n",
      "        [ 0.4398,  1.1013,  1.3917,  1.6706,  2.1130],\n",
      "        [ 0.8243,  1.2782,  0.9522,  0.9939,  1.5071],\n",
      "        [-0.0096, -0.7964, -0.2998,  0.2402,  0.0962]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6173,  0.1120,  0.0280,  0.0082, -0.4787],\n",
      "        [ 0.5416, -0.6219, -0.3910, -0.2357, -0.8516],\n",
      "        [-0.2401,  1.7035, -0.4006,  1.3785,  0.7826],\n",
      "        [ 0.3708,  0.4681, -0.9948, -2.6213, -0.3953]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3448],\n",
      "        [-3.1840],\n",
      "        [ 4.1475],\n",
      "        [-0.7457]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4827,  0.8951,  1.0872,  1.0938, -1.7455],\n",
      "        [-0.7683,  0.1047, -2.0654,  1.1930,  1.5645],\n",
      "        [ 0.0516,  0.9375,  0.5916,  1.4038, -0.8939],\n",
      "        [ 1.5613, -1.1459, -0.3690,  0.8548,  1.6254]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4794,  0.8348,  0.4533,  0.0865,  0.9126],\n",
      "        [ 1.6286,  2.3401,  1.3120,  1.5127,  2.5515],\n",
      "        [-0.5901, -1.2098, -1.4586, -1.2438, -1.8819],\n",
      "        [ 0.2899, -0.5075,  0.0750, -0.5415, -0.8031]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4827,  0.8951,  1.0872,  1.0938, -1.7455],\n",
      "        [-0.7683,  0.1047, -2.0654,  1.1930,  1.5645],\n",
      "        [ 0.0516,  0.9375,  0.5916,  1.4038, -0.8939],\n",
      "        [ 1.5613, -1.1459, -0.3690,  0.8548,  1.6254]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9690],\n",
      "        [ 2.0807],\n",
      "        [-2.0916],\n",
      "        [-0.7618]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2512,  2.6954,  0.3359, -0.1298, -0.3050],\n",
      "        [-0.2422,  0.8046,  0.5191, -0.2536, -1.1616],\n",
      "        [ 0.1763, -0.9015,  0.0545,  0.3790, -0.4617],\n",
      "        [-1.8688, -0.5980, -0.8048,  2.3898, -0.0098]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1744,  0.5329,  0.9747,  0.2657,  0.8895],\n",
      "        [-0.0878,  0.0903, -0.1805, -0.2219, -0.1100],\n",
      "        [ 0.5692, -0.5078, -0.9517, -0.5460, -0.8517],\n",
      "        [ 0.1921, -0.0544, -0.6267, -0.0757, -0.3087]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2512,  2.6954,  0.3359, -0.1298, -0.3050],\n",
      "        [-0.2422,  0.8046,  0.5191, -0.2536, -1.1616],\n",
      "        [ 0.1763, -0.9015,  0.0545,  0.3790, -0.4617],\n",
      "        [-1.8688, -0.5980, -0.8048,  2.3898, -0.0098]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.9274e+00],\n",
      "        [ 1.8426e-01],\n",
      "        [ 6.9258e-01],\n",
      "        [ 2.9366e-05]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8604, -0.0466,  0.5793,  0.1394, -1.3709],\n",
      "        [ 0.4099, -2.5669, -0.8059,  1.3481,  2.1903],\n",
      "        [-0.6071, -0.8440, -1.3813, -0.8711, -0.3161],\n",
      "        [ 0.1940, -0.4623,  0.0980,  0.4557,  1.7913]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1578, -1.0348, -0.5405, -0.6727, -2.0472],\n",
      "        [ 0.6345, -0.1751, -0.2457, -0.3633,  0.5354],\n",
      "        [-0.7474, -1.0829, -0.2388, -1.0097, -1.2630],\n",
      "        [-0.1115, -1.0473, -0.4946, -0.2639, -0.9865]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8604, -0.0466,  0.5793,  0.1394, -1.3709],\n",
      "        [ 0.4099, -2.5669, -0.8059,  1.3481,  2.1903],\n",
      "        [-0.6071, -0.8440, -1.3813, -0.8711, -0.3161],\n",
      "        [ 0.1940, -0.4623,  0.0980,  0.4557,  1.7913]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4517],\n",
      "        [ 1.5907],\n",
      "        [ 2.9764],\n",
      "        [-1.4734]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9564, -0.4301,  0.9148, -0.0538,  1.7058],\n",
      "        [-0.6489, -1.3244, -0.1723,  1.5536,  1.2228],\n",
      "        [-0.3776,  0.5121,  2.1154,  1.0515,  0.8101],\n",
      "        [-0.6873, -0.1253,  0.3541, -0.4384,  1.4093]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5153, -1.0427, -1.7127, -1.6753, -2.2581],\n",
      "        [-0.1178, -0.8954, -0.1117, -0.6578, -1.8377],\n",
      "        [-0.7875, -1.3889, -1.8305, -1.5665, -3.0210],\n",
      "        [ 0.6826,  0.1166, -0.1509, -0.4780, -0.1314]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9564, -0.4301,  0.9148, -0.0538,  1.7058],\n",
      "        [-0.6489, -1.3244, -0.1723,  1.5536,  1.2228],\n",
      "        [-0.3776,  0.5121,  2.1154,  1.0515,  0.8101],\n",
      "        [-0.6873, -0.1253,  0.3541, -0.4384,  1.4093]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.3872],\n",
      "        [-1.9875],\n",
      "        [-8.3807],\n",
      "        [-0.5128]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1440, -1.5460, -0.0077, -1.5950,  2.5570],\n",
      "        [ 1.3827,  1.8401,  0.1006,  0.1122, -1.5026],\n",
      "        [-0.4198,  0.1809,  2.1493, -1.9200, -0.1839],\n",
      "        [-0.9148,  1.4064,  3.3220, -0.2393,  0.0355]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5477,  1.2243,  2.2602,  0.4839,  1.3138],\n",
      "        [ 0.4739,  0.6537,  0.0232,  0.0892,  0.7377],\n",
      "        [ 1.3180,  2.0483,  1.9036,  1.0162,  2.5899],\n",
      "        [ 1.0339, -0.7348,  0.0872,  0.6295, -0.1665]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1440, -1.5460, -0.0077, -1.5950,  2.5570],\n",
      "        [ 1.3827,  1.8401,  0.1006,  0.1122, -1.5026],\n",
      "        [-0.4198,  0.1809,  2.1493, -1.9200, -0.1839],\n",
      "        [-0.9148,  1.4064,  3.3220, -0.2393,  0.0355]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5984],\n",
      "        [ 0.7619],\n",
      "        [ 1.4813],\n",
      "        [-1.8461]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0657, -0.7854, -0.2571, -1.7896, -1.0189],\n",
      "        [ 2.7204,  0.5813, -0.0800,  1.7329,  1.4646],\n",
      "        [ 0.4141,  0.4061,  0.1933, -0.4308,  0.6809],\n",
      "        [-0.9153,  0.0237, -0.0012, -0.9403,  0.1981]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0057,  0.4304,  1.1020,  0.1355,  0.4726],\n",
      "        [-0.1718, -0.4289,  0.1729, -0.0242,  0.1193],\n",
      "        [ 1.0166,  0.1856,  1.4639,  0.7728,  1.4589],\n",
      "        [ 0.5259,  0.6707,  1.0010, -0.2883,  1.7375]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0657, -0.7854, -0.2571, -1.7896, -1.0189],\n",
      "        [ 2.7204,  0.5813, -0.0800,  1.7329,  1.4646],\n",
      "        [ 0.4141,  0.4061,  0.1933, -0.4308,  0.6809],\n",
      "        [-0.9153,  0.0237, -0.0012, -0.9403,  0.1981]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3394],\n",
      "        [-0.5977],\n",
      "        [ 1.4398],\n",
      "        [ 0.1485]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4226,  0.3458, -0.9924, -0.0595,  2.0376],\n",
      "        [ 0.9572,  1.2396,  0.0628, -0.5659,  0.0589],\n",
      "        [ 0.6132,  0.3799, -0.0663,  2.6418,  1.1935],\n",
      "        [-2.0049,  0.3384,  1.4074,  1.4229,  1.8281]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7623,  0.4610,  0.8351,  0.9725,  1.7354],\n",
      "        [ 0.1466,  0.0228,  0.1990,  0.2419,  0.1297],\n",
      "        [ 0.4488, -0.1536,  0.9258,  0.6944,  0.7991],\n",
      "        [ 0.5209, -0.1243,  0.2780, -0.1196,  0.7369]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4226,  0.3458, -0.9924, -0.0595,  2.0376],\n",
      "        [ 0.9572,  1.2396,  0.0628, -0.5659,  0.0589],\n",
      "        [ 0.6132,  0.3799, -0.0663,  2.6418,  1.1935],\n",
      "        [-2.0049,  0.3384,  1.4074,  1.4229,  1.8281]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.1310],\n",
      "        [ 0.0519],\n",
      "        [ 2.9436],\n",
      "        [ 0.4818]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0226,  1.4387, -0.3466,  0.5009,  1.0411],\n",
      "        [ 0.8377,  0.3189,  0.2542, -0.4564,  1.0969],\n",
      "        [-0.7577,  1.0721,  2.0709,  0.8139,  0.6949],\n",
      "        [-1.4330, -0.4764,  0.4106, -0.3549,  1.1037]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4735, -0.4949, -0.9475, -0.1731, -1.9933],\n",
      "        [ 0.0949,  0.0499, -0.9946, -0.3593, -0.1849],\n",
      "        [-0.2698, -0.6877, -1.0082, -1.1015, -1.4211],\n",
      "        [ 0.7052, -1.1114,  0.4474, -0.6536, -0.6622]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0226,  1.4387, -0.3466,  0.5009,  1.0411],\n",
      "        [ 0.8377,  0.3189,  0.2542, -0.4564,  1.0969],\n",
      "        [-0.7577,  1.0721,  2.0709,  0.8139,  0.6949],\n",
      "        [-1.4330, -0.4764,  0.4106, -0.3549,  1.1037]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5349],\n",
      "        [-0.1963],\n",
      "        [-4.5048],\n",
      "        [-0.7962]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5393, -0.0085, -1.0591,  0.1743, -0.4051],\n",
      "        [ 0.2008,  0.1862,  0.8992,  0.5704,  0.2990],\n",
      "        [-0.1441,  0.4340,  0.5549, -1.2222,  0.2490],\n",
      "        [ 1.2044,  0.1815, -0.9200,  0.6398,  1.5179]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6496,  0.5103,  1.1397,  0.7066,  1.1844],\n",
      "        [ 0.0214, -0.2675, -0.1921, -1.2531, -0.8523],\n",
      "        [ 0.7426,  1.4969,  1.2900,  0.9363,  1.3530],\n",
      "        [ 0.1427, -0.3684,  0.5061, -0.9372, -0.2809]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5393, -0.0085, -1.0591,  0.1743, -0.4051],\n",
      "        [ 0.2008,  0.1862,  0.8992,  0.5704,  0.2990],\n",
      "        [-0.1441,  0.4340,  0.5549, -1.2222,  0.2490],\n",
      "        [ 1.2044,  0.1815, -0.9200,  0.6398,  1.5179]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2177],\n",
      "        [-1.1878],\n",
      "        [ 0.4511],\n",
      "        [-1.3867]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0883, -1.1630,  0.0660,  1.0947, -0.5089],\n",
      "        [ 0.0127, -0.7749,  0.3019,  0.5042,  0.2090],\n",
      "        [-1.5725,  0.0819, -1.1969,  0.3568,  0.7727],\n",
      "        [-2.0308,  1.0039,  1.1666,  0.7126, -0.3543]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6739,  0.9140,  1.5969,  0.4876,  1.4848],\n",
      "        [-0.1490, -0.3900,  0.3583,  0.2120,  0.3434],\n",
      "        [ 0.5271,  1.5498,  1.6833,  0.7974,  1.9267],\n",
      "        [ 0.6080,  0.4374,  0.3656,  0.7554,  1.1981]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0883, -1.1630,  0.0660,  1.0947, -0.5089],\n",
      "        [ 0.0127, -0.7749,  0.3019,  0.5042,  0.2090],\n",
      "        [-1.5725,  0.0819, -1.1969,  0.3568,  0.7727],\n",
      "        [-2.0308,  1.0039,  1.1666,  0.7126, -0.3543]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2388],\n",
      "        [ 0.5872],\n",
      "        [-0.9436],\n",
      "        [-0.2554]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3014e+00, -5.5254e-05,  5.1807e-01, -3.6150e-01,  1.6560e+00],\n",
      "        [-1.3275e-01,  7.5872e-01,  1.3229e+00,  9.0542e-01,  8.3974e-01],\n",
      "        [-1.1825e+00, -1.2582e+00,  2.1860e+00,  1.0512e+00, -1.1228e+00],\n",
      "        [ 7.7049e-01, -5.3107e-01,  1.5038e+00, -2.1397e-02,  1.1228e+00]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0472,  1.3738,  1.3166,  1.2533,  1.6567],\n",
      "        [ 0.7205, -0.5786, -0.2690,  0.1377,  0.0829],\n",
      "        [-0.0694,  0.1639,  0.0696, -0.4159, -0.3857],\n",
      "        [ 0.5457,  0.3453,  0.1443,  0.6260,  1.1909]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3014e+00, -5.5254e-05,  5.1807e-01, -3.6150e-01,  1.6560e+00],\n",
      "        [-1.3275e-01,  7.5872e-01,  1.3229e+00,  9.0542e-01,  8.3974e-01],\n",
      "        [-1.1825e+00, -1.2582e+00,  2.1860e+00,  1.0512e+00, -1.1228e+00],\n",
      "        [ 7.7049e-01, -5.3107e-01,  1.5038e+00, -2.1397e-02,  1.1228e+00]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.3351],\n",
      "        [-0.6962],\n",
      "        [ 0.0239],\n",
      "        [ 1.7779]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8881,  1.0326, -0.7870,  0.6024,  1.3285],\n",
      "        [-1.4305,  0.4075, -0.4807, -1.7096,  1.6362],\n",
      "        [-0.0192,  0.8436, -0.6400,  0.9460, -0.5442],\n",
      "        [ 0.4240,  0.1110,  0.6152, -0.0394,  0.1401]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0334, -1.0991, -0.9444, -1.4238, -1.2239],\n",
      "        [ 0.1023, -0.7871, -0.6967,  0.4395, -0.1745],\n",
      "        [-0.2693,  0.7754, -0.1546, -0.0650,  0.2208],\n",
      "        [ 0.0286,  0.0513, -0.0401, -0.0603,  0.0350]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8881,  1.0326, -0.7870,  0.6024,  1.3285],\n",
      "        [-1.4305,  0.4075, -0.4807, -1.7096,  1.6362],\n",
      "        [-0.0192,  0.8436, -0.6400,  0.9460, -0.5442],\n",
      "        [ 0.4240,  0.1110,  0.6152, -0.0394,  0.1401]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9051],\n",
      "        [-1.1690],\n",
      "        [ 0.5766],\n",
      "        [ 0.0005]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1943, -2.3360,  0.1563, -0.4371,  0.0870],\n",
      "        [ 0.8317, -2.8293, -0.4993,  2.8871, -1.8626],\n",
      "        [ 0.9350,  1.0439, -1.2754,  1.0885, -1.6261],\n",
      "        [ 0.4845,  0.1047, -1.7059,  1.3663,  1.6006]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8752,  0.8825,  1.1880,  1.1248,  1.0261],\n",
      "        [ 0.3660,  0.9168,  0.3215,  0.5103,  0.4358],\n",
      "        [-0.2539, -0.2880,  0.4565,  0.1753,  0.0867],\n",
      "        [ 0.1857, -0.4072,  0.1514, -0.5685,  0.0557]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1943, -2.3360,  0.1563, -0.4371,  0.0870],\n",
      "        [ 0.8317, -2.8293, -0.4993,  2.8871, -1.8626],\n",
      "        [ 0.9350,  1.0439, -1.2754,  1.0885, -1.6261],\n",
      "        [ 0.4845,  0.1047, -1.7059,  1.3663,  1.6006]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1083],\n",
      "        [-1.7882],\n",
      "        [-1.0704],\n",
      "        [-0.8984]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3122,  1.3302,  1.8897,  2.9296,  0.8321],\n",
      "        [ 1.8443,  0.9244, -0.1639,  1.1846,  0.7511],\n",
      "        [ 1.4339,  0.8356,  1.8338, -1.4310,  0.6817],\n",
      "        [ 0.0907, -0.2965, -0.8216, -0.3927, -0.8115]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1105,  1.7454,  1.4116,  1.4248,  1.4972],\n",
      "        [ 1.3617,  2.1097,  0.5692,  0.2061,  1.7950],\n",
      "        [ 0.2819,  0.5629,  0.5923,  0.5582,  0.8773],\n",
      "        [ 1.0019,  0.4614,  0.0888,  0.2322,  0.3524]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3122,  1.3302,  1.8897,  2.9296,  0.8321],\n",
      "        [ 1.8443,  0.9244, -0.1639,  1.1846,  0.7511],\n",
      "        [ 1.4339,  0.8356,  1.8338, -1.4310,  0.6817],\n",
      "        [ 0.0907, -0.2965, -0.8216, -0.3927, -0.8115]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 10.0622],\n",
      "        [  5.9606],\n",
      "        [  1.7602],\n",
      "        [ -0.4961]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7853, -0.0873,  0.7519, -2.1863, -0.8942],\n",
      "        [ 0.4700, -1.6365,  0.7129,  0.8071,  1.4815],\n",
      "        [-1.5226, -0.5493, -0.4452,  0.2023,  1.2425],\n",
      "        [-0.7948, -0.6918,  0.6799, -0.2879,  0.7624]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.7308, -1.9302, -2.2181, -2.7787, -4.7863],\n",
      "        [-1.0674, -1.1945, -2.0546, -1.3731, -2.9667],\n",
      "        [-0.3835, -0.6711,  0.1210, -0.4020, -1.0127],\n",
      "        [ 0.4262,  0.6773,  0.4247, -0.6499,  1.0411]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7853, -0.0873,  0.7519, -2.1863, -0.8942],\n",
      "        [ 0.4700, -1.6365,  0.7129,  0.8071,  1.4815],\n",
      "        [-1.5226, -0.5493, -0.4452,  0.2023,  1.2425],\n",
      "        [-0.7948, -0.6918,  0.6799, -0.2879,  0.7624]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 10.2147],\n",
      "        [ -5.5151],\n",
      "        [ -0.4409],\n",
      "        [  0.4623]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0177,  2.3083,  0.7415, -2.4239, -0.4217],\n",
      "        [ 0.9150,  0.8537,  0.0697,  1.9546,  0.9876],\n",
      "        [ 0.4164, -0.6687, -0.0021,  1.8533,  1.6802],\n",
      "        [-0.1214,  0.1440, -0.6215, -1.8631,  0.0683]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1608, -0.4402, -0.4737,  0.8579,  0.2499],\n",
      "        [ 0.4962,  0.5541,  0.8694, -0.0800,  1.3907],\n",
      "        [ 0.2637, -0.0913,  0.0985,  0.1590,  0.1382],\n",
      "        [ 0.8200, -0.4558,  0.3809,  0.1612,  0.2369]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0177,  2.3083,  0.7415, -2.4239, -0.4217],\n",
      "        [ 0.9150,  0.8537,  0.0697,  1.9546,  0.9876],\n",
      "        [ 0.4164, -0.6687, -0.0021,  1.8533,  1.6802],\n",
      "        [-0.1214,  0.1440, -0.6215, -1.8631,  0.0683]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.5551],\n",
      "        [ 2.2050],\n",
      "        [ 0.6976],\n",
      "        [-0.6860]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4216,  0.9917, -2.1326,  1.8799, -1.1636],\n",
      "        [-0.1869,  1.4188, -0.1613, -0.1100,  0.8249],\n",
      "        [ 0.7148, -0.3095,  1.3821, -0.8256,  0.4872],\n",
      "        [ 0.8266, -0.4418, -0.3993, -0.4871,  0.6555]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.0276,  1.6685,  0.9515,  1.2038,  1.6382],\n",
      "        [-0.0337,  0.0603, -0.2066, -0.4155, -1.1090],\n",
      "        [ 0.2491, -0.0347, -0.1529,  0.3700,  0.0541],\n",
      "        [-0.2733,  0.3255,  0.1817,  0.7610,  0.1719]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4216,  0.9917, -2.1326,  1.8799, -1.1636],\n",
      "        [-0.1869,  1.4188, -0.1613, -0.1100,  0.8249],\n",
      "        [ 0.7148, -0.3095,  1.3821, -0.8256,  0.4872],\n",
      "        [ 0.8266, -0.4418, -0.3993, -0.4871,  0.6555]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8724],\n",
      "        [-0.7439],\n",
      "        [-0.3016],\n",
      "        [-0.7003]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0885,  0.4294,  1.2313,  1.4324,  0.6381],\n",
      "        [-0.3317,  0.4994, -1.2542,  0.0928,  0.7227],\n",
      "        [ 0.8876, -0.5397,  0.8604,  1.6308,  0.6776],\n",
      "        [ 1.3432,  1.5018,  0.3998, -0.0819,  0.0546]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4765,  2.0099,  0.9981,  1.5542,  2.3706],\n",
      "        [ 0.2354, -0.5251, -0.6504, -0.2841, -1.1875],\n",
      "        [ 0.3664,  0.8974, -0.5664, -0.0118,  0.0559],\n",
      "        [ 0.3117, -0.5489,  0.2783, -0.2866,  0.1770]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0885,  0.4294,  1.2313,  1.4324,  0.6381],\n",
      "        [-0.3317,  0.4994, -1.2542,  0.0928,  0.7227],\n",
      "        [ 0.8876, -0.5397,  0.8604,  1.6308,  0.6776],\n",
      "        [ 1.3432,  1.5018,  0.3998, -0.0819,  0.0546]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.9617],\n",
      "        [-0.4091],\n",
      "        [-0.6278],\n",
      "        [-0.2613]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3671,  0.1527,  0.6036,  2.3305, -0.4463],\n",
      "        [-0.2712, -0.4999, -0.3558,  0.5747,  0.0342],\n",
      "        [-1.6102,  0.3993,  1.8603, -1.0321,  0.0370],\n",
      "        [ 0.2553, -0.7884,  0.1515,  0.9225, -0.4898]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4248, -0.2603, -0.2044, -0.3345, -1.2955],\n",
      "        [ 0.1205, -0.3848, -0.0608, -0.3935, -0.4612],\n",
      "        [ 0.6920,  0.2139, -0.6703,  0.2830,  0.0480],\n",
      "        [ 0.1811,  0.6947, -0.0413,  0.0992,  0.1862]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3671,  0.1527,  0.6036,  2.3305, -0.4463],\n",
      "        [-0.2712, -0.4999, -0.3558,  0.5747,  0.0342],\n",
      "        [-1.6102,  0.3993,  1.8603, -1.0321,  0.0370],\n",
      "        [ 0.2553, -0.7884,  0.1515,  0.9225, -0.4898]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9453],\n",
      "        [-0.0605],\n",
      "        [-2.5661],\n",
      "        [-0.5075]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1673,  0.0783, -1.4857,  0.1131,  1.3513],\n",
      "        [ 1.3465,  0.0144, -1.2436, -0.0454, -0.4969],\n",
      "        [-0.2167, -0.8898,  0.9148,  0.1143, -0.3460],\n",
      "        [-0.5477, -1.0029,  0.3259,  0.5807,  0.7009]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1525,  1.0533,  0.8059,  1.2110,  1.0990],\n",
      "        [ 0.2240, -0.4712, -0.7001, -0.7961, -0.6725],\n",
      "        [ 0.6523,  0.8966,  1.3133,  1.0007,  2.2983],\n",
      "        [ 0.4699, -0.1252, -0.1274,  0.7367,  0.3893]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1673,  0.0783, -1.4857,  0.1131,  1.3513],\n",
      "        [ 1.3465,  0.0144, -1.2436, -0.0454, -0.4969],\n",
      "        [-0.2167, -0.8898,  0.9148,  0.1143, -0.3460],\n",
      "        [-0.5477, -1.0029,  0.3259,  0.5807,  0.7009]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4818],\n",
      "        [ 1.5359],\n",
      "        [-0.4186],\n",
      "        [ 0.5273]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2737,  1.3779,  0.5004, -1.0908,  0.5647],\n",
      "        [-0.3656, -1.2271,  1.6753, -1.2562,  0.4913],\n",
      "        [ 1.1579, -0.0507,  1.2639, -0.1956, -1.8019],\n",
      "        [-0.0247, -0.5256,  1.3507,  0.6847, -0.9215]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4183,  0.6869,  0.8271,  1.4361,  1.8270],\n",
      "        [-0.7209, -0.6032, -0.8619, -1.8861, -1.4525],\n",
      "        [ 1.1306,  1.5333,  1.5722,  0.9222,  1.4594],\n",
      "        [ 0.1678, -0.3974, -0.6128,  0.5898, -0.1670]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2737,  1.3779,  0.5004, -1.0908,  0.5647],\n",
      "        [-0.3656, -1.2271,  1.6753, -1.2562,  0.4913],\n",
      "        [ 1.1579, -0.0507,  1.2639, -0.1956, -1.8019],\n",
      "        [-0.0247, -0.5256,  1.3507,  0.6847, -0.9215]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2927],\n",
      "        [ 1.2154],\n",
      "        [ 0.4084],\n",
      "        [-0.0652]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7584, -0.7162, -0.3899, -0.3200, -1.6853],\n",
      "        [ 1.2436,  0.1692,  0.6450, -0.3513, -0.0509],\n",
      "        [ 0.0850,  0.2548,  1.1371,  1.5882, -0.8016],\n",
      "        [-2.5345,  1.7492,  1.1210, -0.9344,  1.5784]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1209,  1.1393,  1.5338,  0.6769,  0.8099],\n",
      "        [-0.6573, -1.3332, -2.3291, -0.9870, -2.5549],\n",
      "        [ 0.0735,  0.9811,  1.2831,  1.4191,  1.1414],\n",
      "        [-0.3666,  0.1362, -0.9386,  0.2065,  0.0689]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7584, -0.7162, -0.3899, -0.3200, -1.6853],\n",
      "        [ 1.2436,  0.1692,  0.6450, -0.3513, -0.0509],\n",
      "        [ 0.0850,  0.2548,  1.1371,  1.5882, -0.8016],\n",
      "        [-2.5345,  1.7492,  1.1210, -0.9344,  1.5784]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.0873],\n",
      "        [-2.0684],\n",
      "        [ 3.0539],\n",
      "        [ 0.0309]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8685,  1.0966,  0.0476,  0.4666,  1.2363],\n",
      "        [-0.8564, -0.7661, -1.8721,  0.7318, -0.5788],\n",
      "        [-1.4100,  0.6123,  1.0666,  0.0719,  2.3793],\n",
      "        [ 0.6627,  0.0060,  0.9869,  0.5321,  0.2196]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4669,  1.8652,  1.9385,  1.7462,  2.6161],\n",
      "        [ 0.3386, -0.2988, -0.6852, -0.2212, -0.5523],\n",
      "        [-0.0119,  0.1664, -0.4977, -0.2051, -1.4874],\n",
      "        [ 0.4052,  0.0294,  0.3726,  0.3748, -0.3848]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8685,  1.0966,  0.0476,  0.4666,  1.2363],\n",
      "        [-0.8564, -0.7661, -1.8721,  0.7318, -0.5788],\n",
      "        [-1.4100,  0.6123,  1.0666,  0.0719,  2.3793],\n",
      "        [ 0.6627,  0.0060,  0.9869,  0.5321,  0.2196]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.9129],\n",
      "        [ 1.3794],\n",
      "        [-3.9658],\n",
      "        [ 0.7513]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8851,  0.6813,  0.5064, -0.7684,  1.0943],\n",
      "        [-1.6142, -1.0665, -0.7969, -0.0855, -0.9225],\n",
      "        [ 1.6076,  1.0541, -2.2503, -0.1010, -0.3090],\n",
      "        [ 1.1501,  1.3540,  0.6710,  0.8549, -0.4884]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1753,  0.7309, -0.1191,  0.1706,  0.0781],\n",
      "        [-0.3010, -0.8365, -1.7410, -1.2305, -1.9599],\n",
      "        [ 1.4835,  1.0794,  1.1417,  1.8306,  2.1174],\n",
      "        [ 0.2591, -0.3323,  0.0477,  0.3095, -0.2621]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8851,  0.6813,  0.5064, -0.7684,  1.0943],\n",
      "        [-1.6142, -1.0665, -0.7969, -0.0855, -0.9225],\n",
      "        [ 1.6076,  1.0541, -2.2503, -0.1010, -0.3090],\n",
      "        [ 1.1501,  1.3540,  0.6710,  0.8549, -0.4884]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2369],\n",
      "        [ 4.6786],\n",
      "        [ 0.1142],\n",
      "        [ 0.2727]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2152,  0.0390, -0.6337, -0.4339,  0.4289],\n",
      "        [-0.9320, -0.0336, -2.0121, -0.6009, -0.5798],\n",
      "        [ 1.3221,  0.4154, -0.6533,  0.5500, -1.2790],\n",
      "        [-0.5881,  0.7035,  0.3743, -0.7765, -0.6136]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5847, -0.3000, -0.8636,  0.3066, -0.4673],\n",
      "        [-1.5071, -2.4846, -1.7129, -1.7762, -4.4520],\n",
      "        [-0.1803,  0.0695,  0.0901, -0.0449,  0.8819],\n",
      "        [-0.0408,  0.2212,  0.4462,  0.0674,  0.2546]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2152,  0.0390, -0.6337, -0.4339,  0.4289],\n",
      "        [-0.9320, -0.0336, -2.0121, -0.6009, -0.5798],\n",
      "        [ 1.3221,  0.4154, -0.6533,  0.5500, -1.2790],\n",
      "        [-0.5881,  0.7035,  0.3743, -0.7765, -0.6136]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5084],\n",
      "        [ 8.5830],\n",
      "        [-1.4211],\n",
      "        [ 0.1381]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.1416, -0.1614,  1.1280,  1.5176, -1.2549],\n",
      "        [-0.8849,  1.3108, -1.2049,  0.3295, -0.5527],\n",
      "        [ 0.0381,  1.0125,  0.7693,  1.6570, -0.6433],\n",
      "        [ 1.8330,  0.3425,  1.3008, -0.6954,  1.4179]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1193,  0.6243, -0.6996, -0.5067,  0.3802],\n",
      "        [ 0.0344, -0.1746, -0.1158,  0.0745,  0.5702],\n",
      "        [ 0.5478,  0.7444,  1.4190,  0.5962,  1.3735],\n",
      "        [ 0.1188,  0.9685,  0.0691, -0.1802,  0.1700]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.1416, -0.1614,  1.1280,  1.5176, -1.2549],\n",
      "        [-0.8849,  1.3108, -1.2049,  0.3295, -0.5527],\n",
      "        [ 0.0381,  1.0125,  0.7693,  1.6570, -0.6433],\n",
      "        [ 1.8330,  0.3425,  1.3008, -0.6954,  1.4179]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8806],\n",
      "        [-0.4104],\n",
      "        [ 1.9705],\n",
      "        [ 1.0058]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3761, -0.1019, -0.4451, -0.2959,  0.7735],\n",
      "        [-0.2696,  0.0726, -1.3045, -1.3943,  0.2948],\n",
      "        [ 0.3227,  0.1067, -0.5511,  0.3806,  0.3982],\n",
      "        [ 0.0921,  0.2088,  0.2749, -2.3345, -0.2770]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0278,  0.9865,  0.8224,  0.0673,  0.6644],\n",
      "        [ 0.1937, -0.3780,  0.3920, -0.6789, -0.2577],\n",
      "        [-0.2812, -0.2639, -0.2238, -0.2716, -0.1347],\n",
      "        [-0.4565,  0.0533, -0.0156, -0.0343, -0.0807]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3761, -0.1019, -0.4451, -0.2959,  0.7735],\n",
      "        [-0.2696,  0.0726, -1.3045, -1.3943,  0.2948],\n",
      "        [ 0.3227,  0.1067, -0.5511,  0.3806,  0.3982],\n",
      "        [ 0.0921,  0.2088,  0.2749, -2.3345, -0.2770]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3590],\n",
      "        [ 0.2796],\n",
      "        [-0.1526],\n",
      "        [ 0.0672]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3113,  1.0986, -1.0908,  0.0834,  0.7350],\n",
      "        [ 1.2783, -1.8207,  0.6040,  0.6855,  1.3125],\n",
      "        [-0.6284, -1.3512,  0.8048,  0.3144, -0.8429],\n",
      "        [-0.6410,  1.5299,  0.8333, -0.0918, -0.8697]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0363,  1.1865,  1.4612,  0.3726,  1.2949],\n",
      "        [ 0.3783,  0.2088,  0.1796, -0.5856, -0.3609],\n",
      "        [ 0.1535, -0.0295,  0.0240, -0.1931,  0.1747],\n",
      "        [ 0.3058,  0.9043,  0.8956,  0.5520,  0.1788]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3113,  1.0986, -1.0908,  0.0834,  0.7350],\n",
      "        [ 1.2783, -1.8207,  0.6040,  0.6855,  1.3125],\n",
      "        [-0.6284, -1.3512,  0.8048,  0.3144, -0.8429],\n",
      "        [-0.6410,  1.5299,  0.8333, -0.0918, -0.8697]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3700],\n",
      "        [-0.6631],\n",
      "        [-0.2454],\n",
      "        [ 1.7276]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7185, -0.5099,  1.8106, -0.4504,  1.6897],\n",
      "        [-1.0693, -1.0168, -0.4691, -0.1011, -0.4384],\n",
      "        [ 0.4822, -1.7235,  0.0059,  1.2161,  1.8181],\n",
      "        [-1.9888, -1.1275, -1.2651, -0.6344, -2.5295]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3855, -0.1561,  0.6798,  0.1327,  0.5661],\n",
      "        [-0.4732, -0.0069, -0.7332,  0.6413, -0.6824],\n",
      "        [ 0.4339,  0.5089,  0.0737, -0.3511,  0.1415],\n",
      "        [-0.7316,  0.2043,  0.0829, -0.2182, -1.2152]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7185, -0.5099,  1.8106, -0.4504,  1.6897],\n",
      "        [-1.0693, -1.0168, -0.4691, -0.1011, -0.4384],\n",
      "        [ 0.4822, -1.7235,  0.0059,  1.2161,  1.8181],\n",
      "        [-1.9888, -1.1275, -1.2651, -0.6344, -2.5295]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4842],\n",
      "        [ 1.0913],\n",
      "        [-0.8370],\n",
      "        [ 4.3319]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5470,  0.1313, -0.6666, -1.7531, -0.5628],\n",
      "        [ 1.5399,  0.2740,  1.7197, -0.2509, -1.5835],\n",
      "        [ 0.7145,  1.1568,  0.9204,  0.2429,  1.0607],\n",
      "        [ 0.0296, -2.1884,  2.2719,  0.0186, -1.1071]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7641, -0.5466, -1.2825, -1.3725, -1.2228],\n",
      "        [ 0.1805, -1.2948, -0.3962, -0.4640, -0.8189],\n",
      "        [-0.2145,  0.2724,  0.3659,  0.1878,  0.4643],\n",
      "        [-1.4379, -1.1143, -2.4025, -2.2738, -4.3129]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5470,  0.1313, -0.6666, -1.7531, -0.5628],\n",
      "        [ 1.5399,  0.2740,  1.7197, -0.2509, -1.5835],\n",
      "        [ 0.7145,  1.1568,  0.9204,  0.2429,  1.0607],\n",
      "        [ 0.0296, -2.1884,  2.2719,  0.0186, -1.1071]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.2955],\n",
      "        [ 0.6551],\n",
      "        [ 1.0367],\n",
      "        [ 1.6704]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7623, -1.3363,  1.2045, -0.2612, -0.0631],\n",
      "        [ 1.3441, -1.0821,  2.2439,  1.0794, -0.2965],\n",
      "        [ 1.6768, -0.0357,  1.6564, -0.5236,  1.4581],\n",
      "        [ 0.1728,  0.2761, -0.0319, -1.4641, -0.1619]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.8049, -2.4303, -2.3017, -1.5476, -4.5894],\n",
      "        [-0.0108, -0.9641, -0.2386, -0.5656, -1.3363],\n",
      "        [-0.1272,  0.6449, -0.4160,  0.4319, -0.3869],\n",
      "        [ 0.3489,  0.8892, -0.0429,  0.1233,  0.3409]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7623, -1.3363,  1.2045, -0.2612, -0.0631],\n",
      "        [ 1.3441, -1.0821,  2.2439,  1.0794, -0.2965],\n",
      "        [ 1.6768, -0.0357,  1.6564, -0.5236,  1.4581],\n",
      "        [ 0.1728,  0.2761, -0.0319, -1.4641, -0.1619]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2067],\n",
      "        [ 0.2791],\n",
      "        [-1.7157],\n",
      "        [ 0.0714]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7374,  0.1075,  0.0876, -1.1785, -1.0343],\n",
      "        [ 0.1633,  0.8537, -0.6188, -0.0353, -0.0185],\n",
      "        [ 0.6536,  2.8186,  1.4642, -0.8011,  0.0601],\n",
      "        [-0.8236,  0.7747, -1.5612,  1.9851,  1.8700]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0586, -0.1459,  0.4689, -0.3111, -0.0159],\n",
      "        [-0.3598,  0.0872,  0.5584,  0.2056, -0.1961],\n",
      "        [ 0.6874,  0.6950,  1.1757,  0.2033,  1.1286],\n",
      "        [-0.3572,  0.1942,  0.4754,  0.3387, -0.3389]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7374,  0.1075,  0.0876, -1.1785, -1.0343],\n",
      "        [ 0.1633,  0.8537, -0.6188, -0.0353, -0.0185],\n",
      "        [ 0.6536,  2.8186,  1.4642, -0.8011,  0.0601],\n",
      "        [-0.8236,  0.7747, -1.5612,  1.9851,  1.8700]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4516],\n",
      "        [-0.3335],\n",
      "        [ 4.0345],\n",
      "        [-0.2591]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0281, -0.4648,  0.4393, -0.9227, -0.4259],\n",
      "        [ 0.1236,  1.7603,  1.0719, -1.6010,  0.2045],\n",
      "        [-0.6413, -0.7452,  0.0050, -0.6972, -1.5727],\n",
      "        [-0.1445, -0.4891, -0.7198,  1.6494, -1.4765]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3786, -0.0577, -0.4999,  0.1159,  0.0654],\n",
      "        [ 0.0929, -0.8969,  0.1151, -0.4409, -0.5032],\n",
      "        [-0.8748, -1.3769, -1.1630, -1.5055, -1.6815],\n",
      "        [ 0.2979, -0.1877,  0.2705,  0.1770, -0.2713]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0281, -0.4648,  0.4393, -0.9227, -0.4259],\n",
      "        [ 0.1236,  1.7603,  1.0719, -1.6010,  0.2045],\n",
      "        [-0.6413, -0.7452,  0.0050, -0.6972, -1.5727],\n",
      "        [-0.1445, -0.4891, -0.7198,  1.6494, -1.4765]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0616],\n",
      "        [-0.8411],\n",
      "        [ 5.2754],\n",
      "        [ 0.5465]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2767,  2.5575,  0.5140,  0.7598, -0.8594],\n",
      "        [-1.1032,  1.7096, -0.3330,  1.8398, -0.1574],\n",
      "        [ 0.9150,  0.6341, -1.2536, -0.9915, -0.4702],\n",
      "        [ 0.5161, -1.0128,  0.7858,  0.7940, -1.4794]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0455,  0.6981, -0.7122,  0.5764,  0.4505],\n",
      "        [-0.1082, -0.2158,  0.0168,  0.1002,  0.6762],\n",
      "        [-1.8502, -2.3454, -3.3345, -4.0656, -5.3644],\n",
      "        [ 0.0117,  0.1898,  0.6092,  0.2088,  0.0812]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2767,  2.5575,  0.5140,  0.7598, -0.8594],\n",
      "        [-1.1032,  1.7096, -0.3330,  1.8398, -0.1574],\n",
      "        [ 0.9150,  0.6341, -1.2536, -0.9915, -0.4702],\n",
      "        [ 0.5161, -1.0128,  0.7858,  0.7940, -1.4794]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4827],\n",
      "        [-0.1772],\n",
      "        [ 7.5532],\n",
      "        [ 0.3382]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6890,  1.4738,  0.0710,  1.1338,  0.6556],\n",
      "        [ 0.0076,  0.2451,  0.3540,  1.6384,  0.1550],\n",
      "        [ 0.1553, -0.1986, -0.7076,  1.0467, -0.2249],\n",
      "        [-0.5001,  0.5781,  1.4810, -0.2904, -0.1415]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2039, -0.3025, -1.6016,  0.1363, -1.0586],\n",
      "        [ 0.4252, -0.1676, -0.0666, -0.1280, -0.1351],\n",
      "        [ 0.4481, -0.1877, -0.9237, -0.1756,  0.5814],\n",
      "        [ 0.0223,  0.2017,  0.4834,  0.6529, -0.0134]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6890,  1.4738,  0.0710,  1.1338,  0.6556],\n",
      "        [ 0.0076,  0.2451,  0.3540,  1.6384,  0.1550],\n",
      "        [ 0.1553, -0.1986, -0.7076,  1.0467, -0.2249],\n",
      "        [-0.5001,  0.5781,  1.4810, -0.2904, -0.1415]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7545],\n",
      "        [-0.2920],\n",
      "        [ 0.4460],\n",
      "        [ 0.6338]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1490,  0.8192, -0.5112,  0.2361, -1.0393],\n",
      "        [ 1.8962, -0.0915, -1.0492, -1.1790,  0.4633],\n",
      "        [ 0.7911,  0.4238,  1.3783, -1.1568, -0.3244],\n",
      "        [ 0.0969,  0.5884, -0.0072,  0.7513,  0.8757]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0542, -0.1067,  0.5830, -0.0342, -0.5409],\n",
      "        [ 0.4143, -1.3708, -0.4580, -0.3741, -0.0781],\n",
      "        [-0.0444, -0.5524, -0.5252,  0.1351,  0.0229],\n",
      "        [ 0.1436,  0.4856, -0.0705,  0.4482,  0.2121]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1490,  0.8192, -0.5112,  0.2361, -1.0393],\n",
      "        [ 1.8962, -0.0915, -1.0492, -1.1790,  0.4633],\n",
      "        [ 0.7911,  0.4238,  1.3783, -1.1568, -0.3244],\n",
      "        [ 0.0969,  0.5884, -0.0072,  0.7513,  0.8757]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1606],\n",
      "        [ 1.7965],\n",
      "        [-1.1567],\n",
      "        [ 0.8226]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3549, -0.1858, -1.2355,  0.1568, -1.3542],\n",
      "        [ 0.6635,  1.1523,  0.9068, -0.9170,  0.7833],\n",
      "        [ 1.7435,  0.5896, -1.6924,  0.7207,  1.4585],\n",
      "        [-1.9152, -0.3005, -0.3851, -0.6507,  0.0064]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1842,  0.0414,  0.3433,  0.1769, -0.1706],\n",
      "        [-0.5882, -0.9676, -0.1789, -0.9787, -2.4530],\n",
      "        [ 0.1803, -0.2124,  0.3109,  0.5127,  1.2372],\n",
      "        [-0.6175,  0.5054,  0.0511, -0.3681, -0.4355]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3549, -0.1858, -1.2355,  0.1568, -1.3542],\n",
      "        [ 0.6635,  1.1523,  0.9068, -0.9170,  0.7833],\n",
      "        [ 1.7435,  0.5896, -1.6924,  0.7207,  1.4585],\n",
      "        [-1.9152, -0.3005, -0.3851, -0.6507,  0.0064]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1078],\n",
      "        [-2.6914],\n",
      "        [ 1.8370],\n",
      "        [ 1.2479]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2620,  0.5567, -1.1226,  1.2561,  0.7318],\n",
      "        [-0.5123,  0.1668,  0.7935, -0.3699, -1.4054],\n",
      "        [ 0.4230, -0.1380,  1.0466, -0.6727, -0.2456],\n",
      "        [ 0.7763, -1.6859, -1.2720, -1.3352, -1.1757]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1054, -0.5881, -0.2294, -0.1920,  0.1136],\n",
      "        [ 0.4860,  0.1466,  0.9251,  0.0985,  0.8565],\n",
      "        [-0.3009, -0.5950,  0.0496, -0.5386, -1.1105],\n",
      "        [-0.2501, -0.7528,  0.1899, -0.2910, -0.5938]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2620,  0.5567, -1.1226,  1.2561,  0.7318],\n",
      "        [-0.5123,  0.1668,  0.7935, -0.3699, -1.4054],\n",
      "        [ 0.4230, -0.1380,  1.0466, -0.6727, -0.2456],\n",
      "        [ 0.7763, -1.6859, -1.2720, -1.3352, -1.1757]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2554],\n",
      "        [-0.7306],\n",
      "        [ 0.6418],\n",
      "        [ 1.9202]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2408,  0.7843, -0.2492,  0.5315,  0.9638],\n",
      "        [ 0.8646,  0.4364,  2.1824,  0.1560,  1.4934],\n",
      "        [ 0.0167,  1.3552, -0.0971,  0.1478,  0.5262],\n",
      "        [ 1.2587,  0.8028, -0.4263, -0.4144,  1.4023]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4906,  0.0272, -0.3397, -0.4695,  0.0757],\n",
      "        [ 0.7565,  0.4318,  0.3324, -0.0753,  0.8040],\n",
      "        [-0.7960, -0.2438, -0.4562, -0.4263, -0.8553],\n",
      "        [-0.8510, -0.3112, -1.1592, -1.5469, -2.7822]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2408,  0.7843, -0.2492,  0.5315,  0.9638],\n",
      "        [ 0.8646,  0.4364,  2.1824,  0.1560,  1.4934],\n",
      "        [ 0.0167,  1.3552, -0.0971,  0.1478,  0.5262],\n",
      "        [ 1.2587,  0.8028, -0.4263, -0.4144,  1.4023]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0476],\n",
      "        [ 2.7569],\n",
      "        [-0.8125],\n",
      "        [-4.0872]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4261,  1.0504, -0.3959, -1.3487, -0.4873],\n",
      "        [ 0.1162, -2.0584, -0.4952, -1.4567,  1.2727],\n",
      "        [ 0.6562,  0.1541,  0.0517,  0.7205,  0.3847],\n",
      "        [-0.0379, -0.0148,  0.5733,  0.2605, -0.8869]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2464,  0.6565, -0.0018,  0.0616, -0.6383],\n",
      "        [-0.3523, -0.8797, -0.8703, -1.6130, -1.9437],\n",
      "        [-0.0268,  0.1527, -0.4002,  0.2579,  0.1512],\n",
      "        [ 0.3860,  1.0866,  0.7856,  0.9538,  1.2630]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4261,  1.0504, -0.3959, -1.3487, -0.4873],\n",
      "        [ 0.1162, -2.0584, -0.4952, -1.4567,  1.2727],\n",
      "        [ 0.6562,  0.1541,  0.0517,  0.7205,  0.3847],\n",
      "        [-0.0379, -0.0148,  0.5733,  0.2605, -0.8869]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0233],\n",
      "        [ 2.0768],\n",
      "        [ 0.2293],\n",
      "        [-0.4520]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2666, -0.0716,  0.4356,  0.3740, -0.9188],\n",
      "        [-0.3904, -0.6553, -0.0380,  0.8839,  1.5776],\n",
      "        [ 1.0138,  2.3726,  0.4392,  0.9838, -0.3317],\n",
      "        [ 1.4880,  0.0532, -1.7975, -1.0075, -0.2568]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2697,  0.5007, -0.0506, -0.1401, -1.1234],\n",
      "        [-1.3791, -0.7794, -1.3887, -1.0162, -2.9656],\n",
      "        [ 0.0332, -0.1940, -0.1405, -0.4932, -0.0957],\n",
      "        [ 0.5672,  0.8189,  0.5169,  0.5462,  1.4703]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2666, -0.0716,  0.4356,  0.3740, -0.9188],\n",
      "        [-0.3904, -0.6553, -0.0380,  0.8839,  1.5776],\n",
      "        [ 1.0138,  2.3726,  0.4392,  0.9838, -0.3317],\n",
      "        [ 1.4880,  0.0532, -1.7975, -1.0075, -0.2568]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8499],\n",
      "        [-4.4751],\n",
      "        [-0.9419],\n",
      "        [-0.9693]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0766,  0.0464,  1.3603,  0.0036,  1.0369],\n",
      "        [-0.9330, -0.4453, -0.1441,  0.3676,  0.5983],\n",
      "        [ 0.9928, -0.8866, -0.3223, -0.0529, -2.0308],\n",
      "        [ 1.2655, -0.8759, -1.2788, -0.0098, -0.1288]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2865, -0.3611, -0.4050, -1.1326, -1.2079],\n",
      "        [ 0.6501, -0.0736, -0.3520, -0.3578,  0.3609],\n",
      "        [ 0.4078, -0.1087,  0.7431, -0.2673,  0.3618],\n",
      "        [ 0.4342,  0.9562,  0.4561,  0.5823,  1.0448]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0766,  0.0464,  1.3603,  0.0036,  1.0369],\n",
      "        [-0.9330, -0.4453, -0.1441,  0.3676,  0.5983],\n",
      "        [ 0.9928, -0.8866, -0.3223, -0.0529, -2.0308],\n",
      "        [ 1.2655, -0.8759, -1.2788, -0.0098, -0.1288]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8023],\n",
      "        [-0.4386],\n",
      "        [-0.4589],\n",
      "        [-1.0115]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3240, -0.0625,  3.1075, -0.7910,  1.1140],\n",
      "        [-0.2753,  0.6007,  0.6280,  0.5867, -1.6985],\n",
      "        [-0.5401,  0.2239,  0.0280, -0.3077, -1.8305],\n",
      "        [ 0.9052, -0.4887,  0.1022,  0.1086,  1.1920]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7163, -0.1467,  0.0793, -0.1344,  0.7054],\n",
      "        [ 0.2949, -0.3602, -0.7305, -0.1341, -0.6999],\n",
      "        [ 0.3925, -0.3773, -0.4591,  0.0176, -0.0461],\n",
      "        [ 0.5548,  0.9123,  1.0514,  1.4313,  1.7057]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3240, -0.0625,  3.1075, -0.7910,  1.1140],\n",
      "        [-0.2753,  0.6007,  0.6280,  0.5867, -1.6985],\n",
      "        [-0.5401,  0.2239,  0.0280, -0.3077, -1.8305],\n",
      "        [ 0.9052, -0.4887,  0.1022,  0.1086,  1.1920]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3797],\n",
      "        [ 0.3538],\n",
      "        [-0.2303],\n",
      "        [ 2.3523]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2636, -0.0158,  0.3212,  0.0559, -1.7641],\n",
      "        [ 0.6865, -0.8317, -1.0744, -0.4937, -1.0157],\n",
      "        [ 0.0346,  0.7804,  0.0584,  0.6890,  0.3142],\n",
      "        [ 0.3267, -0.5883, -0.7689,  0.2546,  0.1681]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1752, -1.1472, -0.4518, -0.2478, -0.8512],\n",
      "        [-0.1165, -1.1750, -0.8281, -0.9342, -1.2622],\n",
      "        [ 0.4656, -0.2554,  0.0261, -0.4088, -1.2909],\n",
      "        [ 0.1478,  0.1673,  0.5384,  0.0418, -0.4531]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2636, -0.0158,  0.3212,  0.0559, -1.7641],\n",
      "        [ 0.6865, -0.8317, -1.0744, -0.4937, -1.0157],\n",
      "        [ 0.0346,  0.7804,  0.0584,  0.6890,  0.3142],\n",
      "        [ 0.3267, -0.5883, -0.7689,  0.2546,  0.1681]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4069],\n",
      "        [ 3.5303],\n",
      "        [-0.8689],\n",
      "        [-0.5297]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3973, -0.4344,  1.6400,  1.3815, -0.4143],\n",
      "        [-0.9269,  0.4433, -0.3080,  0.8147, -0.0041],\n",
      "        [-0.9184,  1.0973,  0.0101, -0.1917,  0.9525],\n",
      "        [-1.2260,  0.1958,  1.1653,  0.5733,  0.3676]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7542, -0.9044, -0.7048, -1.1907, -0.6937],\n",
      "        [-0.8121, -1.7031, -1.7003, -2.1120, -3.3922],\n",
      "        [ 0.0886, -0.4661, -0.5411, -0.2474, -0.1899],\n",
      "        [ 0.3067,  0.1900,  0.9485,  0.3685,  0.7891]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3973, -0.4344,  1.6400,  1.3815, -0.4143],\n",
      "        [-0.9269,  0.4433, -0.3080,  0.8147, -0.0041],\n",
      "        [-0.9184,  1.0973,  0.0101, -0.1917,  0.9525],\n",
      "        [-1.2260,  0.1958,  1.1653,  0.5733,  0.3676]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0666],\n",
      "        [-1.1852],\n",
      "        [-0.7318],\n",
      "        [ 1.2678]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1937,  0.9345, -0.0639,  1.4671,  0.1903],\n",
      "        [-0.3515, -0.8460, -0.6299,  0.2531, -0.0129],\n",
      "        [ 0.1173, -1.1915,  0.6002,  0.5810, -1.0575],\n",
      "        [ 0.4202, -0.6187, -0.4871, -0.2805,  0.8401]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7150, -0.3147, -0.4143, -0.2997, -0.9557],\n",
      "        [ 0.3189,  0.0038, -0.1824, -0.6031,  0.9856],\n",
      "        [ 0.8131, -0.4655, -0.3766, -0.7416, -0.9186],\n",
      "        [-0.2517, -0.0346, -0.7577, -0.0693, -1.0738]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1937,  0.9345, -0.0639,  1.4671,  0.1903],\n",
      "        [-0.3515, -0.8460, -0.6299,  0.2531, -0.0129],\n",
      "        [ 0.1173, -1.1915,  0.6002,  0.5810, -1.0575],\n",
      "        [ 0.4202, -0.6187, -0.4871, -0.2805,  0.8401]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0276],\n",
      "        [-0.1658],\n",
      "        [ 0.9645],\n",
      "        [-0.5979]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3780, -0.0986,  0.0569, -1.1099,  0.2153],\n",
      "        [-0.7653, -0.7957,  0.5818, -1.3284,  0.7154],\n",
      "        [ 1.3980,  0.3491,  0.4276,  0.0329,  1.5029],\n",
      "        [-0.3705, -0.9734,  2.3226,  0.8080,  2.0551]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3275, -0.0643,  0.0310,  0.2400, -0.1418],\n",
      "        [-0.3517, -0.2467,  0.8922,  0.5150, -0.4546],\n",
      "        [-0.1664, -0.6954, -0.2086, -1.4398, -1.4102],\n",
      "        [ 0.3044,  0.5154,  0.6769,  0.6530,  0.3494]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3780, -0.0986,  0.0569, -1.1099,  0.2153],\n",
      "        [-0.7653, -0.7957,  0.5818, -1.3284,  0.7154],\n",
      "        [ 1.3980,  0.3491,  0.4276,  0.0329,  1.5029],\n",
      "        [-0.3705, -0.9734,  2.3226,  0.8080,  2.0551]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1651],\n",
      "        [-0.0249],\n",
      "        [-2.7315],\n",
      "        [ 2.2032]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2587,  0.0683, -0.9269,  2.4970,  1.4304],\n",
      "        [-1.4811, -0.5744,  0.3393, -0.6906,  0.4720],\n",
      "        [-0.0675,  0.2201, -0.4629, -2.0147,  1.1763],\n",
      "        [-0.0989, -2.1301,  1.3408,  1.6745,  1.8100]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4124, -0.9041, -0.8305, -0.2584, -0.7623],\n",
      "        [ 0.5493,  0.3585,  0.1010,  0.3295,  0.0451],\n",
      "        [ 1.0252,  0.5369,  0.6160,  0.7332,  1.4765],\n",
      "        [-0.4647, -0.3791, -0.4134, -0.7130, -1.3893]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2587,  0.0683, -0.9269,  2.4970,  1.4304],\n",
      "        [-1.4811, -0.5744,  0.3393, -0.6906,  0.4720],\n",
      "        [-0.0675,  0.2201, -0.4629, -2.0147,  1.1763],\n",
      "        [-0.0989, -2.1301,  1.3408,  1.6745,  1.8100]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5467],\n",
      "        [-1.1916],\n",
      "        [ 0.0236],\n",
      "        [-3.4097]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3402,  0.1022, -0.2197,  1.6477, -1.0997],\n",
      "        [-0.0378,  0.3535,  0.4258, -0.7319,  0.7548],\n",
      "        [-1.7849,  0.0006,  0.9337,  0.7025, -1.8581],\n",
      "        [-0.9230, -0.0799,  0.3799, -1.0363, -0.4709]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5111, -0.0112,  0.5381,  0.7777,  0.8243],\n",
      "        [ 0.7171,  0.2023,  1.3322,  0.3086,  0.6971],\n",
      "        [ 1.1473,  0.8413,  1.4666,  0.8253,  0.9707],\n",
      "        [ 0.5492,  0.9339,  1.0981,  1.3525,  1.5879]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3402,  0.1022, -0.2197,  1.6477, -1.0997],\n",
      "        [-0.0378,  0.3535,  0.4258, -0.7319,  0.7548],\n",
      "        [-1.7849,  0.0006,  0.9337,  0.7025, -1.8581],\n",
      "        [-0.9230, -0.0799,  0.3799, -1.0363, -0.4709]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4295],\n",
      "        [ 0.9118],\n",
      "        [-1.9018],\n",
      "        [-2.3136]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4085, -0.3935, -0.8755, -0.2567, -0.3326],\n",
      "        [ 1.4191, -1.8755, -0.9128,  0.6685,  0.2428],\n",
      "        [-0.7569, -0.4386, -0.2590,  1.0417, -0.6819],\n",
      "        [ 0.2353,  2.0943, -0.0745,  0.5911,  1.1072]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.2172, -0.1729, -0.4297, -0.3353, -0.9322],\n",
      "        [ 0.0979,  0.1975,  0.2768, -0.3426,  0.4706],\n",
      "        [ 1.4760,  2.0511,  1.1806,  1.1797,  1.7907],\n",
      "        [ 1.1416,  1.2993,  0.9360,  0.7852,  1.5920]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4085, -0.3935, -0.8755, -0.2567, -0.3326],\n",
      "        [ 1.4191, -1.8755, -0.9128,  0.6685,  0.2428],\n",
      "        [-0.7569, -0.4386, -0.2590,  1.0417, -0.6819],\n",
      "        [ 0.2353,  2.0943, -0.0745,  0.5911,  1.1072]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9291],\n",
      "        [-0.5989],\n",
      "        [-2.3148],\n",
      "        [ 5.1468]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1311, -0.0284,  0.6762, -0.6437, -1.0510],\n",
      "        [ 0.0396, -1.3024,  0.0976, -1.3236,  0.7858],\n",
      "        [-1.0193, -0.5044,  0.6147, -1.2460,  0.5190],\n",
      "        [-0.0805, -2.0939,  1.1861, -0.4899, -2.0238]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8252, -0.9254, -0.4645, -0.5546, -0.7990],\n",
      "        [ 0.3074, -0.1437,  0.0584,  0.0616,  0.3619],\n",
      "        [ 0.0091,  0.3490, -0.3747,  0.2735,  0.3181],\n",
      "        [-0.7951, -0.0694, -0.7209, -0.6355, -0.9615]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1311, -0.0284,  0.6762, -0.6437, -1.0510],\n",
      "        [ 0.0396, -1.3024,  0.0976, -1.3236,  0.7858],\n",
      "        [-1.0193, -0.5044,  0.6147, -1.2460,  0.5190],\n",
      "        [-0.0805, -2.0939,  1.1861, -0.4899, -2.0238]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0172],\n",
      "        [ 0.4078],\n",
      "        [-0.5912],\n",
      "        [ 1.6116]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.2221,  0.4538,  0.9967,  0.5029, -0.5798],\n",
      "        [ 0.9675, -0.1995,  0.7668,  1.1157, -0.1511],\n",
      "        [-0.2463,  1.0642,  2.9713,  0.7441, -0.6138],\n",
      "        [-0.3018,  0.9315,  0.4236,  0.5522, -0.3808]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7669, -1.1448, -0.3680, -1.0390, -0.6557],\n",
      "        [ 0.4476,  0.2978, -0.0350,  0.1765,  0.1123],\n",
      "        [ 0.5482,  0.5917,  0.1146, -0.0010, -0.0536],\n",
      "        [-0.5890, -1.3772, -2.0154, -1.3308, -1.4027]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.2221,  0.4538,  0.9967,  0.5029, -0.5798],\n",
      "        [ 0.9675, -0.1995,  0.7668,  1.1157, -0.1511],\n",
      "        [-0.2463,  1.0642,  2.9713,  0.7441, -0.6138],\n",
      "        [-0.3018,  0.9315,  0.4236,  0.5522, -0.3808]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.7328],\n",
      "        [ 0.5268],\n",
      "        [ 0.8674],\n",
      "        [-2.1595]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2057,  0.7232,  1.1798,  0.2222,  1.3836],\n",
      "        [-2.4871,  0.0978,  0.7571,  0.9661,  0.6244],\n",
      "        [ 1.8465,  0.0216, -0.0059, -0.2926,  1.1607],\n",
      "        [ 0.3893, -0.0711,  1.2297,  0.2288, -2.1848]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0502,  0.5949,  0.4904,  0.7693,  1.3445],\n",
      "        [ 0.4326, -0.1226,  0.3136,  0.5036,  0.2228],\n",
      "        [ 0.1515, -0.1045,  0.0942, -0.1248, -0.9803],\n",
      "        [ 0.0243,  1.0821,  0.3528,  0.4852,  0.5386]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2057,  0.7232,  1.1798,  0.2222,  1.3836],\n",
      "        [-2.4871,  0.0978,  0.7571,  0.9661,  0.6244],\n",
      "        [ 1.8465,  0.0216, -0.0059, -0.2926,  1.1607],\n",
      "        [ 0.3893, -0.0711,  1.2297,  0.2288, -2.1848]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0295],\n",
      "        [-0.2248],\n",
      "        [-0.8244],\n",
      "        [-0.6993]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2104,  0.9281, -0.7511,  0.3488, -0.2808],\n",
      "        [-0.5080, -0.1770,  1.1425, -0.6521,  0.6123],\n",
      "        [ 0.0478, -0.7140, -0.6203,  0.0681,  0.5589],\n",
      "        [-0.3408, -0.7450,  0.9724,  1.0394,  1.3237]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6743, -1.0617, -1.0404, -1.1613, -2.3084],\n",
      "        [ 0.4774,  0.4278,  0.0123,  0.3632,  0.1583],\n",
      "        [ 0.1663, -0.5386, -0.2307, -0.4590,  0.4406],\n",
      "        [ 0.3235,  0.4195,  0.3933,  0.7669,  0.8441]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2104,  0.9281, -0.7511,  0.3488, -0.2808],\n",
      "        [-0.5080, -0.1770,  1.1425, -0.6521,  0.6123],\n",
      "        [ 0.0478, -0.7140, -0.6203,  0.0681,  0.5589],\n",
      "        [-0.3408, -0.7450,  0.9724,  1.0394,  1.3237]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1812],\n",
      "        [-0.4440],\n",
      "        [ 0.7506],\n",
      "        [ 1.8740]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6729, -2.1426, -0.8809, -0.7988,  0.2432],\n",
      "        [ 0.2132,  0.5260,  2.2696,  0.3982, -0.2902],\n",
      "        [-0.0510,  0.2248,  2.2853, -0.9546, -0.3718],\n",
      "        [ 0.4463,  0.7366, -0.1629,  0.2955,  1.6449]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1550, -0.9099, -0.9050, -0.6687, -2.2637],\n",
      "        [-0.0415, -0.1079, -0.3659,  0.0861, -0.1832],\n",
      "        [-0.1831, -1.0310,  0.1270,  0.4389,  0.4638],\n",
      "        [-0.5592,  0.1052,  0.1528, -0.0901, -1.7554]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6729, -2.1426, -0.8809, -0.7988,  0.2432],\n",
      "        [ 0.2132,  0.5260,  2.2696,  0.3982, -0.2902],\n",
      "        [-0.0510,  0.2248,  2.2853, -0.9546, -0.3718],\n",
      "        [ 0.4463,  0.7366, -0.1629,  0.2955,  1.6449]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.5075],\n",
      "        [-0.8087],\n",
      "        [-0.5236],\n",
      "        [-3.1110]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2030,  0.8696,  1.0646, -0.0562, -2.6472],\n",
      "        [ 1.0631,  1.0535, -0.0762, -1.8789, -1.1832],\n",
      "        [ 0.2744, -0.3217,  0.0055,  1.4172,  0.4158],\n",
      "        [-1.3931,  0.1626, -0.3831,  0.5089, -1.2353]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6362, -0.4157,  0.0244, -0.3578,  0.0301],\n",
      "        [ 0.4850,  0.7875,  0.2411,  0.8360,  0.7598],\n",
      "        [ 0.0320,  0.4207,  0.6407, -0.5411, -0.5380],\n",
      "        [ 0.3545,  1.3474,  0.7239,  1.3664,  2.1724]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2030,  0.8696,  1.0646, -0.0562, -2.6472],\n",
      "        [ 1.0631,  1.0535, -0.0762, -1.8789, -1.1832],\n",
      "        [ 0.2744, -0.3217,  0.0055,  1.4172,  0.4158],\n",
      "        [-1.3931,  0.1626, -0.3831,  0.5089, -1.2353]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5243],\n",
      "        [-1.1429],\n",
      "        [-1.1136],\n",
      "        [-2.5404]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8683,  0.2530,  1.9737, -0.0550,  2.1967],\n",
      "        [ 0.3214, -0.4403,  0.7010, -0.1570,  0.4092],\n",
      "        [ 0.5078,  0.3112, -1.5905,  0.3859, -0.8880],\n",
      "        [-0.6865, -0.1147,  0.1951, -0.2843, -0.7607]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1933, -0.4240,  0.1572, -0.4063, -0.4171],\n",
      "        [ 0.8124,  0.7540,  0.9677,  0.3456,  1.4935],\n",
      "        [ 0.5869,  0.2994,  0.7426, -0.4930,  0.8416],\n",
      "        [ 1.7361,  2.8556,  1.2576,  1.5265,  2.2843]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8683,  0.2530,  1.9737, -0.0550,  2.1967],\n",
      "        [ 0.3214, -0.4403,  0.7010, -0.1570,  0.4092],\n",
      "        [ 0.5078,  0.3112, -1.5905,  0.3859, -0.8880],\n",
      "        [-0.6865, -0.1147,  0.1951, -0.2843, -0.7607]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8586],\n",
      "        [ 1.1643],\n",
      "        [-1.7275],\n",
      "        [-3.4454]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7730, -0.9562, -1.3541, -0.2662,  0.6655],\n",
      "        [-1.7424, -1.3880,  1.0100, -1.4419,  0.1508],\n",
      "        [-0.3750, -0.4315, -0.5788, -0.4510,  0.5351],\n",
      "        [ 0.8177,  1.4904, -0.3440, -0.0516, -0.3087]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0398, -0.2792, -0.0384, -0.3880,  0.1756],\n",
      "        [-0.0976,  0.1253, -0.0203, -0.2677, -0.5849],\n",
      "        [ 0.9808,  1.1487,  1.4036,  1.0739,  1.0185],\n",
      "        [ 0.2335,  0.3088,  0.4344, -0.0154, -0.1274]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7730, -0.9562, -1.3541, -0.2662,  0.6655],\n",
      "        [-1.7424, -1.3880,  1.0100, -1.4419,  0.1508],\n",
      "        [-0.3750, -0.4315, -0.5788, -0.4510,  0.5351],\n",
      "        [ 0.8177,  1.4904, -0.3440, -0.0516, -0.3087]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5698],\n",
      "        [ 0.2733],\n",
      "        [-1.6151],\n",
      "        [ 0.5419]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6488,  0.6470,  0.4559,  0.2870,  1.3077],\n",
      "        [-2.8280, -1.4360,  0.5875, -0.0511, -0.4389],\n",
      "        [-0.4924,  0.8983,  0.8688,  1.2166, -1.3273],\n",
      "        [-0.0722,  1.8940,  0.0056, -0.8092,  1.5454]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2429, -1.4104,  0.0273, -0.2931,  0.0357],\n",
      "        [ 0.1444,  0.4042,  0.4096,  0.0563,  0.9659],\n",
      "        [ 0.9541,  1.5349,  1.3236,  1.5720,  1.7800],\n",
      "        [ 0.2188,  0.2212, -0.4912,  0.8123,  0.4423]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6488,  0.6470,  0.4559,  0.2870,  1.3077],\n",
      "        [-2.8280, -1.4360,  0.5875, -0.0511, -0.4389],\n",
      "        [-0.4924,  0.8983,  0.8688,  1.2166, -1.3273],\n",
      "        [-0.0722,  1.8940,  0.0056, -0.8092,  1.5454]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3380],\n",
      "        [-1.1748],\n",
      "        [ 1.6089],\n",
      "        [ 0.4267]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2490,  0.5250,  2.0590,  1.3572, -0.2296],\n",
      "        [-0.5607, -0.3657,  0.2415, -0.7689,  1.0470],\n",
      "        [-0.3894, -1.4802,  0.2878, -0.4885, -0.4752],\n",
      "        [-0.9620,  1.2190,  0.0027,  1.7300,  1.2434]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6051,  0.8046,  0.7578,  0.4731,  1.7348],\n",
      "        [ 0.6972,  0.3109,  1.0481,  1.0727,  1.6524],\n",
      "        [ 0.8349,  1.3546,  0.9475,  1.0049,  1.4836],\n",
      "        [ 0.0729, -0.3525,  0.0506,  0.0077,  0.4095]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2490,  0.5250,  2.0590,  1.3572, -0.2296],\n",
      "        [-0.5607, -0.3657,  0.2415, -0.7689,  1.0470],\n",
      "        [-0.3894, -1.4802,  0.2878, -0.4885, -0.4752],\n",
      "        [-0.9620,  1.2190,  0.0027,  1.7300,  1.2434]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3771],\n",
      "        [ 0.6537],\n",
      "        [-3.2534],\n",
      "        [ 0.0228]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6956, -0.8356,  0.8246, -0.7116, -1.1125],\n",
      "        [-0.7504,  1.0304,  0.6790,  0.8513,  0.5048],\n",
      "        [-1.8679,  0.7508,  0.5684, -0.3228, -0.2429],\n",
      "        [ 0.3381,  0.3458,  0.1881, -0.5980,  0.5754]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4403, -1.1537, -0.7265, -1.4395, -0.9146],\n",
      "        [ 0.7365,  0.3704, -0.1817, -0.3366,  0.0285],\n",
      "        [ 0.1684,  0.7057, -0.0926, -0.0202,  0.8383],\n",
      "        [ 0.2378, -0.4872,  0.5597,  0.2138,  0.6051]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6956, -0.8356,  0.8246, -0.7116, -1.1125],\n",
      "        [-0.7504,  1.0304,  0.6790,  0.8513,  0.5048],\n",
      "        [-1.8679,  0.7508,  0.5684, -0.3228, -0.2429],\n",
      "        [ 0.3381,  0.3458,  0.1881, -0.5980,  0.5754]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1005],\n",
      "        [-0.5665],\n",
      "        [-0.0343],\n",
      "        [ 0.2376]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9314,  0.6698, -1.5218,  0.0201,  0.1435],\n",
      "        [-1.0301,  2.2024,  2.3237, -1.5151,  0.2159],\n",
      "        [-1.4976, -0.5129, -0.3423, -0.6031,  1.1923],\n",
      "        [-2.3917,  0.0906, -0.2944, -0.8587,  0.2706]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4140, -1.1829, -1.5240, -1.4376, -2.7873],\n",
      "        [ 0.6446, -0.0631,  1.0082,  0.6553,  1.2686],\n",
      "        [ 0.1198,  0.1430, -0.3993,  0.3210,  0.0596],\n",
      "        [-0.2010,  0.3444,  0.0572, -0.2586,  0.1021]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9314,  0.6698, -1.5218,  0.0201,  0.1435],\n",
      "        [-1.0301,  2.2024,  2.3237, -1.5151,  0.2159],\n",
      "        [-1.4976, -0.5129, -0.3423, -0.6031,  1.1923],\n",
      "        [-2.3917,  0.0906, -0.2944, -0.8587,  0.2706]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2985],\n",
      "        [ 0.8208],\n",
      "        [-0.2386],\n",
      "        [ 0.7447]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0224,  0.0035, -1.5641,  1.0149, -0.0269],\n",
      "        [-3.2278,  1.1222,  0.1578, -1.0846, -0.4455],\n",
      "        [ 0.8735, -1.6345,  1.2035,  1.2136, -0.9451],\n",
      "        [-0.0872,  1.4167, -1.3186, -0.4204,  0.5382]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7460, -1.5475, -2.2557, -1.2349, -2.3664],\n",
      "        [ 0.1751,  0.2108, -0.0331, -0.0468,  0.0757],\n",
      "        [ 0.4563,  0.0086, -0.2650, -0.5844,  0.2171],\n",
      "        [-0.2252, -0.2447,  0.6295,  0.2607,  0.4200]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0224,  0.0035, -1.5641,  1.0149, -0.0269],\n",
      "        [-3.2278,  1.1222,  0.1578, -1.0846, -0.4455],\n",
      "        [ 0.8735, -1.6345,  1.2035,  1.2136, -0.9451],\n",
      "        [-0.0872,  1.4167, -1.3186, -0.4204,  0.5382]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0956],\n",
      "        [-0.3166],\n",
      "        [-0.8488],\n",
      "        [-1.0407]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8661, -0.9831,  0.3510,  0.8913, -0.5492],\n",
      "        [ 0.5903, -0.1822, -0.1324,  0.1884,  0.8135],\n",
      "        [-1.6577, -0.5534, -0.8915, -0.8831, -0.7951],\n",
      "        [-0.0928,  0.1537,  1.1847, -1.4705,  1.0584]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1095, -0.5274, -0.0404,  0.2128, -0.1615],\n",
      "        [ 0.4827,  0.1751, -0.1139,  0.4491,  0.9116],\n",
      "        [ 0.2855,  0.1722,  0.5118, -0.3734,  0.9869],\n",
      "        [ 0.4598,  0.2184,  0.6884,  0.1403,  1.1018]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8661, -0.9831,  0.3510,  0.8913, -0.5492],\n",
      "        [ 0.5903, -0.1822, -0.1324,  0.1884,  0.8135],\n",
      "        [-1.6577, -0.5534, -0.8915, -0.8831, -0.7951],\n",
      "        [-0.0928,  0.1537,  1.1847, -1.4705,  1.0584]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5782],\n",
      "        [ 1.0942],\n",
      "        [-1.4798],\n",
      "        [ 1.7661]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5350,  0.1919,  0.6074,  1.0280,  0.5070],\n",
      "        [ 0.7155, -0.3742,  0.4848,  0.8280,  0.5423],\n",
      "        [-0.4508, -0.9645,  0.9274,  0.2276,  0.4165],\n",
      "        [ 0.9147,  0.7410, -0.5414,  0.0541,  0.3730]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2701,  0.3014, -0.2714,  0.1699,  0.2776],\n",
      "        [ 0.0704, -0.5630,  0.2147,  0.0943, -0.0211],\n",
      "        [ 0.8659,  0.4937,  0.7825,  1.0746,  1.6444],\n",
      "        [-0.7069,  0.0783, -0.1144, -0.5159, -1.0582]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5350,  0.1919,  0.6074,  1.0280,  0.5070],\n",
      "        [ 0.7155, -0.3742,  0.4848,  0.8280,  0.5423],\n",
      "        [-0.4508, -0.9645,  0.9274,  0.2276,  0.4165],\n",
      "        [ 0.9147,  0.7410, -0.5414,  0.0541,  0.3730]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3528],\n",
      "        [ 0.4317],\n",
      "        [ 0.7887],\n",
      "        [-0.9492]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0596, -0.1822, -0.7690, -0.5810, -0.8776],\n",
      "        [ 0.0332,  0.3109,  0.0216, -2.3479,  1.1692],\n",
      "        [-2.5479,  0.5162,  0.7526,  0.1035,  1.7602],\n",
      "        [ 0.3007, -1.1677,  1.6368, -0.7481,  0.7127]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0163, -0.4907,  0.2481,  0.2565,  0.3801],\n",
      "        [-0.1283,  1.0038,  0.2073,  0.2692,  0.0592],\n",
      "        [ 0.6694,  0.6108,  0.0391,  0.5930,  0.4943],\n",
      "        [ 0.4014, -0.1798,  0.1533,  0.7168,  0.3010]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0596, -0.1822, -0.7690, -0.5810, -0.8776],\n",
      "        [ 0.0332,  0.3109,  0.0216, -2.3479,  1.1692],\n",
      "        [-2.5479,  0.5162,  0.7526,  0.1035,  1.7602],\n",
      "        [ 0.3007, -1.1677,  1.6368, -0.7481,  0.7127]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5850],\n",
      "        [-0.2505],\n",
      "        [-0.4294],\n",
      "        [ 0.2598]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5768, -1.3042,  0.0427, -0.3253,  1.2580],\n",
      "        [ 1.9076,  0.2687,  0.9815,  0.6217, -0.7512],\n",
      "        [ 0.3883,  1.1115,  0.1852,  1.4630, -0.2856],\n",
      "        [-1.1507, -0.0166, -0.2377,  1.5989,  0.4995]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0201,  0.3225,  0.1451,  0.1176,  0.3933],\n",
      "        [-0.0954,  0.6657,  0.6231,  0.5651,  0.7139],\n",
      "        [ 0.8767,  0.1921,  0.1953, -0.9543, -0.1442],\n",
      "        [ 0.4913, -0.2289,  0.2907, -0.0196, -0.7893]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5768, -1.3042,  0.0427, -0.3253,  1.2580],\n",
      "        [ 1.9076,  0.2687,  0.9815,  0.6217, -0.7512],\n",
      "        [ 0.3883,  1.1115,  0.1852,  1.4630, -0.2856],\n",
      "        [-1.1507, -0.0166, -0.2377,  1.5989,  0.4995]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0537],\n",
      "        [ 0.4235],\n",
      "        [-0.7650],\n",
      "        [-1.0563]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9578, -0.3409,  1.2116, -0.7639,  0.6150],\n",
      "        [ 0.5666, -0.4965,  2.2871,  1.4664, -0.9505],\n",
      "        [-1.0452, -1.7080, -0.3587, -0.0265, -0.0510],\n",
      "        [ 1.1165, -0.1213, -0.9304,  0.6472,  0.6923]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2123,  0.3201, -0.0523, -0.3037, -0.0494],\n",
      "        [-0.0928,  0.8762,  0.3857, -0.0821,  1.1593],\n",
      "        [ 0.4790,  0.1732,  0.4570,  0.7791,  0.7189],\n",
      "        [ 0.6844,  0.5572,  0.7860,  0.6679,  0.7558]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9578, -0.3409,  1.2116, -0.7639,  0.6150],\n",
      "        [ 0.5666, -0.4965,  2.2871,  1.4664, -0.9505],\n",
      "        [-1.0452, -1.7080, -0.3587, -0.0265, -0.0510],\n",
      "        [ 1.1165, -0.1213, -0.9304,  0.6472,  0.6923]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1742],\n",
      "        [-0.8278],\n",
      "        [-1.0177],\n",
      "        [ 0.9207]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6857,  1.1712,  1.8149,  0.0651,  1.1138],\n",
      "        [ 0.7446, -1.2305, -0.4757, -0.1468,  0.5238],\n",
      "        [-0.9809,  0.7238,  0.2303,  0.1170,  0.1973],\n",
      "        [-0.3082,  0.9833,  1.9079,  1.4001,  0.7613]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1854,  0.4586, -0.2735, -0.4931, -0.2498],\n",
      "        [ 0.7989,  1.2018,  1.1474,  0.9958,  1.0407],\n",
      "        [ 0.8185,  0.3409,  0.0531,  0.6829,  1.3034],\n",
      "        [ 0.2043,  0.0556,  0.0702,  0.7486, -0.0904]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6857,  1.1712,  1.8149,  0.0651,  1.1138],\n",
      "        [ 0.7446, -1.2305, -0.4757, -0.1468,  0.5238],\n",
      "        [-0.9809,  0.7238,  0.2303,  0.1170,  0.1973],\n",
      "        [-0.3082,  0.9833,  1.9079,  1.4001,  0.7613]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5822],\n",
      "        [-1.0307],\n",
      "        [-0.2068],\n",
      "        [ 1.1049]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4329,  1.1091,  0.2569,  0.8972,  0.2914],\n",
      "        [-0.7763,  1.2500,  0.4885, -0.4577, -0.3618],\n",
      "        [ 2.0417, -0.4800,  0.0026, -0.5358,  0.9306],\n",
      "        [ 1.0435, -0.6943, -1.0997, -1.4492, -1.1212]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5693, -0.4343,  0.2285,  0.0482,  0.2131],\n",
      "        [ 0.2908,  1.4468,  1.1377,  1.1885,  1.2858],\n",
      "        [ 0.8386,  0.1358,  0.6829,  0.2318,  0.2688],\n",
      "        [ 0.2093, -0.5368, -0.0744,  0.0773, -0.5889]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4329,  1.1091,  0.2569,  0.8972,  0.2914],\n",
      "        [-0.7763,  1.2500,  0.4885, -0.4577, -0.3618],\n",
      "        [ 2.0417, -0.4800,  0.0026, -0.5358,  0.9306],\n",
      "        [ 1.0435, -0.6943, -1.0997, -1.4492, -1.1212]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0712],\n",
      "        [ 1.1292],\n",
      "        [ 1.7747],\n",
      "        [ 1.2212]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3883, -0.6547,  1.1534,  0.6028,  1.5373],\n",
      "        [-1.0530,  0.7045, -0.2669,  0.9147, -0.6635],\n",
      "        [ 0.4573,  1.8307,  0.4422,  1.8088, -0.4374],\n",
      "        [-1.0936,  1.0789, -0.0717,  0.5289, -0.4852]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3162, -0.0164, -0.1504, -0.0346, -0.4807],\n",
      "        [ 0.6331,  1.1817,  0.7994,  0.2758,  0.7651],\n",
      "        [ 0.2869, -1.2628, -0.7455, -0.8361, -1.1026],\n",
      "        [-0.5368, -1.5588, -0.3579, -0.0627, -1.3442]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3883, -0.6547,  1.1534,  0.6028,  1.5373],\n",
      "        [-1.0530,  0.7045, -0.2669,  0.9147, -0.6635],\n",
      "        [ 0.4573,  1.8307,  0.4422,  1.8088, -0.4374],\n",
      "        [-1.0936,  1.0789, -0.0717,  0.5289, -0.4852]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0453],\n",
      "        [-0.3029],\n",
      "        [-3.5403],\n",
      "        [-0.4500]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2480,  0.4517, -0.4377,  0.8839,  0.5954],\n",
      "        [-0.4394,  0.8032,  0.8052, -0.0653, -0.9938],\n",
      "        [-0.1165,  1.3106,  1.5109, -0.9175,  2.1179],\n",
      "        [ 0.4872,  2.0302,  0.1783,  0.7411,  0.4413]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9003,  0.4444, -0.1680,  0.2971,  0.9857],\n",
      "        [ 0.2734,  0.9118,  1.2085,  0.5895,  0.6193],\n",
      "        [ 1.3167,  1.8748,  0.7954,  0.9692,  1.3922],\n",
      "        [-0.2232, -0.1872,  0.5999,  0.6864, -1.0176]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2480,  0.4517, -0.4377,  0.8839,  0.5954],\n",
      "        [-0.4394,  0.8032,  0.8052, -0.0653, -0.9938],\n",
      "        [-0.1165,  1.3106,  1.5109, -0.9175,  2.1179],\n",
      "        [ 0.4872,  2.0302,  0.1783,  0.7411,  0.4413]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3471],\n",
      "        [ 0.9313],\n",
      "        [ 5.5650],\n",
      "        [-0.3221]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3333,  0.0301,  0.3075, -0.1528, -1.3590],\n",
      "        [-1.7413, -0.0475, -1.7764, -1.8098,  1.4000],\n",
      "        [-0.1402, -0.1531, -0.0885, -0.3986, -0.1170],\n",
      "        [-1.0748,  1.2461,  0.2570,  0.6957,  2.0201]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0228, -0.8788, -1.3066, -0.3452,  0.0399],\n",
      "        [ 0.2832,  0.9685,  0.1591,  0.5958,  0.1053],\n",
      "        [-0.5159, -1.1387, -1.5601, -0.4048, -1.9698],\n",
      "        [-0.1703,  0.3453,  0.3051,  0.3723,  0.6279]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3333,  0.0301,  0.3075, -0.1528, -1.3590],\n",
      "        [-1.7413, -0.0475, -1.7764, -1.8098,  1.4000],\n",
      "        [-0.1402, -0.1531, -0.0885, -0.3986, -0.1170],\n",
      "        [-1.0748,  1.2461,  0.2570,  0.6957,  2.0201]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4374],\n",
      "        [-1.7525],\n",
      "        [ 0.7765],\n",
      "        [ 2.2192]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9416,  0.1713,  0.9130, -0.8582, -0.1128],\n",
      "        [ 0.1229,  2.0595, -1.8589,  1.2176,  0.5919],\n",
      "        [ 1.0756, -0.0879,  2.0547, -1.9716, -0.6109],\n",
      "        [ 1.3730, -0.1826, -0.7258,  1.1651, -0.2073]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5450, -0.0659, -0.0952,  0.1182, -0.2380],\n",
      "        [ 0.7640,  1.5498,  1.1028,  1.2746,  1.9952],\n",
      "        [-0.9591, -0.8642, -1.8976, -1.5576, -2.0321],\n",
      "        [-0.9572, -0.4688, -1.4984, -0.2147, -2.4148]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9416,  0.1713,  0.9130, -0.8582, -0.1128],\n",
      "        [ 0.1229,  2.0595, -1.8589,  1.2176,  0.5919],\n",
      "        [ 1.0756, -0.0879,  2.0547, -1.9716, -0.6109],\n",
      "        [ 1.3730, -0.1826, -0.7258,  1.1651, -0.2073]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3404],\n",
      "        [ 3.9686],\n",
      "        [-0.5422],\n",
      "        [ 0.1092]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7482,  0.4340,  0.7322,  0.4111, -0.1456],\n",
      "        [ 1.1248,  0.6242, -1.0353,  0.9093, -0.0499],\n",
      "        [-1.4531, -0.4244, -0.1492, -0.0013, -2.1408],\n",
      "        [ 1.8574, -0.3959, -1.3418,  0.9184,  0.0345]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1585,  0.0998,  0.0254,  0.0949, -0.3219],\n",
      "        [-0.1513, -0.3297, -0.9451, -0.4542, -2.0961],\n",
      "        [-0.1425, -0.5015, -0.4125, -1.4834, -1.0789],\n",
      "        [-0.7628, -0.7391, -0.7999, -0.6402, -1.7416]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7482,  0.4340,  0.7322,  0.4111, -0.1456],\n",
      "        [ 1.1248,  0.6242, -1.0353,  0.9093, -0.0499],\n",
      "        [-1.4531, -0.4244, -0.1492, -0.0013, -2.1408],\n",
      "        [ 1.8574, -0.3959, -1.3418,  0.9184,  0.0345]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0292],\n",
      "        [ 0.2942],\n",
      "        [ 2.7931],\n",
      "        [-0.6990]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4377, -1.3618, -0.8791, -1.2411,  2.0459],\n",
      "        [ 0.5175,  0.7168,  1.5036,  0.6197, -2.0627],\n",
      "        [ 0.5118, -0.1956,  0.3300, -0.6844,  0.9517],\n",
      "        [-0.8058, -1.8540,  1.1368, -0.4060,  1.4400]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0060,  0.0411,  0.2145,  0.4943, -0.3996],\n",
      "        [-0.4577, -0.4326, -0.6230, -0.0437, -1.6886],\n",
      "        [-1.0546, -1.5461, -1.0634, -2.0158, -3.7554],\n",
      "        [-0.3157, -0.4493, -0.9782, -0.3351, -0.1050]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4377, -1.3618, -0.8791, -1.2411,  2.0459],\n",
      "        [ 0.5175,  0.7168,  1.5036,  0.6197, -2.0627],\n",
      "        [ 0.5118, -0.1956,  0.3300, -0.6844,  0.9517],\n",
      "        [-0.8058, -1.8540,  1.1368, -0.4060,  1.4400]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6669],\n",
      "        [ 1.9723],\n",
      "        [-2.7826],\n",
      "        [-0.0398]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2314, -0.4575, -0.3027,  0.1009, -0.4371],\n",
      "        [ 0.0014, -1.3293,  0.7087, -0.4937,  1.2280],\n",
      "        [-1.6453, -1.2388,  1.7450,  1.0253,  0.1962],\n",
      "        [-1.0933, -0.1229,  0.3910, -1.5735, -1.4964]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8197,  0.7714,  0.9636,  0.3664,  0.7525],\n",
      "        [-0.8355, -1.2778, -0.9759, -2.2238, -2.9113],\n",
      "        [-0.3008, -1.2544, -1.7161, -1.5090, -1.5941],\n",
      "        [-0.3766, -0.2217, -0.3379, -0.7911, -0.2939]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2314, -0.4575, -0.3027,  0.1009, -0.4371],\n",
      "        [ 0.0014, -1.3293,  0.7087, -0.4937,  1.2280],\n",
      "        [-1.6453, -1.2388,  1.7450,  1.0253,  0.1962],\n",
      "        [-1.0933, -0.1229,  0.3910, -1.5735, -1.4964]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0729],\n",
      "        [-1.4714],\n",
      "        [-2.8057],\n",
      "        [ 1.9914]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0026,  0.9144, -0.4806,  0.5336,  1.4755],\n",
      "        [ 0.5109, -1.0180,  1.8158,  0.3968, -0.0121],\n",
      "        [ 0.9418,  2.0884,  0.1556, -2.1962,  1.1491],\n",
      "        [-0.1469,  0.5418,  0.1564, -0.4180, -1.2125]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2523,  0.6295,  1.1990,  0.8301,  0.8145],\n",
      "        [-0.7037, -0.5833, -0.6132, -0.6080, -1.7623],\n",
      "        [ 0.6373, -0.3030, -0.0261,  0.5133,  1.0810],\n",
      "        [-0.8568, -0.8010, -1.2959, -1.3956, -2.0989]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0026,  0.9144, -0.4806,  0.5336,  1.4755],\n",
      "        [ 0.5109, -1.0180,  1.8158,  0.3968, -0.0121],\n",
      "        [ 0.9418,  2.0884,  0.1556, -2.1962,  1.1491],\n",
      "        [-0.1469,  0.5418,  0.1564, -0.4180, -1.2125]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6447],\n",
      "        [-1.0991],\n",
      "        [ 0.0783],\n",
      "        [ 2.6175]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4018,  0.6059, -0.1787, -0.4775, -0.3950],\n",
      "        [-2.6575, -0.8041,  1.2722, -2.3855, -0.3163],\n",
      "        [ 0.2106,  1.6338,  1.6452, -0.5895, -0.7011],\n",
      "        [ 0.0536, -0.0294, -0.9029,  0.1099, -0.1842]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5156,  0.0702, -1.3850, -0.3888, -0.3716],\n",
      "        [-0.1952,  0.3094,  0.0861,  0.3581, -0.4026],\n",
      "        [ 0.4025, -0.4083, -1.0657, -0.6374, -0.4964],\n",
      "        [-0.0769,  0.2334,  0.6370,  0.4036, -0.0449]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4018,  0.6059, -0.1787, -0.4775, -0.3950],\n",
      "        [-2.6575, -0.8041,  1.2722, -2.3855, -0.3163],\n",
      "        [ 0.2106,  1.6338,  1.6452, -0.5895, -0.7011],\n",
      "        [ 0.0536, -0.0294, -0.9029,  0.1099, -0.1842]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4153],\n",
      "        [-0.3473],\n",
      "        [-1.6118],\n",
      "        [-0.5336]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9434, -1.1261, -0.8134,  0.2374, -0.3980],\n",
      "        [ 1.6772,  2.3880,  0.9357, -1.5212,  0.5137],\n",
      "        [-0.7726,  0.8014,  1.5743, -0.9573, -1.1794],\n",
      "        [ 0.6686,  0.3024, -0.7054,  1.8047, -0.6635]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6229, -0.1967, -0.0799,  0.0399,  0.1086],\n",
      "        [-0.0567, -0.1678, -0.0297,  0.2494, -0.2639],\n",
      "        [ 1.0891, -0.4276,  0.0292,  0.3212,  0.5466],\n",
      "        [ 0.4849, -0.5071, -0.0488, -0.4674, -0.0533]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9434, -1.1261, -0.8134,  0.2374, -0.3980],\n",
      "        [ 1.6772,  2.3880,  0.9357, -1.5212,  0.5137],\n",
      "        [-0.7726,  0.8014,  1.5743, -0.9573, -1.1794],\n",
      "        [ 0.6686,  0.3024, -0.7054,  1.8047, -0.6635]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3349],\n",
      "        [-1.0386],\n",
      "        [-2.0903],\n",
      "        [-0.6029]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1426,  0.1772, -0.9658,  0.4474, -0.1719],\n",
      "        [-0.3038, -1.2107, -1.3072,  0.3247, -0.9129],\n",
      "        [-0.9286,  0.0012, -1.7198, -0.7704, -0.5515],\n",
      "        [ 1.4933, -0.9920, -0.0856, -1.5123,  0.4212]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3237, -0.0045,  0.0845,  0.2193, -0.0436],\n",
      "        [ 0.0487,  0.2166,  0.1424,  0.2772,  0.5199],\n",
      "        [ 0.9127,  0.9900,  0.8969,  0.3656,  1.9179],\n",
      "        [ 0.3544,  0.1018, -0.2091, -0.0662, -0.5904]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1426,  0.1772, -0.9658,  0.4474, -0.1719],\n",
      "        [-0.3038, -1.2107, -1.3072,  0.3247, -0.9129],\n",
      "        [-0.9286,  0.0012, -1.7198, -0.7704, -0.5515],\n",
      "        [ 1.4933, -0.9920, -0.0856, -1.5123,  0.4212]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0229],\n",
      "        [-0.8478],\n",
      "        [-3.7282],\n",
      "        [ 0.2976]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4527, -0.0927,  1.0638,  1.0389,  1.6055],\n",
      "        [ 0.1165,  0.2694, -0.2246,  0.5444,  1.0187],\n",
      "        [ 0.2517,  0.8509, -1.9010,  1.1505,  1.0792],\n",
      "        [ 0.6758,  0.1211,  1.4385,  0.9958, -2.0433]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4217, -0.3505, -0.1678,  0.1441,  0.5835],\n",
      "        [-0.0037, -0.3335, -0.2925, -0.3825, -0.2805],\n",
      "        [ 2.5955,  2.1070,  2.1894,  1.7002,  3.2591],\n",
      "        [-0.1355,  0.0894, -0.6064, -0.4395,  0.3712]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4527, -0.0927,  1.0638,  1.0389,  1.6055],\n",
      "        [ 0.1165,  0.2694, -0.2246,  0.5444,  1.0187],\n",
      "        [ 0.2517,  0.8509, -1.9010,  1.1505,  1.0792],\n",
      "        [ 0.6758,  0.1211,  1.4385,  0.9958, -2.0433]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1314],\n",
      "        [-0.5185],\n",
      "        [ 3.7574],\n",
      "        [-2.1491]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6851, -0.5808, -0.6199,  1.1592,  0.5400],\n",
      "        [ 0.3744, -0.6112, -1.1826,  1.4153,  0.4748],\n",
      "        [ 1.5997,  1.5213, -0.0087, -0.2828, -0.8379],\n",
      "        [ 0.0423,  0.3975,  0.1039,  1.2388, -0.8175]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6705, -0.0475,  0.2832, -0.5515, -0.4078],\n",
      "        [ 0.4677, -0.8015,  0.4992, -0.5873, -0.4306],\n",
      "        [-0.2158,  0.6924, -0.1106, -0.7442,  0.0773],\n",
      "        [ 1.0663,  1.0844,  0.8923,  0.6953,  1.4231]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6851, -0.5808, -0.6199,  1.1592,  0.5400],\n",
      "        [ 0.3744, -0.6112, -1.1826,  1.4153,  0.4748],\n",
      "        [ 1.5997,  1.5213, -0.0087, -0.2828, -0.8379],\n",
      "        [ 0.0423,  0.3975,  0.1039,  1.2388, -0.8175]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1224],\n",
      "        [-0.9611],\n",
      "        [ 0.8547],\n",
      "        [ 0.2666]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8887,  0.1198,  0.9534,  1.2278,  0.1679],\n",
      "        [ 0.9007,  2.3388, -0.6738,  1.2472, -2.2226],\n",
      "        [ 0.6035, -0.5448,  0.7113, -0.0143,  0.0599],\n",
      "        [ 0.7150,  1.1840,  0.7643, -0.6512,  0.7391]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2239, -0.2201,  0.9322,  1.0369,  0.7125],\n",
      "        [ 0.7909,  0.1139,  0.1487, -0.6739,  0.8425],\n",
      "        [ 0.1637,  0.2158, -0.4732, -0.1407, -0.4331],\n",
      "        [ 1.2262,  0.7739,  1.0443,  1.2522,  1.3464]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8887,  0.1198,  0.9534,  1.2278,  0.1679],\n",
      "        [ 0.9007,  2.3388, -0.6738,  1.2472, -2.2226],\n",
      "        [ 0.6035, -0.5448,  0.7113, -0.0143,  0.0599],\n",
      "        [ 0.7150,  1.1840,  0.7643, -0.6512,  0.7391]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0562],\n",
      "        [-1.8343],\n",
      "        [-0.3793],\n",
      "        [ 2.7709]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[ 0.4642,  0.2717, -0.0489,  0.5840,  0.0455],\n",
      "        [ 0.0865,  0.9706,  1.0821,  2.6211,  0.8272],\n",
      "        [ 0.9924, -0.0465, -0.1908, -0.2730, -0.3955],\n",
      "        [ 0.1789, -0.5019, -1.2285, -0.1975,  0.0616]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4936, -0.8279, -0.8306, -1.1278, -1.5568],\n",
      "        [ 1.4222,  1.1920,  0.4892,  0.6811,  0.9609],\n",
      "        [ 0.5089, -0.8495, -0.1011, -0.2917,  0.1877],\n",
      "        [-0.6372, -0.5296, -0.3425, -0.8469, -1.3967]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4642,  0.2717, -0.0489,  0.5840,  0.0455],\n",
      "        [ 0.0865,  0.9706,  1.0821,  2.6211,  0.8272],\n",
      "        [ 0.9924, -0.0465, -0.1908, -0.2730, -0.3955],\n",
      "        [ 0.1789, -0.5019, -1.2285, -0.1975,  0.0616]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1428],\n",
      "        [ 4.3894],\n",
      "        [ 0.5693],\n",
      "        [ 0.6537]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3037, -0.9361,  1.7391, -0.3754,  0.2563],\n",
      "        [ 1.3162,  0.6900,  0.0470, -0.5740, -1.0160],\n",
      "        [ 0.6030,  0.2131, -0.6549,  0.7270, -1.4009],\n",
      "        [-1.9462, -0.7330, -0.8811,  2.8685,  3.0832]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5527, -0.4099, -0.1156, -0.1676, -0.6874],\n",
      "        [-0.8665, -1.5047, -0.7124, -1.7643, -2.7635],\n",
      "        [ 0.4394,  0.0851, -0.2712,  0.1237,  0.0644],\n",
      "        [-0.5899, -1.3563, -0.7151, -1.2488, -1.7535]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3037, -0.9361,  1.7391, -0.3754,  0.2563],\n",
      "        [ 1.3162,  0.6900,  0.0470, -0.5740, -1.0160],\n",
      "        [ 0.6030,  0.2131, -0.6549,  0.7270, -1.4009],\n",
      "        [-1.9462, -0.7330, -0.8811,  2.8685,  3.0832]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6512],\n",
      "        [ 1.6083],\n",
      "        [ 0.4605],\n",
      "        [-6.2161]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8653, -1.1174, -0.2865,  1.0645,  0.0329],\n",
      "        [-0.1367, -0.2212, -0.7437,  0.9022, -0.6999],\n",
      "        [-2.4730, -0.0261,  1.0585,  0.7717,  1.3070],\n",
      "        [ 0.8216,  0.2557, -0.9679,  1.4376, -0.9847]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2615, -0.0950, -0.1177, -0.2608,  0.2997],\n",
      "        [-1.0874, -1.9602, -1.6032, -1.7872, -3.8482],\n",
      "        [-0.1056,  0.8943,  0.0890, -0.1150, -0.0747],\n",
      "        [ 1.7589,  2.2860,  1.6261,  2.0272,  1.7476]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8653, -1.1174, -0.2865,  1.0645,  0.0329],\n",
      "        [-0.1367, -0.2212, -0.7437,  0.9022, -0.6999],\n",
      "        [-2.4730, -0.0261,  1.0585,  0.7717,  1.3070],\n",
      "        [ 0.8216,  0.2557, -0.9679,  1.4376, -0.9847]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0984],\n",
      "        [ 2.8552],\n",
      "        [ 0.1456],\n",
      "        [ 1.6493]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1868, -0.2012,  0.2740,  0.8330,  0.0129],\n",
      "        [-0.5041,  0.0725, -0.9226, -0.4875,  0.7335],\n",
      "        [ 1.5651, -2.5315,  1.7580,  0.1722,  0.0379],\n",
      "        [ 0.7483,  0.8343, -0.7345, -0.7006,  0.3530]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0045,  0.3429,  0.5438, -0.5173,  0.5220],\n",
      "        [ 0.3708, -0.1841, -0.3509, -0.4208,  0.5495],\n",
      "        [ 0.1281,  0.0960,  0.7808,  0.6515,  0.7424],\n",
      "        [ 0.9512,  2.0261,  1.3377,  1.9875,  1.9367]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1868, -0.2012,  0.2740,  0.8330,  0.0129],\n",
      "        [-0.5041,  0.0725, -0.9226, -0.4875,  0.7335],\n",
      "        [ 1.5651, -2.5315,  1.7580,  0.1722,  0.0379],\n",
      "        [ 0.7483,  0.8343, -0.7345, -0.7006,  0.3530]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3433],\n",
      "        [ 0.7316],\n",
      "        [ 1.4705],\n",
      "        [ 0.7111]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6693,  2.1701, -0.0007,  0.8696, -1.2777],\n",
      "        [ 0.1572,  0.0891,  0.3816, -0.0062, -0.1609],\n",
      "        [-0.8242, -0.6386,  0.5595, -1.2814,  1.7075],\n",
      "        [ 1.2146, -0.4488,  0.0486,  1.2720,  0.7583]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0181,  0.1709, -0.7357, -0.6136,  0.1159],\n",
      "        [-0.1546,  0.4405,  0.1850, -0.4571, -0.0550],\n",
      "        [-0.3475, -0.0001, -0.8529, -0.4960, -0.8465],\n",
      "        [ 0.3057,  0.0957,  0.1931, -0.3521,  0.1693]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6693,  2.1701, -0.0007,  0.8696, -1.2777],\n",
      "        [ 0.1572,  0.0891,  0.3816, -0.0062, -0.1609],\n",
      "        [-0.8242, -0.6386,  0.5595, -1.2814,  1.7075],\n",
      "        [ 1.2146, -0.4488,  0.0486,  1.2720,  0.7583]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3405],\n",
      "        [ 0.0972],\n",
      "        [-1.0004],\n",
      "        [ 0.0182]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1073,  1.7932, -0.6279,  0.1550,  0.7670],\n",
      "        [ 1.5082, -0.5615, -0.0586, -0.2532, -0.2471],\n",
      "        [-0.9474, -0.2030,  0.0749,  0.4051, -0.5244],\n",
      "        [-0.6335, -0.5696,  0.8331,  0.3308,  0.5863]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2481,  0.4653, -0.6000, -0.2655, -0.2588],\n",
      "        [ 0.0514,  0.1486,  0.3841,  0.3150, -0.7723],\n",
      "        [ 0.2267,  0.3215,  0.4717,  0.5797,  0.1397],\n",
      "        [-0.0460,  0.2340, -0.2162, -0.4055, -0.0641]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1073,  1.7932, -0.6279,  0.1550,  0.7670],\n",
      "        [ 1.5082, -0.5615, -0.0586, -0.2532, -0.2471],\n",
      "        [-0.9474, -0.2030,  0.0749,  0.4051, -0.5244],\n",
      "        [-0.6335, -0.5696,  0.8331,  0.3308,  0.5863]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6967],\n",
      "        [ 0.0826],\n",
      "        [-0.0831],\n",
      "        [-0.4560]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2525, -0.3225,  0.4531, -1.2921, -0.4264],\n",
      "        [-0.3300, -0.2665,  0.4369,  1.0419, -0.1455],\n",
      "        [ 2.1523,  0.5530,  0.2904, -0.9862,  0.1797],\n",
      "        [ 0.7045,  0.4391, -1.3442, -0.0020,  1.1497]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1856, -0.6100, -0.7882,  0.1871, -0.8059],\n",
      "        [-0.1274, -0.2435,  0.0200, -0.9161,  0.1427],\n",
      "        [-0.0721,  0.0478, -0.1012,  0.2230, -0.1003],\n",
      "        [ 0.3293, -0.3609, -0.1023, -0.0596,  0.2505]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2525, -0.3225,  0.4531, -1.2921, -0.4264],\n",
      "        [-0.3300, -0.2665,  0.4369,  1.0419, -0.1455],\n",
      "        [ 2.1523,  0.5530,  0.2904, -0.9862,  0.1797],\n",
      "        [ 0.7045,  0.4391, -1.3442, -0.0020,  1.1497]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2910],\n",
      "        [-0.8596],\n",
      "        [-0.3961],\n",
      "        [ 0.4991]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1480, -0.0469, -0.6963,  2.1472, -0.8152],\n",
      "        [ 0.6273,  1.1988,  0.5899,  0.3366,  0.3114],\n",
      "        [ 0.5670,  0.6624, -0.3777,  2.3695,  0.8554],\n",
      "        [-1.1938,  0.7636, -0.0978,  0.9379,  1.7296]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0594, -0.8541, -0.3949, -0.2818, -0.1668],\n",
      "        [ 0.4016,  0.3928,  0.6344,  0.5640,  0.6315],\n",
      "        [-0.1184,  0.3161, -0.3417, -0.1432, -0.0118],\n",
      "        [-0.0064,  0.1874, -0.5245, -0.1757, -0.8317]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1480, -0.0469, -0.6963,  2.1472, -0.8152],\n",
      "        [ 0.6273,  1.1988,  0.5899,  0.3366,  0.3114],\n",
      "        [ 0.5670,  0.6624, -0.3777,  2.3695,  0.8554],\n",
      "        [-1.1938,  0.7636, -0.0978,  0.9379,  1.7296]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2223],\n",
      "        [ 1.4836],\n",
      "        [-0.0780],\n",
      "        [-1.4014]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2982,  1.9138, -0.1869,  1.5157, -0.1913],\n",
      "        [ 0.0917,  1.8930,  1.6243, -0.5548, -0.2834],\n",
      "        [-1.4528,  1.8110,  0.7059,  1.0137, -0.2336],\n",
      "        [-1.4650,  0.7702, -0.4648, -0.4654,  0.9329]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1465, -0.0512,  0.0418, -0.7836, -0.2011],\n",
      "        [-0.5211, -0.3517, -0.2356, -0.3245, -1.0206],\n",
      "        [ 0.7572, -0.6771, -0.3240, -0.1407,  0.0626],\n",
      "        [ 0.0718,  0.5136,  0.7983,  0.4176,  1.0606]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2982,  1.9138, -0.1869,  1.5157, -0.1913],\n",
      "        [ 0.0917,  1.8930,  1.6243, -0.5548, -0.2834],\n",
      "        [-1.4528,  1.8110,  0.7059,  1.0137, -0.2336],\n",
      "        [-1.4650,  0.7702, -0.4648, -0.4654,  0.9329]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2986],\n",
      "        [-0.6271],\n",
      "        [-2.7122],\n",
      "        [ 0.7145]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.4448, -0.3916,  0.8877, -1.2949, -0.4404],\n",
      "        [ 0.7974,  0.0580,  0.0834,  1.0716,  1.3310],\n",
      "        [ 0.7331,  0.4893,  2.0114, -0.4237,  0.2208],\n",
      "        [-2.2700, -0.7453,  1.6776, -0.5058,  2.1581]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3359,  0.1464, -0.0851, -0.0474,  0.1367],\n",
      "        [-0.2516, -0.3668, -0.9520, -0.6285, -0.4566],\n",
      "        [ 0.9206,  0.7321,  0.9814,  0.5165,  0.7467],\n",
      "        [ 0.2388, -0.0270,  0.3336,  0.2857, -0.1201]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.4448, -0.3916,  0.8877, -1.2949, -0.4404],\n",
      "        [ 0.7974,  0.0580,  0.0834,  1.0716,  1.3310],\n",
      "        [ 0.7331,  0.4893,  2.0114, -0.4237,  0.2208],\n",
      "        [-2.2700, -0.7453,  1.6776, -0.5058,  2.1581]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9529],\n",
      "        [-1.5825],\n",
      "        [ 2.9530],\n",
      "        [-0.3660]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.0806, -0.1202, -0.8703,  1.9359,  0.1517],\n",
      "        [-0.5417,  0.8996, -0.5386,  0.9604, -0.0427],\n",
      "        [-0.2679, -0.9674,  1.5916,  1.3585,  1.3228],\n",
      "        [-0.7580, -0.3907,  0.0313, -1.0710,  0.1439]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0332, -0.0942,  0.1588, -0.0153,  0.8565],\n",
      "        [ 0.2611,  0.4718,  0.4009,  1.1618,  0.6778],\n",
      "        [-0.7616, -1.3493, -0.2355,  0.1719, -1.0281],\n",
      "        [ 0.1791,  0.2745,  0.3906,  0.1636,  0.7141]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.0806, -0.1202, -0.8703,  1.9359,  0.1517],\n",
      "        [-0.5417,  0.8996, -0.5386,  0.9604, -0.0427],\n",
      "        [-0.2679, -0.9674,  1.5916,  1.3585,  1.3228],\n",
      "        [-0.7580, -0.3907,  0.0313, -1.0710,  0.1439]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1230],\n",
      "        [ 1.1540],\n",
      "        [ 0.0081],\n",
      "        [-0.3033]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4467, -1.7644,  0.0382,  0.0191,  0.5890],\n",
      "        [-0.2031, -1.5711,  0.7688,  0.7655,  0.6767],\n",
      "        [-0.0554,  1.6009,  1.4173,  0.1101,  0.0662],\n",
      "        [ 0.1858, -1.3231, -0.7454,  1.5868,  0.6287]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3621, -0.9096, -0.8893, -0.8241, -1.1630],\n",
      "        [-0.0420, -0.1555,  0.2305,  0.0136,  0.1488],\n",
      "        [-0.1278,  0.1301,  0.2985,  0.1257, -0.5595],\n",
      "        [ 0.5243,  0.4718, -0.1446, -0.3830, -0.4427]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4467, -1.7644,  0.0382,  0.0191,  0.5890],\n",
      "        [-0.2031, -1.5711,  0.7688,  0.7655,  0.6767],\n",
      "        [-0.0554,  1.6009,  1.4173,  0.1101,  0.0662],\n",
      "        [ 0.1858, -1.3231, -0.7454,  1.5868,  0.6287]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0319],\n",
      "        [ 0.5411],\n",
      "        [ 0.6152],\n",
      "        [-1.3050]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1255, -1.3275,  0.0973,  0.4553, -0.8362],\n",
      "        [ 1.1824,  2.9552,  0.4470, -1.5132,  0.7485],\n",
      "        [ 0.3338,  0.2930, -0.4241, -0.1249,  0.2761],\n",
      "        [-0.4896,  1.0724, -0.1671,  0.8662, -0.1569]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6512, -1.0141, -0.7492, -0.7677, -1.6068],\n",
      "        [ 0.1095, -0.0905,  0.1661, -0.1816,  0.3530],\n",
      "        [-0.0189,  0.4036,  0.0125, -0.0179, -0.5487],\n",
      "        [ 0.6061,  0.2171,  0.2515,  0.6548,  1.4391]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1255, -1.3275,  0.0973,  0.4553, -0.8362],\n",
      "        [ 1.1824,  2.9552,  0.4470, -1.5132,  0.7485],\n",
      "        [ 0.3338,  0.2930, -0.4241, -0.1249,  0.2761],\n",
      "        [-0.4896,  1.0724, -0.1671,  0.8662, -0.1569]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3491],\n",
      "        [ 0.4753],\n",
      "        [-0.0426],\n",
      "        [ 0.2355]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8394, -2.4661,  1.3229,  0.5473,  1.7814],\n",
      "        [-2.3907,  0.8941,  1.3863,  0.8570,  0.5502],\n",
      "        [ 0.1212,  0.9125,  2.7751,  1.1641, -1.1703],\n",
      "        [ 0.1619,  1.2094, -0.8940, -0.4029,  0.4466]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1617, -1.0532, -1.2386, -2.2124, -3.0360],\n",
      "        [ 0.3813,  0.9737,  0.8708,  0.3411, -0.0100],\n",
      "        [-0.1069,  0.2996,  0.5030,  0.0294,  0.3823],\n",
      "        [ 0.1130, -0.4316, -0.3501, -0.5875,  0.1475]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8394, -2.4661,  1.3229,  0.5473,  1.7814],\n",
      "        [-2.3907,  0.8941,  1.3863,  0.8570,  0.5502],\n",
      "        [ 0.1212,  0.9125,  2.7751,  1.1641, -1.1703],\n",
      "        [ 0.1619,  1.2094, -0.8940, -0.4029,  0.4466]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.6854],\n",
      "        [ 1.4530],\n",
      "        [ 1.2433],\n",
      "        [ 0.1120]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3617,  0.4204,  0.4956,  0.0305,  1.3262],\n",
      "        [ 1.5297, -0.7829,  0.4554,  0.8299,  0.6713],\n",
      "        [-1.0092,  1.6884,  1.1564,  2.4097,  0.4373],\n",
      "        [ 1.0036, -0.1317,  0.5659,  0.7994,  0.6944]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6501,  0.3174,  0.0727, -0.1517,  0.3952],\n",
      "        [-0.4339, -0.5738, -0.8114, -0.3465, -1.2409],\n",
      "        [-0.7602, -0.5156, -0.3100, -0.2013, -1.2620],\n",
      "        [ 0.4869, -0.5554, -0.0531, -0.6136,  0.2662]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3617,  0.4204,  0.4956,  0.0305,  1.3262],\n",
      "        [ 1.5297, -0.7829,  0.4554,  0.8299,  0.6713],\n",
      "        [-1.0092,  1.6884,  1.1564,  2.4097,  0.4373],\n",
      "        [ 1.0036, -0.1317,  0.5659,  0.7994,  0.6944]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1963],\n",
      "        [-1.7045],\n",
      "        [-1.4988],\n",
      "        [ 0.2260]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4198,  1.2948, -0.6956,  0.2478,  0.1654],\n",
      "        [ 0.0627, -1.0429,  0.5446,  0.5110, -0.4956],\n",
      "        [-0.2942,  0.3824,  0.0723, -0.3368,  0.1051],\n",
      "        [-0.3873, -2.7425, -0.9899,  1.1574, -1.7147]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7205, -0.6086, -0.5689, -0.8488, -0.7015],\n",
      "        [ 0.0229,  0.6510,  0.8420,  0.5409,  0.7631],\n",
      "        [ 0.1106,  0.5058,  0.5873,  0.5219,  0.9725],\n",
      "        [ 0.3826,  0.2589,  0.4073, -0.2515,  0.3211]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4198,  1.2948, -0.6956,  0.2478,  0.1654],\n",
      "        [ 0.0627, -1.0429,  0.5446,  0.5110, -0.4956],\n",
      "        [-0.2942,  0.3824,  0.0723, -0.3368,  0.1051],\n",
      "        [-0.3873, -2.7425, -0.9899,  1.1574, -1.7147]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4161],\n",
      "        [-0.3207],\n",
      "        [ 0.1298],\n",
      "        [-2.1031]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3605,  1.9525, -1.3563,  0.3825, -0.3071],\n",
      "        [ 0.0927, -0.6523,  0.8401,  1.1573,  0.7985],\n",
      "        [ 0.1359,  0.3935, -1.8484, -1.2276,  1.0364],\n",
      "        [-0.3445,  1.5769,  0.0036, -2.8607,  1.1483]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4761, -1.7372, -1.2785, -0.8745, -1.2533],\n",
      "        [ 0.4474,  0.0779, -0.0117,  0.1358,  0.8828],\n",
      "        [-0.1500,  0.1987, -0.3201, -0.1317,  0.0667],\n",
      "        [ 0.4345,  1.3829,  0.9006,  1.0352,  1.4714]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3605,  1.9525, -1.3563,  0.3825, -0.3071],\n",
      "        [ 0.0927, -0.6523,  0.8401,  1.1573,  0.7985],\n",
      "        [ 0.1359,  0.3935, -1.8484, -1.2276,  1.0364],\n",
      "        [-0.3445,  1.5769,  0.0036, -2.8607,  1.1483]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2552],\n",
      "        [ 0.8428],\n",
      "        [ 0.8804],\n",
      "        [ 0.7624]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6000, -0.1360,  0.5753, -0.1955, -0.9685],\n",
      "        [-0.9241, -0.9830,  2.1941, -0.5540, -1.1185],\n",
      "        [-0.3299, -0.4137,  1.5395, -0.6617, -0.3149],\n",
      "        [-0.3560, -0.1418, -2.0896,  1.7931,  2.6758]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9286, -0.0584,  0.5283,  0.2006,  1.3005],\n",
      "        [-0.4976, -0.1298,  0.4608,  0.4666, -0.8161],\n",
      "        [ 0.2512, -0.0859, -0.1831,  0.6719,  0.1371],\n",
      "        [ 0.4540,  0.3364,  0.1684,  0.6853,  1.4769]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6000, -0.1360,  0.5753, -0.1955, -0.9685],\n",
      "        [-0.9241, -0.9830,  2.1941, -0.5540, -1.1185],\n",
      "        [-0.3299, -0.4137,  1.5395, -0.6617, -0.3149],\n",
      "        [-0.3560, -0.1418, -2.0896,  1.7931,  2.6758]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4726],\n",
      "        [ 2.2527],\n",
      "        [-0.8168],\n",
      "        [ 4.6194]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3036, -0.9421, -1.0203, -0.4667,  0.5688],\n",
      "        [-1.4243,  0.0877, -0.0504,  1.9786, -1.2334],\n",
      "        [-0.2402,  0.3272,  1.2740,  0.9536,  0.6529],\n",
      "        [-1.4699, -0.4102,  0.5893, -0.4711,  0.2999]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5602,  1.3081,  1.2870,  0.7550,  1.5165],\n",
      "        [-0.2766, -1.2778, -0.8349, -0.7612, -1.6947],\n",
      "        [ 0.8957, -0.4287,  0.0137,  0.2259,  0.5213],\n",
      "        [-0.9111, -1.0513, -1.2521, -1.9923, -2.4464]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3036, -0.9421, -1.0203, -0.4667,  0.5688],\n",
      "        [-1.4243,  0.0877, -0.0504,  1.9786, -1.2334],\n",
      "        [-0.2402,  0.3272,  1.2740,  0.9536,  0.6529],\n",
      "        [-1.4699, -0.4102,  0.5893, -0.4711,  0.2999]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.0691],\n",
      "        [ 0.9080],\n",
      "        [ 0.2178],\n",
      "        [ 1.2374]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4826,  1.1289, -1.0985,  0.3597,  0.8878],\n",
      "        [ 1.2658, -1.5072,  0.6300,  1.8979, -0.2369],\n",
      "        [-0.3000,  0.6596,  1.7331, -1.1868, -0.1210],\n",
      "        [-1.8397,  0.2011, -0.1678,  0.4692, -0.9503]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.1856,  2.4726,  2.6005,  1.4474,  3.5096],\n",
      "        [-0.7765, -1.0863, -1.4949, -1.3352, -2.8425],\n",
      "        [-0.2042, -0.1001, -0.1026, -0.0611,  0.1963],\n",
      "        [-1.2253, -1.5859, -1.5155, -2.2273, -3.2249]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4826,  1.1289, -1.0985,  0.3597,  0.8878],\n",
      "        [ 1.2658, -1.5072,  0.6300,  1.8979, -0.2369],\n",
      "        [-0.3000,  0.6596,  1.7331, -1.1868, -0.1210],\n",
      "        [-1.8397,  0.2011, -0.1678,  0.4692, -0.9503]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3308],\n",
      "        [-2.1482],\n",
      "        [-0.1339],\n",
      "        [ 4.2091]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8638,  0.1199,  0.7854,  2.1506,  1.7694],\n",
      "        [ 0.3757,  0.9635, -1.4540, -0.9210, -0.7573],\n",
      "        [ 0.0934,  1.0869,  1.5351, -1.4407,  0.4068],\n",
      "        [ 0.2793, -0.2658,  0.1662,  0.7656, -0.8758]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2259,  0.1388, -0.3186,  0.2852,  0.2252],\n",
      "        [-0.2750, -0.7472, -0.9348, -0.4913, -0.6123],\n",
      "        [ 0.8217, -0.6904,  0.3570,  0.2646, -0.7027],\n",
      "        [ 0.2521, -0.2488, -0.0894,  0.2521, -0.3174]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8638,  0.1199,  0.7854,  2.1506,  1.7694],\n",
      "        [ 0.3757,  0.9635, -1.4540, -0.9210, -0.7573],\n",
      "        [ 0.0934,  1.0869,  1.5351, -1.4407,  0.4068],\n",
      "        [ 0.2793, -0.2658,  0.1662,  0.7656, -0.8758]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3572],\n",
      "        [ 1.4520],\n",
      "        [-0.7928],\n",
      "        [ 0.5927]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1168,  2.4405,  1.2658, -1.4708,  0.2327],\n",
      "        [ 0.8145,  0.5896, -0.8955,  0.5342, -1.8356],\n",
      "        [ 0.1524,  1.9119, -0.5875, -0.0047,  0.6524],\n",
      "        [ 0.6263,  0.4065,  1.0431,  1.0580,  0.2969]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0950, -0.2181, -0.1908, -0.0335,  0.5616],\n",
      "        [-0.8045, -0.4868, -0.8986, -0.7190, -1.5213],\n",
      "        [ 0.5553, -0.7786,  0.2180, -0.1044,  0.5005],\n",
      "        [ 0.2063,  0.2392,  0.6692, -0.3413,  0.4471]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1168,  2.4405,  1.2658, -1.4708,  0.2327],\n",
      "        [ 0.8145,  0.5896, -0.8955,  0.5342, -1.8356],\n",
      "        [ 0.1524,  1.9119, -0.5875, -0.0047,  0.6524],\n",
      "        [ 0.6263,  0.4065,  1.0431,  1.0580,  0.2969]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5828],\n",
      "        [ 2.2708],\n",
      "        [-1.2050],\n",
      "        [ 0.6962]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8539,  0.9873, -1.1030,  0.6308, -0.6589],\n",
      "        [ 0.4407,  0.2598,  0.6033, -0.7389,  0.6506],\n",
      "        [-2.1737, -1.8542,  1.8507, -0.5265,  1.1321],\n",
      "        [ 1.2305,  0.9951, -0.5430,  0.4328,  0.5184]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3419,  0.2236,  0.4615, -1.1315,  0.4114],\n",
      "        [-0.0129, -0.0403,  0.0951,  0.4147, -0.3277],\n",
      "        [ 0.5099,  1.0318,  0.4239,  0.4163,  2.0633],\n",
      "        [-0.2141,  0.1224, -0.2629,  0.1366,  0.1713]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8539,  0.9873, -1.1030,  0.6308, -0.6589],\n",
      "        [ 0.4407,  0.2598,  0.6033, -0.7389,  0.6506],\n",
      "        [-2.1737, -1.8542,  1.8507, -0.5265,  1.1321],\n",
      "        [ 1.2305,  0.9951, -0.5430,  0.4328,  0.5184]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9812],\n",
      "        [-0.4784],\n",
      "        [-0.1205],\n",
      "        [ 0.1490]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5329,  1.1708, -0.7012, -1.0241,  1.0051],\n",
      "        [ 0.5322,  0.8382, -0.7189, -0.5125, -0.1782],\n",
      "        [ 0.1453, -0.5476, -0.8363,  0.7104,  0.2746],\n",
      "        [-1.0336,  0.0290,  0.4831, -0.0181,  0.8652]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3504,  0.1927,  0.8688,  0.0095,  1.0931],\n",
      "        [-0.3438,  0.8588,  0.4836, -0.1608,  0.4388],\n",
      "        [ 0.2979,  0.4873,  0.4234, -0.1139,  1.3410],\n",
      "        [-0.4327,  0.1326,  0.2519, -0.0395,  0.3931]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5329,  1.1708, -0.7012, -1.0241,  1.0051],\n",
      "        [ 0.5322,  0.8382, -0.7189, -0.5125, -0.1782],\n",
      "        [ 0.1453, -0.5476, -0.8363,  0.7104,  0.2746],\n",
      "        [-1.0336,  0.0290,  0.4831, -0.0181,  0.8652]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5186],\n",
      "        [ 0.1933],\n",
      "        [-0.2904],\n",
      "        [ 0.9136]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9172,  0.7995, -0.5613, -0.4962, -0.6719],\n",
      "        [ 0.1728, -1.3191, -1.5423, -0.4488,  0.9615],\n",
      "        [-0.2703,  0.4107, -0.1406,  0.0729, -1.6615],\n",
      "        [ 1.1172,  1.0443,  0.4903,  0.4867, -1.3854]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0181,  0.1416, -0.6621, -0.0655, -0.1334],\n",
      "        [ 0.1149, -0.1698, -0.1029, -0.1399,  0.6687],\n",
      "        [-0.0687, -0.3815, -0.0728, -0.2271, -0.2794],\n",
      "        [ 0.3391,  0.3420, -0.6468, -0.2424, -0.3096]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9172,  0.7995, -0.5613, -0.4962, -0.6719],\n",
      "        [ 0.1728, -1.3191, -1.5423, -0.4488,  0.9615],\n",
      "        [-0.2703,  0.4107, -0.1406,  0.0729, -1.6615],\n",
      "        [ 1.1172,  1.0443,  0.4903,  0.4867, -1.3854]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6235],\n",
      "        [ 1.1083],\n",
      "        [ 0.3198],\n",
      "        [ 0.7298]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5979,  0.9183, -0.1571, -1.0853,  0.5650],\n",
      "        [ 0.7983, -0.2293,  1.1073,  0.7669, -1.2500],\n",
      "        [ 1.3011,  0.5976, -0.0498, -0.0925, -0.3341],\n",
      "        [-0.1576,  0.2836,  1.8365,  0.7385, -0.7317]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1723, -0.2265,  0.6737, -0.1791,  0.6939],\n",
      "        [-0.3525, -0.5306, -0.7282, -0.0430, -1.3582],\n",
      "        [-0.0263, -0.4199, -0.3315, -0.4389, -0.3276],\n",
      "        [ 0.1087,  0.0987, -0.1853,  0.3595, -0.3789]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5979,  0.9183, -0.1571, -1.0853,  0.5650],\n",
      "        [ 0.7983, -0.2293,  1.1073,  0.7669, -1.2500],\n",
      "        [ 1.3011,  0.5976, -0.0498, -0.0925, -0.3341],\n",
      "        [-0.1576,  0.2836,  1.8365,  0.7385, -0.7317]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5479],\n",
      "        [ 0.6987],\n",
      "        [-0.1185],\n",
      "        [ 0.2132]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6662,  0.9125, -2.2944, -1.2130,  1.2699],\n",
      "        [-0.1580, -1.1033, -0.1125,  2.6808,  1.3213],\n",
      "        [ 0.2241,  0.2544,  0.4101,  0.9814,  1.5704],\n",
      "        [-0.5685, -0.5594, -0.8229, -0.2385,  1.0278]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2516,  0.5660, -0.2874, -0.0159,  0.4128],\n",
      "        [-0.5759,  0.4399, -0.3921, -0.1328, -0.8649],\n",
      "        [-0.0061,  0.1565, -0.9901, -0.2563,  0.1278],\n",
      "        [-0.1030,  0.1458,  0.8619,  0.2886,  0.1849]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6662,  0.9125, -2.2944, -1.2130,  1.2699],\n",
      "        [-0.1580, -1.1033, -0.1125,  2.6808,  1.3213],\n",
      "        [ 0.2241,  0.2544,  0.4101,  0.9814,  1.5704],\n",
      "        [-0.5685, -0.5594, -0.8229, -0.2385,  1.0278]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8868],\n",
      "        [-1.8491],\n",
      "        [-0.4184],\n",
      "        [-0.6109]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8513, -0.7898, -1.1112, -1.0972,  1.1883],\n",
      "        [ 1.7965,  0.0883, -0.2400, -0.4572,  1.5765],\n",
      "        [ 1.8603,  0.3226,  0.6159, -0.7399, -1.0701],\n",
      "        [ 0.0561,  1.6701,  0.0825, -1.3140, -0.8278]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6919, -0.7125, -0.9838, -0.6312, -1.4994],\n",
      "        [ 0.2414,  0.8244,  0.4923,  0.6040,  1.0900],\n",
      "        [ 0.5427, -0.2187, -0.3273,  0.4174,  0.3999],\n",
      "        [ 0.1696,  0.5269,  1.2217,  0.7614,  0.2230]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8513, -0.7898, -1.1112, -1.0972,  1.1883],\n",
      "        [ 1.7965,  0.0883, -0.2400, -0.4572,  1.5765],\n",
      "        [ 1.8603,  0.3226,  0.6159, -0.7399, -1.0701],\n",
      "        [ 0.0561,  1.6701,  0.0825, -1.3140, -0.8278]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0222],\n",
      "        [ 1.8306],\n",
      "        [ 0.0006],\n",
      "        [-0.1949]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2474, -0.4552,  0.2279,  0.7913,  1.1150],\n",
      "        [-0.5574, -0.8378, -0.1889,  0.3474,  1.5920],\n",
      "        [-0.3938,  1.5698,  0.2021,  0.0618, -1.7049],\n",
      "        [-0.4104,  0.9190,  0.6646,  1.3529,  1.2438]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6747, -0.5930, -1.2956, -0.7050, -1.1757],\n",
      "        [-0.7853, -0.0880, -0.1171, -0.4052, -0.6009],\n",
      "        [ 0.7250, -0.8506, -0.4667, -0.4287, -0.4020],\n",
      "        [-0.0252,  0.5935, -0.3920,  0.6882,  0.8280]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2474, -0.4552,  0.2279,  0.7913,  1.1150],\n",
      "        [-0.5574, -0.8378, -0.1889,  0.3474,  1.5920],\n",
      "        [-0.3938,  1.5698,  0.2021,  0.0618, -1.7049],\n",
      "        [-0.4104,  0.9190,  0.6646,  1.3529,  1.2438]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7274],\n",
      "        [-0.5637],\n",
      "        [-1.0562],\n",
      "        [ 2.2562]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9592,  0.7810,  0.6031,  0.2755, -0.0496],\n",
      "        [ 0.3750,  1.4308, -0.1035, -0.0771, -0.7530],\n",
      "        [-2.1058,  0.4179,  1.7223,  0.8001, -0.6257],\n",
      "        [-1.4993,  1.2896,  0.6488, -0.1373,  0.3690]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1839,  0.5967,  0.9064,  0.6640,  0.5508],\n",
      "        [-0.3974,  0.0819, -0.0606, -0.4052, -0.3787],\n",
      "        [ 0.5774, -0.0928,  0.4545,  0.3574,  0.6404],\n",
      "        [-0.2167, -0.8001, -0.6658, -0.8935, -1.5879]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9592,  0.7810,  0.6031,  0.2755, -0.0496],\n",
      "        [ 0.3750,  1.4308, -0.1035, -0.0771, -0.7530],\n",
      "        [-2.1058,  0.4179,  1.7223,  0.8001, -0.6257],\n",
      "        [-1.4993,  1.2896,  0.6488, -0.1373,  0.3690]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3446],\n",
      "        [ 0.2909],\n",
      "        [-0.5867],\n",
      "        [-1.6021]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2214,  0.8873, -0.3638, -1.3645,  2.2682],\n",
      "        [-1.0427,  1.1454,  0.4821,  0.5319,  0.0807],\n",
      "        [ 0.1480,  0.4215, -0.5484,  0.0306, -1.2679],\n",
      "        [-0.4308,  0.4961,  1.2391, -1.0298,  0.5809]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2040,  0.2446,  0.3887, -0.3587, -0.7307],\n",
      "        [ 0.1045, -0.6879,  0.0809, -0.1073, -0.0937],\n",
      "        [ 1.4540,  0.0870,  0.3954,  0.5642,  0.5905],\n",
      "        [ 0.1115,  0.1382,  0.5562,  0.3930,  0.0289]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2214,  0.8873, -0.3638, -1.3645,  2.2682],\n",
      "        [-1.0427,  1.1454,  0.4821,  0.5319,  0.0807],\n",
      "        [ 0.1480,  0.4215, -0.5484,  0.0306, -1.2679],\n",
      "        [-0.4308,  0.4961,  1.2391, -1.0298,  0.5809]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8430],\n",
      "        [-0.9224],\n",
      "        [-0.6963],\n",
      "        [ 0.3217]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3211,  0.6869,  0.9079,  0.2522, -1.4883],\n",
      "        [ 0.7463, -0.2028, -0.8289,  2.0809,  0.7551],\n",
      "        [-2.0424,  0.3130,  1.6052, -0.4004,  0.2908],\n",
      "        [ 1.3788, -1.5882, -0.8615,  0.0218,  0.6393]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1509,  0.0899, -0.3521,  0.5277,  0.2126],\n",
      "        [ 0.2851,  0.3247,  0.4517, -0.0759,  0.7235],\n",
      "        [ 0.3943,  0.1014,  0.3466,  0.5855,  1.1658],\n",
      "        [-0.1886,  0.2622,  0.8175, -0.2438, -0.0134]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3211,  0.6869,  0.9079,  0.2522, -1.4883],\n",
      "        [ 0.7463, -0.2028, -0.8289,  2.0809,  0.7551],\n",
      "        [-2.0424,  0.3130,  1.6052, -0.4004,  0.2908],\n",
      "        [ 1.3788, -1.5882, -0.8615,  0.0218,  0.6393]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3928],\n",
      "        [ 0.1609],\n",
      "        [-0.1124],\n",
      "        [-1.3947]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0109,  0.0832, -0.0813,  2.0123, -0.0087],\n",
      "        [-1.3421, -0.1443,  0.1112, -0.4969,  1.0136],\n",
      "        [-0.5320,  0.1239, -0.6536, -2.3383, -0.4078],\n",
      "        [ 1.4965, -0.1850, -1.2632,  1.3367, -0.2540]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2553, -0.1104,  0.1951, -0.9121,  0.1613],\n",
      "        [ 0.3947, -0.4823, -0.3263,  0.4056, -0.0370],\n",
      "        [ 0.3563,  0.1054, -0.1641,  0.8418,  1.0202],\n",
      "        [ 0.2042,  0.4595,  1.2274, -0.0389,  0.9910]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0109,  0.0832, -0.0813,  2.0123, -0.0087],\n",
      "        [-1.3421, -0.1443,  0.1112, -0.4969,  1.0136],\n",
      "        [-0.5320,  0.1239, -0.6536, -2.3383, -0.4078],\n",
      "        [ 1.4965, -0.1850, -1.2632,  1.3367, -0.2540]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8591],\n",
      "        [-0.7355],\n",
      "        [-2.4536],\n",
      "        [-1.6337]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.8441,  0.9049,  0.0375, -1.4575, -0.9433],\n",
      "        [ 0.3353,  0.9170,  0.2629,  0.6187,  2.2603],\n",
      "        [ 1.5482,  0.3258,  1.9247,  1.1114,  0.4371],\n",
      "        [-0.2854, -0.3667, -0.6426,  0.2272,  0.4376]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6361,  1.5011,  0.7930,  0.9175,  1.0734],\n",
      "        [ 0.0344, -0.6043, -0.5242, -0.2011, -0.5566],\n",
      "        [ 1.1732,  1.3819,  1.2367,  0.6528,  1.5451],\n",
      "        [ 1.0863,  0.9838,  0.8181,  1.3120,  1.2112]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.8441,  0.9049,  0.0375, -1.4575, -0.9433],\n",
      "        [ 0.3353,  0.9170,  0.2629,  0.6187,  2.2603],\n",
      "        [ 1.5482,  0.3258,  1.9247,  1.1114,  0.4371],\n",
      "        [-0.2854, -0.3667, -0.6426,  0.2272,  0.4376]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8473],\n",
      "        [-2.0631],\n",
      "        [ 6.0480],\n",
      "        [-0.3683]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.1626,  1.0646,  0.9032,  0.9255,  0.4512],\n",
      "        [ 0.5382,  1.3145,  0.7867, -0.1388, -0.8970],\n",
      "        [ 0.9731,  0.4047,  1.7369,  0.4847,  0.7351],\n",
      "        [ 0.4758,  0.8981, -0.9422, -0.8410,  1.0640]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8718, -0.1471,  1.0052,  0.1544,  1.0436],\n",
      "        [ 0.8793,  0.8674,  1.6331,  1.1890,  1.2123],\n",
      "        [-0.4918, -1.4276, -1.7000, -1.1229, -2.8125],\n",
      "        [ 0.9239,  1.3751,  1.7104,  1.1287,  0.8749]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1626,  1.0646,  0.9032,  0.9255,  0.4512],\n",
      "        [ 0.5382,  1.3145,  0.7867, -0.1388, -0.8970],\n",
      "        [ 0.9731,  0.4047,  1.7369,  0.4847,  0.7351],\n",
      "        [ 0.4758,  0.8981, -0.9422, -0.8410,  1.0640]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2234],\n",
      "        [ 1.6457],\n",
      "        [-6.6206],\n",
      "        [ 0.0446]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1668,  1.5478,  0.3396,  0.2563,  0.1627],\n",
      "        [ 0.9084,  1.4143,  1.4842,  1.4921,  0.7249],\n",
      "        [-0.3186,  0.5556, -0.5463, -0.1435,  0.5631],\n",
      "        [-0.7428, -0.6322,  0.1235,  0.5623,  0.6687]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3362,  0.3010, -1.0980, -0.7168, -0.8062],\n",
      "        [ 0.0477, -0.4143, -0.2730, -0.0712, -0.3315],\n",
      "        [ 1.4373,  1.0086,  1.8593,  1.2408,  1.3236],\n",
      "        [ 0.1136,  0.8864,  0.9305,  1.2721,  2.1920]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1668,  1.5478,  0.3396,  0.2563,  0.1627],\n",
      "        [ 0.9084,  1.4143,  1.4842,  1.4921,  0.7249],\n",
      "        [-0.3186,  0.5556, -0.5463, -0.1435,  0.5631],\n",
      "        [-0.7428, -0.6322,  0.1235,  0.5623,  0.6687]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2780],\n",
      "        [-1.2943],\n",
      "        [-0.3460],\n",
      "        [ 1.6513]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8954,  1.6875,  0.8511,  0.0197,  1.9159],\n",
      "        [ 0.4205, -0.2124,  1.3252, -0.7549,  0.1339],\n",
      "        [-0.8834, -0.7980,  0.1602,  2.1744,  0.5380],\n",
      "        [ 0.3423,  0.0453,  0.4327,  0.5830,  0.6054]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4630, -0.9294,  0.6648,  0.0384, -0.4083],\n",
      "        [ 1.2857,  0.5580,  0.8449,  0.9156,  0.4625],\n",
      "        [ 0.1628, -0.3320, -0.1727,  0.0955,  0.0376],\n",
      "        [-0.0426,  0.6721,  0.9535,  0.2093,  0.5001]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8954,  1.6875,  0.8511,  0.0197,  1.9159],\n",
      "        [ 0.4205, -0.2124,  1.3252, -0.7549,  0.1339],\n",
      "        [-0.8834, -0.7980,  0.1602,  2.1744,  0.5380],\n",
      "        [ 0.3423,  0.0453,  0.4327,  0.5830,  0.6054]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3695],\n",
      "        [ 0.9124],\n",
      "        [ 0.3214],\n",
      "        [ 0.8532]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8335, -1.0776, -0.1850,  0.1494, -1.4264],\n",
      "        [ 1.5529,  1.2906,  0.6316, -0.8224,  0.3978],\n",
      "        [ 0.4737,  0.3503,  2.0911,  1.3982,  0.7778],\n",
      "        [ 1.6284, -0.5985,  0.3612, -0.0892,  0.8864]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5656,  1.3490,  0.6574,  0.6848,  1.3625],\n",
      "        [ 0.2811,  0.3601, -0.4878,  0.4593,  0.3429],\n",
      "        [-0.0720, -0.1509,  0.1089, -0.1084,  0.0322],\n",
      "        [-0.2003,  0.6875,  0.0497,  0.6876,  0.3304]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8335, -1.0776, -0.1850,  0.1494, -1.4264],\n",
      "        [ 1.5529,  1.2906,  0.6316, -0.8224,  0.3978],\n",
      "        [ 0.4737,  0.3503,  2.0911,  1.3982,  0.7778],\n",
      "        [ 1.6284, -0.5985,  0.3612, -0.0892,  0.8864]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.4534],\n",
      "        [ 0.3517],\n",
      "        [ 0.0143],\n",
      "        [-0.4882]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1556, -0.9486, -0.5558,  0.6963,  0.9008],\n",
      "        [-1.3059,  0.1770,  1.6063, -0.0385, -0.6187],\n",
      "        [-0.3769, -0.9149,  0.8014,  0.4518,  0.7819],\n",
      "        [ 0.2580, -0.1548,  0.2017, -1.0523, -0.4742]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6076,  2.3235,  1.4529,  2.3674,  3.0515],\n",
      "        [ 0.6099,  0.3689, -0.2772,  0.1570,  0.5135],\n",
      "        [-0.2123, -0.1194, -0.0216, -0.3634,  0.0816],\n",
      "        [ 0.4140,  0.3820,  0.4667,  0.2524,  1.1349]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1556, -0.9486, -0.5558,  0.6963,  0.9008],\n",
      "        [-1.3059,  0.1770,  1.6063, -0.0385, -0.6187],\n",
      "        [-0.3769, -0.9149,  0.8014,  0.4518,  0.7819],\n",
      "        [ 0.2580, -0.1548,  0.2017, -1.0523, -0.4742]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6357],\n",
      "        [-1.5001],\n",
      "        [ 0.0716],\n",
      "        [-0.6620]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3428,  1.2846,  0.2528,  0.7585,  0.2738],\n",
      "        [ 0.4982,  0.1088,  0.5214,  0.5508,  2.2749],\n",
      "        [ 0.3576,  0.9014,  0.6942, -1.0278,  1.8941],\n",
      "        [ 1.8303,  1.0386,  1.1444,  1.2833,  0.5603]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0647, -0.4658,  0.0358,  0.1720, -0.3517],\n",
      "        [ 0.8127,  0.3067,  0.9282,  0.5024,  0.6786],\n",
      "        [-0.1879, -0.0860, -0.2776, -0.0600, -0.7041],\n",
      "        [ 0.5362,  1.6554,  0.4654,  0.9831,  1.0444]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3428,  1.2846,  0.2528,  0.7585,  0.2738],\n",
      "        [ 0.4982,  0.1088,  0.5214,  0.5508,  2.2749],\n",
      "        [ 0.3576,  0.9014,  0.6942, -1.0278,  1.8941],\n",
      "        [ 1.8303,  1.0386,  1.1444,  1.2833,  0.5603]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5330],\n",
      "        [ 2.7427],\n",
      "        [-1.6094],\n",
      "        [ 5.0801]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2325, -0.8605, -0.5699, -0.5451,  0.7092],\n",
      "        [ 1.6927, -1.3572, -0.4146,  0.0317,  0.2345],\n",
      "        [ 1.2037,  0.6917,  1.2577, -0.6246, -1.5294],\n",
      "        [-1.3164,  0.0387, -0.4638, -0.5166, -0.0815]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4337, -0.7644, -0.5695, -0.1215, -0.1256],\n",
      "        [-0.5773, -1.2364, -0.4764, -1.1689, -1.4062],\n",
      "        [ 0.5976,  0.1366,  0.6581,  0.3298,  2.1483],\n",
      "        [-1.3132, -1.5361, -1.1908, -2.3098, -3.7562]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2325, -0.8605, -0.5699, -0.5451,  0.7092],\n",
      "        [ 1.6927, -1.3572, -0.4146,  0.0317,  0.2345],\n",
      "        [ 1.2037,  0.6917,  1.2577, -0.6246, -1.5294],\n",
      "        [-1.3164,  0.0387, -0.4638, -0.5166, -0.0815]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4940],\n",
      "        [ 0.5317],\n",
      "        [-1.8500],\n",
      "        [ 3.7209]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6614,  0.0265, -0.8241, -0.5343,  1.3146],\n",
      "        [-2.7309, -0.1085,  0.3469, -0.4074,  0.0116],\n",
      "        [ 1.8119, -0.0182, -1.0236, -0.3316, -0.1733],\n",
      "        [ 0.8507, -0.5612,  2.0305,  0.1710, -0.1863]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3435, -0.3757, -0.5496, -0.5241, -1.1325],\n",
      "        [-0.0103, -0.4237, -0.2864, -0.3387, -0.8265],\n",
      "        [ 0.7274,  1.4856,  1.3602,  0.8138,  2.0464],\n",
      "        [-1.9138, -1.9548, -2.5985, -4.0380, -6.0527]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6614,  0.0265, -0.8241, -0.5343,  1.3146],\n",
      "        [-2.7309, -0.1085,  0.3469, -0.4074,  0.0116],\n",
      "        [ 1.8119, -0.0182, -1.0236, -0.3316, -0.1733],\n",
      "        [ 0.8507, -0.5612,  2.0305,  0.1710, -0.1863]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9930],\n",
      "        [ 0.1033],\n",
      "        [-0.7259],\n",
      "        [-5.3706]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4919,  0.1320, -1.1587, -0.7022,  0.5447],\n",
      "        [ 0.9544, -1.5511, -3.3190,  1.2773,  1.6421],\n",
      "        [ 1.2680,  1.0038,  0.4990, -0.3108, -1.0188],\n",
      "        [-1.1892,  0.2617, -0.4762, -0.5453, -0.1549]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1493, -0.2569, -0.1146, -0.2009, -1.0062],\n",
      "        [ 0.4991, -0.4720,  0.7097, -0.4746, -0.6977],\n",
      "        [ 0.8991,  1.7794,  1.1400,  1.5978,  1.9661],\n",
      "        [ 0.5260,  0.3784,  0.2270, -0.1207, -0.2724]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4919,  0.1320, -1.1587, -0.7022,  0.5447],\n",
      "        [ 0.9544, -1.5511, -3.3190,  1.2773,  1.6421],\n",
      "        [ 1.2680,  1.0038,  0.4990, -0.3108, -1.0188],\n",
      "        [-1.1892,  0.2617, -0.4762, -0.5453, -0.1549]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3817],\n",
      "        [-2.8989],\n",
      "        [ 0.9953],\n",
      "        [-0.5265]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5045, -0.1383,  1.5492, -0.8286, -1.6906],\n",
      "        [-0.3536, -0.4821, -0.5363,  1.4424,  2.0017],\n",
      "        [ 0.8064, -1.2451,  2.5200,  0.2473,  0.5001],\n",
      "        [ 0.0860, -0.0922,  1.5925, -0.3954,  0.4356]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1368, -0.1422, -0.8997, -0.4351, -0.8790],\n",
      "        [ 1.0180,  1.0605,  0.7913,  1.0487,  1.2214],\n",
      "        [ 0.7493,  1.1070,  1.0719,  1.1717,  1.2907],\n",
      "        [ 0.2479,  0.2269,  0.1591, -0.0536, -0.1102]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5045, -0.1383,  1.5492, -0.8286, -1.6906],\n",
      "        [-0.3536, -0.4821, -0.5363,  1.4424,  2.0017],\n",
      "        [ 0.8064, -1.2451,  2.5200,  0.2473,  0.5001],\n",
      "        [ 0.0860, -0.0922,  1.5925, -0.3954,  0.4356]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5414],\n",
      "        [ 2.6619],\n",
      "        [ 2.8625],\n",
      "        [ 0.2269]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5778,  0.3552,  1.2424,  0.5754, -0.1929],\n",
      "        [ 0.8362,  0.1699,  1.8774,  0.0535,  0.8411],\n",
      "        [ 0.2174, -0.6536,  0.8479,  1.0928, -1.6343],\n",
      "        [ 1.2738, -0.0429,  0.8326, -0.0759, -0.1609]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2730,  0.0355,  0.0554,  0.2243, -0.9881],\n",
      "        [ 0.0898,  0.3994, -0.3353, -0.1130, -1.2420],\n",
      "        [-0.5951,  0.3603,  0.2764,  0.0429, -0.2591],\n",
      "        [ 0.1362,  0.2116, -0.2575,  0.4541, -0.4073]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5778,  0.3552,  1.2424,  0.5754, -0.1929],\n",
      "        [ 0.8362,  0.1699,  1.8774,  0.0535,  0.8411],\n",
      "        [ 0.2174, -0.6536,  0.8479,  1.0928, -1.6343],\n",
      "        [ 1.2738, -0.0429,  0.8326, -0.0759, -0.1609]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2435],\n",
      "        [-1.5374],\n",
      "        [ 0.3398],\n",
      "        [-0.0189]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0554,  0.8114, -1.1104,  1.2089,  0.1590],\n",
      "        [ 1.6051, -0.0936, -0.6239, -0.2135,  0.1495],\n",
      "        [-0.3475,  2.3414, -0.6623, -0.8318, -0.0034],\n",
      "        [ 0.8399,  0.5141,  0.4615, -0.1115,  0.8465]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0870,  0.3374,  0.0897, -0.1582,  0.6523],\n",
      "        [ 0.5967,  1.3028,  0.8192,  0.6375,  1.3842],\n",
      "        [ 0.1631,  0.9664,  1.7049,  0.8614,  0.8879],\n",
      "        [ 0.0422,  0.1573, -0.0088, -0.0228,  0.7228]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0554,  0.8114, -1.1104,  1.2089,  0.1590],\n",
      "        [ 1.6051, -0.0936, -0.6239, -0.2135,  0.1495],\n",
      "        [-0.3475,  2.3414, -0.6623, -0.8318, -0.0034],\n",
      "        [ 0.8399,  0.5141,  0.4615, -0.1115,  0.8465]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0914],\n",
      "        [ 0.3958],\n",
      "        [ 0.3572],\n",
      "        [ 0.7266]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4766, -1.0836,  0.8370, -0.1680, -1.8861],\n",
      "        [-0.8410,  0.4167,  1.0741, -1.2048, -0.4346],\n",
      "        [ 1.3286, -0.9247, -0.9107,  1.5049, -0.0310],\n",
      "        [ 0.2263,  1.1334,  0.8499,  0.0365,  0.0856]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2779,  0.5884,  0.7562, -0.0451,  0.3336],\n",
      "        [ 0.3159,  0.6642,  0.6784, -0.0365,  0.7515],\n",
      "        [ 0.3270,  0.9753,  0.6151,  1.0220,  0.9840],\n",
      "        [-0.0418,  0.4185, -0.4234,  0.1410, -0.0303]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4766, -1.0836,  0.8370, -0.1680, -1.8861],\n",
      "        [-0.8410,  0.4167,  1.0741, -1.2048, -0.4346],\n",
      "        [ 1.3286, -0.9247, -0.9107,  1.5049, -0.0310],\n",
      "        [ 0.2263,  1.1334,  0.8499,  0.0365,  0.0856]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4939],\n",
      "        [ 0.4571],\n",
      "        [ 0.4797],\n",
      "        [ 0.1075]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2918,  1.6972, -0.3293, -0.4759,  0.4120],\n",
      "        [-0.9602, -0.6373, -0.4757,  1.3250, -0.9883],\n",
      "        [-0.2281,  1.0050, -0.2201, -0.4789, -1.0948],\n",
      "        [-0.9062, -0.5231,  1.0134, -0.3980,  0.6014]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2490, -0.2908, -0.3351, -0.0761, -0.5736],\n",
      "        [-0.1596,  0.2863,  0.5011,  0.3402,  1.0133],\n",
      "        [-0.0471,  0.9935,  0.7685,  1.1454,  0.8429],\n",
      "        [ 0.0466,  0.0092,  0.0734,  0.2863,  0.4086]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2918,  1.6972, -0.3293, -0.4759,  0.4120],\n",
      "        [-0.9602, -0.6373, -0.4757,  1.3250, -0.9883],\n",
      "        [-0.2281,  1.0050, -0.2201, -0.4789, -1.0948],\n",
      "        [-0.9062, -0.5231,  1.0134, -0.3980,  0.6014]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6559],\n",
      "        [-0.8183],\n",
      "        [-0.6313],\n",
      "        [ 0.1592]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2243, -0.1268, -0.3862,  0.2563,  0.8870],\n",
      "        [-0.5288, -0.6073, -0.5454, -0.3781,  1.1872],\n",
      "        [-1.3080,  0.5469,  2.6661,  0.3062,  0.1617],\n",
      "        [-2.4681,  0.0641,  0.4653,  0.7425,  1.7606]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4810, -1.1845, -0.2701,  0.1064, -0.5635],\n",
      "        [-0.1479,  0.4524,  1.0798,  0.2783,  0.8599],\n",
      "        [ 0.6888,  1.5050,  1.3267,  1.0986,  1.9454],\n",
      "        [ 0.4001, -0.3898,  0.2980, -0.2326,  0.2505]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2243, -0.1268, -0.3862,  0.2563,  0.8870],\n",
      "        [-0.5288, -0.6073, -0.5454, -0.3781,  1.1872],\n",
      "        [-1.3080,  0.5469,  2.6661,  0.3062,  0.1617],\n",
      "        [-2.4681,  0.0641,  0.4653,  0.7425,  1.7606]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3259],\n",
      "        [ 0.1303],\n",
      "        [ 4.1103],\n",
      "        [-0.6057]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9409,  0.5861,  0.0606, -1.9431,  1.5267],\n",
      "        [-1.1554, -1.3294, -0.0055, -1.3436,  1.1967],\n",
      "        [ 0.1080,  1.7075,  0.4113, -0.4063,  0.1551],\n",
      "        [-1.4176, -0.2134,  0.3320, -0.6737,  0.0448]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2017, -0.0417, -0.4163, -0.8140, -0.0849],\n",
      "        [ 0.0243,  0.5515,  0.2119,  0.8483,  0.9714],\n",
      "        [-0.3307, -0.8371, -0.1202, -1.1729, -1.5732],\n",
      "        [-0.1572,  0.0727,  0.3954, -0.1248,  0.2801]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9409,  0.5861,  0.0606, -1.9431,  1.5267],\n",
      "        [-1.1554, -1.3294, -0.0055, -1.3436,  1.1967],\n",
      "        [ 0.1080,  1.7075,  0.4113, -0.4063,  0.1551],\n",
      "        [-1.4176, -0.2134,  0.3320, -0.6737,  0.0448]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2126],\n",
      "        [-0.7397],\n",
      "        [-1.2819],\n",
      "        [ 0.4352]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5825,  0.4838, -1.8965, -1.0740,  0.8336],\n",
      "        [ 1.6412, -0.7910, -0.1754, -0.1042, -0.2768],\n",
      "        [-2.2331,  1.0000,  2.1136,  0.2671,  0.3024],\n",
      "        [-0.5676, -0.2798,  0.1117,  1.5207,  2.0466]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0393, -1.3796, -0.7182, -1.0311, -1.0778],\n",
      "        [ 0.2383,  0.0469,  1.3629,  0.1342,  0.8846],\n",
      "        [-0.0907,  0.5767,  0.2769,  0.5316,  0.3439],\n",
      "        [ 0.2599, -0.0832,  0.1927,  0.7651,  0.0902]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5825,  0.4838, -1.8965, -1.0740,  0.8336],\n",
      "        [ 1.6412, -0.7910, -0.1754, -0.1042, -0.2768],\n",
      "        [-2.2331,  1.0000,  2.1136,  0.2671,  0.3024],\n",
      "        [-0.5676, -0.2798,  0.1117,  1.5207,  2.0466]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9263],\n",
      "        [-0.1439],\n",
      "        [ 1.6104],\n",
      "        [ 1.2453]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4228, -1.1018,  0.9900, -0.8764,  0.7415],\n",
      "        [-0.0936,  0.1010, -0.1232,  0.7476,  0.2221],\n",
      "        [-0.4133, -1.4724,  0.1788, -0.9764,  0.6580],\n",
      "        [-0.2818, -0.6964, -1.1978, -0.1597, -0.6814]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2188,  0.2131, -1.5574, -0.9084, -1.8931],\n",
      "        [ 0.3010, -0.1515,  0.2940,  0.0339,  0.6526],\n",
      "        [-0.7479, -0.3142, -0.8230, -0.2540, -0.7353],\n",
      "        [-0.2745, -0.2109,  0.3585,  0.0820, -0.6884]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4228, -1.1018,  0.9900, -0.8764,  0.7415],\n",
      "        [-0.0936,  0.1010, -0.1232,  0.7476,  0.2221],\n",
      "        [-0.4133, -1.4724,  0.1788, -0.9764,  0.6580],\n",
      "        [-0.2818, -0.6964, -1.1978, -0.1597, -0.6814]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2919],\n",
      "        [ 0.0905],\n",
      "        [ 0.3887],\n",
      "        [ 0.2508]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2228,  0.0065,  1.6181, -0.0990,  0.8426],\n",
      "        [ 1.6804, -1.0192, -0.7910, -1.5976,  0.3828],\n",
      "        [ 0.1926, -0.5823,  0.9388, -0.1853, -1.3369],\n",
      "        [-0.3553,  1.3040,  1.1895, -0.4143,  1.4624]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9204,  0.5752,  0.2781, -0.6658,  0.2549],\n",
      "        [ 0.4691,  0.6479,  0.0242,  0.0275,  0.5204],\n",
      "        [-0.0544,  0.3051,  0.1750,  0.5369, -0.4949],\n",
      "        [-0.2925,  0.4219,  0.1061,  0.1213, -0.1213]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2228,  0.0065,  1.6181, -0.0990,  0.8426],\n",
      "        [ 1.6804, -1.0192, -0.7910, -1.5976,  0.3828],\n",
      "        [ 0.1926, -0.5823,  0.9388, -0.1853, -1.3369],\n",
      "        [-0.3553,  1.3040,  1.1895, -0.4143,  1.4624]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9396],\n",
      "        [ 0.2642],\n",
      "        [ 0.5384],\n",
      "        [ 0.5525]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4955,  0.3347,  2.4406,  0.6280,  0.1559],\n",
      "        [ 0.4784,  0.4995,  1.8832, -0.4872,  0.4304],\n",
      "        [-0.8009,  1.0582,  0.6082, -0.1561,  0.7728],\n",
      "        [ 0.1622,  0.0368, -0.5357,  1.0972, -0.7308]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0001, -0.6636, -1.2742, -0.5783,  0.2584],\n",
      "        [ 0.1592,  0.7980,  0.4312,  0.1940,  0.1248],\n",
      "        [-0.2304,  0.0955,  0.1619, -0.0869, -0.0603],\n",
      "        [-0.3057,  0.4310, -0.0597, -0.2309, -0.3293]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4955,  0.3347,  2.4406,  0.6280,  0.1559],\n",
      "        [ 0.4784,  0.4995,  1.8832, -0.4872,  0.4304],\n",
      "        [-0.8009,  1.0582,  0.6082, -0.1561,  0.7728],\n",
      "        [ 0.1622,  0.0368, -0.5357,  1.0972, -0.7308]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.6548],\n",
      "        [ 1.2459],\n",
      "        [ 0.3511],\n",
      "        [-0.0144]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1770,  0.0677,  0.9245, -0.7287,  2.3579],\n",
      "        [-1.2135, -0.6402,  1.4252, -1.1459,  0.3766],\n",
      "        [ 0.8656, -0.1547,  1.5624,  1.1050,  0.2476],\n",
      "        [ 0.4654, -0.2435, -0.3314, -0.4969,  1.4550]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7527,  1.5085,  0.6604,  1.2399,  1.9653],\n",
      "        [-0.0615,  0.1115,  0.4115,  0.3921, -0.5434],\n",
      "        [-0.1494,  0.4231, -0.3103, -0.2558, -0.8603],\n",
      "        [-0.1530,  0.3448, -0.2702,  0.1735,  0.3018]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1770,  0.0677,  0.9245, -0.7287,  2.3579],\n",
      "        [-1.2135, -0.6402,  1.4252, -1.1459,  0.3766],\n",
      "        [ 0.8656, -0.1547,  1.5624,  1.1050,  0.2476],\n",
      "        [ 0.4654, -0.2435, -0.3314, -0.4969,  1.4550]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.3099],\n",
      "        [-0.0643],\n",
      "        [-1.1753],\n",
      "        [ 0.2873]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4544,  1.5895,  1.2851, -0.4136,  0.9940],\n",
      "        [-0.5293, -0.8684,  0.8325, -0.8583,  0.8315],\n",
      "        [-0.7941,  0.0250,  0.8045, -0.3088, -0.1069],\n",
      "        [ 0.4317,  0.1848,  0.8623, -1.5929,  0.0078]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4315, -0.5685, -1.3322, -0.8765, -1.7572],\n",
      "        [ 0.0406,  0.6804,  0.3177,  0.5956,  0.3442],\n",
      "        [-0.2648,  0.7200,  0.6063,  0.3200,  0.8276],\n",
      "        [ 0.0200,  0.2198,  0.6913, -0.0198, -0.2251]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4544,  1.5895,  1.2851, -0.4136,  0.9940],\n",
      "        [-0.5293, -0.8684,  0.8325, -0.8583,  0.8315],\n",
      "        [-0.7941,  0.0250,  0.8045, -0.3088, -0.1069],\n",
      "        [ 0.4317,  0.1848,  0.8623, -1.5929,  0.0078]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.8035],\n",
      "        [-0.5729],\n",
      "        [ 0.5289],\n",
      "        [ 0.6752]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5030, -0.9073, -0.0308,  1.6950, -0.6077],\n",
      "        [ 0.6090,  0.5337,  0.9118,  1.4246, -1.1353],\n",
      "        [-0.1893,  1.1000,  0.2856, -0.4492,  0.8510],\n",
      "        [-0.5617, -0.3871,  1.2240,  0.3176,  0.7438]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.7207,  1.2592,  1.3619,  1.0820,  1.9273],\n",
      "        [ 0.1816,  0.6437,  0.6062,  0.7246,  1.3803],\n",
      "        [-0.3871, -0.1137,  0.1573,  0.5286,  0.3239],\n",
      "        [ 0.0984,  0.6476,  0.7683,  0.3268, -0.1002]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5030, -0.9073, -0.0308,  1.6950, -0.6077],\n",
      "        [ 0.6090,  0.5337,  0.9118,  1.4246, -1.1353],\n",
      "        [-0.1893,  1.1000,  0.2856, -0.4492,  0.8510],\n",
      "        [-0.5617, -0.3871,  1.2240,  0.3176,  0.7438]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3872],\n",
      "        [ 0.4722],\n",
      "        [ 0.0313],\n",
      "        [ 0.6638]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0400, -0.3253,  0.1194,  1.6443,  0.1531],\n",
      "        [-0.3022, -0.6664,  0.7408, -0.0175, -0.9789],\n",
      "        [ 0.1197,  0.1964, -0.6388,  0.2672,  0.1516],\n",
      "        [ 1.3887, -0.8368, -0.0051,  0.4590,  0.3129]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2674,  1.8626,  1.4316,  1.1182,  1.9307],\n",
      "        [ 0.6286,  0.5804,  0.1362,  0.2099,  0.3422],\n",
      "        [-0.0337,  0.3203,  1.2408,  0.7068,  0.8058],\n",
      "        [-0.6862,  0.1496,  0.2245,  0.0749, -0.1276]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0400, -0.3253,  0.1194,  1.6443,  0.1531],\n",
      "        [-0.3022, -0.6664,  0.7408, -0.0175, -0.9789],\n",
      "        [ 0.1197,  0.1964, -0.6388,  0.2672,  0.1516],\n",
      "        [ 1.3887, -0.8368, -0.0051,  0.4590,  0.3129]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3811],\n",
      "        [-0.8145],\n",
      "        [-0.4227],\n",
      "        [-1.0847]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9485,  0.9667, -0.9171, -0.3231,  2.7257],\n",
      "        [ 0.7019,  0.9774, -1.0702, -0.1081, -1.5443],\n",
      "        [ 0.6915,  0.8148,  0.4060, -0.7133,  0.3401],\n",
      "        [ 0.2901,  0.1124,  0.8936,  0.4787,  0.7376]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8693,  1.7182,  1.1630,  1.5548,  1.9243],\n",
      "        [ 1.0680,  0.7427,  1.0361,  0.8367,  0.9153],\n",
      "        [ 0.5124,  0.3008,  0.2571,  0.5581,  0.2529],\n",
      "        [ 0.2154,  0.9860,  1.5122,  0.3698,  0.9952]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9485,  0.9667, -0.9171, -0.3231,  2.7257],\n",
      "        [ 0.7019,  0.9774, -1.0702, -0.1081, -1.5443],\n",
      "        [ 0.6915,  0.8148,  0.4060, -0.7133,  0.3401],\n",
      "        [ 0.2901,  0.1124,  0.8936,  0.4787,  0.7376]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.6432],\n",
      "        [-1.1374],\n",
      "        [ 0.3917],\n",
      "        [ 2.4357]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4803, -0.8133, -0.5692,  0.8394,  1.1299],\n",
      "        [ 0.1032, -2.5296,  1.4496,  1.4422,  1.0332],\n",
      "        [ 1.8809,  0.1536,  0.8067,  0.8010, -0.0342],\n",
      "        [ 2.0668, -1.5085, -0.5505,  0.1820,  0.2045]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5285,  0.4846, -0.3365, -0.1145,  0.0969],\n",
      "        [ 0.5273,  1.1692,  1.3613,  0.8251,  1.2399],\n",
      "        [-0.6511,  1.0735,  0.2563,  0.3060, -0.6472],\n",
      "        [-0.7482, -0.6178, -1.1829, -0.5957, -1.4988]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4803, -0.8133, -0.5692,  0.8394,  1.1299],\n",
      "        [ 0.1032, -2.5296,  1.4496,  1.4422,  1.0332],\n",
      "        [ 1.8809,  0.1536,  0.8067,  0.8010, -0.0342],\n",
      "        [ 2.0668, -1.5085, -0.5505,  0.1820,  0.2045]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9716],\n",
      "        [ 1.5411],\n",
      "        [-0.5856],\n",
      "        [-0.3783]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0832,  0.1351, -0.3692, -0.3236, -1.9066],\n",
      "        [-0.7521,  1.7799,  0.6523, -0.3471, -0.2859],\n",
      "        [-1.4103,  0.1664, -0.0524, -0.0177,  1.5541],\n",
      "        [-1.2974,  0.7256,  0.2245, -2.2845,  0.3146]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2917,  0.3188,  0.5281,  0.3273,  1.2117],\n",
      "        [ 0.3420,  0.6029,  0.1418,  0.4104,  0.5554],\n",
      "        [-0.1767,  0.9634,  0.0458, -0.0023, -0.0320],\n",
      "        [-0.4932,  0.2722,  0.6664,  0.0870, -0.1242]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0832,  0.1351, -0.3692, -0.3236, -1.9066],\n",
      "        [-0.7521,  1.7799,  0.6523, -0.3471, -0.2859],\n",
      "        [-1.4103,  0.1664, -0.0524, -0.0177,  1.5541],\n",
      "        [-1.2974,  0.7256,  0.2245, -2.2845,  0.3146]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5923],\n",
      "        [ 0.6071],\n",
      "        [ 0.3574],\n",
      "        [ 0.7492]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0631, -2.3208, -0.7436, -2.0508, -0.8696],\n",
      "        [-1.3025,  0.8440, -0.1651, -0.0873,  0.4244],\n",
      "        [-0.7801, -0.7390,  0.9653,  0.9486, -1.0601],\n",
      "        [-0.1931,  0.0697,  2.1598,  0.0249,  0.9888]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6612,  1.4730,  2.1332,  0.4457,  1.0872],\n",
      "        [-0.3805,  0.9558,  1.3973,  0.8738,  1.0407],\n",
      "        [ 0.2744,  0.1088,  0.7786,  0.4162,  0.6826],\n",
      "        [-0.7173, -0.2165, -0.2284,  0.6733, -0.4484]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0631, -2.3208, -0.7436, -2.0508, -0.8696],\n",
      "        [-1.3025,  0.8440, -0.1651, -0.0873,  0.4244],\n",
      "        [-0.7801, -0.7390,  0.9653,  0.9486, -1.0601],\n",
      "        [-0.1931,  0.0697,  2.1598,  0.0249,  0.9888]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-5.0980],\n",
      "        [ 1.4370],\n",
      "        [ 0.1284],\n",
      "        [-0.7965]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0846, -0.2867, -0.9654, -1.2423, -0.0169],\n",
      "        [-0.1564, -1.7306, -0.3704, -1.0731,  0.0962],\n",
      "        [-0.9100, -0.7475,  1.3105, -1.0092,  1.1652],\n",
      "        [-0.3810,  0.0129, -1.3088,  0.5737,  0.6629]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2799, -0.4875, -0.6675, -0.2795, -0.2111],\n",
      "        [-0.3150,  0.7105, -0.0958,  1.0088, -0.4823],\n",
      "        [-0.2354,  0.0665, -0.0038,  0.3824,  0.1731],\n",
      "        [-0.2771, -0.3707,  0.3356,  0.1609, -0.0586]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0846, -0.2867, -0.9654, -1.2423, -0.0169],\n",
      "        [-0.1564, -1.7306, -0.3704, -1.0731,  0.0962],\n",
      "        [-0.9100, -0.7475,  1.3105, -1.0092,  1.1652],\n",
      "        [-0.3810,  0.0129, -1.3088,  0.5737,  0.6629]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1586],\n",
      "        [-2.2738],\n",
      "        [-0.0247],\n",
      "        [-0.2850]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1296,  0.7491, -0.0664,  0.4358,  0.1769],\n",
      "        [-0.4124, -0.3661,  1.4342,  0.7426,  0.2055],\n",
      "        [-1.2828,  1.4508, -1.8170,  1.2893,  0.2889],\n",
      "        [-0.3603,  1.7226,  0.1101, -1.5855, -0.0301]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2584,  0.3687, -0.5754, -0.3497, -1.1173],\n",
      "        [ 1.1380,  1.3883,  1.8581,  0.9686,  1.2172],\n",
      "        [-0.2704, -0.1385, -0.0863,  0.4696,  0.6166],\n",
      "        [-0.0443,  0.2574, -0.0969,  0.5987,  0.5975]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1296,  0.7491, -0.0664,  0.4358,  0.1769],\n",
      "        [-0.4124, -0.3661,  1.4342,  0.7426,  0.2055],\n",
      "        [-1.2828,  1.4508, -1.8170,  1.2893,  0.2889],\n",
      "        [-0.3603,  1.7226,  0.1101, -1.5855, -0.0301]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3276],\n",
      "        [ 2.6567],\n",
      "        [ 1.0863],\n",
      "        [-0.5186]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1847,  0.4278,  0.4649,  1.1093,  0.3664],\n",
      "        [-0.4121,  0.1203,  0.8143,  0.3317,  1.7138],\n",
      "        [ 0.5456, -0.6588,  1.8624,  0.5264,  0.6549],\n",
      "        [-0.4878,  0.1118, -0.5804,  0.2724,  1.2493]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1260, -0.0781,  0.6977,  0.3381, -0.7795],\n",
      "        [-0.8738, -0.2175,  0.5237,  0.8486, -0.5011],\n",
      "        [-0.1248,  0.2770, -0.0714,  0.2868,  0.0066],\n",
      "        [-0.1536, -0.2479, -0.1093, -0.7138, -0.0370]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1847,  0.4278,  0.4649,  1.1093,  0.3664],\n",
      "        [-0.4121,  0.1203,  0.8143,  0.3317,  1.7138],\n",
      "        [ 0.5456, -0.6588,  1.8624,  0.5264,  0.6549],\n",
      "        [-0.4878,  0.1118, -0.5804,  0.2724,  1.2493]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4038],\n",
      "        [ 0.1831],\n",
      "        [-0.2282],\n",
      "        [-0.1300]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3161,  1.1746, -0.0660, -0.5533, -0.0970],\n",
      "        [-0.3538, -0.1258,  0.1117, -0.8841,  1.0743],\n",
      "        [-1.2515, -1.6245, -0.6361, -0.5788,  0.5820],\n",
      "        [ 0.0489, -0.9307,  0.2905, -1.0702, -1.0753]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4189, -0.2091,  1.3789, -0.0355,  0.4186],\n",
      "        [-0.0941,  0.9118,  0.4374,  0.5214,  1.0815],\n",
      "        [-0.2960,  0.1334, -0.4826, -0.0477, -0.4337],\n",
      "        [ 0.1987,  0.0801, -0.2566, -0.0956, -0.4961]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3161,  1.1746, -0.0660, -0.5533, -0.0970],\n",
      "        [-0.3538, -0.1258,  0.1117, -0.8841,  1.0743],\n",
      "        [-1.2515, -1.6245, -0.6361, -0.5788,  0.5820],\n",
      "        [ 0.0489, -0.9307,  0.2905, -1.0702, -1.0753]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4900],\n",
      "        [ 0.6683],\n",
      "        [ 0.2359],\n",
      "        [ 0.4965]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1719,  2.5786,  0.7070, -0.2705,  0.8825],\n",
      "        [ 0.4531,  2.0962,  0.9070,  0.4455,  1.9154],\n",
      "        [ 0.0043, -0.5324,  0.7772, -0.6006,  0.2139],\n",
      "        [-0.8964,  0.3682, -0.3266,  0.0593, -1.1364]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2366,  0.5041,  0.0380, -0.6413, -0.2719],\n",
      "        [-0.0387,  0.7978,  0.9863,  0.9592,  0.1609],\n",
      "        [-0.2601,  0.1005, -0.1205,  0.6607, -0.0090],\n",
      "        [-0.2782, -0.2067,  0.5072, -0.3637, -0.4175]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1719,  2.5786,  0.7070, -0.2705,  0.8825],\n",
      "        [ 0.4531,  2.0962,  0.9070,  0.4455,  1.9154],\n",
      "        [ 0.0043, -0.5324,  0.7772, -0.6006,  0.2139],\n",
      "        [-0.8964,  0.3682, -0.3266,  0.0593, -1.1364]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2195],\n",
      "        [ 3.2850],\n",
      "        [-0.5470],\n",
      "        [ 0.4605]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7215, -2.1251,  0.4145,  0.3746, -1.3291],\n",
      "        [ 1.2149,  0.6402,  1.0583, -1.9849, -1.3013],\n",
      "        [-0.9146, -0.0164,  2.2345, -0.7070, -1.2779],\n",
      "        [-0.0855,  1.0169, -0.5800,  1.5158,  0.9688]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0103, -0.1589, -0.1842, -0.3334, -0.7003],\n",
      "        [-0.2777, -0.8871, -0.7292, -0.7539, -1.9459],\n",
      "        [ 0.3242,  0.3277,  0.8176,  0.2366, -0.3222],\n",
      "        [-0.3282,  0.2663, -0.4751,  0.0498, -0.6935]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7215, -2.1251,  0.4145,  0.3746, -1.3291],\n",
      "        [ 1.2149,  0.6402,  1.0583, -1.9849, -1.3013],\n",
      "        [-0.9146, -0.0164,  2.2345, -0.7070, -1.2779],\n",
      "        [-0.0855,  1.0169, -0.5800,  1.5158,  0.9688]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0597],\n",
      "        [ 2.3517],\n",
      "        [ 1.7695],\n",
      "        [-0.0221]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7061,  1.1630,  1.0192, -0.5726, -1.3885],\n",
      "        [ 1.4480, -0.9929, -1.2657,  0.1847,  1.5479],\n",
      "        [-0.2759,  1.2886, -1.5814, -0.8045,  0.2862],\n",
      "        [-1.9483,  0.9267,  2.0543, -0.7472, -0.5028]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3647, -0.4080, -0.5713, -0.7727, -1.5051],\n",
      "        [-0.8574, -1.6763, -1.7621, -1.5889, -3.3592],\n",
      "        [-0.5609, -0.4559, -0.4248, -0.6590, -1.0339],\n",
      "        [ 0.0243, -0.2031,  0.1066, -0.0408, -0.1579]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7061,  1.1630,  1.0192, -0.5726, -1.3885],\n",
      "        [ 1.4480, -0.9929, -1.2657,  0.1847,  1.5479],\n",
      "        [-0.2759,  1.2886, -1.5814, -0.8045,  0.2862],\n",
      "        [-1.9483,  0.9267,  2.0543, -0.7472, -0.5028]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0978],\n",
      "        [-2.8401],\n",
      "        [ 0.4734],\n",
      "        [ 0.0934]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4242, -0.7490, -0.9115, -2.2588,  1.6062],\n",
      "        [-1.4885,  1.2236, -0.6877, -0.1445,  0.4741],\n",
      "        [ 0.1167,  0.4222,  0.5809,  0.3469, -0.6089],\n",
      "        [ 0.1963,  0.8439, -0.3012,  0.2697, -0.2507]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1218, -1.0558, -1.6036, -1.2772, -2.3237],\n",
      "        [ 0.0022, -0.2668, -0.3883, -0.0020, -1.2497],\n",
      "        [-0.0941,  0.0198, -0.0703, -0.3946, -1.4905],\n",
      "        [-0.0324,  0.0636,  0.2953,  0.2594, -0.0810]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4242, -0.7490, -0.9115, -2.2588,  1.6062],\n",
      "        [-1.4885,  1.2236, -0.6877, -0.1445,  0.4741],\n",
      "        [ 0.1167,  0.4222,  0.5809,  0.3469, -0.6089],\n",
      "        [ 0.1963,  0.8439, -0.3012,  0.2697, -0.2507]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8809],\n",
      "        [-0.6548],\n",
      "        [ 0.7271],\n",
      "        [ 0.0486]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5741,  0.9606, -0.9123,  1.2218, -0.6862],\n",
      "        [ 0.5987,  1.1489,  1.7965,  0.1172,  0.5132],\n",
      "        [-0.1201,  0.6368, -0.2138,  1.7550, -0.6043],\n",
      "        [ 1.1137,  0.5559, -0.6181, -0.6714,  0.6221]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0413, -0.1117,  0.4654,  0.5061,  0.1895],\n",
      "        [-0.0485, -0.1911, -0.4634, -0.1138, -0.2882],\n",
      "        [-0.8303, -0.8816, -0.7308, -1.0121, -1.7492],\n",
      "        [-0.3636,  0.2425,  0.1174, -0.3858, -0.3701]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5741,  0.9606, -0.9123,  1.2218, -0.6862],\n",
      "        [ 0.5987,  1.1489,  1.7965,  0.1172,  0.5132],\n",
      "        [-0.1201,  0.6368, -0.2138,  1.7550, -0.6043],\n",
      "        [ 1.1137,  0.5559, -0.6181, -0.6714,  0.6221]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0673],\n",
      "        [-1.2424],\n",
      "        [-1.0246],\n",
      "        [-0.3139]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2179, -1.2275,  1.2692,  1.6686, -1.7764],\n",
      "        [-0.2278,  1.8839, -0.5103, -0.2976, -0.4289],\n",
      "        [-1.3163,  1.0441, -0.4230,  0.5111,  1.6073],\n",
      "        [-0.4014,  0.9941, -0.0552, -0.0915,  1.4454]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8460, -0.5028, -0.0794,  0.0187,  0.7900],\n",
      "        [ 0.0179,  0.2363, -1.0037,  0.5297,  0.6035],\n",
      "        [ 0.4041,  0.2369,  0.0377, -0.3612, -0.8281],\n",
      "        [-0.0456,  0.0051,  0.3009, -0.0158, -0.4385]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2179, -1.2275,  1.2692,  1.6686, -1.7764],\n",
      "        [-0.2278,  1.8839, -0.5103, -0.2976, -0.4289],\n",
      "        [-1.3163,  1.0441, -0.4230,  0.5111,  1.6073],\n",
      "        [-0.4014,  0.9941, -0.0552, -0.0915,  1.4454]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8861],\n",
      "        [ 0.5369],\n",
      "        [-1.8160],\n",
      "        [-0.6255]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4455,  0.5239,  0.2272,  1.4130, -0.2007],\n",
      "        [ 0.1859, -0.0210,  0.1280,  0.6741, -0.5660],\n",
      "        [-0.3315, -0.7118, -0.2774,  1.9496,  1.5581],\n",
      "        [ 1.3144,  0.6795,  1.5280,  0.9004,  0.0782]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1723,  1.4391,  0.9202,  1.2151,  1.2528],\n",
      "        [-0.2751,  0.1514,  0.3093,  0.0877,  0.0140],\n",
      "        [ 0.3410,  1.8459,  1.1850,  0.7736,  1.6000],\n",
      "        [ 0.1996, -0.2229,  0.1695, -0.3827, -0.1127]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4455,  0.5239,  0.2272,  1.4130, -0.2007],\n",
      "        [ 0.1859, -0.0210,  0.1280,  0.6741, -0.5660],\n",
      "        [-0.3315, -0.7118, -0.2774,  1.9496,  1.5581],\n",
      "        [ 1.3144,  0.6795,  1.5280,  0.9004,  0.0782]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5054],\n",
      "        [ 0.0364],\n",
      "        [ 2.2455],\n",
      "        [ 0.0165]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1853, -0.0985, -1.3375,  0.1521,  0.3854],\n",
      "        [-0.9307, -0.0650,  1.6048, -0.9719,  0.4971],\n",
      "        [-0.6157,  0.1363,  0.9241,  1.7667, -0.3086],\n",
      "        [ 0.1577,  1.6151, -0.3161, -0.6459, -0.3378]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2018, -0.4361, -0.5416, -0.5021, -1.2171],\n",
      "        [ 0.0292,  0.0659,  0.0674, -0.4428,  0.2072],\n",
      "        [-0.0780,  0.1840,  0.0188,  0.0167, -0.8700],\n",
      "        [-0.1488, -0.6313, -0.1979, -0.2825, -0.9292]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1853, -0.0985, -1.3375,  0.1521,  0.3854],\n",
      "        [-0.9307, -0.0650,  1.6048, -0.9719,  0.4971],\n",
      "        [-0.6157,  0.1363,  0.9241,  1.7667, -0.3086],\n",
      "        [ 0.1577,  1.6151, -0.3161, -0.6459, -0.3378]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0172],\n",
      "        [ 0.6100],\n",
      "        [ 0.3885],\n",
      "        [-0.4841]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2335,  0.6013,  0.9147,  0.8502, -0.4462],\n",
      "        [-0.1801, -0.9102,  0.4876,  1.5466,  0.9914],\n",
      "        [-1.4953,  0.1879, -0.1050, -0.3201,  0.2865],\n",
      "        [-0.1627,  0.4250,  0.8086,  1.7196,  0.4866]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3310,  0.2054,  0.5725,  0.2193, -0.0029],\n",
      "        [-0.8531, -0.0114,  0.4236, -0.1851, -0.2035],\n",
      "        [-0.2693, -0.2926,  0.2033, -0.0289, -0.5703],\n",
      "        [ 0.1975, -0.5259, -0.3783,  0.2862, -0.7859]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2335,  0.6013,  0.9147,  0.8502, -0.4462],\n",
      "        [-0.1801, -0.9102,  0.4876,  1.5466,  0.9914],\n",
      "        [-1.4953,  0.1879, -0.1050, -0.3201,  0.2865],\n",
      "        [-0.1627,  0.4250,  0.8086,  1.7196,  0.4866]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7576],\n",
      "        [-0.1175],\n",
      "        [ 0.1722],\n",
      "        [-0.4517]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3223, -0.1509,  0.6367,  0.8280,  0.8840],\n",
      "        [ 0.0137,  1.4206,  1.4620, -0.0008,  2.2375],\n",
      "        [ 0.0495,  0.9016, -0.7704, -1.5063,  0.8165],\n",
      "        [-1.7929,  2.0653,  1.2856,  0.1297, -0.4226]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4204,  0.3108, -0.2295,  0.1946, -0.5014],\n",
      "        [-0.4280, -0.2214,  0.2608,  0.5405,  0.0143],\n",
      "        [ 0.1815,  0.3580,  0.2210,  0.6918,  0.6048],\n",
      "        [ 0.3430, -0.3023, -0.3701, -0.2213, -1.2077]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3223, -0.1509,  0.6367,  0.8280,  0.8840],\n",
      "        [ 0.0137,  1.4206,  1.4620, -0.0008,  2.2375],\n",
      "        [ 0.0495,  0.9016, -0.7704, -1.5063,  0.8165],\n",
      "        [-1.7929,  2.0653,  1.2856,  0.1297, -0.4226]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6105],\n",
      "        [ 0.0924],\n",
      "        [-0.3868],\n",
      "        [-1.2333]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0230,  1.7481,  0.3154,  0.2124, -1.8082],\n",
      "        [-0.3348,  1.2749,  0.0798,  0.8827,  0.1747],\n",
      "        [ 1.0997,  0.1717,  2.4853,  0.8252, -1.1200],\n",
      "        [ 0.0916, -0.3685,  1.3909,  0.7195, -0.7104]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7981,  0.1348,  0.5478,  0.3690,  0.8061],\n",
      "        [-0.2255, -0.2744,  0.1362, -0.3250, -0.4760],\n",
      "        [ 0.0714,  0.9218,  0.1963, -0.0864,  0.7064],\n",
      "        [ 0.5220, -1.1582, -0.2577, -0.8495,  0.4018]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0230,  1.7481,  0.3154,  0.2124, -1.8082],\n",
      "        [-0.3348,  1.2749,  0.0798,  0.8827,  0.1747],\n",
      "        [ 1.0997,  0.1717,  2.4853,  0.8252, -1.1200],\n",
      "        [ 0.0916, -0.3685,  1.3909,  0.7195, -0.7104]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9891],\n",
      "        [-0.6335],\n",
      "        [-0.1377],\n",
      "        [-0.7804]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0085,  1.1065,  1.1389,  0.2685,  0.3633],\n",
      "        [-1.5306,  0.5527, -1.2235, -1.2278, -0.1444],\n",
      "        [ 0.2187, -1.0737,  0.6895, -1.0217,  0.6027],\n",
      "        [-1.1391,  0.3607,  0.2236,  1.9836,  0.3095]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6285,  1.0425,  1.2288,  1.0063,  0.9029],\n",
      "        [-0.5210, -0.6684,  0.0998,  0.4313, -0.3047],\n",
      "        [ 0.5763,  0.6857,  0.1986,  0.5097,  0.2563],\n",
      "        [ 0.8554, -0.7703, -0.3217, -0.3269, -0.7372]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0085,  1.1065,  1.1389,  0.2685,  0.3633],\n",
      "        [-1.5306,  0.5527, -1.2235, -1.2278, -0.1444],\n",
      "        [ 0.2187, -1.0737,  0.6895, -1.0217,  0.6027],\n",
      "        [-1.1391,  0.3607,  0.2236,  1.9836,  0.3095]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.7852],\n",
      "        [-0.1797],\n",
      "        [-0.8396],\n",
      "        [-2.2008]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3432, -1.3781,  1.3316, -0.4689,  1.0933],\n",
      "        [-0.1274,  0.8164,  2.5693,  1.0792,  0.4929],\n",
      "        [-0.2506, -0.1987,  0.8334,  0.3242,  0.1658],\n",
      "        [-0.8712, -1.0124,  0.3725, -0.3201, -1.2149]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8162, -0.8256, -0.2137, -1.9465, -2.3538],\n",
      "        [-0.3678, -0.4527,  0.0642,  0.2549, -0.7623],\n",
      "        [-0.1145,  0.0465,  0.3937,  0.0770,  1.0245],\n",
      "        [ 0.7806,  1.0742,  1.1220,  0.6400,  1.1567]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3432, -1.3781,  1.3316, -0.4689,  1.0933],\n",
      "        [-0.1274,  0.8164,  2.5693,  1.0792,  0.4929],\n",
      "        [-0.2506, -0.1987,  0.8334,  0.3242,  0.1658],\n",
      "        [-0.8712, -1.0124,  0.3725, -0.3201, -1.2149]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2888],\n",
      "        [-0.2585],\n",
      "        [ 0.5423],\n",
      "        [-2.9598]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8225,  0.7039,  1.5825,  0.5416,  3.2289],\n",
      "        [-1.5408, -0.4081,  0.2657, -1.9198,  0.6443],\n",
      "        [ 0.2315, -0.2266,  0.5889, -1.4352,  1.0693],\n",
      "        [-1.8863,  1.6660, -0.2607, -1.2304,  2.8863]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7603, -0.9794, -1.1565, -1.4768, -2.6361],\n",
      "        [ 0.0423, -0.2108,  0.3816, -0.3338, -0.3885],\n",
      "        [ 0.2037,  0.3875,  0.3823, -0.2341,  0.9675],\n",
      "        [ 1.2103,  1.6255,  2.0078,  1.2858,  3.0247]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8225,  0.7039,  1.5825,  0.5416,  3.2289],\n",
      "        [-1.5408, -0.4081,  0.2657, -1.9198,  0.6443],\n",
      "        [ 0.2315, -0.2266,  0.5889, -1.4352,  1.0693],\n",
      "        [-1.8863,  1.6660, -0.2607, -1.2304,  2.8863]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-12.4564],\n",
      "        [  0.5127],\n",
      "        [  1.5551],\n",
      "        [  7.0499]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1727, -0.7383, -1.4774,  0.9283, -0.3219],\n",
      "        [ 0.8420, -0.6621, -0.8603,  1.2600,  0.8939],\n",
      "        [ 0.7773, -2.0698, -0.6806, -0.9385,  0.6736],\n",
      "        [ 0.3769,  0.5220,  0.1198,  1.1545,  1.0005]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.9256,  3.2039,  2.9847,  2.7608,  5.6063],\n",
      "        [ 0.0039,  0.2382, -0.6402, -0.3625, -0.9334],\n",
      "        [-0.5772, -0.5331, -0.4231, -0.4019, -0.5584],\n",
      "        [-0.3152, -0.7474, -0.5211, -1.2963, -2.6223]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1727, -0.7383, -1.4774,  0.9283, -0.3219],\n",
      "        [ 0.8420, -0.6621, -0.8603,  1.2600,  0.8939],\n",
      "        [ 0.7773, -2.0698, -0.6806, -0.9385,  0.6736],\n",
      "        [ 0.3769,  0.5220,  0.1198,  1.1545,  1.0005]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-9.4472],\n",
      "        [-0.8948],\n",
      "        [ 0.9437],\n",
      "        [-4.6915]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6383, -0.3111,  1.0997,  0.8034,  1.3738],\n",
      "        [-1.9268,  0.6827, -1.6432, -0.1487,  1.2385],\n",
      "        [ 0.6889, -0.7983,  0.9800, -0.8213, -0.0207],\n",
      "        [-1.3464, -0.1683, -0.7779,  0.3308, -1.2704]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9428, -0.2620, -0.5337,  0.3812,  0.3476],\n",
      "        [-0.3109, -1.2100, -0.7400, -0.5250, -0.3236],\n",
      "        [-0.5845, -0.0987, -0.7793, -0.0229, -1.7889],\n",
      "        [ 1.4064,  0.9037,  1.6259,  1.0973,  1.8008]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6383, -0.3111,  1.0997,  0.8034,  1.3738],\n",
      "        [-1.9268,  0.6827, -1.6432, -0.1487,  1.2385],\n",
      "        [ 0.6889, -0.7983,  0.9800, -0.8213, -0.0207],\n",
      "        [-1.3464, -0.1683, -0.7779,  0.3308, -1.2704]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8800],\n",
      "        [ 0.6664],\n",
      "        [-1.0317],\n",
      "        [-5.2352]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0643,  0.7813,  2.1053,  0.0514,  0.5600],\n",
      "        [ 0.1871,  1.2394,  0.4026, -0.4911,  1.4777],\n",
      "        [-0.1948,  0.4775, -0.1243,  1.0557,  0.6363],\n",
      "        [-1.6520,  0.9958,  0.5319,  0.2771,  1.4470]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1116, -0.7178, -0.1207, -0.2773, -0.1423],\n",
      "        [-0.3293, -1.4478, -0.4643, -0.8128, -1.2678],\n",
      "        [ 0.2869,  0.0978,  0.4878,  0.2087, -0.3079],\n",
      "        [-0.2134,  0.1400, -0.2879, -0.2931,  0.1056]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0643,  0.7813,  2.1053,  0.0514,  0.5600],\n",
      "        [ 0.1871,  1.2394,  0.4026, -0.4911,  1.4777],\n",
      "        [-0.1948,  0.4775, -0.1243,  1.0557,  0.6363],\n",
      "        [-1.6520,  0.9958,  0.5319,  0.2771,  1.4470]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0277],\n",
      "        [-3.5172],\n",
      "        [-0.0454],\n",
      "        [ 0.4103]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4403, -0.8204,  0.5269, -0.8436,  0.2050],\n",
      "        [-0.4190,  0.4195,  0.7317, -1.1999,  1.1170],\n",
      "        [ 0.7548, -2.1682, -0.1146,  0.8099,  0.6463],\n",
      "        [-0.6591,  1.0051, -1.0824,  0.8464, -0.2747]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2082, -0.1774,  0.4785,  0.0254,  0.4580],\n",
      "        [ 0.4611,  0.1867,  0.4591,  0.6837,  1.2522],\n",
      "        [ 0.2406,  0.3387, -0.0592,  0.4536, -0.5047],\n",
      "        [ 0.2661,  0.8115,  0.5552,  0.0688,  0.0398]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4403, -0.8204,  0.5269, -0.8436,  0.2050],\n",
      "        [-0.4190,  0.4195,  0.7317, -1.1999,  1.1170],\n",
      "        [ 0.7548, -2.1682, -0.1146,  0.8099,  0.6463],\n",
      "        [-0.6591,  1.0051, -1.0824,  0.8464, -0.2747]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3785],\n",
      "        [ 0.7994],\n",
      "        [-0.5048],\n",
      "        [ 0.0866]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4765, -1.9498,  0.4635,  1.0862, -0.2573],\n",
      "        [-1.5040, -1.7256, -1.0289, -0.4017, -0.0133],\n",
      "        [-0.1855, -0.1821, -1.3375,  0.1925,  0.2146],\n",
      "        [-0.4170,  2.1379,  1.1524,  0.4347, -0.6583]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2918,  0.3799,  0.7088, -0.4851,  0.1244],\n",
      "        [ 0.2644, -0.5769,  0.2659, -0.0747,  0.9816],\n",
      "        [-0.0845, -0.5426, -0.3629,  0.0609,  0.2458],\n",
      "        [-0.1153, -0.0962, -0.0183,  0.0269, -0.4149]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4765, -1.9498,  0.4635,  1.0862, -0.2573],\n",
      "        [-1.5040, -1.7256, -1.0289, -0.4017, -0.0133],\n",
      "        [-0.1855, -0.1821, -1.3375,  0.1925,  0.2146],\n",
      "        [-0.4170,  2.1379,  1.1524,  0.4347, -0.6583]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1102],\n",
      "        [ 0.3412],\n",
      "        [ 0.6643],\n",
      "        [ 0.1061]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5307,  1.6092,  0.4731, -0.4629, -0.9095],\n",
      "        [-0.6155,  0.0348,  0.1911, -0.9149,  0.5295],\n",
      "        [-0.4628,  0.7644,  0.4492, -0.8666, -0.4415],\n",
      "        [ 0.9746,  0.0401,  0.7078,  1.8434, -1.1767]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0224,  0.9570,  0.4459,  1.0318,  0.8724],\n",
      "        [ 0.3731, -0.1393, -0.3127, -0.2263, -0.2445],\n",
      "        [ 0.7504, -0.6587,  0.4766, -0.3936, -0.8155],\n",
      "        [-0.0819,  0.2855,  0.1500,  0.4974,  0.1262]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5307,  1.6092,  0.4731, -0.4629, -0.9095],\n",
      "        [-0.6155,  0.0348,  0.1911, -0.9149,  0.5295],\n",
      "        [-0.4628,  0.7644,  0.4492, -0.8666, -0.4415],\n",
      "        [ 0.9746,  0.0401,  0.7078,  1.8434, -1.1767]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4456],\n",
      "        [-0.2167],\n",
      "        [ 0.0644],\n",
      "        [ 0.8061]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8099,  1.0420, -0.3922,  0.4118, -0.4024],\n",
      "        [ 0.6741, -0.4071, -1.8566, -1.2018, -1.9689],\n",
      "        [ 0.9328, -0.4451, -0.7781,  2.0261, -0.8277],\n",
      "        [-0.1608, -0.1484,  0.1837,  1.8020, -0.3402]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2916,  0.1741,  0.0514, -0.3478,  0.1332],\n",
      "        [-0.2833, -0.7053,  0.0099, -0.9583, -0.3708],\n",
      "        [ 0.2076, -0.6760,  0.1269, -0.1020, -0.1304],\n",
      "        [ 0.0101, -0.3042, -0.1799, -0.1550, -0.7234]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8099,  1.0420, -0.3922,  0.4118, -0.4024],\n",
      "        [ 0.6741, -0.4071, -1.8566, -1.2018, -1.9689],\n",
      "        [ 0.9328, -0.4451, -0.7781,  2.0261, -0.8277],\n",
      "        [-0.1608, -0.1484,  0.1837,  1.8020, -0.3402]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2005],\n",
      "        [ 1.9598],\n",
      "        [ 0.2970],\n",
      "        [-0.0227]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2618, -1.4027,  0.5226,  0.4535,  0.3960],\n",
      "        [ 1.0499,  0.3230,  0.0502, -1.0508,  0.5084],\n",
      "        [ 0.1905,  0.3742,  1.1277,  1.7213, -0.8695],\n",
      "        [ 0.8013, -1.3024,  1.0766, -0.6536,  0.2256]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1600,  0.0268,  0.4355,  0.1150, -0.0281],\n",
      "        [-0.6140, -1.1442, -0.2679, -1.4353, -2.3606],\n",
      "        [ 0.2283,  0.1934,  0.1812, -0.2511, -0.2327],\n",
      "        [ 0.2060, -0.1128,  0.1452,  0.4466, -0.0436]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2618, -1.4027,  0.5226,  0.4535,  0.3960],\n",
      "        [ 1.0499,  0.3230,  0.0502, -1.0508,  0.5084],\n",
      "        [ 0.1905,  0.3742,  1.1277,  1.7213, -0.8695],\n",
      "        [ 0.8013, -1.3024,  1.0766, -0.6536,  0.2256]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1891],\n",
      "        [-0.7199],\n",
      "        [ 0.0902],\n",
      "        [ 0.1666]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8626, -1.4114,  1.3821, -1.4196, -0.0130],\n",
      "        [-1.6987,  1.6312, -3.0469, -1.0217,  1.0268],\n",
      "        [-0.6037,  1.2291,  1.3278,  1.6192,  1.3359],\n",
      "        [-2.1226, -0.0227,  0.1199,  0.7844,  1.2552]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1726, -0.4087, -0.2515, -0.0961,  0.1143],\n",
      "        [ 0.1131, -1.0436, -0.3652, -1.0791, -1.4798],\n",
      "        [-0.0963,  0.2846,  0.1812,  0.3421, -0.0125],\n",
      "        [ 0.4881,  0.0222, -0.2657,  0.0320, -0.0953]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8626, -1.4114,  1.3821, -1.4196, -0.0130],\n",
      "        [-1.6987,  1.6312, -3.0469, -1.0217,  1.0268],\n",
      "        [-0.6037,  1.2291,  1.3278,  1.6192,  1.3359],\n",
      "        [-2.1226, -0.0227,  0.1199,  0.7844,  1.2552]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0427],\n",
      "        [-1.1986],\n",
      "        [ 1.1858],\n",
      "        [-1.1628]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0531, -0.7305,  0.2906, -0.3479,  1.4766],\n",
      "        [-2.7607, -0.0201, -1.7922, -0.7709, -0.9371],\n",
      "        [-0.3115, -0.6031,  0.5490, -0.5435,  2.2864],\n",
      "        [-0.9440,  0.2147, -0.1758,  2.0717, -0.2454]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4421, -0.0029,  0.2260, -0.7271,  0.2092],\n",
      "        [ 0.0560, -0.8665,  0.1526, -0.5095, -0.5620],\n",
      "        [-0.3923, -0.9832, -0.4337, -0.7699, -0.9076],\n",
      "        [ 0.6456,  0.2689,  0.4015,  0.1984,  0.7947]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0531, -0.7305,  0.2906, -0.3479,  1.4766],\n",
      "        [-2.7607, -0.0201, -1.7922, -0.7709, -0.9371],\n",
      "        [-0.3115, -0.6031,  0.5490, -0.5435,  2.2864],\n",
      "        [-0.9440,  0.2147, -0.1758,  2.0717, -0.2454]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6531],\n",
      "        [ 0.5088],\n",
      "        [-1.1797],\n",
      "        [-0.4063]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1169,  0.0646, -0.2823, -0.2805,  0.6575],\n",
      "        [ 0.2291, -0.1657,  1.5840, -0.4268,  2.3009],\n",
      "        [ 1.0614,  0.7548,  1.2591, -1.0764, -0.4490],\n",
      "        [ 0.1436,  1.3219,  1.0134, -0.0042,  0.9177]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4982,  0.1175, -0.1020, -0.0201,  0.4660],\n",
      "        [-0.2956, -0.5700,  0.2476, -0.0790, -1.5857],\n",
      "        [ 0.4248,  0.0782, -0.0750,  0.2703, -0.0474],\n",
      "        [ 0.7579,  0.7775,  0.0615,  0.4217,  0.9355]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1169,  0.0646, -0.2823, -0.2805,  0.6575],\n",
      "        [ 0.2291, -0.1657,  1.5840, -0.4268,  2.3009],\n",
      "        [ 1.0614,  0.7548,  1.2591, -1.0764, -0.4490],\n",
      "        [ 0.1436,  1.3219,  1.0134, -0.0042,  0.9177]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2080],\n",
      "        [-3.1959],\n",
      "        [ 0.1458],\n",
      "        [ 2.0557]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2030,  0.4547, -0.6680, -1.1543,  1.5136],\n",
      "        [ 0.1621,  0.8514, -1.2548, -0.6954, -0.9964],\n",
      "        [-1.4568,  0.6338,  1.3089, -0.5090,  0.5584],\n",
      "        [-0.0137,  0.0277, -1.1758,  0.4129,  0.8112]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3438, -0.2444,  0.2664, -0.2174,  0.0373],\n",
      "        [ 0.7891,  1.1936,  0.6496,  0.8852,  2.1621],\n",
      "        [ 0.2482,  0.0516,  0.0723, -0.4222, -0.5595],\n",
      "        [-0.2873, -1.3149, -0.5989, -0.5372, -0.6625]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2030,  0.4547, -0.6680, -1.1543,  1.5136],\n",
      "        [ 0.1621,  0.8514, -1.2548, -0.6954, -0.9964],\n",
      "        [-1.4568,  0.6338,  1.3089, -0.5090,  0.5584],\n",
      "        [-0.0137,  0.0277, -1.1758,  0.4129,  0.8112]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0881],\n",
      "        [-2.4409],\n",
      "        [-0.3319],\n",
      "        [-0.0876]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0263, -0.0365,  0.6001,  2.0426,  1.8470],\n",
      "        [ 0.7825, -1.0388, -0.5470,  0.9391, -0.1787],\n",
      "        [ 0.4769, -1.1053,  0.4567,  0.3181,  1.3516],\n",
      "        [-1.8929, -0.6290, -1.1457,  1.6641,  2.2196]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0447, -0.2141,  0.2204,  0.2132, -0.1390],\n",
      "        [ 1.2247,  1.5731,  1.6671,  1.1863,  2.1982],\n",
      "        [ 0.2689, -0.3858, -0.1426, -0.3136, -1.0807],\n",
      "        [-0.5622,  0.0604, -0.3293,  0.1623, -0.5309]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0263, -0.0365,  0.6001,  2.0426,  1.8470],\n",
      "        [ 0.7825, -1.0388, -0.5470,  0.9391, -0.1787],\n",
      "        [ 0.4769, -1.1053,  0.4567,  0.3181,  1.3516],\n",
      "        [-1.8929, -0.6290, -1.1457,  1.6641,  2.2196]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3201],\n",
      "        [-0.8664],\n",
      "        [-1.0708],\n",
      "        [ 0.4952]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4714, -0.9295, -0.0858,  0.1199,  0.3966],\n",
      "        [-1.3199,  0.1019, -0.7755,  0.4107,  1.1557],\n",
      "        [-0.6303,  1.4510, -0.9747,  0.3708,  0.4158],\n",
      "        [-1.4503, -0.3054, -0.2331,  0.4134,  0.1894]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1022, -0.2810,  0.0656,  0.2691,  1.0117],\n",
      "        [ 1.0539,  2.3710,  2.2159,  1.2792,  3.2533],\n",
      "        [ 0.3493,  0.5840, -0.3203, -0.1877, -0.3066],\n",
      "        [-0.0946,  0.0566, -0.2066,  0.1328, -0.9688]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4714, -0.9295, -0.0858,  0.1199,  0.3966],\n",
      "        [-1.3199,  0.1019, -0.7755,  0.4107,  1.1557],\n",
      "        [-0.6303,  1.4510, -0.9747,  0.3708,  0.4158],\n",
      "        [-1.4503, -0.3054, -0.2331,  0.4134,  0.1894]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5387],\n",
      "        [ 1.4173],\n",
      "        [ 0.7423],\n",
      "        [ 0.0396]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4477, -1.4028,  0.3531, -0.0274,  2.8247],\n",
      "        [-1.9731,  0.1938, -0.8720, -1.3146,  0.2191],\n",
      "        [ 0.4586, -3.5872,  0.3031,  1.0764, -0.6698],\n",
      "        [-0.0183, -0.4612, -0.2948, -0.3993, -1.1204]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1705,  0.4737, -0.2409,  0.0615,  0.6920],\n",
      "        [ 0.0507,  0.1409,  0.2352,  0.1908, -0.1558],\n",
      "        [ 0.5275, -0.1591, -0.4998, -0.1535, -0.6134],\n",
      "        [ 0.1522,  0.3655,  0.0589,  0.1439, -0.0511]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4477, -1.4028,  0.3531, -0.0274,  2.8247],\n",
      "        [-1.9731,  0.1938, -0.8720, -1.3146,  0.2191],\n",
      "        [ 0.4586, -3.5872,  0.3031,  1.0764, -0.6698],\n",
      "        [-0.0183, -0.4612, -0.2948, -0.3993, -1.1204]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9565],\n",
      "        [-0.5627],\n",
      "        [ 0.9068],\n",
      "        [-0.1889]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5152, -0.9561,  0.9632,  0.4228,  0.1716],\n",
      "        [-0.0362, -1.0084, -0.1109,  1.1360,  1.2120],\n",
      "        [-0.4557,  0.8112,  0.7559,  0.4931,  1.8955],\n",
      "        [-0.8405,  1.0986,  0.6288,  0.9090,  0.6049]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4752,  0.0606,  0.1239,  0.3708, -0.4595],\n",
      "        [-0.2801, -0.1333, -0.0832, -0.1624, -0.3863],\n",
      "        [ 0.0605, -1.0955, -0.0369, -0.2163, -0.6483],\n",
      "        [-0.5112, -0.5148,  0.3112,  0.1841,  0.5550]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5152, -0.9561,  0.9632,  0.4228,  0.1716],\n",
      "        [-0.0362, -1.0084, -0.1109,  1.1360,  1.2120],\n",
      "        [-0.4557,  0.8112,  0.7559,  0.4931,  1.8955],\n",
      "        [-0.8405,  1.0986,  0.6288,  0.9090,  0.6049]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8594],\n",
      "        [-0.4988],\n",
      "        [-2.2797],\n",
      "        [ 0.5628]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1935,  0.2768,  0.4367,  1.1476, -2.4674],\n",
      "        [-0.5809, -2.0414,  0.4445,  0.7507,  0.7701],\n",
      "        [-0.8450, -1.1298,  0.9786,  1.2728, -1.1772],\n",
      "        [ 0.0748,  0.8455, -0.4608,  1.6526, -1.2790]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2742, -0.2024, -0.1986,  0.1612, -0.6712],\n",
      "        [ 0.1173, -0.6697, -0.2824,  0.3741, -0.2401],\n",
      "        [ 0.5644,  0.9054,  0.9799,  0.4957,  1.7945],\n",
      "        [-0.2236,  0.0796, -0.1413,  0.3380,  0.1135]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1935,  0.2768,  0.4367,  1.1476, -2.4674],\n",
      "        [-0.5809, -2.0414,  0.4445,  0.7507,  0.7701],\n",
      "        [-0.8450, -1.1298,  0.9786,  1.2728, -1.1772],\n",
      "        [ 0.0748,  0.8455, -0.4608,  1.6526, -1.2790]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2998],\n",
      "        [ 1.2694],\n",
      "        [-2.0226],\n",
      "        [ 0.5291]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3373, -0.0675,  2.2806,  0.1535,  0.0128],\n",
      "        [-0.3923, -0.0569, -0.6174,  1.3422, -1.2912],\n",
      "        [-0.9277, -0.0347,  0.5085,  0.8897,  0.8145],\n",
      "        [ 1.2470,  1.6443,  1.3903, -0.1236, -0.3221]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0660, -0.5519, -1.3013, -1.6970, -2.2830],\n",
      "        [-0.2934, -0.7789, -1.0866, -0.7154, -0.8915],\n",
      "        [ 1.3123,  1.0532,  1.9757,  1.0995,  2.2930],\n",
      "        [-0.0802,  0.4075, -0.0494, -0.1499,  0.1717]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3373, -0.0675,  2.2806,  0.1535,  0.0128],\n",
      "        [-0.3923, -0.0569, -0.6174,  1.3422, -1.2912],\n",
      "        [-0.9277, -0.0347,  0.5085,  0.8897,  0.8145],\n",
      "        [ 1.2470,  1.6443,  1.3903, -0.1236, -0.3221]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8604],\n",
      "        [ 1.0213],\n",
      "        [ 2.5967],\n",
      "        [ 0.4646]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1305,  1.1114, -0.3871,  1.0382,  0.2715],\n",
      "        [ 0.4002,  0.3530,  1.7453, -0.5245,  0.0835],\n",
      "        [ 0.5248, -2.1016, -0.0335, -0.1773,  0.4707],\n",
      "        [-0.6453,  1.3273,  1.2856,  0.4437,  1.3484]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2101, -0.2266,  0.3079, -0.1369,  1.1300],\n",
      "        [-0.1037, -0.8713, -0.5279,  0.0724, -1.3122],\n",
      "        [ 0.6407,  0.4421,  0.8211,  0.3688,  0.8206],\n",
      "        [-0.5452, -0.9289,  0.1596,  0.1614,  0.4510]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1305,  1.1114, -0.3871,  1.0382,  0.2715],\n",
      "        [ 0.4002,  0.3530,  1.7453, -0.5245,  0.0835],\n",
      "        [ 0.5248, -2.1016, -0.0335, -0.1773,  0.4707],\n",
      "        [-0.6453,  1.3273,  1.2856,  0.4437,  1.3484]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2337],\n",
      "        [-1.4180],\n",
      "        [-0.2994],\n",
      "        [ 0.0038]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1623,  0.0753,  0.6590,  1.5552, -1.3010],\n",
      "        [ 0.8470, -0.4857,  1.1191,  0.7697,  0.7444],\n",
      "        [ 0.2348, -0.1054,  0.1834, -0.5825,  0.3298],\n",
      "        [-0.4603, -0.5834, -0.7472, -0.4829, -1.2103]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0047,  0.5890, -0.1345, -0.2090, -0.5908],\n",
      "        [ 0.5633, -0.8758, -0.3297, -0.4460, -1.1087],\n",
      "        [ 0.4554,  0.9040,  0.6289,  0.3374,  0.0412],\n",
      "        [ 0.3623,  0.3125,  0.5008, -0.3331,  0.0671]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1623,  0.0753,  0.6590,  1.5552, -1.3010],\n",
      "        [ 0.8470, -0.4857,  1.1191,  0.7697,  0.7444],\n",
      "        [ 0.2348, -0.1054,  0.1834, -0.5825,  0.3298],\n",
      "        [-0.4603, -0.5834, -0.7472, -0.4829, -1.2103]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4047],\n",
      "        [-0.6350],\n",
      "        [-0.0560],\n",
      "        [-0.6436]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1687,  0.7686,  0.3650, -0.3749,  2.2761],\n",
      "        [-1.0439,  0.3144, -0.1518, -0.6485,  0.0745],\n",
      "        [ 0.2701, -0.3385,  1.1590,  0.0361, -0.5644],\n",
      "        [-1.9846,  1.5535, -1.8456, -1.4156,  0.1567]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5368, -0.2289, -0.4082, -0.1745, -0.0256],\n",
      "        [ 0.2697, -0.6154, -0.5271, -0.9489, -0.8666],\n",
      "        [ 1.0244,  0.6584,  0.5845,  0.3206,  0.8175],\n",
      "        [-0.0080, -0.0877,  0.2470,  0.7507,  0.8032]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1687,  0.7686,  0.3650, -0.3749,  2.2761],\n",
      "        [-1.0439,  0.3144, -0.1518, -0.6485,  0.0745],\n",
      "        [ 0.2701, -0.3385,  1.1590,  0.0361, -0.5644],\n",
      "        [-1.9846,  1.5535, -1.8456, -1.4156,  0.1567]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2273],\n",
      "        [ 0.1558],\n",
      "        [ 0.2814],\n",
      "        [-1.5130]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5204,  0.3366,  0.4536, -0.9668, -0.2724],\n",
      "        [ 0.8520,  0.1733,  1.0590,  0.5076,  2.0171],\n",
      "        [-0.1919,  1.1809, -1.1704, -0.2043,  0.2126],\n",
      "        [-0.1361, -0.5980,  1.9486, -0.0030,  1.8806]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0382, -0.0391, -0.3558, -0.3656, -0.6751],\n",
      "        [-0.0301, -0.3638, -0.8717, -0.6151, -0.4459],\n",
      "        [ 0.0382,  0.7817,  0.5725,  0.4863,  1.2014],\n",
      "        [ 0.4571,  0.9109,  0.6953,  0.5239,  1.7809]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5204,  0.3366,  0.4536, -0.9668, -0.2724],\n",
      "        [ 0.8520,  0.1733,  1.0590,  0.5076,  2.0171],\n",
      "        [-0.1919,  1.1809, -1.1704, -0.2043,  0.2126],\n",
      "        [-0.1361, -0.5980,  1.9486, -0.0030,  1.8806]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3047],\n",
      "        [-2.2235],\n",
      "        [ 0.4018],\n",
      "        [ 4.0954]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1407, -0.2552,  0.8999, -0.5068, -0.0722],\n",
      "        [-1.5276,  0.1393, -0.3844,  2.2830,  0.8148],\n",
      "        [-0.7216,  0.5067, -0.0695,  0.9705, -1.0745],\n",
      "        [ 0.4553, -0.3080, -0.2142,  1.0844, -0.4058]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3523, -0.4833,  0.2980, -0.3980, -0.4824],\n",
      "        [ 0.6658,  0.4799,  0.9050,  0.7653,  1.3178],\n",
      "        [-0.0649,  1.0809,  0.5690,  0.1516,  1.5298],\n",
      "        [-0.9130, -1.1443, -1.4926, -1.5579, -2.2357]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1407, -0.2552,  0.8999, -0.5068, -0.0722],\n",
      "        [-1.5276,  0.1393, -0.3844,  2.2830,  0.8148],\n",
      "        [-0.7216,  0.5067, -0.0695,  0.9705, -1.0745],\n",
      "        [ 0.4553, -0.3080, -0.2142,  1.0844, -0.4058]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2261],\n",
      "        [ 1.5229],\n",
      "        [-0.9417],\n",
      "        [-0.5258]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1530,  0.9957,  0.0574, -0.0820,  0.4402],\n",
      "        [-0.8398,  2.1058, -2.4404,  0.1951, -0.5072],\n",
      "        [-1.0375, -0.8036,  0.0321, -0.9420,  0.2846],\n",
      "        [ 1.0432,  1.1277, -0.8496,  0.7787, -0.5673]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8220,  0.1370, -0.0668, -0.0451, -0.4812],\n",
      "        [ 0.7989, -0.6839, -0.7529, -0.2406, -0.7279],\n",
      "        [ 0.5964,  0.6819,  1.0909,  1.0505,  0.9869],\n",
      "        [-0.5914, -1.1477, -1.1100, -0.7225, -1.9037]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1530,  0.9957,  0.0574, -0.0820,  0.4402],\n",
      "        [-0.8398,  2.1058, -2.4404,  0.1951, -0.5072],\n",
      "        [-1.0375, -0.8036,  0.0321, -0.9420,  0.2846],\n",
      "        [ 1.0432,  1.1277, -0.8496,  0.7787, -0.5673]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0233],\n",
      "        [ 0.0487],\n",
      "        [-1.8403],\n",
      "        [-0.4508]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2985, -1.3641, -0.3391, -0.6450, -1.6104],\n",
      "        [-2.0050,  0.8164,  1.1976, -0.2112, -0.0413],\n",
      "        [ 0.8290,  0.1549,  0.6938,  0.5475, -1.1385],\n",
      "        [ 1.2020, -0.3014, -0.6076, -0.1542, -0.8279]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3287,  0.2548, -0.0439,  0.2778,  0.8044],\n",
      "        [ 0.4722, -0.0014, -0.4077, -0.3289, -0.8082],\n",
      "        [ 0.9383,  1.3791,  1.1146,  1.0404,  1.4729],\n",
      "        [-0.4284, -0.3832, -0.8639, -0.6403, -0.9920]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2985, -1.3641, -0.3391, -0.6450, -1.6104],\n",
      "        [-2.0050,  0.8164,  1.1976, -0.2112, -0.0413],\n",
      "        [ 0.8290,  0.1549,  0.6938,  0.5475, -1.1385],\n",
      "        [ 1.2020, -0.3014, -0.6076, -0.1542, -0.8279]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7092],\n",
      "        [-1.3333],\n",
      "        [ 0.6574],\n",
      "        [ 1.0455]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8103, -2.2325, -2.7772,  1.3972,  0.5115],\n",
      "        [ 0.6942, -0.7538, -0.5117, -0.9156,  0.5329],\n",
      "        [ 0.4134, -0.9967,  0.8370, -0.2358, -0.8328],\n",
      "        [-0.4968, -0.0105, -0.5324, -0.3077,  0.8134]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2955,  0.4247,  0.6842,  0.9186,  1.6914],\n",
      "        [ 0.7461,  0.9146, -0.0560,  0.3069,  0.7784],\n",
      "        [ 1.2484,  1.2726,  2.1611,  0.8198,  1.2341],\n",
      "        [-0.5708,  0.0017, -1.0824, -0.9161, -1.6116]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8103, -2.2325, -2.7772,  1.3972,  0.5115],\n",
      "        [ 0.6942, -0.7538, -0.5117, -0.9156,  0.5329],\n",
      "        [ 0.4134, -0.9967,  0.8370, -0.2358, -0.8328],\n",
      "        [-0.4968, -0.0105, -0.5324, -0.3077,  0.8134]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9391],\n",
      "        [-0.0090],\n",
      "        [-0.1645],\n",
      "        [-0.1692]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5717, -0.8754, -0.5698,  1.0102, -0.7397],\n",
      "        [ 0.0648,  1.4972,  0.5972,  0.2464, -0.1433],\n",
      "        [ 0.0650,  0.2500,  1.6013,  0.3102,  1.5519],\n",
      "        [-1.0532, -1.1829,  0.9895,  0.1006, -0.1655]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5201,  1.3252,  0.9578,  0.8888,  1.6282],\n",
      "        [ 0.0877,  0.7248,  0.0417, -1.0306,  0.9122],\n",
      "        [ 1.0049,  1.2304,  2.1791,  0.7381,  0.5363],\n",
      "        [-0.3125, -0.2363, -1.7919, -0.4404, -1.7642]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5717, -0.8754, -0.5698,  1.0102, -0.7397],\n",
      "        [ 0.0648,  1.4972,  0.5972,  0.2464, -0.1433],\n",
      "        [ 0.0650,  0.2500,  1.6013,  0.3102,  1.5519],\n",
      "        [-1.0532, -1.1829,  0.9895,  0.1006, -0.1655]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.3097],\n",
      "        [ 0.7312],\n",
      "        [ 4.9237],\n",
      "        [-0.9168]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8123,  0.0266,  1.0779, -1.1429, -1.0820],\n",
      "        [ 0.1178,  1.4095,  0.1075,  0.0405,  1.6973],\n",
      "        [-0.8618,  0.4087, -0.2358, -1.4598,  0.5548],\n",
      "        [-0.0384, -0.4449, -0.7092, -0.2084,  0.8434]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5896,  1.6202,  1.4470,  1.0768,  2.4134],\n",
      "        [ 0.2049, -0.3953,  0.3065, -0.3423, -1.1247],\n",
      "        [-0.8915, -0.7193, -0.5564, -1.4740, -2.2676],\n",
      "        [-0.3667, -0.0721,  0.7725, -0.8586, -0.3801]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8123,  0.0266,  1.0779, -1.1429, -1.0820],\n",
      "        [ 0.1178,  1.4095,  0.1075,  0.0405,  1.6973],\n",
      "        [-0.8618,  0.4087, -0.2358, -1.4598,  0.5548],\n",
      "        [-0.0384, -0.4449, -0.7092, -0.2084,  0.8434]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9481],\n",
      "        [-2.4228],\n",
      "        [ 1.4991],\n",
      "        [-0.6434]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4029,  1.4499, -1.0071, -1.0293, -0.8876],\n",
      "        [ 1.2040, -0.3924,  0.8670,  1.6051,  2.2249],\n",
      "        [ 0.2338,  0.8817, -0.1862, -1.0527,  0.3058],\n",
      "        [-0.1613,  0.0332, -0.6237, -0.9137,  0.7626]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.8054,  2.4409,  1.6529,  1.5158,  2.2005],\n",
      "        [ 0.6856,  0.5131,  0.8739,  0.8632,  1.1572],\n",
      "        [-0.8916, -1.5023, -2.0391, -1.5439, -3.2560],\n",
      "        [ 0.2515, -0.9429, -1.2861, -0.3761, -1.0988]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4029,  1.4499, -1.0071, -1.0293, -0.8876],\n",
      "        [ 1.2040, -0.3924,  0.8670,  1.6051,  2.2249],\n",
      "        [ 0.2338,  0.8817, -0.1862, -1.0527,  0.3058],\n",
      "        [-0.1613,  0.0332, -0.6237, -0.9137,  0.7626]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.1715],\n",
      "        [ 5.3421],\n",
      "        [-0.5239],\n",
      "        [ 0.2359]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4811, -0.5142, -0.7826, -1.1497, -0.2266],\n",
      "        [ 0.9563,  0.8447, -1.0542,  0.5224, -3.2161],\n",
      "        [-0.4201,  1.2047,  0.3289, -0.5261,  1.5706],\n",
      "        [-1.2740, -0.1312,  0.9236, -1.0217,  1.3373]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2611,  0.1677,  0.3316, -0.5170,  0.6316],\n",
      "        [-1.0301, -1.9242, -1.1741, -1.3241, -2.2414],\n",
      "        [-0.8926, -1.0178, -1.0574, -1.2622, -2.9260],\n",
      "        [ 0.1830, -0.7984, -0.8307, -0.6645, -1.3668]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4811, -0.5142, -0.7826, -1.1497, -0.2266],\n",
      "        [ 0.9563,  0.8447, -1.0542,  0.5224, -3.2161],\n",
      "        [-0.4201,  1.2047,  0.3289, -0.5261,  1.5706],\n",
      "        [-1.2740, -0.1312,  0.9236, -1.0217,  1.3373]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2311],\n",
      "        [ 5.1441],\n",
      "        [-5.1306],\n",
      "        [-2.0444]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1246, -0.1188,  1.3200,  1.8316,  1.0605],\n",
      "        [ 1.3096,  1.5774, -0.4002, -1.3973,  1.4345],\n",
      "        [ 0.9136,  1.2306,  1.0025,  2.1210,  0.4735],\n",
      "        [ 0.0624, -0.2906,  1.8848, -0.6815, -0.1108]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3395,  0.3449,  0.0562, -0.0919, -0.2054],\n",
      "        [-1.9725, -3.1148, -3.3940, -3.3964, -5.8766],\n",
      "        [ 0.9356,  0.6937,  0.2259,  0.3299,  1.1308],\n",
      "        [ 0.6343, -0.4284, -0.4689,  0.0454,  0.5349]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1246, -0.1188,  1.3200,  1.8316,  1.0605],\n",
      "        [ 1.3096,  1.5774, -0.4002, -1.3973,  1.4345],\n",
      "        [ 0.9136,  1.2306,  1.0025,  2.1210,  0.4735],\n",
      "        [ 0.0624, -0.2906,  1.8848, -0.6815, -0.1108]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3953],\n",
      "        [-9.8220],\n",
      "        [ 3.1702],\n",
      "        [-0.8099]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8717,  0.4012, -0.7889,  0.3356,  1.7979],\n",
      "        [-1.8077, -1.0120,  0.2625,  0.7543,  0.5742],\n",
      "        [-0.3472,  1.0692,  1.6763,  1.2516,  1.4351],\n",
      "        [-2.6523,  0.2037, -0.1103,  0.4440, -0.3076]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0532, -0.2126, -0.2432, -0.2365, -0.0100],\n",
      "        [ 0.8209, -0.5288,  0.7552,  0.2346,  0.2965],\n",
      "        [-0.5626, -0.9371, -0.8710, -0.6673, -0.9715],\n",
      "        [ 0.5564, -0.3791,  0.0398,  0.0162,  0.4816]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8717,  0.4012, -0.7889,  0.3356,  1.7979],\n",
      "        [-1.8077, -1.0120,  0.2625,  0.7543,  0.5742],\n",
      "        [-0.3472,  1.0692,  1.6763,  1.2516,  1.4351],\n",
      "        [-2.6523,  0.2037, -0.1103,  0.4440, -0.3076]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0556],\n",
      "        [-0.4034],\n",
      "        [-4.4961],\n",
      "        [-1.6983]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0116,  0.4557, -1.0442,  1.4366,  0.8519],\n",
      "        [-0.2953,  1.8948, -0.2415,  1.0722,  1.0719],\n",
      "        [ 0.9186, -1.7187, -0.7395,  0.0935, -0.7404],\n",
      "        [ 0.3114, -1.9674, -1.7066, -0.3145,  0.4209]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3093, -0.1996, -0.0718,  0.0593, -0.2209],\n",
      "        [ 0.1611,  0.2286,  0.5891,  0.1403,  0.4671],\n",
      "        [ 1.2203,  1.0875,  1.2190,  0.7357,  2.3451],\n",
      "        [ 0.9054,  0.3008,  0.5338,  0.3568,  0.9397]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0116,  0.4557, -1.0442,  1.4366,  0.8519],\n",
      "        [-0.2953,  1.8948, -0.2415,  1.0722,  1.0719],\n",
      "        [ 0.9186, -1.7187, -0.7395,  0.0935, -0.7404],\n",
      "        [ 0.3114, -1.9674, -1.7066, -0.3145,  0.4209]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1226],\n",
      "        [ 0.8945],\n",
      "        [-3.3173],\n",
      "        [-0.9376]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2598, -0.5550,  0.4162, -1.5257,  0.5428],\n",
      "        [ 0.4140,  0.2949,  1.1284, -0.1456, -1.2780],\n",
      "        [-0.1602,  0.0858, -0.8672,  1.0201,  0.1970],\n",
      "        [ 0.7391,  1.6481,  1.5059, -0.1122,  0.4019]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2808, -0.2502, -0.6017, -0.3740,  0.2577],\n",
      "        [-0.3439,  0.2863, -0.0090, -0.0699, -0.0222],\n",
      "        [ 1.6939,  2.4606,  2.6458,  1.6753,  2.6508],\n",
      "        [ 0.6777,  0.5928,  0.2969, -0.0674,  1.1456]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2598, -0.5550,  0.4162, -1.5257,  0.5428],\n",
      "        [ 0.4140,  0.2949,  1.1284, -0.1456, -1.2780],\n",
      "        [-0.1602,  0.0858, -0.8672,  1.0201,  0.1970],\n",
      "        [ 0.7391,  1.6481,  1.5059, -0.1122,  0.4019]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9527],\n",
      "        [-0.0296],\n",
      "        [-0.1235],\n",
      "        [ 2.3930]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0167, -0.5042, -0.7923, -0.2871,  0.8363],\n",
      "        [ 0.1533, -0.2427,  0.6937, -1.2168,  2.0790],\n",
      "        [ 0.1766,  0.7041,  1.8926, -0.1155,  0.4934],\n",
      "        [ 0.2114,  0.6247, -1.9140,  0.7254, -0.8154]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4664, -0.5924,  0.2955, -0.2317, -0.9105],\n",
      "        [ 0.2350,  0.3443,  0.2980, -0.1042,  0.6063],\n",
      "        [ 0.3204,  0.0517, -0.0157,  0.6446, -1.0851],\n",
      "        [ 0.1063, -0.6491, -1.1870, -1.0783, -1.6920]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0167, -0.5042, -0.7923, -0.2871,  0.8363],\n",
      "        [ 0.1533, -0.2427,  0.6937, -1.2168,  2.0790],\n",
      "        [ 0.1766,  0.7041,  1.8926, -0.1155,  0.4934],\n",
      "        [ 0.2114,  0.6247, -1.9140,  0.7254, -0.8154]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6225],\n",
      "        [ 1.5465],\n",
      "        [-0.5467],\n",
      "        [ 2.4864]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0593,  0.7343,  0.7258,  0.0042,  0.6658],\n",
      "        [-0.7682,  1.0099,  0.3252, -1.1339, -0.9211],\n",
      "        [-0.2210,  0.1632,  1.0523,  0.2472,  0.5592],\n",
      "        [-0.7643, -0.4311, -1.5933, -3.3344,  0.4514]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1521,  0.6842,  0.2821, -0.0111, -0.2744],\n",
      "        [-0.1477, -0.7191, -0.7498, -0.4515, -1.7838],\n",
      "        [ 0.5421,  0.0681,  0.0441, -0.6562, -0.3201],\n",
      "        [-0.5942, -1.1193, -1.3458, -1.5966, -2.5939]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0593,  0.7343,  0.7258,  0.0042,  0.6658],\n",
      "        [-0.7682,  1.0099,  0.3252, -1.1339, -0.9211],\n",
      "        [-0.2210,  0.1632,  1.0523,  0.2472,  0.5592],\n",
      "        [-0.7643, -0.4311, -1.5933, -3.3344,  0.4514]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5154],\n",
      "        [ 1.2984],\n",
      "        [-0.4034],\n",
      "        [ 7.2338]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8099, -0.9790, -0.6568,  0.1910,  2.3698],\n",
      "        [ 0.3844, -0.3986, -0.2264,  0.4295,  0.0807],\n",
      "        [-0.1682,  1.0442, -0.4486, -0.3080, -0.3907],\n",
      "        [-0.7310,  0.1145, -0.7423, -3.1387, -2.3887]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3785,  0.6586,  0.0893, -0.0905, -0.1959],\n",
      "        [-0.7420, -0.6956, -0.9675, -1.2513, -1.1523],\n",
      "        [ 0.4243, -0.1930,  0.7067,  0.0329, -0.1777],\n",
      "        [ 0.5895, -0.0271, -0.3783,  0.0561,  0.0182]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8099, -0.9790, -0.6568,  0.1910,  2.3698],\n",
      "        [ 0.3844, -0.3986, -0.2264,  0.4295,  0.0807],\n",
      "        [-0.1682,  1.0442, -0.4486, -0.3080, -0.3907],\n",
      "        [-0.7310,  0.1145, -0.7423, -3.1387, -2.3887]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8784],\n",
      "        [-0.4193],\n",
      "        [-0.5306],\n",
      "        [-0.3728]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9557, -1.2251,  1.6784, -2.6735,  0.8577],\n",
      "        [-0.0551, -1.3852, -0.1319,  0.0406,  0.9826],\n",
      "        [ 0.1857, -1.5543, -0.7241,  1.1398,  1.7110],\n",
      "        [ 0.2943,  1.0844, -1.0143,  0.6758,  0.2754]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4925, -0.0660,  0.4111,  0.8617,  0.0271],\n",
      "        [-0.5382, -0.9189, -0.5527, -0.6359, -1.3238],\n",
      "        [ 0.3967,  0.1281, -0.3042, -0.5290, -1.2376],\n",
      "        [-0.1015, -0.7447, -0.4432,  0.0459, -0.1218]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9557, -1.2251,  1.6784, -2.6735,  0.8577],\n",
      "        [-0.0551, -1.3852, -0.1319,  0.0406,  0.9826],\n",
      "        [ 0.1857, -1.5543, -0.7241,  1.1398,  1.7110],\n",
      "        [ 0.2943,  1.0844, -1.0143,  0.6758,  0.2754]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0388],\n",
      "        [ 0.0488],\n",
      "        [-2.6256],\n",
      "        [-0.3904]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3945, -1.0974,  0.7578, -0.1590, -0.9757],\n",
      "        [-1.0663, -0.9106, -1.3473,  0.9196,  1.3632],\n",
      "        [ 0.3967,  1.1016, -0.3913,  0.1016,  1.0237],\n",
      "        [-0.6096, -0.1336,  0.7385,  1.8037,  1.4505]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7343,  0.7412,  0.4123,  0.0615,  0.5722],\n",
      "        [-0.5434, -0.1018, -0.6309, -0.5137, -0.8808],\n",
      "        [ 1.4215,  0.9675,  1.2344,  0.6274,  1.5576],\n",
      "        [ 0.1267, -0.2515, -0.3298, -0.0559, -0.3549]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3945, -1.0974,  0.7578, -0.1590, -0.9757],\n",
      "        [-1.0663, -0.9106, -1.3473,  0.9196,  1.3632],\n",
      "        [ 0.3967,  1.1016, -0.3913,  0.1016,  1.0237],\n",
      "        [-0.6096, -0.1336,  0.7385,  1.8037,  1.4505]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7793],\n",
      "        [-0.1509],\n",
      "        [ 2.8049],\n",
      "        [-0.9028]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8249,  1.3163, -0.1399,  2.0176, -0.9126],\n",
      "        [-0.3513,  0.2241,  0.1012,  0.8510, -1.6866],\n",
      "        [-0.5825, -0.0179, -0.9273,  0.1768,  0.3986],\n",
      "        [-1.6444, -1.3354,  0.5476,  0.0305,  2.0895]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7088,  0.4129,  0.4835,  1.6118,  1.5725],\n",
      "        [-0.4856, -0.1043, -0.3720, -0.3174, -0.4558],\n",
      "        [ 0.2219, -0.7146, -0.2165, -0.9412, -1.3741],\n",
      "        [ 0.2918,  0.2832,  0.3856,  0.3436,  0.5020]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8249,  1.3163, -0.1399,  2.0176, -0.9126],\n",
      "        [-0.3513,  0.2241,  0.1012,  0.8510, -1.6866],\n",
      "        [-0.5825, -0.0179, -0.9273,  0.1768,  0.3986],\n",
      "        [-1.6444, -1.3354,  0.5476,  0.0305,  2.0895]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9994],\n",
      "        [ 0.6082],\n",
      "        [-0.6298],\n",
      "        [ 0.4124]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1583,  0.0170,  3.3120, -0.8759,  1.5666],\n",
      "        [-0.8532,  1.5187,  0.3956,  0.7370, -0.2375],\n",
      "        [-1.0529,  1.5038,  1.3169,  0.9081,  1.2882],\n",
      "        [-0.8992,  1.9430,  0.6094, -0.5829, -0.3886]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8861, -0.2125,  0.1683, -0.1136,  0.3138],\n",
      "        [-0.0363, -0.1538, -0.8545,  0.3818, -0.8759],\n",
      "        [ 0.4300,  0.1245,  0.3236,  0.1076,  0.2364],\n",
      "        [-0.0503, -0.0094, -0.1046, -0.4074, -0.6329]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1583,  0.0170,  3.3120, -0.8759,  1.5666],\n",
      "        [-0.8532,  1.5187,  0.3956,  0.7370, -0.2375],\n",
      "        [-1.0529,  1.5038,  1.3169,  0.9081,  1.2882],\n",
      "        [-0.8992,  1.9430,  0.6094, -0.5829, -0.3886]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1186],\n",
      "        [-0.0513],\n",
      "        [ 0.5629],\n",
      "        [ 0.4467]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4844,  0.7296, -0.7198, -0.2612, -0.8485],\n",
      "        [ 1.0589,  0.8174,  0.8779, -0.2527, -0.2633],\n",
      "        [ 0.3913,  0.1687,  0.6891, -0.3615, -1.0514],\n",
      "        [-2.5371, -0.2267,  1.6342,  0.6086,  0.8875]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0198,  0.2441, -1.0525,  0.7727,  0.2412],\n",
      "        [-0.6543, -0.2446,  0.0500, -0.0350, -0.1815],\n",
      "        [-0.0484, -0.2870, -0.1379, -0.8928, -0.5315],\n",
      "        [ 0.1551, -0.9562, -0.1955,  0.2842, -0.1946]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4844,  0.7296, -0.7198, -0.2612, -0.8485],\n",
      "        [ 1.0589,  0.8174,  0.8779, -0.2527, -0.2633],\n",
      "        [ 0.3913,  0.1687,  0.6891, -0.3615, -1.0514],\n",
      "        [-2.5371, -0.2267,  1.6342,  0.6086,  0.8875]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5195],\n",
      "        [-0.7923],\n",
      "        [ 0.7192],\n",
      "        [-0.4960]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0160, -0.7885,  2.1849,  0.7101, -2.0495],\n",
      "        [ 0.5456, -1.3560, -1.2708,  1.3230, -1.0764],\n",
      "        [ 0.0254, -1.4537,  1.9093, -0.9011, -0.2631],\n",
      "        [-1.6620,  0.1558, -1.4256,  0.6862,  0.1535]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1547,  0.0854,  0.3483, -0.0455,  0.5939],\n",
      "        [-0.4469, -0.3545, -0.5158,  0.3340,  0.2943],\n",
      "        [-0.2929, -0.6710, -0.8681, -0.3524, -0.6009],\n",
      "        [ 0.3835, -1.0182,  0.2424, -0.0393, -0.2575]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0160, -0.7885,  2.1849,  0.7101, -2.0495],\n",
      "        [ 0.5456, -1.3560, -1.2708,  1.3230, -1.0764],\n",
      "        [ 0.0254, -1.4537,  1.9093, -0.9011, -0.2631],\n",
      "        [-1.6620,  0.1558, -1.4256,  0.6862,  0.1535]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5534],\n",
      "        [ 1.0172],\n",
      "        [-0.2139],\n",
      "        [-1.2082]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6521,  1.2490, -1.4235, -0.3030, -0.0142],\n",
      "        [-0.9551,  0.2617, -1.6785, -0.3955,  0.3221],\n",
      "        [ 0.8725, -1.0461,  2.0633, -0.2852,  0.1296],\n",
      "        [ 0.2323, -0.7615, -1.1890, -1.1184, -0.0848]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0452,  0.2632,  0.9250,  0.4572,  0.5550],\n",
      "        [-0.4086,  0.2868,  0.0304,  0.2064, -0.9945],\n",
      "        [ 0.4973, -0.3158,  0.0192, -0.2450,  0.3106],\n",
      "        [ 0.6739, -0.0201,  0.2815,  0.4454,  0.8386]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6521,  1.2490, -1.4235, -0.3030, -0.0142],\n",
      "        [-0.9551,  0.2617, -1.6785, -0.3955,  0.3221],\n",
      "        [ 0.8725, -1.0461,  2.0633, -0.2852,  0.1296],\n",
      "        [ 0.2323, -0.7615, -1.1890, -1.1184, -0.0848]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1050],\n",
      "        [ 0.0124],\n",
      "        [ 0.9138],\n",
      "        [-0.7320]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 3.0731,  0.8782,  0.9279, -0.3404,  1.3744],\n",
      "        [ 0.5818, -0.1291, -0.3882, -0.4715, -2.0950],\n",
      "        [-0.1246,  1.8950,  0.3495,  0.6439, -0.2540],\n",
      "        [-2.1272,  0.8350,  1.4176, -0.2523,  0.6673]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4113,  0.1812,  0.4617,  0.3958,  1.0690],\n",
      "        [ 0.0099, -0.6582, -0.2022,  0.2236,  0.3672],\n",
      "        [-0.2915,  0.3693,  0.3358, -0.2458, -0.0702],\n",
      "        [ 0.3084,  0.4510,  0.6392,  0.6531,  0.7693]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 3.0731,  0.8782,  0.9279, -0.3404,  1.3744],\n",
      "        [ 0.5818, -0.1291, -0.3882, -0.4715, -2.0950],\n",
      "        [-0.1246,  1.8950,  0.3495,  0.6439, -0.2540],\n",
      "        [-2.1272,  0.8350,  1.4176, -0.2523,  0.6673]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.1861],\n",
      "        [-0.7055],\n",
      "        [ 0.7130],\n",
      "        [ 0.9753]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.6727, -0.0892, -1.9117,  0.3369,  1.4197],\n",
      "        [ 0.4951, -0.0533, -0.0572,  0.6577, -0.7123],\n",
      "        [-0.0903,  1.1663, -1.5192,  0.2233,  0.6495],\n",
      "        [-0.1222, -0.8672,  0.7989,  0.4404,  0.0942]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6196, -0.3512, -1.9436, -0.7684, -1.8860],\n",
      "        [-0.2100, -0.5393,  0.2221, -0.1411, -0.5886],\n",
      "        [-0.2113, -0.6329, -0.3559,  0.2289, -0.1237],\n",
      "        [ 0.6827, -0.4969, -0.4261,  0.4698, -0.2655]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.6727, -0.0892, -1.9117,  0.3369,  1.4197],\n",
      "        [ 0.4951, -0.0533, -0.0572,  0.6577, -0.7123],\n",
      "        [-0.0903,  1.1663, -1.5192,  0.2233,  0.6495],\n",
      "        [-0.1222, -0.8672,  0.7989,  0.4404,  0.0942]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8455],\n",
      "        [ 0.2385],\n",
      "        [-0.2075],\n",
      "        [ 0.1889]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4368,  1.0078, -0.4168,  0.0229,  0.4631],\n",
      "        [-0.4309,  0.6987, -1.3705, -0.8732,  0.7190],\n",
      "        [-1.6487, -0.6200, -0.1752,  0.1350,  0.8934],\n",
      "        [-1.2211,  1.1866, -0.9279,  0.5995,  0.5652]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2101, -0.3818, -0.2357,  0.1840, -0.6748],\n",
      "        [-0.4420, -0.4813, -0.6934, -0.2794, -0.7009],\n",
      "        [-0.0457,  0.0732,  0.9640,  0.6036,  0.7867],\n",
      "        [ 0.6245, -0.2046,  0.2886, -0.2940,  1.0160]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4368,  1.0078, -0.4168,  0.0229,  0.4631],\n",
      "        [-0.4309,  0.6987, -1.3705, -0.8732,  0.7190],\n",
      "        [-1.6487, -0.6200, -0.1752,  0.1350,  0.8934],\n",
      "        [-1.2211,  1.1866, -0.9279,  0.5995,  0.5652]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6865],\n",
      "        [ 0.5446],\n",
      "        [ 0.6454],\n",
      "        [-0.8752]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0908,  0.4466,  0.5199, -0.7587,  1.5996],\n",
      "        [-2.0579,  0.7567, -0.2029, -0.8238,  0.3008],\n",
      "        [ 0.4696,  0.6159, -1.0240,  1.4101, -0.2484],\n",
      "        [-1.0688, -1.4617, -0.4612,  0.4532, -0.2441]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5657, -0.7463, -0.0165, -0.2042, -0.4187],\n",
      "        [-0.2384, -0.6213, -0.4340, -0.4110, -0.9863],\n",
      "        [ 0.1004,  0.4467,  0.3735,  1.1057,  0.8431],\n",
      "        [-0.3164,  0.6898,  0.1252,  0.5634,  0.3117]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0908,  0.4466,  0.5199, -0.7587,  1.5996],\n",
      "        [-2.0579,  0.7567, -0.2029, -0.8238,  0.3008],\n",
      "        [ 0.4696,  0.6159, -1.0240,  1.4101, -0.2484],\n",
      "        [-1.0688, -1.4617, -0.4612,  0.4532, -0.2441]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8053],\n",
      "        [ 0.1504],\n",
      "        [ 1.2895],\n",
      "        [-0.5485]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4284,  0.6885,  0.5356,  0.4474,  0.7006],\n",
      "        [-0.4643,  1.0896, -0.0962,  0.8166,  0.4553],\n",
      "        [ 0.3035,  2.1958,  1.0535,  0.9622,  0.7294],\n",
      "        [ 0.4979, -2.3930,  1.7656, -1.1969, -0.7363]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6507,  0.3473,  0.6740, -0.4879, -0.3669],\n",
      "        [ 0.0586, -0.6544, -0.5177,  0.1012, -0.7401],\n",
      "        [-0.4133, -0.2240, -0.1875,  0.1380, -0.7467],\n",
      "        [ 0.2104,  0.0067,  0.0714, -0.0827,  0.9829]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4284,  0.6885,  0.5356,  0.4474,  0.7006],\n",
      "        [-0.4643,  1.0896, -0.0962,  0.8166,  0.4553],\n",
      "        [ 0.3035,  2.1958,  1.0535,  0.9622,  0.7294],\n",
      "        [ 0.4979, -2.3930,  1.7656, -1.1969, -0.7363]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1540],\n",
      "        [-0.9447],\n",
      "        [-1.2266],\n",
      "        [-0.4100]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0187, -0.0986,  0.5141, -0.0926,  0.4174],\n",
      "        [ 0.5558, -0.5961,  0.1363, -2.5969,  0.1290],\n",
      "        [-0.6932, -0.9320,  0.0083, -0.4291,  0.5970],\n",
      "        [ 0.1076,  1.6793, -0.9169,  0.3840,  1.6943]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6907, -0.1369,  0.1021, -0.5854, -0.3724],\n",
      "        [ 0.3707, -0.6736,  0.4702, -0.4083, -0.0220],\n",
      "        [ 0.0250,  1.0915, -0.1139,  0.2165,  0.6354],\n",
      "        [ 0.7366,  0.3883,  0.2152,  0.3774,  0.1486]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0187, -0.0986,  0.5141, -0.0926,  0.4174],\n",
      "        [ 0.5558, -0.5961,  0.1363, -2.5969,  0.1290],\n",
      "        [-0.6932, -0.9320,  0.0083, -0.4291,  0.5970],\n",
      "        [ 0.1076,  1.6793, -0.9169,  0.3840,  1.6943]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0224],\n",
      "        [ 1.7291],\n",
      "        [-0.7490],\n",
      "        [ 0.9307]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1264,  0.0176, -0.9998, -1.0193,  1.9667],\n",
      "        [-1.8362, -0.3587,  0.2582, -0.6178,  0.9902],\n",
      "        [-0.1255, -0.3636, -0.4732, -0.0167, -0.9836],\n",
      "        [ 0.9513,  1.0561,  2.3792, -1.3053, -1.3292]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3685, -0.5771, -0.1129, -0.3273, -0.0983],\n",
      "        [-0.5438, -0.4626, -1.0046, -1.0664, -2.3274],\n",
      "        [-0.0037,  1.2920,  0.5829,  0.6099,  0.3183],\n",
      "        [-0.2279,  0.0133, -0.5076,  0.1125, -1.1178]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1264,  0.0176, -0.9998, -1.0193,  1.9667],\n",
      "        [-1.8362, -0.3587,  0.2582, -0.6178,  0.9902],\n",
      "        [-0.1255, -0.3636, -0.4732, -0.0167, -0.9836],\n",
      "        [ 0.9513,  1.0561,  2.3792, -1.3053, -1.3292]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2895],\n",
      "        [-0.7408],\n",
      "        [-1.0684],\n",
      "        [-0.0716]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8560,  0.5482, -0.9225, -0.9012,  0.5138],\n",
      "        [-0.8437,  1.3787,  0.8311,  0.5232, -0.1335],\n",
      "        [-1.7316, -0.0505,  0.2142,  1.3039, -0.1155],\n",
      "        [ 1.1093,  1.0830,  0.5436, -0.5472, -1.5932]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5171, -0.3183,  0.0760,  0.0357,  0.0220],\n",
      "        [-0.1308, -0.6995, -0.7055, -0.9980, -1.3432],\n",
      "        [ 0.6344,  1.0462,  1.1538,  0.8304,  1.5300],\n",
      "        [-0.1080,  0.5131, -0.5627, -0.0385, -0.0896]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8560,  0.5482, -0.9225, -0.9012,  0.5138],\n",
      "        [-0.8437,  1.3787,  0.8311,  0.5232, -0.1335],\n",
      "        [-1.7316, -0.0505,  0.2142,  1.3039, -0.1155],\n",
      "        [ 1.1093,  1.0830,  0.5436, -0.5472, -1.5932]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6942],\n",
      "        [-1.7832],\n",
      "        [ 0.0020],\n",
      "        [ 0.2938]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3805, -0.9459, -0.9658,  0.4903,  0.8673],\n",
      "        [ 0.0099, -0.9143,  0.6683,  0.9913, -0.3515],\n",
      "        [ 0.8074, -0.8179, -0.0605, -1.9287, -1.4014],\n",
      "        [-0.9988,  0.8681,  0.1994,  1.3398, -0.0434]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2753,  0.1048,  0.4215,  0.4280,  0.1629],\n",
      "        [ 0.7554, -0.1190,  0.3135,  0.1936,  0.2350],\n",
      "        [ 0.5484,  1.5194,  0.8699,  1.1625,  1.1689],\n",
      "        [ 0.2097,  0.9507,  0.7027,  0.8408,  0.3476]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3805, -0.9459, -0.9658,  0.4903,  0.8673],\n",
      "        [ 0.0099, -0.9143,  0.6683,  0.9913, -0.3515],\n",
      "        [ 0.8074, -0.8179, -0.0605, -1.9287, -1.4014],\n",
      "        [-0.9988,  0.8681,  0.1994,  1.3398, -0.0434]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2249],\n",
      "        [ 0.4351],\n",
      "        [-4.7328],\n",
      "        [ 1.8675]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1816, -0.3925,  0.4806,  0.6312,  0.7109],\n",
      "        [-0.8601,  0.8034,  0.6293, -0.4354,  0.4882],\n",
      "        [-1.3382, -0.2651,  0.5667,  1.1425,  0.8501],\n",
      "        [-0.0582, -0.7362,  0.5309,  0.2865,  1.2183]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0241,  0.0779, -0.2034,  0.0429,  0.6297],\n",
      "        [-0.1485, -0.5452, -0.4149, -0.7178, -0.8614],\n",
      "        [ 1.6702,  2.1438,  2.3300,  1.9888,  2.4498],\n",
      "        [-0.2820,  0.4941, -0.5829, -0.5083, -1.6103]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1816, -0.3925,  0.4806,  0.6312,  0.7109],\n",
      "        [-0.8601,  0.8034,  0.6293, -0.4354,  0.4882],\n",
      "        [-1.3382, -0.2651,  0.5667,  1.1425,  0.8501],\n",
      "        [-0.0582, -0.7362,  0.5309,  0.2865,  1.2183]], device='cuda:0') torch.Size([4, 5])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:\n",
      "tensor([[ 0.3420],\n",
      "        [-0.6793],\n",
      "        [ 2.8719],\n",
      "        [-2.7642]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6454,  1.0346,  0.2229,  1.4426,  0.4419],\n",
      "        [ 0.5789,  1.0435, -0.2897, -0.2433,  1.2442],\n",
      "        [-2.3706, -0.1592, -1.9359, -1.0726, -0.6717],\n",
      "        [-0.1036, -0.9904,  1.0811, -0.3606, -0.3258]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0098,  0.0283, -0.1781, -0.0900, -0.4694],\n",
      "        [ 0.3628, -0.7285, -0.6197, -1.2988, -0.6296],\n",
      "        [ 0.0927,  0.3927, -0.0584,  0.0403,  0.4621],\n",
      "        [ 0.8301,  1.3478,  2.1486,  1.0165,  1.2455]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6454,  1.0346,  0.2229,  1.4426,  0.4419],\n",
      "        [ 0.5789,  1.0435, -0.2897, -0.2433,  1.2442],\n",
      "        [-2.3706, -0.1592, -1.9359, -1.0726, -0.6717],\n",
      "        [-0.1036, -0.9904,  1.0811, -0.3606, -0.3258]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3638],\n",
      "        [-0.8380],\n",
      "        [-0.5230],\n",
      "        [ 0.1298]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1820,  1.8756, -0.9058,  1.1249,  0.8736],\n",
      "        [ 0.7226,  0.6843, -1.0124,  1.9619,  0.4775],\n",
      "        [-0.5893,  1.1106,  0.2771,  2.1659, -0.2830],\n",
      "        [ 1.1771, -1.5543,  1.4949, -0.5695,  0.1022]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2433,  0.4889,  0.1071, -0.2028,  0.1374],\n",
      "        [ 0.1854, -0.9198, -1.0580, -1.6301, -0.8432],\n",
      "        [ 0.1568,  0.2506,  0.0343,  0.1292,  0.5742],\n",
      "        [ 0.5871,  1.0254,  0.9491, -0.1589,  0.2921]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1820,  1.8756, -0.9058,  1.1249,  0.8736],\n",
      "        [ 0.7226,  0.6843, -1.0124,  1.9619,  0.4775],\n",
      "        [-0.5893,  1.1106,  0.2771,  2.1659, -0.2830],\n",
      "        [ 1.1771, -1.5543,  1.4949, -0.5695,  0.1022]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7561],\n",
      "        [-3.0251],\n",
      "        [ 0.3126],\n",
      "        [ 0.6364]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-3.0269, -1.4781, -0.1484, -0.3914,  0.2631],\n",
      "        [-0.1058,  1.6578,  1.0866,  1.6000, -0.2166],\n",
      "        [-1.0801,  0.2736,  0.8666, -0.2781,  0.3174],\n",
      "        [-1.4806, -0.6191, -1.3531, -1.0187,  0.3958]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5091,  0.5912, -0.1346,  0.1081,  0.0569],\n",
      "        [ 0.5256,  0.0622,  0.9749,  0.7283,  1.3642],\n",
      "        [ 0.1237,  0.3185,  0.4852, -0.4076,  0.4858],\n",
      "        [ 0.5302,  0.4062,  0.5944,  0.2177,  0.6835]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-3.0269, -1.4781, -0.1484, -0.3914,  0.2631],\n",
      "        [-0.1058,  1.6578,  1.0866,  1.6000, -0.2166],\n",
      "        [-1.0801,  0.2736,  0.8666, -0.2781,  0.3174],\n",
      "        [-1.4806, -0.6191, -1.3531, -1.0187,  0.3958]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4224],\n",
      "        [ 1.9766],\n",
      "        [ 0.6415],\n",
      "        [-1.7920]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8954,  0.4285,  0.2202, -0.2209,  1.6310],\n",
      "        [-1.0573, -0.4594, -0.4591, -0.1318,  1.2040],\n",
      "        [-1.5238,  2.1307,  0.1431, -1.0840,  1.7464],\n",
      "        [-0.2789, -1.0754, -0.6976,  1.8037,  0.8938]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4087,  0.9241,  1.5260,  1.3324,  0.8971],\n",
      "        [ 0.0082, -0.4615, -0.3363, -1.2209, -0.8313],\n",
      "        [-0.4228, -0.4487,  0.1644,  0.2293,  0.0461],\n",
      "        [ 0.9265,  1.0702, -0.2693,  1.1563,  1.6279]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8954,  0.4285,  0.2202, -0.2209,  1.6310],\n",
      "        [-1.0573, -0.4594, -0.4591, -0.1318,  1.2040],\n",
      "        [-1.5238,  2.1307,  0.1431, -1.0840,  1.7464],\n",
      "        [-0.2789, -1.0754, -0.6976,  1.8037,  0.8938]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1261],\n",
      "        [-0.4822],\n",
      "        [-0.4565],\n",
      "        [ 2.3193]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5208,  1.0648,  0.4748,  0.6060,  1.3612],\n",
      "        [ 0.9436, -0.5772,  0.3182, -0.7737, -1.8843],\n",
      "        [-0.6977,  0.2737,  0.1574, -0.9193,  2.1461],\n",
      "        [-1.8025,  0.1630,  1.5641,  1.1668,  0.1772]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3380,  0.3623,  0.5491,  0.5055,  1.0151],\n",
      "        [ 0.4760, -0.9902, -0.5640, -0.6394, -1.3941],\n",
      "        [ 0.3238, -0.3705,  0.2601, -0.2773,  0.1126],\n",
      "        [-0.1155, -0.1106,  0.2229,  0.3694, -0.2817]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5208,  1.0648,  0.4748,  0.6060,  1.3612],\n",
      "        [ 0.9436, -0.5772,  0.3182, -0.7737, -1.8843],\n",
      "        [-0.6977,  0.2737,  0.1574, -0.9193,  2.1461],\n",
      "        [-1.8025,  0.1630,  1.5641,  1.1668,  0.1772]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5107],\n",
      "        [ 3.9628],\n",
      "        [ 0.2102],\n",
      "        [ 0.9199]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3417,  0.7550, -1.1206,  1.1512,  1.9309],\n",
      "        [-0.7201,  1.6917,  0.4555, -0.4413,  1.6423],\n",
      "        [-0.7200,  0.0159, -0.0369,  1.8497, -0.2597],\n",
      "        [ 0.2082, -1.5018, -1.6673, -1.7466, -0.4351]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7593, -0.9408, -0.3450, -0.4554, -1.9106],\n",
      "        [-1.4712, -1.3811, -1.7170, -2.1912, -4.0341],\n",
      "        [-0.1807, -0.1290, -0.0832,  0.2010,  0.5223],\n",
      "        [ 0.2436, -0.6527,  0.2934,  0.1440, -0.7406]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3417,  0.7550, -1.1206,  1.1512,  1.9309],\n",
      "        [-0.7201,  1.6917,  0.4555, -0.4413,  1.6423],\n",
      "        [-0.7200,  0.0159, -0.0369,  1.8497, -0.2597],\n",
      "        [ 0.2082, -1.5018, -1.6673, -1.7466, -0.4351]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.2777],\n",
      "        [-7.7172],\n",
      "        [ 0.3673],\n",
      "        [ 0.6124]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.1258,  0.6139, -1.0473,  2.7401,  0.9935],\n",
      "        [-2.0899, -0.8382,  1.5212,  1.0296, -1.2559],\n",
      "        [ 0.5218, -0.1072, -0.7603, -0.2608, -1.6726],\n",
      "        [ 0.3285,  0.9356,  0.0576, -0.8733, -1.3413]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1398,  1.5825,  1.0887,  0.8353,  1.9472],\n",
      "        [ 1.3713,  1.0455,  0.8346,  0.4024,  1.6018],\n",
      "        [ 0.6410, -0.4630,  0.0349, -0.1790, -0.5405],\n",
      "        [-0.7340, -0.0672,  0.2344, -0.0198, -0.1108]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.1258,  0.6139, -1.0473,  2.7401,  0.9935],\n",
      "        [-2.0899, -0.8382,  1.5212,  1.0296, -1.2559],\n",
      "        [ 0.5218, -0.1072, -0.7603, -0.2608, -1.6726],\n",
      "        [ 0.3285,  0.9356,  0.0576, -0.8733, -1.3413]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 6.4775],\n",
      "        [-4.0698],\n",
      "        [ 1.3083],\n",
      "        [-0.1246]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8725, -0.7284, -0.0256, -0.5352, -0.1264],\n",
      "        [ 0.4524,  0.9445, -1.2350,  0.4750,  0.8342],\n",
      "        [ 0.9371,  1.8236, -0.0496, -0.4968, -1.4560],\n",
      "        [ 0.3326,  0.2647,  0.1802,  0.2846, -0.4319]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7653, -1.6396, -1.3476, -0.7881, -2.7108],\n",
      "        [ 2.1224,  2.3100,  2.2743,  1.3021,  2.9983],\n",
      "        [-0.2922, -1.0464, -1.1371, -0.4700, -1.3085],\n",
      "        [-0.3099,  1.2324,  1.0951,  0.4299,  0.7229]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8725, -0.7284, -0.0256, -0.5352, -0.1264],\n",
      "        [ 0.4524,  0.9445, -1.2350,  0.4750,  0.8342],\n",
      "        [ 0.9371,  1.8236, -0.0496, -0.4968, -1.4560],\n",
      "        [ 0.3326,  0.2647,  0.1802,  0.2846, -0.4319]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.6611],\n",
      "        [ 3.4528],\n",
      "        [ 0.0131],\n",
      "        [ 0.2306]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5147,  0.1574,  0.2788,  0.1113, -0.4869],\n",
      "        [-0.5116,  0.7627,  1.6460,  1.3556, -0.5866],\n",
      "        [-0.6290, -0.4664, -0.3956, -0.5974,  0.0513],\n",
      "        [-0.8524, -0.6272, -2.0052, -0.0883,  0.0520]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1736, -1.5409, -1.8103, -2.1523, -5.1022],\n",
      "        [ 1.1496,  1.3393,  2.2163,  0.5429,  1.7673],\n",
      "        [ 0.1252, -0.1439, -0.0591,  0.1346, -0.1951],\n",
      "        [ 0.7853, -0.4471,  0.7613,  1.3176,  0.3988]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5147,  0.1574,  0.2788,  0.1113, -0.4869],\n",
      "        [-0.5116,  0.7627,  1.6460,  1.3556, -0.5866],\n",
      "        [-0.6290, -0.4664, -0.3956, -0.5974,  0.0513],\n",
      "        [-0.8524, -0.6272, -2.0052, -0.0883,  0.0520]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1012],\n",
      "        [ 3.7806],\n",
      "        [-0.0787],\n",
      "        [-2.0111]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6213,  0.4019,  0.4838,  1.4254, -1.5294],\n",
      "        [ 0.2963, -0.5294, -0.4224,  0.8605,  0.6346],\n",
      "        [ 0.6857,  0.5442,  0.4927, -1.2665,  0.2650],\n",
      "        [-0.0715,  0.3094,  0.0525, -0.6257,  0.8043]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0826, -0.1378, -0.2432,  0.4060,  0.2359],\n",
      "        [ 0.1200,  0.8532, -0.6460,  0.2409, -0.1669],\n",
      "        [-0.2667,  0.2005, -0.0045, -0.0300, -0.2051],\n",
      "        [ 0.6363,  0.9361,  1.5285,  0.9625,  1.6010]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6213,  0.4019,  0.4838,  1.4254, -1.5294],\n",
      "        [ 0.2963, -0.5294, -0.4224,  0.8605,  0.6346],\n",
      "        [ 0.6857,  0.5442,  0.4927, -1.2665,  0.2650],\n",
      "        [-0.0715,  0.3094,  0.0525, -0.6257,  0.8043]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0962],\n",
      "        [-0.0419],\n",
      "        [-0.0923],\n",
      "        [ 1.0096]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1707,  0.1447,  0.8632,  0.5799, -0.4628],\n",
      "        [ 0.1888, -1.6532, -0.3524, -1.3750,  0.5101],\n",
      "        [-1.3362, -1.4531, -0.8499,  0.6214, -0.2574],\n",
      "        [ 0.6585, -1.8305, -1.4888,  0.8863,  0.8363]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0444, -0.4691,  0.2359, -0.1282, -0.0517],\n",
      "        [ 0.1648,  0.0018,  0.3806,  0.7796,  0.5103],\n",
      "        [-0.3821,  0.0748,  0.0111, -0.1453, -0.0300],\n",
      "        [ 0.2855,  1.6007,  0.7894,  0.9730,  0.7774]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1707,  0.1447,  0.8632,  0.5799, -0.4628],\n",
      "        [ 0.1888, -1.6532, -0.3524, -1.3750,  0.5101],\n",
      "        [-1.3362, -1.4531, -0.8499,  0.6214, -0.2574],\n",
      "        [ 0.6585, -1.8305, -1.4888,  0.8863,  0.8363]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0333],\n",
      "        [-0.9177],\n",
      "        [ 0.3098],\n",
      "        [-2.4047]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2038,  1.4062,  0.9807, -0.7319, -0.3913],\n",
      "        [ 0.4820, -1.1187,  0.5140, -0.3990,  1.3831],\n",
      "        [ 0.0914, -0.4123, -0.2572, -0.5437,  0.0463],\n",
      "        [ 0.0169, -0.6733,  0.0852,  1.3148, -0.2196]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1383, -0.1086,  0.3625,  0.1859,  0.0289],\n",
      "        [ 0.6684,  0.2708,  0.3951, -0.1874,  0.5628],\n",
      "        [-0.0901, -0.0972,  0.2020,  0.5727,  0.3313],\n",
      "        [ 0.3164,  1.2852,  2.6473,  2.2836,  2.5228]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2038,  1.4062,  0.9807, -0.7319, -0.3913],\n",
      "        [ 0.4820, -1.1187,  0.5140, -0.3990,  1.3831],\n",
      "        [ 0.0914, -0.4123, -0.2572, -0.5437,  0.0463],\n",
      "        [ 0.0169, -0.6733,  0.0852,  1.3148, -0.2196]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1110],\n",
      "        [ 1.0755],\n",
      "        [-0.3162],\n",
      "        [ 1.8140]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7915,  0.5140,  0.5925,  0.1045, -2.2814],\n",
      "        [-0.0043, -0.7507, -0.0488,  1.5277, -0.8338],\n",
      "        [ 0.5440, -0.5509,  1.1634, -1.6275, -0.4643],\n",
      "        [ 0.1847, -0.8992, -0.6197,  0.8560,  1.5046]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0783, -0.8581,  0.5725, -0.2080,  0.9676],\n",
      "        [ 0.0045,  0.4369, -0.0024, -0.4083, -0.1230],\n",
      "        [ 0.3508, -0.1665,  0.6404, -0.4116, -0.0060],\n",
      "        [ 1.1091,  1.4763,  0.7215,  1.5343,  1.2921]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7915,  0.5140,  0.5925,  0.1045, -2.2814],\n",
      "        [-0.0043, -0.7507, -0.0488,  1.5277, -0.8338],\n",
      "        [ 0.5440, -0.5509,  1.1634, -1.6275, -0.4643],\n",
      "        [ 0.1847, -0.8992, -0.6197,  0.8560,  1.5046]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2692],\n",
      "        [-0.8490],\n",
      "        [ 1.7003],\n",
      "        [ 1.6877]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3753, -0.8593,  0.9411, -1.6644, -0.3364],\n",
      "        [-0.1680, -0.3181,  0.0895, -1.3778,  0.0190],\n",
      "        [ 1.6588, -0.4433,  1.1369, -0.6887, -0.5454],\n",
      "        [ 0.1121, -0.6947,  0.9017,  0.2156, -0.3519]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8552,  1.3167,  0.5092,  1.3079,  1.4110],\n",
      "        [ 0.1359,  0.0222, -0.4047, -0.0245,  0.0712],\n",
      "        [-0.3007, -0.4352, -1.2468, -0.2531, -2.0314],\n",
      "        [-0.5096,  0.9263,  1.2700,  0.7711,  1.0502]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3753, -0.8593,  0.9411, -1.6644, -0.3364],\n",
      "        [-0.1680, -0.3181,  0.0895, -1.3778,  0.0190],\n",
      "        [ 1.6588, -0.4433,  1.1369, -0.6887, -0.5454],\n",
      "        [ 0.1121, -0.6947,  0.9017,  0.2156, -0.3519]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.6249],\n",
      "        [-0.0310],\n",
      "        [-0.4412],\n",
      "        [ 0.2412]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0726, -0.8767,  0.8709,  0.3209, -0.4658],\n",
      "        [-0.5665,  0.8443, -0.0092,  0.8126, -0.4853],\n",
      "        [-0.9427, -0.7456,  0.9658, -2.0177, -0.2812],\n",
      "        [ 1.4914, -1.2653, -0.7763,  0.6489, -0.1387]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.0431,  1.3487,  2.8174,  1.9431,  2.6616],\n",
      "        [ 0.6069,  0.0899,  1.0181, -0.0871,  0.4563],\n",
      "        [ 0.0002, -0.5981, -0.0512, -0.0388, -0.8510],\n",
      "        [ 0.0923,  0.6726,  0.3294,  0.5152,  1.1541]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0726, -0.8767,  0.8709,  0.3209, -0.4658],\n",
      "        [-0.5665,  0.8443, -0.0092,  0.8126, -0.4853],\n",
      "        [-0.9427, -0.7456,  0.9658, -2.0177, -0.2812],\n",
      "        [ 1.4914, -1.2653, -0.7763,  0.6489, -0.1387]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.8465],\n",
      "        [-0.5696],\n",
      "        [ 0.7138],\n",
      "        [-0.7950]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.3162, -0.0523, -0.8869, -0.8235,  0.9317],\n",
      "        [ 0.3743, -1.1570, -0.0683,  0.5364,  0.0773],\n",
      "        [ 0.0727,  0.7933,  1.2764, -0.5369,  1.5029],\n",
      "        [-2.3132,  1.2257,  1.4089,  1.4922, -0.1603]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1098, -0.0761, -0.0457, -0.4983, -0.5833],\n",
      "        [ 0.4674,  0.3793,  0.6378, -0.4698,  0.1081],\n",
      "        [-0.5923, -0.4713,  0.1694, -0.3482, -0.4414],\n",
      "        [ 0.9081,  1.8759,  1.7476,  0.7839,  1.2028]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.3162, -0.0523, -0.8869, -0.8235,  0.9317],\n",
      "        [ 0.3743, -1.1570, -0.0683,  0.5364,  0.0773],\n",
      "        [ 0.0727,  0.7933,  1.2764, -0.5369,  1.5029],\n",
      "        [-2.3132,  1.2257,  1.4089,  1.4922, -0.1603]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1656],\n",
      "        [-0.5511],\n",
      "        [-0.6772],\n",
      "        [ 3.6379]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2314,  2.3813,  1.3440,  2.1405, -0.2243],\n",
      "        [ 1.2531, -0.2360,  0.6863, -0.8032,  0.6689],\n",
      "        [-1.2741,  0.3645,  0.7041, -0.3181,  1.9986],\n",
      "        [-2.0447, -1.1267, -0.6084,  0.3503,  0.2913]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4199,  0.3488, -0.1958, -0.2251, -0.2272],\n",
      "        [ 0.5587, -0.2466, -0.6468, -0.1004, -0.1915],\n",
      "        [-0.2269, -0.4782,  0.3978,  0.4556, -0.3996],\n",
      "        [-0.3023, -1.0310, -0.3127, -0.4402, -0.5258]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2314,  2.3813,  1.3440,  2.1405, -0.2243],\n",
      "        [ 1.2531, -0.2360,  0.6863, -0.8032,  0.6689],\n",
      "        [-1.2741,  0.3645,  0.7041, -0.3181,  1.9986],\n",
      "        [-2.0447, -1.1267, -0.6084,  0.3503,  0.2913]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0394],\n",
      "        [ 0.2669],\n",
      "        [-0.5487],\n",
      "        [ 1.6625]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2837, -0.0930, -0.4616, -0.1431, -1.5700],\n",
      "        [ 0.1905, -0.5140,  0.1063, -0.3212, -0.5803],\n",
      "        [-0.8297,  0.0772, -0.6290,  0.6349, -0.2266],\n",
      "        [ 0.9994,  0.7568,  1.2933, -1.2586, -0.8261]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0291,  0.2353, -0.4945, -0.1269, -0.0557],\n",
      "        [ 0.3686,  0.1831, -0.2360, -0.5282,  0.5926],\n",
      "        [ 0.1137, -0.3976, -0.1638, -0.4593, -0.4427],\n",
      "        [-1.1302, -0.3036, -1.1958, -1.6413, -1.5014]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2837, -0.0930, -0.4616, -0.1431, -1.5700],\n",
      "        [ 0.1905, -0.5140,  0.1063, -0.3212, -0.5803],\n",
      "        [-0.8297,  0.0772, -0.6290,  0.6349, -0.2266],\n",
      "        [ 0.9994,  0.7568,  1.2933, -1.2586, -0.8261]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3038],\n",
      "        [-0.2232],\n",
      "        [-0.2133],\n",
      "        [ 0.4002]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.3265,  1.1671, -0.1079,  0.4128, -0.5291],\n",
      "        [ 0.0045, -1.2113,  0.4718, -0.5001,  0.3878],\n",
      "        [ 2.5953, -0.2226, -1.0038, -1.5075,  0.7155],\n",
      "        [ 0.3108,  0.3134,  0.0962,  0.4136,  0.8133]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5067, -0.2738,  0.2568, -0.1560,  0.0644],\n",
      "        [ 0.2470, -0.2075,  0.1252, -0.1969,  0.0162],\n",
      "        [ 0.2460, -0.3297, -0.7724, -0.9111, -0.5961],\n",
      "        [-0.7896, -0.4470, -0.3596, -1.1700, -1.9603]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.3265,  1.1671, -0.1079,  0.4128, -0.5291],\n",
      "        [ 0.0045, -1.2113,  0.4718, -0.5001,  0.3878],\n",
      "        [ 2.5953, -0.2226, -1.0038, -1.5075,  0.7155],\n",
      "        [ 0.3108,  0.3134,  0.0962,  0.4136,  0.8133]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6246],\n",
      "        [ 0.4163],\n",
      "        [ 2.4341],\n",
      "        [-2.4984]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2836, -0.5582,  1.3700, -0.5777,  0.7495],\n",
      "        [-1.2392,  1.0210,  0.8678, -0.5493, -0.8553],\n",
      "        [-0.6513,  0.0918, -2.0913, -0.9618,  3.3455],\n",
      "        [-0.8247,  1.5591,  0.4713, -0.3876,  0.6001]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8305,  1.0612,  0.6773,  0.3710,  2.2123],\n",
      "        [ 0.5292, -0.0635,  0.2005, -0.0639, -0.2854],\n",
      "        [-0.2327, -0.9057, -0.9579, -1.2686, -2.1788],\n",
      "        [ 0.0996, -0.3368,  0.8712,  0.1805,  1.5696]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2836, -0.5582,  1.3700, -0.5777,  0.7495],\n",
      "        [-1.2392,  1.0210,  0.8678, -0.5493, -0.8553],\n",
      "        [-0.6513,  0.0918, -2.0913, -0.9618,  3.3455],\n",
      "        [-0.8247,  1.5591,  0.4713, -0.3876,  0.6001]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0148],\n",
      "        [-0.2675],\n",
      "        [-3.9974],\n",
      "        [ 0.6754]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3904,  0.4982,  2.0639,  0.9357, -0.3928],\n",
      "        [ 1.1996,  0.1750,  0.2335,  0.6115,  0.2527],\n",
      "        [ 0.0021,  0.7974,  0.1381,  1.6788,  1.3302],\n",
      "        [-0.4378, -0.7480, -0.1499,  0.0048,  0.5509]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4891, -0.0789,  0.1400, -0.1967, -0.6232],\n",
      "        [ 0.1890,  0.0876, -0.2604, -0.3421, -0.2696],\n",
      "        [ 0.6736,  0.2043,  0.5070,  0.3617,  1.0852],\n",
      "        [-0.9366,  1.1254,  0.2060,  0.1815,  0.4593]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3904,  0.4982,  2.0639,  0.9357, -0.3928],\n",
      "        [ 1.1996,  0.1750,  0.2335,  0.6115,  0.2527],\n",
      "        [ 0.0021,  0.7974,  0.1381,  1.6788,  1.3302],\n",
      "        [-0.4378, -0.7480, -0.1499,  0.0048,  0.5509]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5012],\n",
      "        [-0.0960],\n",
      "        [ 2.2852],\n",
      "        [-0.2087]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0227,  1.7178,  2.5862,  0.1985, -1.2191],\n",
      "        [ 0.8819,  0.6055, -2.8885, -0.2214, -0.0115],\n",
      "        [-0.6649, -0.3225, -0.6190, -0.3359, -0.3607],\n",
      "        [-0.4402, -0.7182,  0.3620,  1.6746,  0.5658]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1607,  0.2661, -0.7342, -0.4799, -0.2951],\n",
      "        [ 0.8079, -0.3596, -0.2519, -0.1040,  0.7558],\n",
      "        [-0.6162, -0.7955, -0.9067, -0.9791, -1.6189],\n",
      "        [-0.0584,  1.1175, -0.0127,  0.5887,  0.7961]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0227,  1.7178,  2.5862,  0.1985, -1.2191],\n",
      "        [ 0.8819,  0.6055, -2.8885, -0.2214, -0.0115],\n",
      "        [-0.6649, -0.3225, -0.6190, -0.3359, -0.3607],\n",
      "        [-0.4402, -0.7182,  0.3620,  1.6746,  0.5658]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3414],\n",
      "        [ 1.2368],\n",
      "        [ 2.1402],\n",
      "        [ 0.6547]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0109,  0.0563, -0.1926, -0.5596,  1.2895],\n",
      "        [ 0.1798, -0.1937,  0.6572, -0.6771,  0.4137],\n",
      "        [ 0.1844,  1.9286, -2.6979, -0.9807,  0.3806],\n",
      "        [ 0.7133,  0.0504, -0.9043,  0.1397,  0.5360]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2338,  0.6967,  0.4393,  0.9687,  0.7687],\n",
      "        [ 0.0675, -0.2504, -0.3679, -0.3178, -1.3237],\n",
      "        [-1.0419, -1.5452, -1.1606, -1.2596, -2.5718],\n",
      "        [ 0.1189,  0.9574,  0.9706,  0.7588,  0.9615]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0109,  0.0563, -0.1926, -0.5596,  1.2895],\n",
      "        [ 0.1798, -0.1937,  0.6572, -0.6771,  0.4137],\n",
      "        [ 0.1844,  1.9286, -2.6979, -0.9807,  0.3806],\n",
      "        [ 0.7133,  0.0504, -0.9043,  0.1397,  0.5360]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4012],\n",
      "        [-0.5134],\n",
      "        [ 0.2155],\n",
      "        [-0.1233]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7404,  1.3062,  1.1633, -0.7431, -0.6201],\n",
      "        [ 1.5355,  0.8996, -0.5525, -1.2850,  0.3717],\n",
      "        [ 0.7745,  0.9787, -0.8057,  0.9260, -1.5621],\n",
      "        [-0.0684, -0.3090, -1.0494,  0.2230,  0.2761]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4630, -0.2049,  0.5444,  0.4334,  0.3511],\n",
      "        [-0.0247,  0.0056, -0.0460, -0.1820,  0.1791],\n",
      "        [ 0.3735, -0.1568, -0.7012,  0.4307, -0.4498],\n",
      "        [-0.1818, -0.3240,  1.1333,  0.5947,  0.7797]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7404,  1.3062,  1.1633, -0.7431, -0.6201],\n",
      "        [ 1.5355,  0.8996, -0.5525, -1.2850,  0.3717],\n",
      "        [ 0.7745,  0.9787, -0.8057,  0.9260, -1.5621],\n",
      "        [-0.0684, -0.3090, -1.0494,  0.2230,  0.2761]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9799],\n",
      "        [ 0.2931],\n",
      "        [ 1.8020],\n",
      "        [-0.7289]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3029,  1.2513,  0.8170, -1.0448, -0.1161],\n",
      "        [-0.4737,  0.2936, -1.2300,  0.1407,  1.2791],\n",
      "        [-0.1275,  1.2566,  1.4459,  1.3416,  0.1845],\n",
      "        [ 0.6137,  0.0353, -0.1919,  1.2818, -0.2162]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6194,  0.8800,  0.8618,  0.8542,  0.4805],\n",
      "        [-0.0371,  0.2223,  0.0222, -0.2456,  0.4821],\n",
      "        [-0.6370, -1.1987, -1.0255, -0.8777, -1.2894],\n",
      "        [-0.2296,  0.9139,  0.7546,  0.8307,  2.4284]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3029,  1.2513,  0.8170, -1.0448, -0.1161],\n",
      "        [-0.4737,  0.2936, -1.2300,  0.1407,  1.2791],\n",
      "        [-0.1275,  1.2566,  1.4459,  1.3416,  0.1845],\n",
      "        [ 0.6137,  0.0353, -0.1919,  1.2818, -0.2162]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6641],\n",
      "        [ 0.6376],\n",
      "        [-4.3233],\n",
      "        [ 0.2863]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3322, -0.5534,  1.1625,  0.1625, -2.6705],\n",
      "        [ 0.9672, -1.9162, -0.4638,  0.2909, -0.8481],\n",
      "        [ 0.8334, -0.3483, -1.0303,  0.5863, -0.3788],\n",
      "        [ 0.9954,  0.4494,  0.5483,  0.4798, -1.5506]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7821, -0.2918, -0.0695, -0.1276, -0.6630],\n",
      "        [-0.1504, -0.1272,  0.0731, -0.4041,  0.7585],\n",
      "        [ 0.7843,  1.2504,  0.9121,  1.0571,  1.8393],\n",
      "        [-1.0353,  1.0109,  0.7760,  0.4810,  0.7151]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3322, -0.5534,  1.1625,  0.1625, -2.6705],\n",
      "        [ 0.9672, -1.9162, -0.4638,  0.2909, -0.8481],\n",
      "        [ 0.8334, -0.3483, -1.0303,  0.5863, -0.3788],\n",
      "        [ 0.9954,  0.4494,  0.5483,  0.4798, -1.5506]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5708],\n",
      "        [-0.6964],\n",
      "        [-0.7985],\n",
      "        [-1.0287]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6655, -0.2935,  0.2656, -0.1612,  1.3892],\n",
      "        [ 1.6909,  1.0955, -0.3431,  1.6857,  0.9197],\n",
      "        [-1.8871, -0.4827, -0.5424,  0.0642, -0.6774],\n",
      "        [-0.0092, -0.3472, -1.1619,  0.3858,  1.0750]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4164, -1.2137, -1.3160, -0.8518, -1.6171],\n",
      "        [-0.0790,  0.1885,  0.2310,  0.4381,  0.8954],\n",
      "        [ 1.3437,  1.7206,  0.3074,  1.9025,  2.4586],\n",
      "        [ 0.2019,  1.1471,  0.9428,  0.8093,  0.5682]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6655, -0.2935,  0.2656, -0.1612,  1.3892],\n",
      "        [ 1.6909,  1.0955, -0.3431,  1.6857,  0.9197],\n",
      "        [-1.8871, -0.4827, -0.5424,  0.0642, -0.6774],\n",
      "        [-0.0092, -0.3472, -1.1619,  0.3858,  1.0750]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.7960],\n",
      "        [ 1.5556],\n",
      "        [-5.0764],\n",
      "        [-0.5726]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0797,  1.1796,  1.1961,  0.4446, -0.4032],\n",
      "        [ 0.3604,  0.2203,  1.5038, -1.6895,  0.8434],\n",
      "        [-0.0928, -1.9011,  2.5846, -0.3845, -0.9767],\n",
      "        [ 0.9489,  0.5539,  1.2278,  1.7063, -0.0540]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7798,  0.9264,  0.9165,  0.4333,  1.1346],\n",
      "        [-0.2289, -1.1368, -0.0336, -0.6662, -1.1574],\n",
      "        [ 0.1379,  0.0166,  0.5833,  0.2282,  0.2836],\n",
      "        [ 0.3516,  0.8604,  0.9018,  1.0742,  1.4860]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0797,  1.1796,  1.1961,  0.4446, -0.4032],\n",
      "        [ 0.3604,  0.2203,  1.5038, -1.6895,  0.8434],\n",
      "        [-0.0928, -1.9011,  2.5846, -0.3845, -0.9767],\n",
      "        [ 0.9489,  0.5539,  1.2278,  1.7063, -0.0540]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0822],\n",
      "        [-0.2343],\n",
      "        [ 1.0985],\n",
      "        [ 3.6700]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8321, -0.1256, -0.0911, -0.3170,  0.7019],\n",
      "        [-0.9847,  0.8299,  1.2829,  1.8514,  1.0681],\n",
      "        [-0.5722, -2.0415,  0.6205,  0.5356, -1.4254],\n",
      "        [ 1.4061,  0.2500, -0.9984, -2.0345,  0.3734]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2750, -0.0027, -0.1627,  0.2170,  0.0160],\n",
      "        [ 0.5651,  0.0558, -0.5855,  0.5566, -0.3140],\n",
      "        [-0.6654, -0.0190, -0.6057, -0.3125, -0.9658],\n",
      "        [-0.8164, -0.4641, -0.5184, -0.6773, -0.8385]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8321, -0.1256, -0.0911, -0.3170,  0.7019],\n",
      "        [-0.9847,  0.8299,  1.2829,  1.8514,  1.0681],\n",
      "        [-0.5722, -2.0415,  0.6205,  0.5356, -1.4254],\n",
      "        [ 1.4061,  0.2500, -0.9984, -2.0345,  0.3734]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1864],\n",
      "        [-0.5663],\n",
      "        [ 1.2530],\n",
      "        [ 0.3184]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1948,  1.3553, -1.0362,  1.2936, -0.5706],\n",
      "        [-2.3153,  0.8173,  0.0040,  1.2702, -1.0522],\n",
      "        [-1.2220, -0.3734, -0.8136, -0.2287,  1.8832],\n",
      "        [ 1.3865, -0.0202, -1.2978, -1.4843, -1.0941]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1302, -0.1183,  0.1946,  0.1741, -0.0410],\n",
      "        [-0.0012, -0.7741,  0.0390, -0.6394, -0.1281],\n",
      "        [-0.5512, -0.4285, -0.6894, -0.3437, -1.6290],\n",
      "        [-0.9576, -0.1684, -0.2938,  0.0947, -1.9152]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1948,  1.3553, -1.0362,  1.2936, -0.5706],\n",
      "        [-2.3153,  0.8173,  0.0040,  1.2702, -1.0522],\n",
      "        [-1.2220, -0.3734, -0.8136, -0.2287,  1.8832],\n",
      "        [ 1.3865, -0.0202, -1.2978, -1.4843, -1.0941]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0879],\n",
      "        [-1.3073],\n",
      "        [-1.5948],\n",
      "        [ 1.0118]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3789,  0.4701,  1.4734,  0.6402,  1.3459],\n",
      "        [-0.4635,  0.0107,  2.0234,  0.3246, -2.7196],\n",
      "        [-0.3565,  1.5878, -1.6383,  1.3244,  0.8199],\n",
      "        [-0.5645,  0.1610,  0.2111,  1.3170,  0.6994]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1355,  0.5484, -0.3842, -0.4325,  0.6221],\n",
      "        [ 0.4626,  0.8661,  0.4924,  0.7954,  1.2092],\n",
      "        [ 0.0724,  0.2426,  0.2559, -0.2283,  0.2630],\n",
      "        [-0.6183, -0.4891, -0.4901, -0.6787, -2.6510]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3789,  0.4701,  1.4734,  0.6402,  1.3459],\n",
      "        [-0.4635,  0.0107,  2.0234,  0.3246, -2.7196],\n",
      "        [-0.3565,  1.5878, -1.6383,  1.3244,  0.8199],\n",
      "        [-0.5645,  0.1610,  0.2111,  1.3170,  0.6994]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0653],\n",
      "        [-2.2391],\n",
      "        [-0.1465],\n",
      "        [-2.5812]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1156,  1.4815,  0.1441,  0.2294, -0.5429],\n",
      "        [-0.3788, -1.0596, -1.1844,  0.2781,  0.7863],\n",
      "        [-2.4307,  1.6286,  1.1933, -1.4809, -0.0109],\n",
      "        [-0.3666, -1.7368, -2.0093, -0.7593,  0.2569]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0402, -0.0479, -0.1598, -0.1696, -0.4490],\n",
      "        [ 1.2129,  1.1894,  1.3288,  1.6548,  1.6013],\n",
      "        [ 0.3684, -0.4725,  0.3796, -0.8054, -0.7363],\n",
      "        [ 0.0210,  0.3344,  0.7314, -0.9211,  0.4426]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1156,  1.4815,  0.1441,  0.2294, -0.5429],\n",
      "        [-0.3788, -1.0596, -1.1844,  0.2781,  0.7863],\n",
      "        [-2.4307,  1.6286,  1.1933, -1.4809, -0.0109],\n",
      "        [-0.3666, -1.7368, -2.0093, -0.7593,  0.2569]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1557],\n",
      "        [-1.5743],\n",
      "        [-0.0114],\n",
      "        [-1.2449]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1990,  2.3428, -1.5737,  0.4835, -0.2518],\n",
      "        [-1.2076,  0.2557, -0.4475, -1.2242, -0.8555],\n",
      "        [ 0.2576,  1.0547, -0.6266, -0.4788,  1.8299],\n",
      "        [-0.5687,  1.9668,  0.9016,  0.3546,  0.4961]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1618, -0.4346, -0.2529, -0.6067, -0.2183],\n",
      "        [ 1.5435,  1.3535,  2.0978,  1.0958,  3.1774],\n",
      "        [ 0.3151,  0.1057,  0.1373,  0.3171, -0.4780],\n",
      "        [ 0.0836,  0.2652,  0.5406,  0.0567,  1.2379]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1990,  2.3428, -1.5737,  0.4835, -0.2518],\n",
      "        [-1.2076,  0.2557, -0.4475, -1.2242, -0.8555],\n",
      "        [ 0.2576,  1.0547, -0.6266, -0.4788,  1.8299],\n",
      "        [-0.5687,  1.9668,  0.9016,  0.3546,  0.4961]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8908],\n",
      "        [-6.5164],\n",
      "        [-0.9200],\n",
      "        [ 1.5957]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-1.2981, -1.1308,  1.7097,  0.3642,  0.8468],\n",
      "        [-0.1110,  0.6620, -0.0985, -0.5757,  0.0472],\n",
      "        [-0.7579, -0.9743, -3.6550, -0.8086,  0.7578],\n",
      "        [ 0.9542, -1.5818,  3.0749,  0.8622,  0.1927]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2670,  0.2353, -0.9133,  0.0726,  0.6435],\n",
      "        [-0.3514, -0.6660, -0.4262, -0.0721,  0.3649],\n",
      "        [ 0.6471, -0.5772,  0.4816, -0.0523,  0.5317],\n",
      "        [-0.4809, -0.3362, -0.5058,  0.1068,  0.1969]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2981, -1.1308,  1.7097,  0.3642,  0.8468],\n",
      "        [-0.1110,  0.6620, -0.0985, -0.5757,  0.0472],\n",
      "        [-0.7579, -0.9743, -3.6550, -0.8086,  0.7578],\n",
      "        [ 0.9542, -1.5818,  3.0749,  0.8622,  0.1927]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6027],\n",
      "        [-0.3011],\n",
      "        [-1.2430],\n",
      "        [-1.3523]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2553,  1.1999,  1.4254,  0.9005, -0.3798],\n",
      "        [-0.7702,  1.1227,  0.3652, -1.3526,  0.4254],\n",
      "        [ 0.2957, -0.0189, -1.0939, -0.6182,  1.0566],\n",
      "        [-1.0676,  1.0978, -0.5057, -0.4024,  2.0508]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8851,  0.5409,  1.1337,  0.8721,  1.1079],\n",
      "        [ 0.1852,  0.2753,  0.2650,  0.1067,  0.2532],\n",
      "        [ 0.4636,  0.7848, -0.2986, -0.0702,  0.2851],\n",
      "        [-0.1061,  0.3599,  0.7165, -0.0666,  0.2846]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2553,  1.1999,  1.4254,  0.9005, -0.3798],\n",
      "        [-0.7702,  1.1227,  0.3652, -1.3526,  0.4254],\n",
      "        [ 0.2957, -0.0189, -1.0939, -0.6182,  1.0566],\n",
      "        [-1.0676,  1.0978, -0.5057, -0.4024,  2.0508]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.8556],\n",
      "        [ 0.2267],\n",
      "        [ 0.7935],\n",
      "        [ 0.7564]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2273,  0.2930, -1.0187,  0.4975,  0.3679],\n",
      "        [ 0.5899, -0.4603,  0.3644, -1.8458, -1.6216],\n",
      "        [ 0.5490,  0.8861,  0.2172, -0.0267, -0.6497],\n",
      "        [-1.4591,  0.6488, -0.3809, -1.1138, -0.3216]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0656, -0.8271, -0.9884, -0.4338, -1.5853],\n",
      "        [-0.1944,  0.0956,  0.1779,  0.0113, -0.0230],\n",
      "        [ 0.0265, -0.9852, -0.3410,  0.2086, -0.1081],\n",
      "        [-0.2051,  0.4247,  0.3327,  0.1959, -0.6731]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2273,  0.2930, -1.0187,  0.4975,  0.3679],\n",
      "        [ 0.5899, -0.4603,  0.3644, -1.8458, -1.6216],\n",
      "        [ 0.5490,  0.8861,  0.2172, -0.0267, -0.6497],\n",
      "        [-1.4591,  0.6488, -0.3809, -1.1138, -0.3216]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0196],\n",
      "        [-0.0773],\n",
      "        [-0.8679],\n",
      "        [ 0.4463]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4657,  0.6461,  0.4795, -1.4698, -0.1262],\n",
      "        [ 0.8236, -0.3705,  0.1084, -0.0936, -0.5207],\n",
      "        [ 0.1394, -0.2535, -0.6635, -0.3188,  0.5481],\n",
      "        [-1.8303,  0.8848, -0.2590,  1.1782, -1.0909]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1305,  0.1713, -0.4121,  0.3113, -0.8081],\n",
      "        [ 0.3003,  0.4467, -0.0301,  0.0224, -0.2521],\n",
      "        [ 0.0612, -0.0575, -0.1806, -0.1597,  0.0390],\n",
      "        [ 0.1548, -0.1253,  0.4663, -0.0413, -0.1764]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4657,  0.6461,  0.4795, -1.4698, -0.1262],\n",
      "        [ 0.8236, -0.3705,  0.1084, -0.0936, -0.5207],\n",
      "        [ 0.1394, -0.2535, -0.6635, -0.3188,  0.5481],\n",
      "        [-1.8303,  0.8848, -0.2590,  1.1782, -1.0909]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3817],\n",
      "        [ 0.2078],\n",
      "        [ 0.2153],\n",
      "        [-0.3713]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8003,  2.1528, -0.4404,  1.4719, -0.5157],\n",
      "        [ 0.1620,  0.5340, -0.3369,  0.4156,  1.5173],\n",
      "        [-0.2257, -0.3068,  0.6982,  2.2078,  0.5824],\n",
      "        [ 0.5444, -0.5321, -0.4395, -1.2776, -0.5533]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2276, -0.2374,  0.5260,  0.5833, -0.3047],\n",
      "        [-0.1502,  0.0085,  0.6505,  0.3350, -0.0549],\n",
      "        [ 0.0160, -0.4678, -1.1349, -0.5465, -1.0087],\n",
      "        [-0.6101, -0.8374, -0.5625, -0.5316, -0.8026]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8003,  2.1528, -0.4404,  1.4719, -0.5157],\n",
      "        [ 0.1620,  0.5340, -0.3369,  0.4156,  1.5173],\n",
      "        [-0.2257, -0.3068,  0.6982,  2.2078,  0.5824],\n",
      "        [ 0.5444, -0.5321, -0.4395, -1.2776, -0.5533]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0909],\n",
      "        [-0.1829],\n",
      "        [-2.4465],\n",
      "        [ 1.4839]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0992, -0.1590,  0.9653,  1.6464,  0.4370],\n",
      "        [-1.9989, -1.5843,  0.1196,  0.9840,  0.7015],\n",
      "        [-1.2146, -0.6923, -1.0499,  1.2323,  0.6869],\n",
      "        [ 0.9051, -1.2375,  1.0159,  1.1412, -0.3675]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4476, -0.4483,  0.0065, -0.4540, -1.1931],\n",
      "        [-0.2675,  0.0019,  0.7202, -0.1499, -0.6163],\n",
      "        [ 0.8742,  1.1882,  1.0610,  0.3149,  1.5680],\n",
      "        [-0.2595, -0.4142,  0.1312, -0.5646, -1.5548]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0992, -0.1590,  0.9653,  1.6464,  0.4370],\n",
      "        [-1.9989, -1.5843,  0.1196,  0.9840,  0.7015],\n",
      "        [-1.2146, -0.6923, -1.0499,  1.2323,  0.6869],\n",
      "        [ 0.9051, -1.2375,  1.0159,  1.1412, -0.3675]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2357],\n",
      "        [ 0.0379],\n",
      "        [-1.5332],\n",
      "        [ 0.3379]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1879,  1.1049,  0.0866, -0.3970, -1.6034],\n",
      "        [ 0.6295, -1.0463, -1.0763,  1.2450,  0.9620],\n",
      "        [-0.1908, -1.9506, -2.2573,  1.1110,  1.1141],\n",
      "        [-0.4727, -1.8504,  1.2504,  2.3011,  1.1484]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2192,  0.6615,  0.5889,  1.0353,  0.0805],\n",
      "        [ 0.1024, -0.0523, -0.4201, -0.3821,  0.0520],\n",
      "        [ 1.2245,  1.5311,  1.4716,  1.3400,  1.6208],\n",
      "        [-0.5001, -0.5108,  0.4394, -0.0012, -0.9002]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1879,  1.1049,  0.0866, -0.3970, -1.6034],\n",
      "        [ 0.6295, -1.0463, -1.0763,  1.2450,  0.9620],\n",
      "        [-0.1908, -1.9506, -2.2573,  1.1110,  1.1141],\n",
      "        [-0.4727, -1.8504,  1.2504,  2.3011,  1.1484]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0187],\n",
      "        [ 0.1457],\n",
      "        [-3.2476],\n",
      "        [ 0.6945]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9028,  0.9985, -0.5185,  0.7266,  0.3430],\n",
      "        [ 2.4853,  2.0379,  0.2719,  1.1752, -0.7666],\n",
      "        [ 1.4584,  1.2540, -0.8316, -0.4461, -0.0212],\n",
      "        [-1.8538,  1.3537,  1.1163,  0.2433,  1.0029]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4927, -0.0129, -0.1852, -0.3965, -0.1672],\n",
      "        [ 0.3462, -0.5672, -0.3214,  0.3545, -0.3753],\n",
      "        [ 0.0687,  0.4082,  0.2020, -0.0680,  0.1048],\n",
      "        [-0.5420, -0.5741, -0.0661, -0.3930, -0.7138]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9028,  0.9985, -0.5185,  0.7266,  0.3430],\n",
      "        [ 2.4853,  2.0379,  0.2719,  1.1752, -0.7666],\n",
      "        [ 1.4584,  1.2540, -0.8316, -0.4461, -0.0212],\n",
      "        [-1.8538,  1.3537,  1.1163,  0.2433,  1.0029]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7070],\n",
      "        [ 0.3213],\n",
      "        [ 0.4722],\n",
      "        [-0.6575]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5834, -1.1894,  1.3203, -1.4345, -0.4919],\n",
      "        [-0.4098, -0.2123, -0.6824, -1.2255,  0.9229],\n",
      "        [-1.0868, -0.3425,  1.5329,  0.2406,  1.4894],\n",
      "        [-0.4070, -0.3466,  0.6973,  0.5807,  0.6448]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1187,  0.6958, -0.0804,  0.0119, -0.1150],\n",
      "        [ 0.3686, -0.2073, -0.6795,  0.3089,  0.4873],\n",
      "        [ 0.3620,  0.7398,  0.1401, -0.3253,  0.5987],\n",
      "        [-0.4509, -0.0990, -0.6033, -0.0695, -0.2424]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5834, -1.1894,  1.3203, -1.4345, -0.4919],\n",
      "        [-0.4098, -0.2123, -0.6824, -1.2255,  0.9229],\n",
      "        [-1.0868, -0.3425,  1.5329,  0.2406,  1.4894],\n",
      "        [-0.4070, -0.3466,  0.6973,  0.5807,  0.6448]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2416],\n",
      "        [ 0.4278],\n",
      "        [ 0.3813],\n",
      "        [-0.3995]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3530, -0.5040,  0.7880,  1.6853, -0.1271],\n",
      "        [-1.1036,  0.0678,  0.5216,  0.7954,  0.7423],\n",
      "        [-0.6226, -1.1405,  1.7230, -1.3723, -1.6529],\n",
      "        [-0.1066,  0.9278, -0.9508, -1.0582,  1.3901]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3920, -0.2047, -0.5863, -0.4884, -1.0373],\n",
      "        [-0.5256, -0.0566,  0.5694,  0.3049,  0.2936],\n",
      "        [ 0.0666, -0.4363,  0.0206, -0.1574,  0.6069],\n",
      "        [-0.2219,  0.4766, -0.2784,  0.2170, -1.2660]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3530, -0.5040,  0.7880,  1.6853, -0.1271],\n",
      "        [-1.1036,  0.0678,  0.5216,  0.7954,  0.7423],\n",
      "        [-0.6226, -1.1405,  1.7230, -1.3723, -1.6529],\n",
      "        [-0.1066,  0.9278, -0.9508, -1.0582,  1.3901]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1885],\n",
      "        [ 1.3336],\n",
      "        [-0.2954],\n",
      "        [-1.2590]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0688, -0.1257,  1.1496, -0.4983, -1.0949],\n",
      "        [-0.5764,  0.4312,  0.1505,  1.3451,  0.4050],\n",
      "        [ 1.2859, -0.3869, -0.7461,  0.4539,  0.1908],\n",
      "        [-0.8075,  1.4312,  0.0588, -0.9131,  0.9982]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7207, -0.0108,  0.6044, -0.0308,  1.1608],\n",
      "        [-0.7264, -0.2187, -0.2332, -0.4809, -1.1687],\n",
      "        [ 0.2637, -0.3203, -0.1361, -0.1084,  0.0773],\n",
      "        [ 0.2126, -0.1431, -0.3003, -0.3220, -0.2189]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0688, -0.1257,  1.1496, -0.4983, -1.0949],\n",
      "        [-0.5764,  0.4312,  0.1505,  1.3451,  0.4050],\n",
      "        [ 1.2859, -0.3869, -0.7461,  0.4539,  0.1908],\n",
      "        [-0.8075,  1.4312,  0.0588, -0.9131,  0.9982]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3297],\n",
      "        [-0.8308],\n",
      "        [ 0.5301],\n",
      "        [-0.3187]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7885,  0.3503,  0.5517,  2.0357,  1.8772],\n",
      "        [-0.1652,  1.2653,  0.7639, -0.3147,  0.1736],\n",
      "        [-0.0257, -0.6365,  0.6618, -1.0326, -1.4643],\n",
      "        [-0.5146,  1.1741,  1.0350,  1.0946,  2.3107]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9346,  0.6257,  0.3617,  0.6917,  1.6327],\n",
      "        [ 0.0138,  0.2095,  0.5043, -0.0380, -0.2424],\n",
      "        [-0.0295,  0.4641,  0.4402, -0.2432,  0.0194],\n",
      "        [-0.3359, -0.8744, -0.3900, -0.5788, -0.3166]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7885,  0.3503,  0.5517,  2.0357,  1.8772],\n",
      "        [-0.1652,  1.2653,  0.7639, -0.3147,  0.1736],\n",
      "        [-0.0257, -0.6365,  0.6618, -1.0326, -1.4643],\n",
      "        [-0.5146,  1.1741,  1.0350,  1.0946,  2.3107]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.6288],\n",
      "        [ 0.6179],\n",
      "        [ 0.2193],\n",
      "        [-2.6225]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7154, -0.8174,  0.1473, -0.4111, -1.6919],\n",
      "        [ 0.0384, -0.1574, -0.0723, -0.2462,  1.4094],\n",
      "        [-0.6737, -0.1662, -0.7713, -0.5225, -0.7478],\n",
      "        [-0.1747,  0.5837,  1.3427,  0.2485, -0.5228]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7059, -1.7054, -1.6707, -1.9640, -3.1091],\n",
      "        [-0.4175, -0.2783,  1.1229,  0.5091,  0.3999],\n",
      "        [-0.4168,  0.0490,  0.0871,  0.2508,  0.2945],\n",
      "        [ 0.7982,  1.0005,  0.6471,  0.8142,  1.4064]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7154, -0.8174,  0.1473, -0.4111, -1.6919],\n",
      "        [ 0.0384, -0.1574, -0.0723, -0.2462,  1.4094],\n",
      "        [-0.6737, -0.1662, -0.7713, -0.5225, -0.7478],\n",
      "        [-0.1747,  0.5837,  1.3427,  0.2485, -0.5228]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 7.7203],\n",
      "        [ 0.3848],\n",
      "        [-0.1459],\n",
      "        [ 0.7806]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5984, -0.4856,  0.2399, -0.0577, -0.6328],\n",
      "        [-1.8474,  0.3276,  1.0544,  0.3009,  0.7168],\n",
      "        [ 0.3847,  1.1958, -0.8869,  0.1982, -2.3749],\n",
      "        [-0.3033,  1.0765,  0.2359,  1.8962,  0.4716]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3443, -0.0050, -0.4404,  0.2690,  0.1554],\n",
      "        [ 0.3396,  0.2986, -0.5639,  0.2256, -0.7618],\n",
      "        [ 0.0510, -0.0407, -0.3002, -0.2440,  0.0593],\n",
      "        [ 0.6881, -0.1758, -0.0494,  0.0312,  0.0767]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5984, -0.4856,  0.2399, -0.0577, -0.6328],\n",
      "        [-1.8474,  0.3276,  1.0544,  0.3009,  0.7168],\n",
      "        [ 0.3847,  1.1958, -0.8869,  0.1982, -2.3749],\n",
      "        [-0.3033,  1.0765,  0.2359,  1.8962,  0.4716]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0111],\n",
      "        [-1.6023],\n",
      "        [ 0.0480],\n",
      "        [-0.3142]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3943, -0.6535, -2.4551,  0.6851, -2.2344],\n",
      "        [-0.3955, -0.8896,  0.3963,  0.1141,  1.2014],\n",
      "        [ 0.1927,  0.1951,  0.1694, -2.3447, -1.2987],\n",
      "        [ 0.6903, -0.2890, -1.2488, -0.7410,  0.7672]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2045, -0.3090, -0.0213,  0.4537,  0.3788],\n",
      "        [ 0.6938,  1.0886,  0.7773,  1.0976,  1.4937],\n",
      "        [ 0.2200,  0.0469,  0.5774,  0.4957,  0.3022],\n",
      "        [-0.1751, -0.0781, -0.3234,  0.0329, -0.6040]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3943, -0.6535, -2.4551,  0.6851, -2.2344],\n",
      "        [-0.3955, -0.8896,  0.3963,  0.1141,  1.2014],\n",
      "        [ 0.1927,  0.1951,  0.1694, -2.3447, -1.2987],\n",
      "        [ 0.6903, -0.2890, -1.2488, -0.7410,  0.7672]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3619],\n",
      "        [ 0.9851],\n",
      "        [-1.4054],\n",
      "        [-0.1822]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6222, -0.7144, -1.5906,  1.0208, -0.5933],\n",
      "        [ 0.2794, -0.1806, -0.4773,  0.2633, -0.6580],\n",
      "        [-1.7507, -0.2668,  0.3889, -0.7723,  1.3818],\n",
      "        [ 1.1585,  0.7056, -0.5853,  0.0122,  1.2531]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1854, -0.2539,  0.0907, -0.0891, -0.0156],\n",
      "        [ 0.1807,  0.6068, -0.0403,  0.2169,  0.4433],\n",
      "        [ 0.8387,  0.4988,  1.0306,  0.5313,  0.8993],\n",
      "        [ 0.0005, -0.4593, -0.3811, -0.5652, -1.7315]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6222, -0.7144, -1.5906,  1.0208, -0.5933],\n",
      "        [ 0.2794, -0.1806, -0.4773,  0.2633, -0.6580],\n",
      "        [-1.7507, -0.2668,  0.3889, -0.7723,  1.3818],\n",
      "        [ 1.1585,  0.7056, -0.5853,  0.0122,  1.2531]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2561],\n",
      "        [-0.2744],\n",
      "        [-0.3683],\n",
      "        [-2.2771]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2520,  0.7110,  0.2886, -0.3168,  1.7028],\n",
      "        [ 0.4103,  0.8948, -1.0201,  1.7000,  0.6298],\n",
      "        [ 1.0752, -0.4108, -1.4751,  0.9842, -0.2912],\n",
      "        [ 0.1597,  1.3108, -0.5380, -1.3996,  0.7220]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2723, -0.1793, -0.1160,  0.2754,  0.4786],\n",
      "        [-0.1569,  0.2272, -0.5640, -0.2656, -0.0186],\n",
      "        [ 0.5816, -0.1609,  0.4067,  0.5380,  1.1199],\n",
      "        [-0.0872,  0.7525,  0.6102,  0.4962,  1.5145]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2520,  0.7110,  0.2886, -0.3168,  1.7028],\n",
      "        [ 0.4103,  0.8948, -1.0201,  1.7000,  0.6298],\n",
      "        [ 1.0752, -0.4108, -1.4751,  0.9842, -0.2912],\n",
      "        [ 0.1597,  1.3108, -0.5380, -1.3996,  0.7220]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4981],\n",
      "        [ 0.2510],\n",
      "        [ 0.2949],\n",
      "        [ 1.0431]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5798,  0.2656, -1.6612, -0.8779,  0.8190],\n",
      "        [ 0.8263, -0.0557,  2.1132,  1.0538,  0.9725],\n",
      "        [-1.3663,  0.8155, -1.1464, -1.2032, -0.9564],\n",
      "        [-0.9817, -0.5609,  1.4899,  0.9807, -0.8141]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2124, -0.0988, -0.5836,  0.3473, -0.0450],\n",
      "        [-0.3435, -0.4250,  0.6431,  0.0895,  0.1814],\n",
      "        [ 0.9363,  0.3122,  0.4739, -0.0610,  0.2612],\n",
      "        [ 0.3980, -0.5398, -0.4728, -0.2841, -0.2204]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5798,  0.2656, -1.6612, -0.8779,  0.8190],\n",
      "        [ 0.8263, -0.0557,  2.1132,  1.0538,  0.9725],\n",
      "        [-1.3663,  0.8155, -1.1464, -1.2032, -0.9564],\n",
      "        [-0.9817, -0.5609,  1.4899,  0.9807, -0.8141]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7246],\n",
      "        [ 1.3695],\n",
      "        [-1.7443],\n",
      "        [-0.8916]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3453,  1.8344,  1.0815, -0.1361,  1.7932],\n",
      "        [-0.0549,  1.3088, -0.4794, -1.3118, -0.0063],\n",
      "        [-0.1152,  0.0986,  0.6925, -0.4456,  0.6857],\n",
      "        [ 0.5818,  0.4981,  1.3024,  0.1230, -0.4773]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0047,  0.1464,  0.1883,  0.3872, -0.5016],\n",
      "        [-0.1635, -0.7808, -0.3470, -0.0881, -0.2524],\n",
      "        [ 0.9451,  0.5234,  1.0451,  1.8530,  1.4672],\n",
      "        [ 0.3190, -0.1558,  0.8911, -0.1946,  0.8572]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3453,  1.8344,  1.0815, -0.1361,  1.7932],\n",
      "        [-0.0549,  1.3088, -0.4794, -1.3118, -0.0063],\n",
      "        [-0.1152,  0.0986,  0.6925, -0.4456,  0.6857],\n",
      "        [ 0.5818,  0.4981,  1.3024,  0.1230, -0.4773]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4815],\n",
      "        [-0.7294],\n",
      "        [ 0.8468],\n",
      "        [ 0.8354]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4712,  2.2466, -1.5535,  0.4604,  0.7480],\n",
      "        [-0.7175, -0.3368,  1.3439, -0.0768,  1.0341],\n",
      "        [-0.1868,  1.1137,  1.5258,  0.1016,  1.3514],\n",
      "        [-1.3216,  0.3883, -0.7636, -0.2183,  1.3176]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0965,  0.1868,  0.5009,  0.2015,  0.1709],\n",
      "        [ 0.4408,  0.3475,  0.3341,  0.0062, -0.3421],\n",
      "        [ 0.6552,  0.7152, -0.0234,  0.1995,  0.4327],\n",
      "        [ 0.1417, -1.0001, -0.8106, -0.2704,  0.0854]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4712,  2.2466, -1.5535,  0.4604,  0.7480],\n",
      "        [-0.7175, -0.3368,  1.3439, -0.0768,  1.0341],\n",
      "        [-0.1868,  1.1137,  1.5258,  0.1016,  1.3514],\n",
      "        [-1.3216,  0.3883, -0.7636, -0.2183,  1.3176]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2798],\n",
      "        [-0.3385],\n",
      "        [ 1.2433],\n",
      "        [ 0.2150]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6754, -0.6195,  0.0476, -1.1273,  0.6499],\n",
      "        [-0.7929, -0.3100,  1.0591, -0.1223, -1.0083],\n",
      "        [ 0.5919,  1.0819, -0.7607,  0.5233, -0.1223],\n",
      "        [ 0.6328,  0.1552, -0.4822, -1.0409, -0.4602]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6509, -0.1109,  0.8080,  0.4315,  0.1476],\n",
      "        [ 0.1192, -0.4238,  0.0367, -0.0952, -0.5130],\n",
      "        [-0.0581, -0.9077, -0.1144, -0.6771, -0.3377],\n",
      "        [ 0.0583, -0.2095, -0.8543, -0.4782, -0.5089]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6754, -0.6195,  0.0476, -1.1273,  0.6499],\n",
      "        [-0.7929, -0.3100,  1.0591, -0.1223, -1.0083],\n",
      "        [ 0.5919,  1.0819, -0.7607,  0.5233, -0.1223],\n",
      "        [ 0.6328,  0.1552, -0.4822, -1.0409, -0.4602]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7230],\n",
      "        [ 0.6046],\n",
      "        [-1.2425],\n",
      "        [ 1.1484]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1395, -0.8633,  0.8585,  2.6282,  2.6754],\n",
      "        [ 0.8609,  1.3465, -0.5706,  0.1065, -0.5704],\n",
      "        [-1.2551,  0.8740,  1.4344, -0.0697, -1.6838],\n",
      "        [-0.6292,  0.4007, -0.5361,  0.3017, -0.3223]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4852, -0.6212, -0.0227, -0.3080,  0.7053],\n",
      "        [-0.3954,  0.1531, -0.1228, -0.2912, -0.4581],\n",
      "        [ 0.0960,  0.7878,  0.4241, -0.3004,  0.8536],\n",
      "        [ 0.3054, -0.0063,  0.0040, -0.2862, -1.4732]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1395, -0.8633,  0.8585,  2.6282,  2.6754],\n",
      "        [ 0.8609,  1.3465, -0.5706,  0.1065, -0.5704],\n",
      "        [-1.2551,  0.8740,  1.4344, -0.0697, -1.6838],\n",
      "        [-0.6292,  0.4007, -0.5361,  0.3017, -0.3223]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1471],\n",
      "        [ 0.1661],\n",
      "        [-0.2399],\n",
      "        [ 0.1917]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6528, -0.3284,  0.6279,  0.1609,  0.1600],\n",
      "        [ 1.1449,  1.8253, -1.0037, -0.5346,  0.5684],\n",
      "        [-1.5363, -0.1495, -0.2266,  0.8690, -0.9169],\n",
      "        [-0.9489, -0.3310, -0.0489,  1.2376,  2.2285]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7389, -1.1108, -0.9995, -0.4029, -1.4673],\n",
      "        [-0.2064, -0.7761,  0.1648,  0.0636, -0.1774],\n",
      "        [ 0.4436,  0.5828,  0.2488,  0.5730,  0.4612],\n",
      "        [ 0.1840, -0.2970, -0.2744,  0.5324, -0.5495]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6528, -0.3284,  0.6279,  0.1609,  0.1600],\n",
      "        [ 1.1449,  1.8253, -1.0037, -0.5346,  0.5684],\n",
      "        [-1.5363, -0.1495, -0.2266,  0.8690, -0.9169],\n",
      "        [-0.9489, -0.3310, -0.0489,  1.2376,  2.2285]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0448],\n",
      "        [-1.9533],\n",
      "        [-0.7500],\n",
      "        [-0.6285]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6788, -0.0281, -0.9631,  0.0899, -0.6812],\n",
      "        [-2.1662,  1.4727, -0.7555, -0.0158, -0.6365],\n",
      "        [-0.1267,  0.4881, -1.0386,  0.7321, -0.4064],\n",
      "        [-0.3012, -0.1037, -0.1757, -0.1630, -1.6429]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1109, -0.4785,  0.0945, -0.4361, -0.6657],\n",
      "        [ 0.6790,  0.8160,  0.7880,  0.6286,  0.9958],\n",
      "        [ 0.5713,  0.8409,  0.4838, -0.1460,  0.7768],\n",
      "        [-0.2369, -0.2410, -0.1945,  0.3651, -0.5867]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6788, -0.0281, -0.9631,  0.0899, -0.6812],\n",
      "        [-2.1662,  1.4727, -0.7555, -0.0158, -0.6365],\n",
      "        [-0.1267,  0.4881, -1.0386,  0.7321, -0.4064],\n",
      "        [-0.3012, -0.1037, -0.1757, -0.1630, -1.6429]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1506],\n",
      "        [-1.5082],\n",
      "        [-0.5870],\n",
      "        [ 1.0348]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0612, -0.8916,  0.3807, -0.7502, -0.3637],\n",
      "        [-0.8656,  0.5086,  1.3259, -0.0273, -0.3429],\n",
      "        [-0.5955, -0.4938,  0.5275,  0.0222,  0.0339],\n",
      "        [ 1.1077, -1.7022, -0.5454,  0.0195,  1.5798]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1348, -0.6043,  0.2498,  0.0252, -0.0114],\n",
      "        [ 0.5531,  0.9698,  1.8725,  1.8363,  2.1411],\n",
      "        [ 0.5632,  0.4591,  1.0186,  0.3252,  0.6942],\n",
      "        [-0.6444, -0.8316, -0.4675,  0.0231, -1.2441]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0612, -0.8916,  0.3807, -0.7502, -0.3637],\n",
      "        [-0.8656,  0.5086,  1.3259, -0.0273, -0.3429],\n",
      "        [-0.5955, -0.4938,  0.5275,  0.0222,  0.0339],\n",
      "        [ 1.1077, -1.7022, -0.5454,  0.0195,  1.5798]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7623],\n",
      "        [ 1.7130],\n",
      "        [ 0.0060],\n",
      "        [-1.0081]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3541,  1.3216, -1.5966,  0.1433,  0.7551],\n",
      "        [-0.4982,  0.4709,  0.8761, -0.7308,  0.8800],\n",
      "        [ 0.1365,  1.3848,  0.3269,  0.5670, -0.8317],\n",
      "        [ 0.8060,  1.7752,  0.1414,  0.1876, -0.7124]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0070, -0.2543, -0.3428, -0.4220, -0.7159],\n",
      "        [ 0.3890,  1.0023,  0.8668,  1.3563,  0.7895],\n",
      "        [ 0.6990,  0.6451,  0.4338,  0.3287,  0.4641],\n",
      "        [ 0.3498, -0.2522, -0.1070,  0.0320, -0.3303]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3541,  1.3216, -1.5966,  0.1433,  0.7551],\n",
      "        [-0.4982,  0.4709,  0.8761, -0.7308,  0.8800],\n",
      "        [ 0.1365,  1.3848,  0.3269,  0.5670, -0.8317],\n",
      "        [ 0.8060,  1.7752,  0.1414,  0.1876, -0.7124]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3924],\n",
      "        [ 0.7412],\n",
      "        [ 0.9310],\n",
      "        [ 0.0603]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9166,  1.1967,  0.8946,  0.2304,  0.0246],\n",
      "        [-0.3886,  3.2568,  0.8405, -0.2274,  0.2009],\n",
      "        [ 0.3950,  0.4824,  1.7407,  0.4737,  0.5165],\n",
      "        [ 0.4202,  1.3362,  0.6752,  1.5593, -0.1484]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0286, -0.1797, -0.1997,  0.4309, -0.4926],\n",
      "        [ 0.3162,  0.4084,  0.8558,  1.0241,  1.1775],\n",
      "        [ 0.1630, -0.5336,  0.2930,  0.2655, -0.1414],\n",
      "        [-0.1766,  0.2607,  0.1206,  0.4453, -0.0787]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9166,  1.1967,  0.8946,  0.2304,  0.0246],\n",
      "        [-0.3886,  3.2568,  0.8405, -0.2274,  0.2009],\n",
      "        [ 0.3950,  0.4824,  1.7407,  0.4737,  0.5165],\n",
      "        [ 0.4202,  1.3362,  0.6752,  1.5593, -0.1484]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3327],\n",
      "        [ 1.9301],\n",
      "        [ 0.3698],\n",
      "        [ 1.0617]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2473,  1.1692,  0.6857,  0.7811, -0.8980],\n",
      "        [ 0.0934,  0.0050,  0.9001,  0.8382, -1.2990],\n",
      "        [ 0.5775,  0.7756, -0.4704,  1.8297, -1.0592],\n",
      "        [ 1.7101,  0.3370, -0.6440,  0.2178, -0.2606]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3536, -0.0086, -0.2896, -0.5330, -0.8031],\n",
      "        [-0.2937, -0.0115, -0.2049, -0.4765, -1.7596],\n",
      "        [ 0.2813, -0.1582,  0.3576, -0.2640,  0.3114],\n",
      "        [-0.7256, -0.0508, -0.6678, -0.8761, -0.8422]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2473,  1.1692,  0.6857,  0.7811, -0.8980],\n",
      "        [ 0.0934,  0.0050,  0.9001,  0.8382, -1.2990],\n",
      "        [ 0.5775,  0.7756, -0.4704,  1.8297, -1.0592],\n",
      "        [ 1.7101,  0.3370, -0.6440,  0.2178, -0.2606]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3448],\n",
      "        [ 1.6744],\n",
      "        [-0.9413],\n",
      "        [-0.7992]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3091, -2.1256,  0.9183, -0.4665,  0.1345],\n",
      "        [ 0.4751, -0.2200, -0.6113,  0.1646, -0.8066],\n",
      "        [-0.8938, -1.3595,  0.1299,  2.0554,  0.7820],\n",
      "        [-0.8065, -0.7428,  0.1253,  0.0778,  2.3846]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2439,  0.0390, -0.2294, -0.7193, -0.8614],\n",
      "        [-1.3381, -0.8647, -0.2809, -0.2918, -1.4365],\n",
      "        [ 0.7385, -0.0004,  0.3203,  0.2333,  0.5236],\n",
      "        [ 0.1529, -0.2827, -0.1880,  0.7214, -0.6063]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3091, -2.1256,  0.9183, -0.4665,  0.1345],\n",
      "        [ 0.4751, -0.2200, -0.6113,  0.1646, -0.8066],\n",
      "        [-0.8938, -1.3595,  0.1299,  2.0554,  0.7820],\n",
      "        [-0.8065, -0.7428,  0.1253,  0.0778,  2.3846]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1492],\n",
      "        [ 0.8368],\n",
      "        [ 0.2711],\n",
      "        [-1.3267]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4328,  0.1687,  0.6573,  0.2546, -0.2693],\n",
      "        [ 1.2794,  2.4179,  0.1770,  0.4968, -0.9076],\n",
      "        [-0.5383,  3.1328,  0.2095, -0.5765,  0.4880],\n",
      "        [ 0.0343,  0.7165, -0.4022,  0.4417,  0.2865]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3672, -0.5558, -0.4481, -0.6481, -1.1651],\n",
      "        [-1.0239, -0.4123, -0.9285, -0.8558, -2.1967],\n",
      "        [ 0.2773,  0.4153,  0.7360,  0.8221,  0.5045],\n",
      "        [ 0.2953,  0.5193, -0.5277, -0.0867,  0.8089]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4328,  0.1687,  0.6573,  0.2546, -0.2693],\n",
      "        [ 1.2794,  2.4179,  0.1770,  0.4968, -0.9076],\n",
      "        [-0.5383,  3.1328,  0.2095, -0.5765,  0.4880],\n",
      "        [ 0.0343,  0.7165, -0.4022,  0.4417,  0.2865]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2865],\n",
      "        [-0.9027],\n",
      "        [ 1.0783],\n",
      "        [ 0.7879]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0831,  0.0023, -0.2317,  1.3610,  1.9018],\n",
      "        [ 0.4963, -0.7354, -0.7513, -0.5412, -0.5109],\n",
      "        [-1.2138, -1.5985,  0.6335,  0.3759, -0.3642],\n",
      "        [-0.9688, -0.4044,  0.5602,  0.3689, -0.3142]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2122, -0.0196, -0.3851, -0.0691, -0.2573],\n",
      "        [ 0.0379, -0.0842,  0.1734, -0.2158, -0.9615],\n",
      "        [ 0.5700,  0.0178,  0.0156,  0.0226, -0.5024],\n",
      "        [ 0.0519, -0.3415, -0.0473,  0.0906, -0.7342]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0831,  0.0023, -0.2317,  1.3610,  1.9018],\n",
      "        [ 0.4963, -0.7354, -0.7513, -0.5412, -0.5109],\n",
      "        [-1.2138, -1.5985,  0.6335,  0.3759, -0.3642],\n",
      "        [-0.9688, -0.4044,  0.5602,  0.3689, -0.3142]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4766],\n",
      "        [ 0.5584],\n",
      "        [-0.5189],\n",
      "        [ 0.3255]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5767,  1.3434, -1.4641,  0.9860,  1.7254],\n",
      "        [-0.2709, -0.7922,  1.0350, -0.8349, -0.4895],\n",
      "        [-0.0131,  0.3585,  0.8726,  0.0799, -0.2167],\n",
      "        [ 0.3579, -0.1478,  1.3828,  0.3261, -2.0250]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0394, -0.6224, -1.1605,  0.0455, -0.3556],\n",
      "        [-0.8033, -0.0883, -0.5649, -0.3315, -1.1437],\n",
      "        [ 0.5680,  0.9914,  1.1764,  0.1369,  0.5447],\n",
      "        [-0.3549,  0.5401,  0.5023, -0.1997, -0.5418]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5767,  1.3434, -1.4641,  0.9860,  1.7254],\n",
      "        [-0.2709, -0.7922,  1.0350, -0.8349, -0.4895],\n",
      "        [-0.0131,  0.3585,  0.8726,  0.0799, -0.2167],\n",
      "        [ 0.3579, -0.1478,  1.3828,  0.3261, -2.0250]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2715],\n",
      "        [ 0.5395],\n",
      "        [ 1.2673],\n",
      "        [ 1.5198]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3139,  0.4369,  0.8850,  0.9747,  0.8127],\n",
      "        [-0.0420,  0.7217,  0.4146, -0.2695, -0.2594],\n",
      "        [ 1.2104, -1.3164,  0.2152,  0.2888, -0.3610],\n",
      "        [-0.1888,  2.3024, -0.6812,  2.3328,  0.1762]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3425, -0.5069, -0.5558, -0.1549, -0.1132],\n",
      "        [-0.4759, -0.3987, -0.6570,  0.0049, -1.2750],\n",
      "        [-0.2579, -0.3412, -0.2638,  0.0516, -0.2292],\n",
      "        [-0.2947, -0.7653, -0.7780, -0.3628, -1.8663]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3139,  0.4369,  0.8850,  0.9747,  0.8127],\n",
      "        [-0.0420,  0.7217,  0.4146, -0.2695, -0.2594],\n",
      "        [ 1.2104, -1.3164,  0.2152,  0.2888, -0.3610],\n",
      "        [-0.1888,  2.3024, -0.6812,  2.3328,  0.1762]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0638],\n",
      "        [-0.2107],\n",
      "        [ 0.1779],\n",
      "        [-2.3515]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0127,  0.7465, -1.3408,  0.8857,  0.6841],\n",
      "        [ 0.4106,  0.9529,  0.8786,  1.3050,  0.5282],\n",
      "        [-1.3303,  0.8983, -0.3796,  1.6031,  1.6751],\n",
      "        [-0.3572,  0.0837, -0.2253,  0.9267,  1.0845]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5205,  0.3727, -0.1344,  0.1412,  0.1062],\n",
      "        [-0.2967,  0.3493, -0.4232, -0.7361, -0.1607],\n",
      "        [-0.0713,  0.6480, -0.1332,  0.7185,  0.0465],\n",
      "        [ 0.1680, -0.1947, -0.1163,  0.9084,  0.5072]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0127,  0.7465, -1.3408,  0.8857,  0.6841],\n",
      "        [ 0.4106,  0.9529,  0.8786,  1.3050,  0.5282],\n",
      "        [-1.3303,  0.8983, -0.3796,  1.6031,  1.6751],\n",
      "        [-0.3572,  0.0837, -0.2253,  0.9267,  1.0845]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6496],\n",
      "        [-1.2063],\n",
      "        [ 1.9573],\n",
      "        [ 1.3417]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0384,  1.7224,  0.7752, -0.6003,  0.0416],\n",
      "        [-0.7178,  0.5474, -0.0989, -0.9102,  1.1742],\n",
      "        [-0.4303,  1.7617,  0.2143,  1.8182,  0.4683],\n",
      "        [-1.2967,  0.0344,  2.4649, -0.6512, -0.1831]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3756, -0.0442, -0.7938,  0.0400, -1.1186],\n",
      "        [ 0.0437, -0.4452,  0.2171,  0.2682, -0.0237],\n",
      "        [-0.6999, -0.0371, -0.7371, -0.0728, -0.9533],\n",
      "        [ 0.1954, -0.3504,  0.7094, -0.2181, -0.5521]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0384,  1.7224,  0.7752, -0.6003,  0.0416],\n",
      "        [-0.7178,  0.5474, -0.0989, -0.9102,  1.1742],\n",
      "        [-0.4303,  1.7617,  0.2143,  1.8182,  0.4683],\n",
      "        [-1.2967,  0.0344,  2.4649, -0.6512, -0.1831]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7477],\n",
      "        [-0.5684],\n",
      "        [-0.5009],\n",
      "        [ 1.7263]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[ 0.3741, -0.0707,  0.1518, -0.7782,  0.5197],\n",
      "        [ 0.7171,  2.0474, -0.5745,  1.2674, -0.0066],\n",
      "        [-1.9268,  1.5934, -0.0969,  0.4063,  1.6043],\n",
      "        [-0.4236, -0.6340, -0.2089,  2.3891, -0.1626]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3155, -0.3397,  0.0002, -0.1341, -0.2199],\n",
      "        [-0.3525,  0.0522,  0.4190, -0.4527, -0.3362],\n",
      "        [-0.5860,  0.6606,  1.1011,  0.3428,  0.0503],\n",
      "        [-0.9243, -1.7150, -0.4918, -0.5209, -2.1435]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3741, -0.0707,  0.1518, -0.7782,  0.5197],\n",
      "        [ 0.7171,  2.0474, -0.5745,  1.2674, -0.0066],\n",
      "        [-1.9268,  1.5934, -0.0969,  0.4063,  1.6043],\n",
      "        [-0.4236, -0.6340, -0.2089,  2.3891, -0.1626]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1321],\n",
      "        [-0.9582],\n",
      "        [ 2.2951],\n",
      "        [ 0.6856]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0288,  0.3197,  0.8658,  0.9843,  0.1510],\n",
      "        [-1.3382,  0.8914,  0.7666, -0.2291,  1.5101],\n",
      "        [-0.6336,  0.4461,  0.8880,  1.3022,  2.9554],\n",
      "        [ 0.8414,  0.7308,  1.0482,  0.6241,  0.6826]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0785, -0.7091,  0.1763, -1.4840, -0.9149],\n",
      "        [-0.0480, -0.5094, -0.3638,  0.0149,  0.0735],\n",
      "        [-0.1892, -1.2362, -0.3495, -1.3415, -1.4602],\n",
      "        [-0.7163, -1.3364, -1.1671, -0.8209, -2.1713]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0288,  0.3197,  0.8658,  0.9843,  0.1510],\n",
      "        [-1.3382,  0.8914,  0.7666, -0.2291,  1.5101],\n",
      "        [-0.6336,  0.4461,  0.8880,  1.3022,  2.9554],\n",
      "        [ 0.8414,  0.7308,  1.0482,  0.6241,  0.6826]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6751],\n",
      "        [-0.5612],\n",
      "        [-6.8043],\n",
      "        [-4.7973]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5405,  0.5020, -0.1992, -0.1973,  0.6217],\n",
      "        [-0.7631,  0.8789,  1.1141, -0.1836, -1.9720],\n",
      "        [-1.0293,  0.5050,  1.3043,  0.2646,  0.2129],\n",
      "        [-0.3181, -1.1104,  0.4239, -0.5382,  1.3560]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6165,  0.1576,  0.9358, -0.2534,  0.6163],\n",
      "        [ 0.4683, -0.4049, -0.7429,  0.1062, -0.0323],\n",
      "        [ 2.0461,  1.9151,  2.0066,  1.6655,  3.0833],\n",
      "        [ 1.0191,  0.5595,  0.5489,  0.8058,  1.2415]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5405,  0.5020, -0.1992, -0.1973,  0.6217],\n",
      "        [-0.7631,  0.8789,  1.1141, -0.1836, -1.9720],\n",
      "        [-1.0293,  0.5050,  1.3043,  0.2646,  0.2129],\n",
      "        [-0.3181, -1.1104,  0.4239, -0.5382,  1.3560]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0074],\n",
      "        [-1.4967],\n",
      "        [ 2.5753],\n",
      "        [ 0.5371]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1349,  0.9743,  1.7252, -0.4357,  0.4224],\n",
      "        [ 1.5197,  1.8760,  0.4002, -0.3547,  0.8134],\n",
      "        [-0.1607,  0.5978,  1.1884,  0.9086, -0.6573],\n",
      "        [-0.1065, -0.9869,  0.2576,  1.0814, -0.4351]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6155, -0.1200, -0.4624,  0.1967,  0.2713],\n",
      "        [ 0.5739,  1.2804,  0.4774, -0.0894,  1.5298],\n",
      "        [ 1.1064,  1.6470,  0.8004,  1.4783,  1.7475],\n",
      "        [ 0.2550,  0.7549,  0.4995,  0.7429,  1.3061]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1349,  0.9743,  1.7252, -0.4357,  0.4224],\n",
      "        [ 1.5197,  1.8760,  0.4002, -0.3547,  0.8134],\n",
      "        [-0.1607,  0.5978,  1.1884,  0.9086, -0.6573],\n",
      "        [-0.1065, -0.9869,  0.2576,  1.0814, -0.4351]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9687],\n",
      "        [ 4.7413],\n",
      "        [ 1.9525],\n",
      "        [-0.4085]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0117,  0.0996,  1.0802, -0.6442,  1.0446],\n",
      "        [ 0.6034, -1.1621,  0.1577,  0.4804, -0.6256],\n",
      "        [ 0.1588, -0.4868,  0.5910, -1.0034, -0.7689],\n",
      "        [ 1.8123, -0.0427,  1.0800,  1.5932,  0.1050]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8355,  0.7125, -0.1368,  0.9748,  0.6566],\n",
      "        [-1.0921, -1.2224, -1.5816, -2.3476, -3.3739],\n",
      "        [-0.2783,  0.9397,  0.8544,  0.4792,  1.4818],\n",
      "        [ 0.3028,  0.5506,  0.2109,  0.7360,  0.4680]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0117,  0.0996,  1.0802, -0.6442,  1.0446],\n",
      "        [ 0.6034, -1.1621,  0.1577,  0.4804, -0.6256],\n",
      "        [ 0.1588, -0.4868,  0.5910, -1.0034, -0.7689],\n",
      "        [ 1.8123, -0.0427,  1.0800,  1.5932,  0.1050]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0092],\n",
      "        [ 1.4949],\n",
      "        [-1.6169],\n",
      "        [ 1.9747]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3261, -0.5095, -0.1524,  0.2577,  2.8700],\n",
      "        [ 1.1305,  2.1063,  0.5173, -0.3143, -0.4536],\n",
      "        [-0.8051, -0.9331, -0.7874, -1.4324, -0.9361],\n",
      "        [ 1.8037, -0.8859,  2.0845,  1.1441, -0.1620]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7702, -0.0902, -0.0817, -0.1988, -0.1622],\n",
      "        [ 0.0025, -0.1943, -0.0719, -0.0524,  0.1209],\n",
      "        [ 1.1525,  1.9427,  0.8949,  1.2214,  1.7318],\n",
      "        [-0.2074, -1.0045, -0.3548, -0.1146, -1.3872]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3261, -0.5095, -0.1524,  0.2577,  2.8700],\n",
      "        [ 1.1305,  2.1063,  0.5173, -0.3143, -0.4536],\n",
      "        [-0.8051, -0.9331, -0.7874, -1.4324, -0.9361],\n",
      "        [ 1.8037, -0.8859,  2.0845,  1.1441, -0.1620]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5631],\n",
      "        [-0.4819],\n",
      "        [-6.8160],\n",
      "        [-0.1302]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5058,  0.1267,  0.0750,  0.9511, -0.5727],\n",
      "        [-1.6098,  0.1769,  0.4938,  0.8474, -0.5242],\n",
      "        [-0.9247, -0.9788,  1.0830, -1.1334, -0.4011],\n",
      "        [ 0.5305,  0.1216,  1.0065, -0.2657,  0.7706]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5371, -1.1688, -0.2509, -0.0271,  0.0620],\n",
      "        [ 0.1692,  0.2171,  0.8982,  0.0535,  0.7166],\n",
      "        [ 0.0640, -0.1500,  0.0054, -0.8328, -0.2363],\n",
      "        [ 0.2561,  0.2592, -1.0292, -0.3310, -0.6250]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5058,  0.1267,  0.0750,  0.9511, -0.5727],\n",
      "        [-1.6098,  0.1769,  0.4938,  0.8474, -0.5242],\n",
      "        [-0.9247, -0.9788,  1.0830, -1.1334, -0.4011],\n",
      "        [ 0.5305,  0.1216,  1.0065, -0.2657,  0.7706]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4999],\n",
      "        [-0.1208],\n",
      "        [ 1.1322],\n",
      "        [-1.2623]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6277,  0.2001,  0.2228, -1.4428,  1.2398],\n",
      "        [-0.0463,  1.0650,  0.5208,  1.9924,  0.4957],\n",
      "        [-0.3128,  0.4945,  0.6898, -0.8404,  0.7680],\n",
      "        [-0.6667,  0.6978, -0.8033, -1.6719,  0.1953]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4333, -0.3315, -0.0393, -0.4737, -0.3433],\n",
      "        [ 0.1125,  0.3497, -0.5620, -0.6615, -0.1238],\n",
      "        [-0.5835, -0.4851, -0.4861,  0.4638, -1.0141],\n",
      "        [-0.0870, -0.2117,  0.2396, -0.0280,  0.9472]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6277,  0.2001,  0.2228, -1.4428,  1.2398],\n",
      "        [-0.0463,  1.0650,  0.5208,  1.9924,  0.4957],\n",
      "        [-0.3128,  0.4945,  0.6898, -0.8404,  0.7680],\n",
      "        [-0.6667,  0.6978, -0.8033, -1.6719,  0.1953]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0893],\n",
      "        [-1.3048],\n",
      "        [-1.5613],\n",
      "        [-0.0504]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0285, -0.4698,  0.9274,  0.4294, -0.0919],\n",
      "        [ 1.3592, -1.1538,  0.4947, -1.3345, -0.9414],\n",
      "        [-0.1068, -0.4564,  1.5986, -0.0184, -2.0732],\n",
      "        [ 1.2234, -0.3449,  0.0298, -0.1581, -0.3378]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0951, -0.8698, -0.5950, -0.9155, -1.0219],\n",
      "        [ 0.5495,  0.4314,  1.2945,  1.1892,  0.7018],\n",
      "        [ 0.4577,  0.5971,  0.5735,  0.3017,  0.6936],\n",
      "        [ 0.1634,  0.1764, -0.5238, -0.0754,  0.1369]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0285, -0.4698,  0.9274,  0.4294, -0.0919],\n",
      "        [ 1.3592, -1.1538,  0.4947, -1.3345, -0.9414],\n",
      "        [-0.1068, -0.4564,  1.5986, -0.0184, -2.0732],\n",
      "        [ 1.2234, -0.3449,  0.0298, -0.1581, -0.3378]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4398],\n",
      "        [-1.3582],\n",
      "        [-0.8481],\n",
      "        [ 0.0892]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7746, -0.5548,  1.5033,  1.5816,  0.8706],\n",
      "        [-0.2192, -0.7524, -0.7193,  0.9255, -0.7946],\n",
      "        [ 1.3278, -0.2994,  0.2879,  0.9602,  0.6190],\n",
      "        [-1.0840,  1.2100,  0.3182,  0.4868,  0.2564]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2482, -0.4554,  0.4701, -0.0791, -0.3379],\n",
      "        [ 0.7660,  0.9815,  0.3628,  0.6397,  1.7067],\n",
      "        [ 0.5058,  0.5450,  0.3429,  0.4363,  1.3586],\n",
      "        [-0.2085,  0.2795, -0.4212, -0.6306, -0.3582]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7746, -0.5548,  1.5033,  1.5816,  0.8706],\n",
      "        [-0.2192, -0.7524, -0.7193,  0.9255, -0.7946],\n",
      "        [ 1.3278, -0.2994,  0.2879,  0.9602,  0.6190],\n",
      "        [-1.0840,  1.2100,  0.3182,  0.4868,  0.2564]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3478],\n",
      "        [-1.9314],\n",
      "        [ 1.8672],\n",
      "        [ 0.0315]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1962, -0.8526, -1.2489,  0.6612,  0.3431],\n",
      "        [-1.8402,  1.5458, -1.4392, -1.1349,  0.6401],\n",
      "        [ 0.8784, -1.0881,  0.6406,  0.8531, -1.5499],\n",
      "        [ 2.1300, -0.7867, -0.5342,  0.9918,  1.8641]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8235,  0.1641, -0.8459,  0.1410,  0.0461],\n",
      "        [ 1.1829,  1.5975,  1.2150,  1.0891,  2.1860],\n",
      "        [-0.0858, -0.8464, -0.2376, -1.1804, -1.3162],\n",
      "        [ 0.2098, -0.6623, -0.4059, -0.0099, -0.6955]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1962, -0.8526, -1.2489,  0.6612,  0.3431],\n",
      "        [-1.8402,  1.5458, -1.4392, -1.1349,  0.6401],\n",
      "        [ 0.8784, -1.0881,  0.6406,  0.8531, -1.5499],\n",
      "        [ 2.1300, -0.7867, -0.5342,  0.9918,  1.8641]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7831],\n",
      "        [-1.2927],\n",
      "        [ 1.7265],\n",
      "        [-0.1214]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2441, -0.5931,  1.5428, -0.5710,  0.9293],\n",
      "        [-0.6209,  0.0376, -1.1338,  0.0275, -2.1784],\n",
      "        [ 1.2605, -0.2757, -0.1923, -0.0143, -0.2838],\n",
      "        [-1.1620, -1.8944,  0.2514, -0.2675,  0.1469]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2970, -0.1497,  0.5825, -0.0097, -0.1304],\n",
      "        [ 0.8787, -0.6224, -0.2014,  0.3339,  0.1534],\n",
      "        [-0.6667, -0.7118, -1.0341, -1.3814, -1.9306],\n",
      "        [-0.1942, -0.1596, -0.0227, -0.3075, -0.4107]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2441, -0.5931,  1.5428, -0.5710,  0.9293],\n",
      "        [-0.6209,  0.0376, -1.1338,  0.0275, -2.1784],\n",
      "        [ 1.2605, -0.2757, -0.1923, -0.0143, -0.2838],\n",
      "        [-1.1620, -1.8944,  0.2514, -0.2675,  0.1469]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7993],\n",
      "        [-0.6657],\n",
      "        [ 0.1226],\n",
      "        [ 0.5441]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2347,  1.1235,  0.9527,  1.5246, -0.6312],\n",
      "        [ 0.2077,  0.2471,  1.5983,  0.0919, -0.6254],\n",
      "        [-1.5639,  0.6342, -0.1006, -1.7785,  0.6211],\n",
      "        [ 0.2594, -0.2645, -1.5329, -4.2599,  1.8856]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4285, -1.0602,  0.0266, -0.6048, -0.4324],\n",
      "        [ 0.3303, -0.1434, -1.1069,  0.2304, -0.8861],\n",
      "        [-0.6038, -1.1236, -0.9053, -0.4242, -1.8309],\n",
      "        [-0.2682,  0.4297, -0.1484,  0.5016, -0.4561]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2347,  1.1235,  0.9527,  1.5246, -0.6312],\n",
      "        [ 0.2077,  0.2471,  1.5983,  0.0919, -0.6254],\n",
      "        [-1.5639,  0.6342, -0.1006, -1.7785,  0.6211],\n",
      "        [ 0.2594, -0.2645, -1.5329, -4.2599,  1.8856]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7144],\n",
      "        [-1.1607],\n",
      "        [-0.0600],\n",
      "        [-2.9526]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5300,  0.8912,  1.0336,  0.0824, -0.3082],\n",
      "        [-0.6977,  0.3195, -1.1656,  0.3101,  1.2189],\n",
      "        [-0.4312,  0.8453, -0.4669,  1.3565, -0.2634],\n",
      "        [ 1.2805, -0.1757,  1.1592,  1.5040, -0.2931]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6311,  0.1750, -0.4423,  0.3208,  0.3575],\n",
      "        [ 0.5993,  0.4428,  0.2809,  0.1487,  1.0440],\n",
      "        [-0.9683,  0.0101, -0.6525, -0.5777, -1.3400],\n",
      "        [ 0.9615,  1.8797,  0.4259,  0.7241,  1.6803]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5300,  0.8912,  1.0336,  0.0824, -0.3082],\n",
      "        [-0.6977,  0.3195, -1.1656,  0.3101,  1.2189],\n",
      "        [-0.4312,  0.8453, -0.4669,  1.3565, -0.2634],\n",
      "        [ 1.2805, -0.1757,  1.1592,  1.5040, -0.2931]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0505],\n",
      "        [ 0.7146],\n",
      "        [ 0.3000],\n",
      "        [ 1.9911]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0459,  0.7928,  0.0385, -0.4653, -1.0418],\n",
      "        [-0.3566,  0.9987,  0.2874,  0.7309,  1.4019],\n",
      "        [-0.0099, -0.3492,  0.0022,  3.3505,  0.1977],\n",
      "        [ 0.6645, -0.4301,  0.0371, -1.0045,  0.2610]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9715,  0.2331,  0.4418, -0.4985,  0.6009],\n",
      "        [-0.0492, -0.4007, -0.3376, -0.6462, -0.4297],\n",
      "        [-0.6775, -0.7791, -1.4628, -0.3309, -2.1320],\n",
      "        [-0.1541,  0.6167,  0.4702,  0.1166, -0.0629]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0459,  0.7928,  0.0385, -0.4653, -1.0418],\n",
      "        [-0.3566,  0.9987,  0.2874,  0.7309,  1.4019],\n",
      "        [-0.0099, -0.3492,  0.0022,  3.3505,  0.1977],\n",
      "        [ 0.6645, -0.4301,  0.0371, -1.0045,  0.2610]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2368],\n",
      "        [-1.5542],\n",
      "        [-1.2547],\n",
      "        [-0.4838]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0286,  0.5630,  0.7510,  0.4728,  1.7700],\n",
      "        [-0.6790, -0.4290,  0.8663,  1.1809,  1.1243],\n",
      "        [-0.4669,  1.3381,  0.9215, -1.1137, -0.9008],\n",
      "        [-0.5889,  0.0140,  0.7620, -0.9033,  0.3014]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4983, -0.4099, -0.1815, -0.6565, -0.0480],\n",
      "        [ 0.9670,  1.1941,  1.2078,  0.7929,  1.2378],\n",
      "        [-0.0295, -0.3780, -0.8546, -0.2076, -0.6993],\n",
      "        [ 0.4539,  0.7219,  0.5216,  0.9946,  1.3872]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0286,  0.5630,  0.7510,  0.4728,  1.7700],\n",
      "        [-0.6790, -0.4290,  0.8663,  1.1809,  1.1243],\n",
      "        [-0.4669,  1.3381,  0.9215, -1.1137, -0.9008],\n",
      "        [-0.5889,  0.0140,  0.7620, -0.9033,  0.3014]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7767],\n",
      "        [ 2.2055],\n",
      "        [-0.4183],\n",
      "        [-0.3401]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2398, -0.9826, -0.1455,  1.6688,  0.6585],\n",
      "        [ 0.0942, -0.7210, -0.4119,  0.5869,  0.4533],\n",
      "        [ 0.5378, -1.1721,  0.7325, -0.9210,  0.6877],\n",
      "        [ 0.7628, -1.4060,  0.4761,  0.7302, -0.2455]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7848, -0.2562, -0.6848, -0.5534, -0.0306],\n",
      "        [-0.1706, -0.3067, -0.8126, -0.5655, -1.6174],\n",
      "        [-0.0924, -0.7400, -0.5798, -0.9430, -1.2178],\n",
      "        [ 0.7417,  0.1113,  0.6808,  0.8090,  0.9941]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2398, -0.9826, -0.1455,  1.6688,  0.6585],\n",
      "        [ 0.0942, -0.7210, -0.4119,  0.5869,  0.4533],\n",
      "        [ 0.5378, -1.1721,  0.7325, -0.9210,  0.6877],\n",
      "        [ 0.7628, -1.4060,  0.4761,  0.7302, -0.2455]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3806],\n",
      "        [-0.5252],\n",
      "        [ 0.4239],\n",
      "        [ 1.0801]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2890, -0.8868,  0.5951,  3.4353,  0.4370],\n",
      "        [ 0.4869, -0.8660,  0.0399, -1.0314,  2.1408],\n",
      "        [-1.3960, -1.9482, -0.4205, -0.5566,  0.2094],\n",
      "        [ 1.3658,  2.7452, -0.1437, -2.0427, -0.4761]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6718, -0.5583, -0.0412, -0.0322, -0.6588],\n",
      "        [ 0.5683,  0.4980, -0.0784, -0.0827, -0.5080],\n",
      "        [-0.0113, -0.8742, -0.9983, -1.0667, -1.4460],\n",
      "        [ 0.0866,  0.5805,  0.6452, -0.2717,  0.3721]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2890, -0.8868,  0.5951,  3.4353,  0.4370],\n",
      "        [ 0.4869, -0.8660,  0.0399, -1.0314,  2.1408],\n",
      "        [-1.3960, -1.9482, -0.4205, -0.5566,  0.2094],\n",
      "        [ 1.3658,  2.7452, -0.1437, -2.0427, -0.4761]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1220],\n",
      "        [-1.1600],\n",
      "        [ 2.4295],\n",
      "        [ 1.9969]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6319, -1.0202,  2.5027, -0.3053,  0.3387],\n",
      "        [-0.4956, -1.7833,  0.5242,  0.8575, -0.3276],\n",
      "        [-0.6639,  1.1854, -0.8612, -1.0478,  0.1448],\n",
      "        [ 0.3934,  0.6526,  0.3114, -0.5785,  0.2230]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6579, -0.9622, -0.2234,  0.0882,  0.4288],\n",
      "        [ 0.4434,  0.4787,  0.3762,  0.2870,  1.2541],\n",
      "        [-0.9139, -1.4493, -1.3386, -1.8724, -3.0161],\n",
      "        [-0.5254, -0.3802, -0.7625, -0.5740, -1.0639]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6319, -1.0202,  2.5027, -0.3053,  0.3387],\n",
      "        [-0.4956, -1.7833,  0.5242,  0.8575, -0.3276],\n",
      "        [-0.6639,  1.1854, -0.8612, -1.0478,  0.1448],\n",
      "        [ 0.3934,  0.6526,  0.3114, -0.5785,  0.2230]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5328],\n",
      "        [-1.0411],\n",
      "        [ 1.5667],\n",
      "        [-0.5975]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9471, -0.5977, -1.2502, -1.7094, -0.6957],\n",
      "        [-1.0221,  1.7245,  0.6459, -0.0709,  0.6056],\n",
      "        [ 1.8708,  0.5922, -1.2385,  1.0964,  1.0491],\n",
      "        [-1.6758,  0.1250,  0.6270,  0.8289, -0.3497]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6086, -0.0767, -0.3264, -0.2510, -0.4568],\n",
      "        [ 0.8847,  1.2963, -0.0645,  0.5269,  1.1250],\n",
      "        [ 0.4539, -0.1715,  0.6265, -0.0662,  0.3000],\n",
      "        [-0.4187,  0.5017, -0.0462,  0.4246,  0.0882]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9471, -0.5977, -1.2502, -1.7094, -0.6957],\n",
      "        [-1.0221,  1.7245,  0.6459, -0.0709,  0.6056],\n",
      "        [ 1.8708,  0.5922, -1.2385,  1.0964,  1.0491],\n",
      "        [-1.6758,  0.1250,  0.6270,  0.8289, -0.3497]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0160],\n",
      "        [ 1.9335],\n",
      "        [ 0.2137],\n",
      "        [ 1.0565]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8041, -0.3898,  0.5488, -1.3601, -1.4736],\n",
      "        [-0.4031, -1.8173,  2.2758,  1.7052, -0.4474],\n",
      "        [-0.4719,  0.5183, -1.9194,  0.2517, -0.3977],\n",
      "        [ 0.6996, -1.7521, -1.2500, -0.4302,  1.3039]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0877, -0.3287, -0.2903, -0.0232, -0.7961],\n",
      "        [ 0.0297,  0.0521,  0.1311, -0.2006, -0.6346],\n",
      "        [ 0.1094,  0.2794,  0.9192, -0.4348,  0.5610],\n",
      "        [-0.1707, -0.2578,  0.2096,  0.3458, -0.8633]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8041, -0.3898,  0.5488, -1.3601, -1.4736],\n",
      "        [-0.4031, -1.8173,  2.2758,  1.7052, -0.4474],\n",
      "        [-0.4719,  0.5183, -1.9194,  0.2517, -0.3977],\n",
      "        [ 0.6996, -1.7521, -1.2500, -0.4302,  1.3039]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2989],\n",
      "        [ 0.1335],\n",
      "        [-2.0038],\n",
      "        [-1.2041]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7952, -1.6372, -1.7630,  0.4349,  0.2787],\n",
      "        [-1.1959,  0.2600,  1.5023, -1.0723,  0.7265],\n",
      "        [-1.5564,  0.2763,  0.2806, -0.5443,  0.9440],\n",
      "        [-0.0868,  0.1493, -1.1665,  2.1221,  0.6067]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3792,  0.1478,  0.1706, -0.5364,  0.2977],\n",
      "        [ 0.0904,  0.8289, -0.1582, -0.3110, -0.3423],\n",
      "        [ 0.9984,  1.8293,  0.9874,  0.7281,  1.3797],\n",
      "        [-0.2839, -0.1012, -0.1736, -0.1412,  0.7324]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7952, -1.6372, -1.7630,  0.4349,  0.2787],\n",
      "        [-1.1959,  0.2600,  1.5023, -1.0723,  0.7265],\n",
      "        [-1.5564,  0.2763,  0.2806, -0.5443,  0.9440],\n",
      "        [-0.0868,  0.1493, -1.1665,  2.1221,  0.6067]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3916],\n",
      "        [-0.0455],\n",
      "        [ 0.1347],\n",
      "        [ 0.3566]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2360, -0.1090,  1.7233,  1.1541,  0.2962],\n",
      "        [ 0.2785,  0.5338,  1.5263, -1.4243,  0.7658],\n",
      "        [ 1.1342,  1.7207, -0.5470, -0.8647,  0.4358],\n",
      "        [-1.1255,  0.4999, -0.5268, -0.4788, -0.4480]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4981,  0.0891, -0.0672, -0.4333,  0.5566],\n",
      "        [ 0.3744,  0.5949,  0.2373,  0.0314,  0.2396],\n",
      "        [ 0.5660,  0.4770,  0.9731,  1.2671,  0.9410],\n",
      "        [-0.0674,  0.6100,  0.0377,  0.0565,  0.1049]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2360, -0.1090,  1.7233,  1.1541,  0.2962],\n",
      "        [ 0.2785,  0.5338,  1.5263, -1.4243,  0.7658],\n",
      "        [ 1.1342,  1.7207, -0.5470, -0.8647,  0.4358],\n",
      "        [-1.1255,  0.4999, -0.5268, -0.4788, -0.4480]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5782],\n",
      "        [ 0.9229],\n",
      "        [ 0.2447],\n",
      "        [ 0.2869]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2939,  0.2001, -0.0825,  1.0144, -0.6137],\n",
      "        [-0.3301,  0.3714, -1.5950,  1.2653,  0.4304],\n",
      "        [-0.7244, -1.0284, -0.4639, -0.3236,  0.5605],\n",
      "        [-0.0764, -0.0346,  1.2332,  1.2384,  0.5095]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8366,  0.4361, -0.5556, -0.6720,  0.2428],\n",
      "        [-0.1704, -0.0392,  0.6838,  0.8639,  0.1893],\n",
      "        [ 0.5654,  0.5897, -0.2430,  0.1238,  0.0480],\n",
      "        [ 0.0333, -0.4213, -0.0008, -0.3081,  0.4068]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2939,  0.2001, -0.0825,  1.0144, -0.6137],\n",
      "        [-0.3301,  0.3714, -1.5950,  1.2653,  0.4304],\n",
      "        [-0.7244, -1.0284, -0.4639, -0.3236,  0.5605],\n",
      "        [-0.0764, -0.0346,  1.2332,  1.2384,  0.5095]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9434],\n",
      "        [ 0.1257],\n",
      "        [-0.9165],\n",
      "        [-0.1632]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4490,  2.3770, -0.4670,  0.2991,  0.0041],\n",
      "        [-0.9182, -0.0317,  0.5084, -0.6451, -0.1973],\n",
      "        [ 1.0327,  1.2600,  0.0591, -0.2064,  0.0842],\n",
      "        [ 0.2625,  0.3244, -0.8363, -0.8908,  1.1008]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8877, -0.0989,  0.1082,  0.1050,  0.1856],\n",
      "        [ 0.2087, -0.0248, -0.4922,  0.4919,  0.2024],\n",
      "        [ 1.1467,  0.9032,  0.2336,  0.1912,  0.7335],\n",
      "        [-0.0642, -0.1092, -0.0507,  0.9232, -0.2529]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4490,  2.3770, -0.4670,  0.2991,  0.0041],\n",
      "        [-0.9182, -0.0317,  0.5084, -0.6451, -0.1973],\n",
      "        [ 1.0327,  1.2600,  0.0591, -0.2064,  0.0842],\n",
      "        [ 0.2625,  0.3244, -0.8363, -0.8908,  1.1008]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6520],\n",
      "        [-0.7983],\n",
      "        [ 2.3584],\n",
      "        [-1.1106]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4641, -0.1440, -0.3810,  0.4403,  0.8187],\n",
      "        [-0.7692, -0.2999, -0.0031, -1.6597,  0.1284],\n",
      "        [-0.2478,  0.6002,  1.1667, -0.0966, -0.0942],\n",
      "        [-1.7123,  1.9192, -0.2772,  0.1351,  0.0746]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6724,  0.1922,  0.6577,  0.1896,  0.8828],\n",
      "        [ 0.4013,  0.5466,  1.0565,  0.0313,  1.2417],\n",
      "        [-0.4627, -0.8896, -0.5723, -0.0457, -0.9312],\n",
      "        [ 0.7628, -0.0531,  0.2804, -0.3725,  1.0307]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4641, -0.1440, -0.3810,  0.4403,  0.8187],\n",
      "        [-0.7692, -0.2999, -0.0031, -1.6597,  0.1284],\n",
      "        [-0.2478,  0.6002,  1.1667, -0.0966, -0.0942],\n",
      "        [-1.7123,  1.9192, -0.2772,  0.1351,  0.0746]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8401],\n",
      "        [-0.3683],\n",
      "        [-0.9948],\n",
      "        [-1.4593]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8041,  0.3042,  1.5499,  1.5226, -0.2376],\n",
      "        [-0.0775,  0.8941,  0.5337,  1.3850,  0.2283],\n",
      "        [-0.0280, -0.2065,  2.0969,  2.5653,  0.7380],\n",
      "        [-1.8557,  1.2483,  0.8138,  0.7811, -0.1208]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4865, -0.2811, -0.0283, -0.1689, -0.2613],\n",
      "        [ 0.5536,  0.6186,  0.4585,  0.7003,  0.8432],\n",
      "        [ 0.6107,  0.1390, -0.0795,  0.5948,  0.1288],\n",
      "        [ 0.4849,  1.0337,  1.3092,  1.0199,  1.4780]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8041,  0.3042,  1.5499,  1.5226, -0.2376],\n",
      "        [-0.0775,  0.8941,  0.5337,  1.3850,  0.2283],\n",
      "        [-0.0280, -0.2065,  2.0969,  2.5653,  0.7380],\n",
      "        [-1.8557,  1.2483,  0.8138,  0.7811, -0.1208]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7157],\n",
      "        [ 1.9172],\n",
      "        [ 1.4083],\n",
      "        [ 2.0740]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8095, -1.6255, -1.2510, -1.8055,  0.9015],\n",
      "        [-1.3456, -1.1509, -0.0537,  0.5886,  0.9382],\n",
      "        [ 0.3297, -0.8897,  0.3534, -0.1530, -0.2865],\n",
      "        [ 0.9798, -0.5227, -1.2331, -0.3751, -0.4885]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7617,  0.5345,  0.2098, -0.7827,  0.2076],\n",
      "        [-0.3194, -0.8475, -0.4096, -0.7584, -1.5939],\n",
      "        [-0.3866, -0.6266,  0.4222, -0.7537, -1.3054],\n",
      "        [ 0.1942,  0.1954, -0.4832, -0.2577,  0.1424]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8095, -1.6255, -1.2510, -1.8055,  0.9015],\n",
      "        [-1.3456, -1.1509, -0.0537,  0.5886,  0.9382],\n",
      "        [ 0.3297, -0.8897,  0.3534, -0.1530, -0.2865],\n",
      "        [ 0.9798, -0.5227, -1.2331, -0.3751, -0.4885]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0856],\n",
      "        [-0.5148],\n",
      "        [ 1.0686],\n",
      "        [ 0.7110]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5146,  0.2720,  0.0028, -0.0562,  0.0601],\n",
      "        [ 0.4761, -0.5252,  0.0203,  0.3032, -1.4720],\n",
      "        [ 0.3749, -0.9623,  1.6611,  0.8630,  0.1555],\n",
      "        [-0.1012,  0.5065, -1.0735,  0.5335,  0.2846]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6847, -0.4119,  0.4861, -0.0874,  0.1121],\n",
      "        [ 0.1452, -0.1148, -0.6496,  0.1965, -0.5556],\n",
      "        [-0.5945, -0.5183, -1.0243, -0.8969, -1.5228],\n",
      "        [-0.2778,  0.1157, -0.1903,  0.9890, -0.1103]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5146,  0.2720,  0.0028, -0.0562,  0.0601],\n",
      "        [ 0.4761, -0.5252,  0.0203,  0.3032, -1.4720],\n",
      "        [ 0.3749, -0.9623,  1.6611,  0.8630,  0.1555],\n",
      "        [-0.1012,  0.5065, -1.0735,  0.5335,  0.2846]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4514],\n",
      "        [ 0.9938],\n",
      "        [-2.4363],\n",
      "        [ 0.7872]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2995,  0.7376,  1.4451, -1.1443, -1.0352],\n",
      "        [ 1.5052, -0.4998, -0.0841, -0.2772, -0.3077],\n",
      "        [-1.3263,  0.4785,  1.0308, -0.0792, -0.9006],\n",
      "        [-0.8954,  0.8805, -0.5124,  2.2172, -0.1907]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5738,  0.2728, -0.2996, -0.1466,  0.2764],\n",
      "        [-0.6619, -0.4518, -0.5327,  0.1246, -1.0889],\n",
      "        [ 0.3719,  0.5278,  0.9658,  1.8941,  1.4163],\n",
      "        [ 0.1062,  0.1134, -0.2296,  0.4872,  0.0548]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2995,  0.7376,  1.4451, -1.1443, -1.0352],\n",
      "        [ 1.5052, -0.4998, -0.0841, -0.2772, -0.3077],\n",
      "        [-1.3263,  0.4785,  1.0308, -0.0792, -0.9006],\n",
      "        [-0.8954,  0.8805, -0.5124,  2.2172, -0.1907]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1782],\n",
      "        [-0.4252],\n",
      "        [-0.6708],\n",
      "        [ 1.1922]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2295, -0.1722, -0.8271,  0.9875, -0.4687],\n",
      "        [-1.0847,  0.7934, -1.4994,  1.1912, -0.8813],\n",
      "        [-1.4239,  1.4591, -0.1896,  0.2423, -0.5434],\n",
      "        [ 1.4413,  0.2222, -1.1324, -0.9917,  0.0535]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5659,  0.6745, -0.2338,  0.2787, -0.2173],\n",
      "        [-0.1971,  0.0124,  0.2620,  0.3388, -0.4045],\n",
      "        [ 0.9123,  0.9673,  1.1235,  1.1304,  1.1187],\n",
      "        [-0.2154,  0.0831, -0.3377, -0.0533, -0.5830]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2295, -0.1722, -0.8271,  0.9875, -0.4687],\n",
      "        [-1.0847,  0.7934, -1.4994,  1.1912, -0.8813],\n",
      "        [-1.4239,  1.4591, -0.1896,  0.2423, -0.5434],\n",
      "        [ 1.4413,  0.2222, -1.1324, -0.9917,  0.0535]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1502],\n",
      "        [ 0.5909],\n",
      "        [-0.4347],\n",
      "        [ 0.1120]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.3809,  1.3633, -0.2986, -0.2677, -0.1194],\n",
      "        [-0.3121, -0.2197, -1.1094,  0.6418, -1.1822],\n",
      "        [-2.2055, -0.5025,  0.2466,  0.4650, -0.7299],\n",
      "        [-0.0203, -1.1174, -0.0901, -0.0810, -1.0009]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0037, -0.1879, -0.2179, -0.9377, -0.8198],\n",
      "        [-0.4601,  0.1349,  0.1034, -0.3773, -0.0261],\n",
      "        [ 0.7321,  0.8095,  0.8905,  0.0660,  1.7809],\n",
      "        [-0.6971,  0.4699,  0.4570,  0.2942,  0.5387]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.3809,  1.3633, -0.2986, -0.2677, -0.1194],\n",
      "        [-0.3121, -0.2197, -1.1094,  0.6418, -1.1822],\n",
      "        [-2.2055, -0.5025,  0.2466,  0.4650, -0.7299],\n",
      "        [-0.0203, -1.1174, -0.0901, -0.0810, -1.0009]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1666],\n",
      "        [-0.2121],\n",
      "        [-3.0710],\n",
      "        [-1.1152]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8935, -0.4111,  2.1705,  0.1529,  0.5896],\n",
      "        [ 0.4280, -0.0008, -0.1309, -0.9431, -0.3500],\n",
      "        [ 0.0227,  0.2135,  0.9271,  0.0933, -0.9391],\n",
      "        [-0.1533, -0.1685, -0.1700, -1.1947, -0.0786]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3680, -0.3027, -1.0755, -0.4236, -0.1767],\n",
      "        [-0.3526, -0.4708,  0.2926,  0.1547,  0.8107],\n",
      "        [ 1.2343,  2.2007,  1.5233,  1.7221,  2.0507],\n",
      "        [-0.0929,  0.4565,  0.9693,  0.0337,  1.2463]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8935, -0.4111,  2.1705,  0.1529,  0.5896],\n",
      "        [ 0.4280, -0.0008, -0.1309, -0.9431, -0.3500],\n",
      "        [ 0.0227,  0.2135,  0.9271,  0.0933, -0.9391],\n",
      "        [-0.1533, -0.1685, -0.1700, -1.1947, -0.0786]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6820],\n",
      "        [-0.6185],\n",
      "        [ 0.1449],\n",
      "        [-0.3657]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4228, -1.0848,  0.2990, -0.0912, -0.3795],\n",
      "        [-1.4440, -0.1792,  0.2056,  0.5638,  1.1807],\n",
      "        [ 0.8889,  1.3165,  0.1812,  1.9790, -0.9349],\n",
      "        [-0.9369,  0.8902,  1.6552,  0.2135,  0.8702]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8217,  0.0928, -0.1722,  1.2326,  1.4390],\n",
      "        [ 0.1464,  0.3325,  0.1088,  0.3925,  1.5575],\n",
      "        [ 0.5778, -0.0653,  0.5827, -0.8138, -0.1371],\n",
      "        [-0.0983,  0.8116,  0.4798,  0.8136,  1.1924]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4228, -1.0848,  0.2990, -0.0912, -0.3795],\n",
      "        [-1.4440, -0.1792,  0.2056,  0.5638,  1.1807],\n",
      "        [ 0.8889,  1.3165,  0.1812,  1.9790, -0.9349],\n",
      "        [-0.9369,  0.8902,  1.6552,  0.2135,  0.8702]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9797],\n",
      "        [ 1.8116],\n",
      "        [-0.9491],\n",
      "        [ 2.8200]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1858,  1.1636, -0.4215, -0.6210,  0.6135],\n",
      "        [-1.9186,  0.9724, -0.3100,  0.3517, -0.3358],\n",
      "        [-0.0893, -0.7428,  0.1320, -0.1225,  1.0665],\n",
      "        [-1.7262,  1.0457, -0.6951, -0.2499,  0.3530]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.2372,  1.1398,  1.7309,  1.0059,  1.6887],\n",
      "        [-0.8550, -0.3416, -0.6977, -0.7369, -0.8792],\n",
      "        [-0.0036,  0.7526,  0.7111,  0.9257,  1.1467],\n",
      "        [-0.4316, -0.5609, -0.2309, -0.4320, -0.8036]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1858,  1.1636, -0.4215, -0.6210,  0.6135],\n",
      "        [-1.9186,  0.9724, -0.3100,  0.3517, -0.3358],\n",
      "        [-0.0893, -0.7428,  0.1320, -0.1225,  1.0665],\n",
      "        [-1.7262,  1.0457, -0.6951, -0.2499,  0.3530]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.6610],\n",
      "        [ 1.5606],\n",
      "        [ 0.6448],\n",
      "        [ 0.1433]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8402,  0.3298, -0.3135,  0.1126, -0.9892],\n",
      "        [-0.7926,  0.7029,  0.5066,  0.0241,  0.6388],\n",
      "        [-0.1744,  1.9850,  0.6099, -0.3887, -2.6045],\n",
      "        [ 0.9065,  0.9183, -1.0089,  1.1466, -1.1743]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1386,  0.8394, -0.9611,  0.1419, -0.4221],\n",
      "        [-0.7698, -0.3196, -1.0791, -0.9538, -2.0405],\n",
      "        [ 0.1422,  0.1859, -0.0067,  0.3808,  0.2367],\n",
      "        [-0.5319, -0.6070,  0.2410, -0.6099, -1.3285]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8402,  0.3298, -0.3135,  0.1126, -0.9892],\n",
      "        [-0.7926,  0.7029,  0.5066,  0.0241,  0.6388],\n",
      "        [-0.1744,  1.9850,  0.6099, -0.3887, -2.6045],\n",
      "        [ 0.9065,  0.9183, -1.0089,  1.1466, -1.1743]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2667],\n",
      "        [-1.4876],\n",
      "        [-0.4244],\n",
      "        [-0.4220]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2013, -0.0367,  0.4196, -0.0175,  0.1912],\n",
      "        [ 0.4392,  0.7702,  0.9347, -1.1007, -0.3073],\n",
      "        [-0.5392, -0.0234, -0.9036,  0.4478, -1.6476],\n",
      "        [ 0.4289,  1.1219, -0.7491, -0.9034, -1.0491]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6090, -0.4915, -0.9586, -0.0641, -0.7971],\n",
      "        [-0.0722, -0.6103,  0.0205, -0.8172, -1.1293],\n",
      "        [ 0.4921, -0.2059, -0.4972,  0.2876, -0.4073],\n",
      "        [-0.7486,  0.0697,  0.0186,  0.6472,  0.1516]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2013, -0.0367,  0.4196, -0.0175,  0.1912],\n",
      "        [ 0.4392,  0.7702,  0.9347, -1.1007, -0.3073],\n",
      "        [-0.5392, -0.0234, -0.9036,  0.4478, -1.6476],\n",
      "        [ 0.4289,  1.1219, -0.7491, -0.9034, -1.0491]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4129],\n",
      "        [ 0.7639],\n",
      "        [ 0.9886],\n",
      "        [-1.0004]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3379,  0.5761, -0.0857, -1.6772, -0.1355],\n",
      "        [-0.3207,  0.2662,  1.9001, -0.2753,  0.3499],\n",
      "        [-0.2940,  1.1543, -0.7678, -0.7649, -1.1251],\n",
      "        [ 0.3770, -0.5759, -1.5805,  0.7712,  2.2235]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1564, -0.1670,  0.2286,  0.4435,  0.5612],\n",
      "        [-0.6203, -0.6539, -0.7658, -0.7289, -0.6274],\n",
      "        [ 0.5031, -0.2714,  0.2450, -0.0698, -1.2391],\n",
      "        [-0.2135,  0.9428,  0.9866,  0.6307,  0.5096]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3379,  0.5761, -0.0857, -1.6772, -0.1355],\n",
      "        [-0.3207,  0.2662,  1.9001, -0.2753,  0.3499],\n",
      "        [-0.2940,  1.1543, -0.7678, -0.7649, -1.1251],\n",
      "        [ 0.3770, -0.5759, -1.5805,  0.7712,  2.2235]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8829],\n",
      "        [-1.4491],\n",
      "        [ 0.7982],\n",
      "        [-0.5632]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1608, -0.7219, -0.8143,  0.5140,  1.0963],\n",
      "        [-1.5826,  1.1832,  0.0026,  0.7128, -1.4071],\n",
      "        [-0.1610,  0.8287,  1.8961,  0.4712,  1.4672],\n",
      "        [ 0.1317, -0.9999, -0.7950, -1.4628, -2.0672]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4996,  0.9023,  1.1322,  0.9445,  0.1417],\n",
      "        [ 0.1779, -0.0941, -0.2603, -0.0616,  0.3386],\n",
      "        [-0.1496,  0.2336,  0.2402, -0.4486, -0.7441],\n",
      "        [ 0.1735,  0.2872,  0.8076,  1.1946,  0.8528]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1608, -0.7219, -0.8143,  0.5140,  1.0963],\n",
      "        [-1.5826,  1.1832,  0.0026,  0.7128, -1.4071],\n",
      "        [-0.1610,  0.8287,  1.8961,  0.4712,  1.4672],\n",
      "        [ 0.1317, -0.9999, -0.7950, -1.4628, -2.0672]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3525],\n",
      "        [-0.9141],\n",
      "        [-0.6299],\n",
      "        [-4.4168]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1049, -0.8209, -0.5178, -0.7041,  0.0398],\n",
      "        [ 0.4574, -0.6146,  0.2662, -0.3334,  0.8791],\n",
      "        [ 0.0501,  1.6468, -1.0594,  0.1891,  2.2984],\n",
      "        [-0.1390,  0.2950, -0.7631,  0.5156,  1.3983]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1301,  0.6764,  0.1116,  1.1536,  0.6990],\n",
      "        [ 0.4540,  0.1646, -0.4124, -0.5623, -0.6591],\n",
      "        [-0.1387,  0.2371,  0.0622, -0.0043, -0.0457],\n",
      "        [ 1.8038,  1.5638,  2.2747,  1.6256,  2.7672]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1049, -0.8209, -0.5178, -0.7041,  0.0398],\n",
      "        [ 0.4574, -0.6146,  0.2662, -0.3334,  0.8791],\n",
      "        [ 0.0501,  1.6468, -1.0594,  0.1891,  2.2984],\n",
      "        [-0.1390,  0.2950, -0.7631,  0.5156,  1.3983]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4110],\n",
      "        [-0.3952],\n",
      "        [ 0.2119],\n",
      "        [ 3.1823]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1031,  0.7110, -0.1190,  0.8913,  1.4574],\n",
      "        [ 0.1033, -0.1798, -0.5089,  0.4314,  0.0219],\n",
      "        [-1.3731,  0.2392, -0.0879, -0.7294,  0.8373],\n",
      "        [-0.5387, -0.1004,  0.7147,  0.7165, -0.0005]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5025,  1.0505,  1.0837,  1.3888,  1.7782],\n",
      "        [ 0.4337, -1.2130, -1.0599, -0.1133, -1.3556],\n",
      "        [ 0.2694, -0.4658,  0.1550,  0.1275, -0.1223],\n",
      "        [ 0.2244,  1.3624,  0.9270,  0.8828,  1.8174]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1031,  0.7110, -0.1190,  0.8913,  1.4574],\n",
      "        [ 0.1033, -0.1798, -0.5089,  0.4314,  0.0219],\n",
      "        [-1.3731,  0.2392, -0.0879, -0.7294,  0.8373],\n",
      "        [-0.5387, -0.1004,  0.7147,  0.7165, -0.0005]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.2923],\n",
      "        [ 0.7237],\n",
      "        [-0.6902],\n",
      "        [ 1.0365]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1052,  0.0983, -0.4437, -0.8257, -0.6979],\n",
      "        [-1.0892,  0.8021, -0.6866, -0.1346,  0.8399],\n",
      "        [-0.0189,  1.2098, -0.8324,  1.6711,  0.9023],\n",
      "        [-0.4117,  1.0732,  0.1105,  0.5544, -0.0459]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7208, -0.7668, -1.3867, -0.5353, -1.3715],\n",
      "        [ 0.0753, -0.9537, -0.8232, -0.9012, -0.9261],\n",
      "        [ 0.2440, -0.4552, -0.1662, -0.3527,  0.1383],\n",
      "        [ 0.4024,  0.6990,  1.8105,  0.6917,  1.2920]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1052,  0.0983, -0.4437, -0.8257, -0.6979],\n",
      "        [-1.0892,  0.8021, -0.6866, -0.1346,  0.8399],\n",
      "        [-0.0189,  1.2098, -0.8324,  1.6711,  0.9023],\n",
      "        [-0.4117,  1.0732,  0.1105,  0.5544, -0.0459]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1424],\n",
      "        [-0.9383],\n",
      "        [-0.8816],\n",
      "        [ 1.1087]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1702, -0.3149, -0.3150,  0.7428, -0.5675],\n",
      "        [ 0.2460,  0.2736, -0.2899, -1.2884,  0.5150],\n",
      "        [-0.9852,  0.0920, -1.2430,  0.2381, -0.8591],\n",
      "        [-0.2478,  1.2933,  0.1380,  0.1053,  0.7803]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0233, -1.4304, -1.2383, -0.3987, -1.4073],\n",
      "        [-0.1972, -1.2075, -0.9595, -0.8960, -1.7697],\n",
      "        [ 0.6241, -0.0561,  0.0415, -0.1205,  0.7667],\n",
      "        [ 0.2146,  0.5062,  1.5575,  0.9611,  0.4886]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1702, -0.3149, -0.3150,  0.7428, -0.5675],\n",
      "        [ 0.2460,  0.2736, -0.2899, -1.2884,  0.5150],\n",
      "        [-0.9852,  0.0920, -1.2430,  0.2381, -0.8591],\n",
      "        [-0.2478,  1.2933,  0.1380,  0.1053,  0.7803]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5404],\n",
      "        [ 0.1423],\n",
      "        [-1.3589],\n",
      "        [ 1.2988]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9124, -0.2380, -2.1542,  1.8506, -0.2711],\n",
      "        [-0.6043, -0.2025, -0.6805, -0.4101, -0.0315],\n",
      "        [-1.2062,  1.5279,  0.4709, -0.3357, -0.2003],\n",
      "        [ 1.2690, -0.2919,  0.5193, -1.5909,  0.1820]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7449, -1.2205, -2.8456, -2.0132, -3.4982],\n",
      "        [-0.0721, -1.0706, -0.7252, -0.5892, -1.0793],\n",
      "        [ 0.4142,  0.4379,  0.4409,  1.0544,  0.8381],\n",
      "        [-0.3328,  0.6283,  0.7878,  1.4177,  0.2103]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9124, -0.2380, -2.1542,  1.8506, -0.2711],\n",
      "        [-0.6043, -0.2025, -0.6805, -0.4101, -0.0315],\n",
      "        [-1.2062,  1.5279,  0.4709, -0.3357, -0.2003],\n",
      "        [ 1.2690, -0.2919,  0.5193, -1.5909,  0.1820]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.0674],\n",
      "        [ 1.0295],\n",
      "        [-0.1448],\n",
      "        [-2.4137]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3545, -1.2727, -0.7050, -0.1852,  0.9864],\n",
      "        [-0.4875,  0.3765,  1.3010,  1.0516,  1.9791],\n",
      "        [ 0.5637, -0.1481, -0.2537,  0.7884,  0.4216],\n",
      "        [-0.4264,  0.2445,  1.6852, -0.4452, -0.8633]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0201,  0.3094,  0.0025, -0.4488, -0.0588],\n",
      "        [-0.0058, -1.3072, -1.2480, -1.0965, -1.6230],\n",
      "        [ 0.7497,  0.4731,  0.8156,  0.8094,  0.3510],\n",
      "        [-0.1481,  0.2032, -0.4605, -0.2272,  0.2150]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3545, -1.2727, -0.7050, -0.1852,  0.9864],\n",
      "        [-0.4875,  0.3765,  1.3010,  1.0516,  1.9791],\n",
      "        [ 0.5637, -0.1481, -0.2537,  0.7884,  0.4216],\n",
      "        [-0.4264,  0.2445,  1.6852, -0.4452, -0.8633]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3633],\n",
      "        [-6.4781],\n",
      "        [ 0.9316],\n",
      "        [-0.7477]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7995,  0.6755, -0.7526, -0.8424,  1.0152],\n",
      "        [-1.8040,  0.9213,  0.4679,  0.9623,  1.0046],\n",
      "        [ 0.7793,  0.2268,  1.2749, -0.8161, -1.1222],\n",
      "        [-0.7089,  0.8414, -1.8524,  0.2168,  0.2095]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4798, -0.3342, -0.4515, -0.0047, -0.1248],\n",
      "        [ 1.2018,  1.3781,  1.3328,  1.3215,  3.4615],\n",
      "        [ 0.5143, -0.3097,  0.0541, -0.0373, -0.1210],\n",
      "        [ 0.5667,  0.2507,  0.5841,  0.0528,  0.4735]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7995,  0.6755, -0.7526, -0.8424,  1.0152],\n",
      "        [-1.8040,  0.9213,  0.4679,  0.9623,  1.0046],\n",
      "        [ 0.7793,  0.2268,  1.2749, -0.8161, -1.1222],\n",
      "        [-0.7089,  0.8414, -1.8524,  0.2168,  0.2095]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3749],\n",
      "        [ 4.4746],\n",
      "        [ 0.5658],\n",
      "        [-1.1621]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5438,  1.1181,  0.6269,  0.1166, -0.4088],\n",
      "        [-0.9684, -1.2469,  0.8092, -1.1371, -0.0783],\n",
      "        [ 0.1179,  0.6086, -0.4224,  0.3013,  0.3003],\n",
      "        [-1.5622,  1.2754,  0.2607,  0.3639, -0.1496]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2990,  0.3576, -0.5068, -0.3129,  0.1214],\n",
      "        [ 0.6194,  0.0655, -0.1939, -0.6202,  0.0369],\n",
      "        [ 0.2344, -0.1965, -0.3904, -0.0821, -0.4999],\n",
      "        [ 0.7829,  0.3060,  0.4577,  0.1228,  1.3003]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5438,  1.1181,  0.6269,  0.1166, -0.4088],\n",
      "        [-0.9684, -1.2469,  0.8092, -1.1371, -0.0783],\n",
      "        [ 0.1179,  0.6086, -0.4224,  0.3013,  0.3003],\n",
      "        [-1.5622,  1.2754,  0.2607,  0.3639, -0.1496]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1666],\n",
      "        [-0.1362],\n",
      "        [-0.1019],\n",
      "        [-0.8634]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7756,  0.3180,  2.0349,  0.1977, -0.5896],\n",
      "        [ 1.8716,  0.7323, -0.3581,  1.5266,  0.0006],\n",
      "        [ 0.8906, -0.3359,  0.3944,  0.6061,  0.1093],\n",
      "        [ 0.9614, -0.3392, -0.3377,  1.0900,  0.9506]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4711,  0.4659,  0.3134,  0.3344,  0.5295],\n",
      "        [ 0.0136, -0.9793, -0.9937, -0.4211,  0.2018],\n",
      "        [-0.4934, -0.0198,  0.2095, -0.3800,  0.1505],\n",
      "        [ 0.9324,  1.0222,  1.3167,  0.4764,  1.1286]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7756,  0.3180,  2.0349,  0.1977, -0.5896],\n",
      "        [ 1.8716,  0.7323, -0.3581,  1.5266,  0.0006],\n",
      "        [ 0.8906, -0.3359,  0.3944,  0.6061,  0.1093],\n",
      "        [ 0.9614, -0.3392, -0.3377,  1.0900,  0.9506]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1745],\n",
      "        [-0.9785],\n",
      "        [-0.5640],\n",
      "        [ 1.6972]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0867,  1.3815, -0.1466,  0.2997,  0.3810],\n",
      "        [-0.4584,  1.3191,  0.3091, -1.1598,  0.3886],\n",
      "        [-0.3136,  1.4920, -0.6762, -1.9172,  1.0519],\n",
      "        [-0.4605,  0.9194, -1.8482, -0.5998,  0.8891]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3928,  0.2552, -0.1703, -0.1831, -0.0529],\n",
      "        [ 0.6073,  0.3539, -0.3013,  0.6899,  0.6820],\n",
      "        [ 0.4221, -0.3105, -0.1559,  0.0700,  0.1260],\n",
      "        [ 0.2333, -0.6046, -0.1576, -0.2059, -0.7148]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0867,  1.3815, -0.1466,  0.2997,  0.3810],\n",
      "        [-0.4584,  1.3191,  0.3091, -1.1598,  0.3886],\n",
      "        [-0.3136,  1.4920, -0.6762, -1.9172,  1.0519],\n",
      "        [-0.4605,  0.9194, -1.8482, -0.5998,  0.8891]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3366],\n",
      "        [-0.4398],\n",
      "        [-0.4919],\n",
      "        [-0.8841]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6315,  2.2408,  1.0807,  0.0324,  0.1731],\n",
      "        [ 1.5957,  0.5501,  0.0121, -1.3251,  0.3373],\n",
      "        [ 0.2778, -0.2891, -1.2948, -0.7813,  0.4567],\n",
      "        [ 1.3855,  0.9518,  1.7654,  0.3132, -1.3747]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1065, -0.0583,  0.1032, -0.3763,  0.3244],\n",
      "        [ 0.4304, -0.3642, -0.5654, -0.2009,  0.6113],\n",
      "        [ 0.3628, -0.1908,  0.1082, -0.2100,  0.2517],\n",
      "        [ 0.7757, -0.0254,  0.7088,  0.2077, -0.4223]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6315,  2.2408,  1.0807,  0.0324,  0.1731],\n",
      "        [ 1.5957,  0.5501,  0.0121, -1.3251,  0.3373],\n",
      "        [ 0.2778, -0.2891, -1.2948, -0.7813,  0.4567],\n",
      "        [ 1.3855,  0.9518,  1.7654,  0.3132, -1.3747]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0920],\n",
      "        [ 0.9520],\n",
      "        [ 0.2949],\n",
      "        [ 2.9475]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0502,  0.5453, -1.8724,  1.5270, -0.9373],\n",
      "        [ 0.1303, -0.3556,  1.7995, -0.1118,  0.5502],\n",
      "        [-0.8831, -0.4766, -0.9867,  1.3192,  0.7456],\n",
      "        [-1.7308,  2.0306,  0.5939,  1.1264,  1.8327]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3652, -0.1730,  0.5772, -0.0179,  0.5233],\n",
      "        [ 0.5105, -1.1590, -0.2402, -1.1799, -0.9109],\n",
      "        [ 0.4703, -0.1898, -0.2326,  0.3821,  0.0357],\n",
      "        [-0.8159, -1.3452, -1.1174, -1.4731, -2.0699]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0502,  0.5453, -1.8724,  1.5270, -0.9373],\n",
      "        [ 0.1303, -0.3556,  1.7995, -0.1118,  0.5502],\n",
      "        [-0.8831, -0.4766, -0.9867,  1.3192,  0.7456],\n",
      "        [-1.7308,  2.0306,  0.5939,  1.1264,  1.8327]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7111],\n",
      "        [-0.3229],\n",
      "        [ 0.4354],\n",
      "        [-7.4358]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3218,  1.2288, -0.2309,  0.7592, -1.0228],\n",
      "        [-2.5346, -0.0896,  1.1289,  0.9839,  1.8512],\n",
      "        [-0.0578,  0.5562, -0.1960, -0.4589,  0.5769],\n",
      "        [ 0.0147,  1.6116, -0.4437, -1.0392,  2.0623]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4495,  0.8166,  1.6490,  1.0714,  0.3308],\n",
      "        [ 0.1100, -0.6701, -0.1301, -0.1391, -1.0303],\n",
      "        [ 0.8624,  1.0051, -0.4194, -0.1583, -0.3650],\n",
      "        [ 1.9027,  2.2412,  1.7273,  1.7701,  2.6476]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3218,  1.2288, -0.2309,  0.7592, -1.0228],\n",
      "        [-2.5346, -0.0896,  1.1289,  0.9839,  1.8512],\n",
      "        [-0.0578,  0.5562, -0.1960, -0.4589,  0.5769],\n",
      "        [ 0.0147,  1.6116, -0.4437, -1.0392,  2.0623]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2423],\n",
      "        [-2.4099],\n",
      "        [ 0.4534],\n",
      "        [ 6.4939]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1564, -1.7328, -0.2680,  1.2870,  0.5707],\n",
      "        [-2.1584, -0.1158,  0.0963, -1.3479, -0.9960],\n",
      "        [-0.0092, -0.7081,  0.9897,  0.1026,  0.3544],\n",
      "        [ 1.0687,  0.4438,  0.1481, -1.2828, -0.7952]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0943,  0.1597, -0.6260,  0.2749,  0.5115],\n",
      "        [ 1.2823,  0.6744,  1.6660,  0.1341,  0.9784],\n",
      "        [-0.0691,  0.2689,  0.6092,  0.5104, -0.3333],\n",
      "        [-0.2500, -0.3263,  0.0933, -0.5567, -1.0882]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1564, -1.7328, -0.2680,  1.2870,  0.5707],\n",
      "        [-2.1584, -0.1158,  0.0963, -1.3479, -0.9960],\n",
      "        [-0.0092, -0.7081,  0.9897,  0.1026,  0.3544],\n",
      "        [ 1.0687,  0.4438,  0.1481, -1.2828, -0.7952]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5219],\n",
      "        [-3.8406],\n",
      "        [ 0.3474],\n",
      "        [ 1.1814]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5492,  1.4756, -0.0720,  1.7074,  1.0911],\n",
      "        [-2.1459, -0.3421,  1.0450,  0.1735, -0.1617],\n",
      "        [-2.4508, -0.0549, -0.8168, -0.0517,  0.7606],\n",
      "        [-1.1996, -0.5517,  1.0339,  0.1152, -0.8515]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0443,  0.0590,  0.6295,  0.3975,  0.5610],\n",
      "        [ 1.9462,  2.4391,  1.9590,  1.9094,  2.8517],\n",
      "        [-0.1532,  0.5719,  0.2065,  0.7368,  0.1366],\n",
      "        [-0.5024, -1.1925, -1.2061, -0.5163, -1.0890]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5492,  1.4756, -0.0720,  1.7074,  1.0911],\n",
      "        [-2.1459, -0.3421,  1.0450,  0.1735, -0.1617],\n",
      "        [-2.4508, -0.0549, -0.8168, -0.0517,  0.7606],\n",
      "        [-1.1996, -0.5517,  1.0339,  0.1152, -0.8515]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3570],\n",
      "        [-3.0936],\n",
      "        [ 0.2411],\n",
      "        [ 0.8814]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4634, -0.0432, -0.0706, -1.6570,  2.0787],\n",
      "        [ 1.3392, -0.2422,  0.8805, -0.2242,  0.5392],\n",
      "        [-0.6973, -0.2462,  0.7242,  0.0817, -0.1069],\n",
      "        [ 0.2991, -0.1320, -1.2350,  1.6188,  0.4638]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6898, -0.1070, -0.1714, -0.4910, -0.6343],\n",
      "        [ 0.3447,  0.4003, -0.4517, -0.1495, -0.1002],\n",
      "        [-0.1304,  0.8067,  0.3776,  0.5620,  0.0197],\n",
      "        [-0.4105, -0.8923, -1.1736, -1.7706, -1.5651]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4634, -0.0432, -0.0706, -1.6570,  2.0787],\n",
      "        [ 1.3392, -0.2422,  0.8805, -0.2242,  0.5392],\n",
      "        [-0.6973, -0.2462,  0.7242,  0.0817, -0.1069],\n",
      "        [ 0.2991, -0.1320, -1.2350,  1.6188,  0.4638]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5212],\n",
      "        [-0.0536],\n",
      "        [ 0.2096],\n",
      "        [-2.1478]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1678,  0.2593,  0.7382,  0.5574,  3.2889],\n",
      "        [ 2.0174,  0.7518, -2.1486,  0.5794,  0.9338],\n",
      "        [ 0.6059, -1.7411,  0.0841,  1.3536, -0.3930],\n",
      "        [-1.2257,  0.7655, -0.0506,  1.0576, -0.2850]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2775,  0.0751,  0.4590,  0.1533,  0.1755],\n",
      "        [ 0.1871, -0.3318, -0.2133,  0.4308,  0.2407],\n",
      "        [ 0.0861, -0.0316,  0.4222,  0.0125,  0.9114],\n",
      "        [-0.1656, -0.3299,  0.6880,  0.5428, -0.0158]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1678,  0.2593,  0.7382,  0.5574,  3.2889],\n",
      "        [ 2.0174,  0.7518, -2.1486,  0.5794,  0.9338],\n",
      "        [ 0.6059, -1.7411,  0.0841,  1.3536, -0.3930],\n",
      "        [-1.2257,  0.7655, -0.0506,  1.0576, -0.2850]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0676],\n",
      "        [ 1.0606],\n",
      "        [-0.1986],\n",
      "        [ 0.4942]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7252, -0.3788,  1.5048, -0.3009, -0.3310],\n",
      "        [-1.6017, -1.8590, -0.1795, -0.7474,  0.9538],\n",
      "        [ 3.0436,  0.1770,  2.3689,  1.0586,  1.7713],\n",
      "        [ 0.2104, -0.1779, -0.0815, -0.1206,  0.4478]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9002, -0.3118, -0.0481, -0.0725, -1.2940],\n",
      "        [-0.4021,  0.0949, -0.6305, -1.1848, -0.8114],\n",
      "        [ 0.0839,  0.3720,  0.7118, -0.1623,  0.3204],\n",
      "        [ 0.5768, -0.2020,  0.6724,  0.4213,  0.3944]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7252, -0.3788,  1.5048, -0.3009, -0.3310],\n",
      "        [-1.6017, -1.8590, -0.1795, -0.7474,  0.9538],\n",
      "        [ 3.0436,  0.1770,  2.3689,  1.0586,  1.7713],\n",
      "        [ 0.2104, -0.1779, -0.0815, -0.1206,  0.4478]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1571],\n",
      "        [ 0.6924],\n",
      "        [ 2.4029],\n",
      "        [ 0.2283]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0508,  0.7543,  0.1869,  0.5773,  1.6642],\n",
      "        [-0.5084,  0.6537,  1.4904,  0.2544,  0.0117],\n",
      "        [-0.5281,  2.2362, -0.4375, -1.1913, -0.0674],\n",
      "        [ 0.0380, -0.8623, -0.7470,  1.9110, -0.0061]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4693, -0.1444, -0.0003,  0.1771, -0.4234],\n",
      "        [-0.5966, -0.3567, -0.6577, -0.6343, -0.4735],\n",
      "        [-0.7153, -0.2860, -0.7878, -0.9147, -2.0892],\n",
      "        [ 0.8154,  0.2674,  0.3249,  0.3980,  0.2386]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0508,  0.7543,  0.1869,  0.5773,  1.6642],\n",
      "        [-0.5084,  0.6537,  1.4904,  0.2544,  0.0117],\n",
      "        [-0.5281,  2.2362, -0.4375, -1.1913, -0.0674],\n",
      "        [ 0.0380, -0.8623, -0.7470,  1.9110, -0.0061]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7352],\n",
      "        [-1.0769],\n",
      "        [ 1.3133],\n",
      "        [ 0.3169]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2106, -0.0011,  0.0787,  1.1610, -0.2641],\n",
      "        [ 0.0495,  1.9434,  0.0135,  0.0010, -1.7032],\n",
      "        [ 0.2482,  1.4028,  0.4365, -1.6863,  0.5004],\n",
      "        [ 2.4805, -0.6314, -0.2430,  1.7594, -0.0341]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0968,  0.0935, -0.3958,  0.1442,  0.5730],\n",
      "        [-0.0544,  0.2873, -0.2880, -0.1671, -0.0404],\n",
      "        [-1.1205, -0.9379, -0.7395, -1.1761, -2.7496],\n",
      "        [-0.1166, -0.0413, -0.0832, -0.2882,  0.5006]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2106, -0.0011,  0.0787,  1.1610, -0.2641],\n",
      "        [ 0.0495,  1.9434,  0.0135,  0.0010, -1.7032],\n",
      "        [ 0.2482,  1.4028,  0.4365, -1.6863,  0.5004],\n",
      "        [ 2.4805, -0.6314, -0.2430,  1.7594, -0.0341]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1323],\n",
      "        [ 0.6203],\n",
      "        [-1.3091],\n",
      "        [-0.7673]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2922, -0.0363,  0.8144,  0.6076, -0.5505],\n",
      "        [-1.1499,  0.2251,  1.3963, -2.4580,  0.5340],\n",
      "        [-1.2912,  0.3346,  0.7317,  0.6039,  0.8714],\n",
      "        [ 0.6381,  0.1927, -0.4787, -0.4959,  0.6536]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2782, -0.3672,  0.2317,  0.1935,  0.2758],\n",
      "        [-0.0444,  0.2944,  0.3963,  0.3190,  0.3749],\n",
      "        [-0.0882, -0.4736, -0.8462, -0.8663, -1.5436],\n",
      "        [-0.0714, -0.3513, -0.0379,  1.0893,  1.1310]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2922, -0.0363,  0.8144,  0.6076, -0.5505],\n",
      "        [-1.1499,  0.2251,  1.3963, -2.4580,  0.5340],\n",
      "        [-1.2912,  0.3346,  0.7317,  0.6039,  0.8714],\n",
      "        [ 0.6381,  0.1927, -0.4787, -0.4959,  0.6536]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0864],\n",
      "        [ 0.0868],\n",
      "        [-2.5319],\n",
      "        [ 0.1040]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3520,  0.5575,  0.9887,  0.2701,  0.7242],\n",
      "        [-0.3473, -1.8481, -0.6090,  1.5654,  0.8013],\n",
      "        [-1.5270, -0.6413, -0.3262, -0.0935,  0.3115],\n",
      "        [-0.8051, -0.4540,  0.5204, -0.6560, -0.3973]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1694,  0.0189,  0.0714,  0.6710,  0.0104],\n",
      "        [ 0.0353,  0.1255,  0.3672,  0.0525,  0.2344],\n",
      "        [ 0.2911,  0.1363,  0.0057,  0.4712,  0.4141],\n",
      "        [-0.1273, -0.3802, -0.4259,  0.2161,  0.1320]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3520,  0.5575,  0.9887,  0.2701,  0.7242],\n",
      "        [-0.3473, -1.8481, -0.6090,  1.5654,  0.8013],\n",
      "        [-1.5270, -0.6413, -0.3262, -0.0935,  0.3115],\n",
      "        [-0.8051, -0.4540,  0.5204, -0.6560, -0.3973]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4990],\n",
      "        [-0.1979],\n",
      "        [-0.4488],\n",
      "        [-0.1408]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0492,  0.6389,  1.1712,  0.7572, -1.0090],\n",
      "        [-1.2609, -0.0155, -0.5685,  0.7675,  0.5276],\n",
      "        [-2.3731, -1.0632,  0.2050,  0.2564, -0.1346],\n",
      "        [-1.8105,  2.1601,  2.1193, -0.9842,  1.9683]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2519, -0.1136,  0.0279,  0.6985,  0.0984],\n",
      "        [-0.0628, -0.1271, -0.3788,  0.1070, -0.3821],\n",
      "        [ 0.6384,  0.9859, -0.6350,  0.3427,  1.3641],\n",
      "        [ 0.1263,  0.4650,  0.6157,  0.2275,  0.2314]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0492,  0.6389,  1.1712,  0.7572, -1.0090],\n",
      "        [-1.2609, -0.0155, -0.5685,  0.7675,  0.5276],\n",
      "        [-2.3731, -1.0632,  0.2050,  0.2564, -0.1346],\n",
      "        [-1.8105,  2.1601,  2.1193, -0.9842,  1.9683]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6540],\n",
      "        [ 0.1770],\n",
      "        [-2.7890],\n",
      "        [ 2.3123]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1717,  1.3038,  1.6620,  1.1329,  0.1287],\n",
      "        [-0.6012, -1.1800,  1.8902,  0.8394, -0.7014],\n",
      "        [-0.6096,  1.1073,  1.4052, -0.5326, -0.5356],\n",
      "        [ 2.0703, -0.2126, -0.3464, -1.9840,  0.1360]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3086, -0.1481, -0.2043,  0.4717, -0.6859],\n",
      "        [-0.0466,  0.0250, -0.4431,  0.0228,  0.0165],\n",
      "        [ 1.9068,  1.2818,  0.8022,  0.6947,  1.0360],\n",
      "        [-0.9738, -0.8742, -0.5478, -1.6174, -1.4851]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1717,  1.3038,  1.6620,  1.1329,  0.1287],\n",
      "        [-0.6012, -1.1800,  1.8902,  0.8394, -0.7014],\n",
      "        [-0.6096,  1.1073,  1.4052, -0.5326, -0.5356],\n",
      "        [ 2.0703, -0.2126, -0.3464, -1.9840,  0.1360]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0336],\n",
      "        [-0.8315],\n",
      "        [ 0.4593],\n",
      "        [ 1.3664]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3778,  0.2964,  1.3479, -1.1883, -1.0758],\n",
      "        [ 1.3802,  0.5847,  0.3722, -0.2088, -0.1484],\n",
      "        [ 0.0406, -1.0613, -1.2582,  0.6629,  0.0468],\n",
      "        [ 1.1060,  0.9297,  0.0221,  1.0370, -0.1237]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6649,  0.4279,  0.0305, -0.0042, -0.0634],\n",
      "        [ 0.8186,  0.1500,  0.1169, -0.0036, -0.2795],\n",
      "        [ 0.7512,  2.1262,  1.4867,  0.8210,  1.4955],\n",
      "        [-0.9346, -1.2495, -1.2520, -1.7774, -2.4573]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3778,  0.2964,  1.3479, -1.1883, -1.0758],\n",
      "        [ 1.3802,  0.5847,  0.3722, -0.2088, -0.1484],\n",
      "        [ 0.0406, -1.0613, -1.2582,  0.6629,  0.0468],\n",
      "        [ 1.1060,  0.9297,  0.0221,  1.0370, -0.1237]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4925],\n",
      "        [ 1.3033],\n",
      "        [-3.4825],\n",
      "        [-3.7622]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7992, -0.0146,  0.8727,  0.5720, -0.2926],\n",
      "        [-1.6621, -0.1533, -0.0370, -0.7673,  1.3940],\n",
      "        [ 0.5075, -1.0254,  0.6324,  0.8482,  0.6645],\n",
      "        [-0.3255, -1.6653,  1.3941,  1.5914, -1.4984]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1641, -0.1641, -0.2074,  0.6891,  1.1572],\n",
      "        [-0.4704, -0.6866,  0.1074, -0.3036, -1.7908],\n",
      "        [ 1.7705,  2.9678,  2.2875,  1.5182,  3.0507],\n",
      "        [ 0.3770,  0.0986, -0.0532,  0.2686,  0.3152]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7992, -0.0146,  0.8727,  0.5720, -0.2926],\n",
      "        [-1.6621, -0.1533, -0.0370, -0.7673,  1.3940],\n",
      "        [ 0.5075, -1.0254,  0.6324,  0.8482,  0.6645],\n",
      "        [-0.3255, -1.6653,  1.3941,  1.5914, -1.4984]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2542],\n",
      "        [-1.3803],\n",
      "        [ 2.6168],\n",
      "        [-0.4059]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4107, -0.2391,  1.0042, -0.1338,  1.1446],\n",
      "        [ 0.4035,  0.3053,  0.3263,  0.6668,  0.6817],\n",
      "        [ 1.4933,  0.6465, -0.2567,  0.9055,  1.0853],\n",
      "        [ 0.3137,  0.2636, -0.0192,  0.5897, -1.7887]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2083, -1.0239, -1.0063,  0.2523,  0.1951],\n",
      "        [ 0.5313, -0.2852,  0.7545, -0.3449,  0.0971],\n",
      "        [ 0.1552, -0.4049,  0.0362,  0.6939, -0.5378],\n",
      "        [ 0.7243, -0.6625, -0.9313, -0.4002, -0.0871]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4107, -0.2391,  1.0042, -0.1338,  1.1446],\n",
      "        [ 0.4035,  0.3053,  0.3263,  0.6668,  0.6817],\n",
      "        [ 1.4933,  0.6465, -0.2567,  0.9055,  1.0853],\n",
      "        [ 0.3137,  0.2636, -0.0192,  0.5897, -1.7887]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6616],\n",
      "        [ 0.2097],\n",
      "        [ 0.0052],\n",
      "        [-0.0099]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9426,  0.0628,  1.5966, -0.5256, -2.5695],\n",
      "        [-1.9678, -1.3449, -0.4614,  1.1028,  0.2479],\n",
      "        [ 0.4883,  0.6700, -0.5375, -1.1416, -0.9540],\n",
      "        [-0.1192, -0.4035,  0.2128,  1.4100, -0.6423]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0668,  0.2363,  0.3364, -0.0754,  0.5799],\n",
      "        [ 0.3325, -0.3909, -0.1181, -0.1248,  0.3153],\n",
      "        [-0.1739, -0.1223, -0.6257,  0.0954, -0.1394],\n",
      "        [ 0.1372, -0.7027, -0.5008, -0.8629, -0.9432]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9426,  0.0628,  1.5966, -0.5256, -2.5695],\n",
      "        [-1.9678, -1.3449, -0.4614,  1.1028,  0.2479],\n",
      "        [ 0.4883,  0.6700, -0.5375, -1.1416, -0.9540],\n",
      "        [-0.1192, -0.4035,  0.2128,  1.4100, -0.6423]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0280],\n",
      "        [-0.1336],\n",
      "        [ 0.1935],\n",
      "        [-0.4503]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6419, -0.3634,  0.8729,  0.1158, -0.0776],\n",
      "        [-0.1283,  0.8673,  0.1384,  0.7136,  1.2410],\n",
      "        [ 0.3176,  0.9448,  1.9486, -1.0119, -1.1328],\n",
      "        [-0.5404,  1.4351, -0.5818,  0.0614, -0.0647]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7422,  0.2324,  1.2934,  0.6176,  0.9053],\n",
      "        [-0.1488,  0.3828, -0.9380,  0.1599,  0.3734],\n",
      "        [ 0.2424, -0.0770,  0.0850, -0.2666,  0.0728],\n",
      "        [-0.2906, -1.0942, -0.5397, -0.9989, -0.3800]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6419, -0.3634,  0.8729,  0.1158, -0.0776],\n",
      "        [-0.1283,  0.8673,  0.1384,  0.7136,  1.2410],\n",
      "        [ 0.3176,  0.9448,  1.9486, -1.0119, -1.1328],\n",
      "        [-0.5404,  1.4351, -0.5818,  0.0614, -0.0647]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5223],\n",
      "        [ 0.7988],\n",
      "        [ 0.3572],\n",
      "        [-1.1360]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8153,  0.8050,  0.9001,  0.6721,  0.1768],\n",
      "        [ 0.6200, -0.0880, -0.3557, -1.0121,  0.4056],\n",
      "        [-1.6030, -1.4340, -1.2939, -1.6502,  1.5370],\n",
      "        [-0.7074,  1.3520,  0.2088,  0.6707, -0.9904]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1354, -0.6438,  0.1222, -0.0985, -0.8848],\n",
      "        [-0.2799, -0.4190,  0.5267, -1.1741, -0.8552],\n",
      "        [ 0.1107,  0.4638,  0.3965,  0.0436, -0.3917],\n",
      "        [-0.0448, -1.3563, -1.1969, -1.0793, -0.0245]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8153,  0.8050,  0.9001,  0.6721,  0.1768],\n",
      "        [ 0.6200, -0.0880, -0.3557, -1.0121,  0.4056],\n",
      "        [-1.6030, -1.4340, -1.2939, -1.6502,  1.5370],\n",
      "        [-0.7074,  1.3520,  0.2088,  0.6707, -0.9904]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3851],\n",
      "        [ 0.5174],\n",
      "        [-2.0297],\n",
      "        [-2.7515]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7497, -1.4499,  0.0835,  0.3395, -0.1149],\n",
      "        [ 0.7090, -0.0493, -1.2172,  0.1906, -0.2964],\n",
      "        [ 2.0428,  0.4093,  1.1214,  1.8511,  1.3689],\n",
      "        [ 0.0258, -1.1319,  2.2400,  1.3135, -0.2444]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1671,  0.3244, -0.4229,  0.6669, -0.1752],\n",
      "        [ 0.1830,  0.1933,  0.9101,  0.3063, -0.4016],\n",
      "        [ 0.9290,  0.9935,  0.9962,  0.6183,  0.9975],\n",
      "        [ 0.9583,  0.4159,  1.3660,  0.6049,  1.1962]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7497, -1.4499,  0.0835,  0.3395, -0.1149],\n",
      "        [ 0.7090, -0.0493, -1.2172,  0.1906, -0.2964],\n",
      "        [ 2.0428,  0.4093,  1.1214,  1.8511,  1.3689],\n",
      "        [ 0.0258, -1.1319,  2.2400,  1.3135, -0.2444]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3844],\n",
      "        [-0.8101],\n",
      "        [ 5.9319],\n",
      "        [ 3.1158]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.3947,  1.3713, -1.1317,  1.0346, -0.2590],\n",
      "        [ 0.3255,  0.0422,  0.3518,  0.9952, -0.1425],\n",
      "        [-0.2878, -0.0499, -0.8624,  0.3757,  1.3598],\n",
      "        [ 0.6669, -0.0631,  0.4168, -1.3322, -0.2909]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6966, -0.5429, -0.9519, -0.4324, -0.1410],\n",
      "        [ 0.1083, -0.2207,  0.0561, -0.5511,  0.1369],\n",
      "        [-1.0449, -1.3213, -1.7534, -2.4134, -3.2216],\n",
      "        [-0.2841, -1.0281, -0.5288, -1.0792, -2.0026]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.3947,  1.3713, -1.1317,  1.0346, -0.2590],\n",
      "        [ 0.3255,  0.0422,  0.3518,  0.9952, -0.1425],\n",
      "        [-0.2878, -0.0499, -0.8624,  0.3757,  1.3598],\n",
      "        [ 0.6669, -0.0631,  0.4168, -1.3322, -0.2909]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5901],\n",
      "        [-0.5223],\n",
      "        [-3.4088],\n",
      "        [ 1.6753]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0922,  1.5720, -0.9585, -0.7406,  0.1832],\n",
      "        [-0.4693,  1.4852,  1.0835, -0.8335,  2.1422],\n",
      "        [ 0.8463, -0.9857,  0.9330, -0.3016, -0.4628],\n",
      "        [-0.6987,  0.1377, -1.9048, -0.1780, -0.9794]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5291, -0.5504, -0.3684, -0.6445, -1.6062],\n",
      "        [ 0.6351, -0.3676, -0.6256, -0.3970, -0.4416],\n",
      "        [-0.6148, -1.0855, -0.6858, -0.9770, -1.0929],\n",
      "        [-1.2837, -1.3835, -1.3864, -1.4263, -2.5205]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0922,  1.5720, -0.9585, -0.7406,  0.1832],\n",
      "        [-0.4693,  1.4852,  1.0835, -0.8335,  2.1422],\n",
      "        [ 0.8463, -0.9857,  0.9330, -0.3016, -0.4628],\n",
      "        [-0.6987,  0.1377, -1.9048, -0.1780, -0.9794]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3777],\n",
      "        [-2.1369],\n",
      "        [ 0.7104],\n",
      "        [ 6.0697]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2987, -0.8364,  0.3965, -0.0911, -0.5639],\n",
      "        [ 1.4491, -1.6412, -0.0727,  2.7418, -0.1825],\n",
      "        [-0.2561, -0.9312,  0.4435, -0.1183,  1.1209],\n",
      "        [ 0.8529, -1.2820, -0.9364,  1.3014,  1.9558]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1960, -0.3906,  0.1583, -0.7664, -0.4670],\n",
      "        [ 0.6402,  1.3494,  1.8159,  0.8001,  1.5355],\n",
      "        [-0.4758, -0.0896, -0.9000, -0.9014, -1.0781],\n",
      "        [-1.6407, -2.9085, -3.4907, -3.0108, -6.4815]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2987, -0.8364,  0.3965, -0.0911, -0.5639],\n",
      "        [ 1.4491, -1.6412, -0.0727,  2.7418, -0.1825],\n",
      "        [-0.2561, -0.9312,  0.4435, -0.1183,  1.1209],\n",
      "        [ 0.8529, -1.2820, -0.9364,  1.3014,  1.9558]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[  0.7812],\n",
      "        [  0.4944],\n",
      "        [ -1.2957],\n",
      "        [-10.9969]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6959, -0.2589,  0.5689, -0.6878,  1.3347],\n",
      "        [-1.1107,  1.2071,  1.3917,  2.0363, -1.4585],\n",
      "        [ 1.1754,  2.2565, -0.6647,  1.6962, -0.5048],\n",
      "        [-1.3273, -0.9461,  0.7850,  1.1971, -1.3251]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1341, -0.3824, -0.2894,  0.0616, -0.8105],\n",
      "        [ 0.9872, -0.2522,  0.7807, -0.0417,  0.8436],\n",
      "        [ 0.0764, -0.4783, -0.2878, -0.6697, -0.6155],\n",
      "        [-0.2938, -0.1476,  0.1123, -0.0802,  0.2569]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6959, -0.2589,  0.5689, -0.6878,  1.3347],\n",
      "        [-1.1107,  1.2071,  1.3917,  2.0363, -1.4585],\n",
      "        [ 1.1754,  2.2565, -0.6647,  1.6962, -0.5048],\n",
      "        [-1.3273, -0.9461,  0.7850,  1.1971, -1.3251]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4172],\n",
      "        [-1.6296],\n",
      "        [-1.6235],\n",
      "        [ 0.1814]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3241, -0.7883,  0.2636, -1.3705, -0.2994],\n",
      "        [ 0.1063,  2.0100, -0.5869, -0.5942,  0.7328],\n",
      "        [ 0.2395, -0.7120, -0.3701,  0.4007,  0.1237],\n",
      "        [ 1.0106,  1.7616,  0.0254, -0.1076,  0.9723]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3139, -0.4561,  0.0369,  0.1028,  0.9852],\n",
      "        [ 0.9530,  0.9640,  0.5977,  0.9601,  1.1259],\n",
      "        [ 0.8959,  0.7904,  1.0427,  0.0104,  1.9285],\n",
      "        [ 0.1651,  0.2670, -0.0494, -0.8061,  0.5485]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3241, -0.7883,  0.2636, -1.3705, -0.2994],\n",
      "        [ 0.1063,  2.0100, -0.5869, -0.5942,  0.7328],\n",
      "        [ 0.2395, -0.7120, -0.3701,  0.4007,  0.1237],\n",
      "        [ 1.0106,  1.7616,  0.0254, -0.1076,  0.9723]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1684],\n",
      "        [ 1.9428],\n",
      "        [-0.4913],\n",
      "        [ 1.2559]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0037,  1.3207,  1.3538,  1.1926,  0.8721],\n",
      "        [-0.2601,  0.3576,  1.7652,  1.1276, -0.6114],\n",
      "        [ 1.9909,  1.2915,  0.0091, -0.0757, -0.9140],\n",
      "        [ 0.8827, -1.3536, -0.0567,  0.1351,  1.4042]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2847, -0.3624, -0.1752, -0.2493, -0.3906],\n",
      "        [ 0.6423, -0.6304,  0.1394, -0.2746, -0.3632],\n",
      "        [ 0.6823, -0.0630,  0.2980,  0.0812,  1.1034],\n",
      "        [-0.0975, -0.2688,  0.3202,  0.1079, -0.9713]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0037,  1.3207,  1.3538,  1.1926,  0.8721],\n",
      "        [-0.2601,  0.3576,  1.7652,  1.1276, -0.6114],\n",
      "        [ 1.9909,  1.2915,  0.0091, -0.0757, -0.9140],\n",
      "        [ 0.8827, -1.3536, -0.0567,  0.1351,  1.4042]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9241],\n",
      "        [-0.2341],\n",
      "        [ 0.2650],\n",
      "        [-1.0895]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7497, -1.8926,  1.3115,  0.8234,  0.4711],\n",
      "        [-0.4417,  1.4701,  0.4013, -0.7825,  0.3405],\n",
      "        [ 0.5662,  0.2421,  0.6735,  0.9970, -1.1753],\n",
      "        [-1.5150, -0.7605,  1.3198, -1.0079,  0.2083]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6727,  0.6312,  0.1965, -0.0116,  1.7634],\n",
      "        [ 0.2960, -0.6885, -0.1322,  0.0543,  0.5474],\n",
      "        [-0.3721, -0.1029, -0.4884, -0.9733, -1.3068],\n",
      "        [ 0.2760,  0.0460, -0.0513, -0.0854,  0.1692]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7497, -1.8926,  1.3115,  0.8234,  0.4711],\n",
      "        [-0.4417,  1.4701,  0.4013, -0.7825,  0.3405],\n",
      "        [ 0.5662,  0.2421,  0.6735,  0.9970, -1.1753],\n",
      "        [-1.5150, -0.7605,  1.3198, -1.0079,  0.2083]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3887],\n",
      "        [-1.0521],\n",
      "        [ 0.0008],\n",
      "        [-0.3994]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1319,  0.0391,  2.0042,  0.0539, -1.7972],\n",
      "        [ 0.4059,  0.2430, -0.4380,  0.2946,  1.0881],\n",
      "        [-0.8401, -0.5359,  1.3381, -0.5968, -0.8376],\n",
      "        [ 2.7818,  1.3293,  0.1018, -0.9159, -1.0720]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2856,  0.1235,  0.3552, -0.3452,  0.5117],\n",
      "        [ 0.7711,  1.0095,  0.3544,  0.7744,  0.5454],\n",
      "        [ 0.2818, -0.5335,  0.4364, -0.6109, -1.6137],\n",
      "        [ 0.0662,  0.0218,  0.5869, -0.6643, -0.1095]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1319,  0.0391,  2.0042,  0.0539, -1.7972],\n",
      "        [ 0.4059,  0.2430, -0.4380,  0.2946,  1.0881],\n",
      "        [-0.8401, -0.5359,  1.3381, -0.5968, -0.8376],\n",
      "        [ 2.7818,  1.3293,  0.1018, -0.9159, -1.0720]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5449],\n",
      "        [ 1.2247],\n",
      "        [ 2.3494],\n",
      "        [ 0.9987]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0655,  1.5009, -0.1145, -2.0316, -0.0898],\n",
      "        [ 1.6832,  0.3108, -1.0135, -0.5757,  0.7828],\n",
      "        [-0.1267,  0.3114, -0.1162,  0.4585,  1.4599],\n",
      "        [-0.4052,  0.6011,  1.3418,  1.2694, -0.0566]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3899, -0.4060, -0.5161, -0.4407, -0.5666],\n",
      "        [ 0.2165, -0.2366,  0.4662,  0.0473, -0.0693],\n",
      "        [-0.5610, -1.5968, -1.5679, -1.9693, -2.3215],\n",
      "        [-0.3568, -0.1512, -0.0685, -0.6746, -1.1568]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0655,  1.5009, -0.1145, -2.0316, -0.0898],\n",
      "        [ 1.6832,  0.3108, -1.0135, -0.5757,  0.7828],\n",
      "        [-0.1267,  0.3114, -0.1162,  0.4585,  1.4599],\n",
      "        [-0.4052,  0.6011,  1.3418,  1.2694, -0.0566]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4214],\n",
      "        [-0.2630],\n",
      "        [-4.5358],\n",
      "        [-0.8291]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4664,  0.4666,  0.3800, -0.6569, -0.4760],\n",
      "        [-1.3728, -0.1174, -0.0717, -1.6184,  0.1334],\n",
      "        [-1.9422, -0.4735,  1.0218,  0.6304, -0.5843],\n",
      "        [-0.2575,  1.6441,  0.7024,  0.2615,  0.2898]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3725, -0.5034, -0.8177, -0.6800,  0.2368],\n",
      "        [ 0.2880,  0.1389, -0.1833,  0.2071, -0.1525],\n",
      "        [ 1.0730,  0.8635,  0.6932,  0.4222,  1.1935],\n",
      "        [ 0.4929,  0.0544, -0.6209,  0.1650,  0.3929]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4664,  0.4666,  0.3800, -0.6569, -0.4760],\n",
      "        [-1.3728, -0.1174, -0.0717, -1.6184,  0.1334],\n",
      "        [-1.9422, -0.4735,  1.0218,  0.6304, -0.5843],\n",
      "        [-0.2575,  1.6441,  0.7024,  0.2615,  0.2898]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3853],\n",
      "        [-0.7540],\n",
      "        [-2.2158],\n",
      "        [-0.3166]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8439,  1.1546,  0.1603, -0.4171, -1.4210],\n",
      "        [ 1.5564, -1.9435,  1.2061,  0.0967,  0.1620],\n",
      "        [-1.3109,  0.6781,  1.0417, -0.7453,  0.1805],\n",
      "        [-0.6369, -0.1337, -0.7580,  0.4175,  0.9383]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5872, -0.1163, -0.6019, -0.5463, -0.6849],\n",
      "        [ 0.2101,  0.1970,  0.2768,  0.0142,  0.3509],\n",
      "        [ 1.2602,  1.2331,  1.3112,  0.2792,  1.4235],\n",
      "        [-0.0527, -0.4043, -0.0663, -0.0773, -0.9736]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8439,  1.1546,  0.1603, -0.4171, -1.4210],\n",
      "        [ 1.5564, -1.9435,  1.2061,  0.0967,  0.1620],\n",
      "        [-1.3109,  0.6781,  1.0417, -0.7453,  0.1805],\n",
      "        [-0.6369, -0.1337, -0.7580,  0.4175,  0.9383]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4747],\n",
      "        [ 0.3362],\n",
      "        [ 0.5989],\n",
      "        [-0.8079]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9974, -0.7920, -0.2205, -0.4243,  1.2265],\n",
      "        [-0.5964,  1.5179,  0.2804,  1.0157,  0.5217],\n",
      "        [-1.7500,  1.0436, -1.8582,  2.2890,  2.2525],\n",
      "        [-0.1793,  0.1874, -0.6213, -0.8605,  1.2252]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1429, -0.7560, -0.4541, -0.2021, -0.9380],\n",
      "        [ 0.3113,  0.0326, -0.1566,  0.8436,  0.3723],\n",
      "        [ 1.0356,  1.6619,  1.2754,  1.1808,  1.9950],\n",
      "        [ 0.0718, -0.0399, -0.0596, -0.7952,  0.5555]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9974, -0.7920, -0.2205, -0.4243,  1.2265],\n",
      "        [-0.5964,  1.5179,  0.2804,  1.0157,  0.5217],\n",
      "        [-1.7500,  1.0436, -1.8582,  2.2890,  2.2525],\n",
      "        [-0.1793,  0.1874, -0.6213, -0.8605,  1.2252]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2233],\n",
      "        [ 0.8710],\n",
      "        [ 4.7487],\n",
      "        [ 1.3815]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5068, -0.3364,  0.1022, -0.2621, -0.6559],\n",
      "        [ 0.1614, -0.3509,  0.2046, -1.9602,  0.5770],\n",
      "        [-1.8847,  0.3116, -0.2944,  0.7925,  0.1669],\n",
      "        [-0.0250,  1.7539, -0.9793, -0.1562,  0.3290]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4463, -0.4255, -0.0221, -0.8220, -0.2073],\n",
      "        [-0.2956,  0.2310,  0.7187,  0.2082, -0.5293],\n",
      "        [-0.6598, -0.6617, -0.9966, -0.7054, -1.6061],\n",
      "        [-0.3031, -0.6675, -0.3959, -0.4132, -0.7658]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5068, -0.3364,  0.1022, -0.2621, -0.6559],\n",
      "        [ 0.1614, -0.3509,  0.2046, -1.9602,  0.5770],\n",
      "        [-1.8847,  0.3116, -0.2944,  0.7925,  0.1669],\n",
      "        [-0.0250,  1.7539, -0.9793, -0.1562,  0.3290]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1648],\n",
      "        [-0.6952],\n",
      "        [ 0.5036],\n",
      "        [-0.9629]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4163, -0.1833, -0.0022,  1.2866,  2.4275],\n",
      "        [ 1.4723,  0.8271,  1.3361,  0.7595,  1.7205],\n",
      "        [-0.6243, -0.4438, -0.4627,  0.1087, -0.6267],\n",
      "        [ 2.0701,  0.6040, -0.0719, -0.8016,  1.0537]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8250, -0.8464, -1.0578, -0.0520, -1.3435],\n",
      "        [ 0.1079,  0.2991,  0.0376,  0.4137,  0.9735],\n",
      "        [-0.2112, -0.2828, -0.4257, -0.9668, -1.6669],\n",
      "        [ 0.1738, -0.3650, -0.4605, -0.3305, -0.5624]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4163, -0.1833, -0.0022,  1.2866,  2.4275],\n",
      "        [ 1.4723,  0.8271,  1.3361,  0.7595,  1.7205],\n",
      "        [-0.6243, -0.4438, -0.4627,  0.1087, -0.6267],\n",
      "        [ 2.0701,  0.6040, -0.0719, -0.8016,  1.0537]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.3394],\n",
      "        [ 2.4456],\n",
      "        [ 1.3940],\n",
      "        [-0.1554]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0936, -0.7949,  1.4233, -0.6713,  0.3613],\n",
      "        [-0.4429, -1.4909, -0.1510, -1.0792,  1.5783],\n",
      "        [-0.9908,  0.3466, -0.9777, -0.5381, -1.2592],\n",
      "        [-0.9801,  2.1171,  0.8017,  0.5223,  1.6715]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3662,  1.5410,  0.8501,  1.3694,  1.3222],\n",
      "        [-0.4115, -0.8910, -1.0248, -1.0402, -1.5595],\n",
      "        [-1.0292, -0.4492, -1.0953, -0.7754, -2.3708],\n",
      "        [ 0.5147, -0.7787,  0.1896, -0.6940, -0.4318]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0936, -0.7949,  1.4233, -0.6713,  0.3613],\n",
      "        [-0.4429, -1.4909, -0.1510, -1.0792,  1.5783],\n",
      "        [-0.9908,  0.3466, -0.9777, -0.5381, -1.2592],\n",
      "        [-0.9801,  2.1171,  0.8017,  0.5223,  1.6715]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0375],\n",
      "        [ 0.3267],\n",
      "        [ 5.3374],\n",
      "        [-3.0853]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8039, -1.0674, -0.0888, -0.5976,  1.5663],\n",
      "        [-0.0566, -0.0170,  0.0179,  0.1413,  0.1764],\n",
      "        [-0.6355,  0.8591, -0.9560, -1.0600, -0.1878],\n",
      "        [ 0.6658,  0.5822,  1.7325,  0.0026, -0.1745]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7644,  1.2055,  1.3237,  0.4180,  1.4773],\n",
      "        [-0.0008, -0.8541, -0.6834, -0.6664, -1.2482],\n",
      "        [-2.0455, -2.3936, -3.1020, -3.4458, -5.4679],\n",
      "        [ 2.1152,  0.8290,  1.1370,  0.3990,  1.8857]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8039, -1.0674, -0.0888, -0.5976,  1.5663],\n",
      "        [-0.0566, -0.0170,  0.0179,  0.1413,  0.1764],\n",
      "        [-0.6355,  0.8591, -0.9560, -1.0600, -0.1878],\n",
      "        [ 0.6658,  0.5822,  1.7325,  0.0026, -0.1745]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2743],\n",
      "        [-0.3120],\n",
      "        [ 6.8885],\n",
      "        [ 3.5328]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1121,  1.3154, -1.5458, -0.9253, -0.5635],\n",
      "        [-2.3829,  0.1908, -0.2109,  0.4805,  2.3309],\n",
      "        [ 0.4064, -0.5250, -1.4799,  1.3190, -0.1667],\n",
      "        [ 1.0588, -1.5094, -0.4161,  0.2897,  0.3592]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1967,  0.1047,  0.3702,  0.5625,  1.1221],\n",
      "        [-0.5815, -0.1575,  0.4445, -0.3022, -0.7432],\n",
      "        [ 0.1774,  0.2702,  0.1347,  0.2067,  0.2084],\n",
      "        [-0.2867, -0.9383, -0.0621, -0.4765, -1.3487]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1121,  1.3154, -1.5458, -0.9253, -0.5635],\n",
      "        [-2.3829,  0.1908, -0.2109,  0.4805,  2.3309],\n",
      "        [ 0.4064, -0.5250, -1.4799,  1.3190, -0.1667],\n",
      "        [ 1.0588, -1.5094, -0.4161,  0.2897,  0.3592]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5652],\n",
      "        [-0.6156],\n",
      "        [-0.0312],\n",
      "        [ 0.5159]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2921,  0.1386, -0.4364, -0.2496,  0.0490],\n",
      "        [-0.9321, -0.7809,  0.9059,  0.9888, -0.6237],\n",
      "        [ 0.9469, -0.7410, -1.4912,  0.5952, -0.3146],\n",
      "        [-0.4953, -0.9798, -1.0608, -0.3119,  1.5219]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4681,  0.8212,  1.3643,  1.1365,  1.2746],\n",
      "        [ 0.0734, -0.1793,  0.7466,  0.0245, -0.1989],\n",
      "        [ 0.0672,  0.3252, -0.4049,  0.0255,  0.1141],\n",
      "        [-0.4970, -0.6296,  0.1562, -0.5415, -1.4161]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2921,  0.1386, -0.4364, -0.2496,  0.0490],\n",
      "        [-0.9321, -0.7809,  0.9059,  0.9888, -0.6237],\n",
      "        [ 0.9469, -0.7410, -1.4912,  0.5952, -0.3146],\n",
      "        [-0.4953, -0.9798, -1.0608, -0.3119,  1.5219]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3077],\n",
      "        [ 0.8962],\n",
      "        [ 0.4057],\n",
      "        [-1.2889]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2356,  0.0302, -0.5149,  0.1670, -0.0770],\n",
      "        [-0.9018,  0.2276,  0.0377, -0.8943, -1.0335],\n",
      "        [-0.4286,  0.8672,  0.3720,  0.8018,  0.0542],\n",
      "        [-1.4386, -1.4661,  1.1078,  0.7592,  0.0735]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6738,  1.8771,  1.6030,  1.9782,  2.1604],\n",
      "        [-0.1760, -0.6173,  0.1275, -0.1012,  0.0896],\n",
      "        [ 0.1602,  0.8007,  0.3900,  0.3335,  0.6760],\n",
      "        [ 0.2402,  0.3326, -0.1520,  0.2170,  0.3609]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2356,  0.0302, -0.5149,  0.1670, -0.0770],\n",
      "        [-0.9018,  0.2276,  0.0377, -0.8943, -1.0335],\n",
      "        [-0.4286,  0.8672,  0.3720,  0.8018,  0.0542],\n",
      "        [-1.4386, -1.4661,  1.1078,  0.7592,  0.0735]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7634],\n",
      "        [ 0.0209],\n",
      "        [ 1.0749],\n",
      "        [-0.8102]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4168, -0.5898,  0.0590,  1.3324,  0.2928],\n",
      "        [-1.2887, -0.8734,  0.3786,  0.3010, -0.5968],\n",
      "        [ 0.2484,  1.3098, -0.2772,  0.6231,  0.1971],\n",
      "        [-0.5029,  0.1585, -1.6234, -0.3761, -1.2993]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2084,  0.3898, -0.2452,  0.6931, -0.3599],\n",
      "        [-0.4466, -0.5806,  0.2887, -0.0086, -0.3847],\n",
      "        [-0.5903, -0.7070,  0.3072,  0.0975, -0.9812],\n",
      "        [ 0.7454,  0.5183,  0.6701,  0.2462,  0.3300]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4168, -0.5898,  0.0590,  1.3324,  0.2928],\n",
      "        [-1.2887, -0.8734,  0.3786,  0.3010, -0.5968],\n",
      "        [ 0.2484,  1.3098, -0.2772,  0.6231,  0.1971],\n",
      "        [-0.5029,  0.1585, -1.6234, -0.3761, -1.2993]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2785],\n",
      "        [ 1.4189],\n",
      "        [-1.2904],\n",
      "        [-1.9019]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4438, -0.3453, -0.3162, -0.9596,  0.3899],\n",
      "        [ 0.0321,  1.1300,  0.9080,  0.0130,  1.8411],\n",
      "        [ 0.6405,  0.4012, -0.6337, -0.4007, -0.7057],\n",
      "        [-0.7001, -0.2381,  0.1744,  0.4220, -1.7995]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0019,  0.3611, -1.0652,  0.7745,  0.3168],\n",
      "        [-0.6529, -0.0849,  0.0790, -0.8868, -1.7004],\n",
      "        [ 0.3862, -0.1296,  1.5171,  0.4435,  0.8517],\n",
      "        [ 0.8582,  1.5187,  0.9583,  0.6584,  1.5429]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4438, -0.3453, -0.3162, -0.9596,  0.3899],\n",
      "        [ 0.0321,  1.1300,  0.9080,  0.0130,  1.8411],\n",
      "        [ 0.6405,  0.4012, -0.6337, -0.4007, -0.7057],\n",
      "        [-0.7001, -0.2381,  0.1744,  0.4220, -1.7995]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4048],\n",
      "        [-3.1874],\n",
      "        [-1.5448],\n",
      "        [-3.2940]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2987,  1.3744,  1.2767, -1.5252, -0.7322],\n",
      "        [-1.2776,  0.3047,  0.0836,  1.3458, -0.0066],\n",
      "        [-0.5141, -0.2439, -0.2228, -1.3533, -0.9931],\n",
      "        [-1.1755, -1.0401,  0.3953,  0.9896, -0.6907]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0819,  0.6808,  0.3429,  0.9015,  0.4773],\n",
      "        [ 0.5764,  1.3665,  1.1105,  0.8259,  1.1457],\n",
      "        [ 0.5950,  1.5990,  1.7041,  0.5493,  1.4770],\n",
      "        [ 1.5358,  1.8642,  1.7439,  1.6176,  3.0019]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2987,  1.3744,  1.2767, -1.5252, -0.7322],\n",
      "        [-1.2776,  0.3047,  0.0836,  1.3458, -0.0066],\n",
      "        [-0.5141, -0.2439, -0.2228, -1.3533, -0.9931],\n",
      "        [-1.1755, -1.0401,  0.3953,  0.9896, -0.6907]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4575],\n",
      "        [ 0.8767],\n",
      "        [-3.2856],\n",
      "        [-3.5276]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3370, -0.2932, -0.8838, -0.8188,  0.9491],\n",
      "        [ 0.3345, -0.5444,  0.9638,  1.0043,  1.3722],\n",
      "        [-1.2099,  1.3208, -0.5558, -0.4142, -1.1258],\n",
      "        [ 0.1822,  0.0727,  0.3540,  0.2718,  1.2607]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4864, -0.7071, -0.1956,  0.7702,  0.0113],\n",
      "        [-0.2316,  0.3617,  0.3962,  0.0917,  0.9130],\n",
      "        [ 1.7062,  1.8717,  1.6786,  1.5146,  2.4135],\n",
      "        [ 0.2209,  0.1221,  0.4173,  0.0276,  0.4452]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3370, -0.2932, -0.8838, -0.8188,  0.9491],\n",
      "        [ 0.3345, -0.5444,  0.9638,  1.0043,  1.3722],\n",
      "        [-1.2099,  1.3208, -0.5558, -0.4142, -1.1258],\n",
      "        [ 0.1822,  0.0727,  0.3540,  0.2718,  1.2607]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8901],\n",
      "        [ 1.4524],\n",
      "        [-3.8696],\n",
      "        [ 0.7657]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0451,  1.0109,  1.5219, -0.7520,  0.0622],\n",
      "        [-2.6734,  0.9614,  0.9191, -0.4437, -0.3249],\n",
      "        [-0.0170, -0.2113,  0.3255, -0.0362, -0.1850],\n",
      "        [-0.0874, -1.7950,  0.1416,  0.3816, -0.4599]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1986,  0.6555,  0.3556,  0.3380,  1.0306],\n",
      "        [ 0.0011, -1.0323, -0.8725, -0.4954, -0.7717],\n",
      "        [ 0.4724,  0.0633, -0.4165,  0.3560, -0.0515],\n",
      "        [-0.5004, -0.0704, -0.2993,  0.2140,  0.1988]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0451,  1.0109,  1.5219, -0.7520,  0.0622],\n",
      "        [-2.6734,  0.9614,  0.9191, -0.4437, -0.3249],\n",
      "        [-0.0170, -0.2113,  0.3255, -0.0362, -0.1850],\n",
      "        [-0.0874, -1.7950,  0.1416,  0.3816, -0.4599]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0227],\n",
      "        [-1.3267],\n",
      "        [-0.1604],\n",
      "        [ 0.1180]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2779, -0.0756, -0.5981, -1.3357, -0.3752],\n",
      "        [-0.0868,  0.0774,  1.3331, -1.3237, -0.1763],\n",
      "        [ 0.7117, -0.9050, -0.7780, -0.7198,  0.8131],\n",
      "        [ 0.1736,  0.9883,  1.0320, -2.0069,  2.5157]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6932,  0.0705, -0.3692, -0.5059, -0.3544],\n",
      "        [ 0.1535, -0.0564,  0.0107,  0.1232,  0.7053],\n",
      "        [-0.1255, -0.0509,  0.2001,  0.1689,  0.2341],\n",
      "        [ 0.3482, -0.0350, -1.0317,  1.0267,  0.4099]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2779, -0.0756, -0.5981, -1.3357, -0.3752],\n",
      "        [-0.0868,  0.0774,  1.3331, -1.3237, -0.1763],\n",
      "        [ 0.7117, -0.9050, -0.7780, -0.7198,  0.8131],\n",
      "        [ 0.1736,  0.9883,  1.0320, -2.0069,  2.5157]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2169],\n",
      "        [-0.2908],\n",
      "        [-0.1301],\n",
      "        [-2.0683]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9082,  0.5033,  2.0594, -1.1902,  0.6124],\n",
      "        [-1.6881,  0.2619, -0.5964,  1.3209, -0.7656],\n",
      "        [-0.8182,  0.3368,  0.7347, -0.9493,  0.9528],\n",
      "        [ 0.0531,  0.8798,  1.1851,  0.1469, -1.2246]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2646, -0.1553,  0.1314, -0.6146, -1.0711],\n",
      "        [-0.0045, -0.6073, -0.5563,  0.2701, -0.7549],\n",
      "        [-0.1482,  0.1563, -0.2437,  0.0124,  0.2652],\n",
      "        [ 0.7652,  0.9045,  0.3714,  0.4331,  1.2189]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9082,  0.5033,  2.0594, -1.1902,  0.6124],\n",
      "        [-1.6881,  0.2619, -0.5964,  1.3209, -0.7656],\n",
      "        [-0.8182,  0.3368,  0.7347, -0.9493,  0.9528],\n",
      "        [ 0.0531,  0.8798,  1.1851,  0.1469, -1.2246]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0276],\n",
      "        [ 1.1150],\n",
      "        [ 0.2358],\n",
      "        [-0.1524]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7519, -0.4842,  2.5521,  1.0249,  1.3850],\n",
      "        [ 0.1523, -1.1283, -0.1834,  1.3738, -1.1077],\n",
      "        [ 1.4039, -1.0013, -1.4997,  1.1092, -0.0793],\n",
      "        [-1.9136,  0.0061,  0.2843,  0.8376, -0.8173]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0276, -0.3775,  0.4081, -0.0510, -0.4402],\n",
      "        [-0.1047, -0.6466, -0.4307, -0.9354, -1.2724],\n",
      "        [-0.2314,  0.0854, -0.0307, -0.0610,  0.2199],\n",
      "        [ 0.9272,  1.5684,  0.4486,  1.1413,  0.8755]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7519, -0.4842,  2.5521,  1.0249,  1.3850],\n",
      "        [ 0.1523, -1.1283, -0.1834,  1.3738, -1.1077],\n",
      "        [ 1.4039, -1.0013, -1.4997,  1.1092, -0.0793],\n",
      "        [-1.9136,  0.0061,  0.2843,  0.8376, -0.8173]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5830],\n",
      "        [ 0.9169],\n",
      "        [-0.4495],\n",
      "        [-1.3966]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8322, -0.1902,  1.8975,  1.5195,  0.8070],\n",
      "        [ 0.2997, -1.1071,  0.0574, -1.4662,  0.1235],\n",
      "        [ 0.3302,  0.0582,  1.0028,  0.1123, -0.8053],\n",
      "        [-1.9739, -0.8477, -2.0731, -0.4037, -0.0417]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4274,  0.0197,  0.3559,  0.0305, -0.6926],\n",
      "        [-0.8118, -0.5620, -0.9243, -0.7802, -1.0116],\n",
      "        [ 0.1861,  0.1771, -0.4042,  0.0032,  0.1788],\n",
      "        [ 1.1082,  0.9313,  1.4180,  0.4343,  1.8016]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8322, -0.1902,  1.8975,  1.5195,  0.8070],\n",
      "        [ 0.2997, -1.1071,  0.0574, -1.4662,  0.1235],\n",
      "        [ 0.3302,  0.0582,  1.0028,  0.1123, -0.8053],\n",
      "        [-1.9739, -0.8477, -2.0731, -0.4037, -0.0417]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9421],\n",
      "        [ 1.3448],\n",
      "        [-0.4771],\n",
      "        [-6.1668]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8709,  0.2018,  1.4633,  2.3664,  0.2898],\n",
      "        [ 0.9237,  1.6037,  0.4639, -1.0708,  0.4981],\n",
      "        [-0.8317, -0.0443,  1.0263, -0.8561,  1.3719],\n",
      "        [-0.2680,  0.0143,  0.2350,  0.8259, -2.0393]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4032, -0.5044,  0.2755, -0.4805, -1.6490],\n",
      "        [-0.5346, -0.7483, -0.6062, -1.4170, -1.6131],\n",
      "        [ 0.1346, -0.0731,  0.2798, -0.2015, -1.0536],\n",
      "        [ 0.3732,  0.0275,  0.0646,  0.4249,  0.5666]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8709,  0.2018,  1.4633,  2.3664,  0.2898],\n",
      "        [ 0.9237,  1.6037,  0.4639, -1.0708,  0.4981],\n",
      "        [-0.8317, -0.0443,  1.0263, -0.8561,  1.3719],\n",
      "        [-0.2680,  0.0143,  0.2350,  0.8259, -2.0393]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6647],\n",
      "        [-1.2612],\n",
      "        [-1.0946],\n",
      "        [-0.8890]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5496,  1.7243,  0.4867, -1.4831, -0.9578],\n",
      "        [ 0.9677, -0.2610,  0.7133, -0.4746,  1.9689],\n",
      "        [-2.1337, -0.5702,  0.6648, -1.1775, -0.1621],\n",
      "        [-0.7548,  1.6611, -1.1599, -0.7984, -0.1497]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4510,  0.3775,  0.5458,  0.7980,  0.7716],\n",
      "        [-0.7384, -0.5747, -1.1040, -0.4936, -1.9401],\n",
      "        [ 0.3889, -0.0560,  0.6947,  0.2272,  0.5542],\n",
      "        [ 0.4970,  0.4440,  0.2582,  0.1674, -0.0680]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5496,  1.7243,  0.4867, -1.4831, -0.9578],\n",
      "        [ 0.9677, -0.2610,  0.7133, -0.4746,  1.9689],\n",
      "        [-2.1337, -0.5702,  0.6648, -1.1775, -0.1621],\n",
      "        [-0.7548,  1.6611, -1.1599, -0.7984, -0.1497]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2537],\n",
      "        [-4.9377],\n",
      "        [-0.6933],\n",
      "        [-0.0606]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9161, -0.0915, -0.1090,  1.3027,  1.2185],\n",
      "        [ 0.0759,  1.1306, -0.0004,  0.6278, -1.5187],\n",
      "        [-0.6355, -0.1000, -0.6859,  0.0824,  2.3768],\n",
      "        [-0.0406, -0.6543, -0.0596,  0.6498, -1.9000]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9593,  1.5755,  0.0457, -0.2312,  1.1783],\n",
      "        [ 1.1349,  1.5608,  1.7111,  1.7076,  1.9769],\n",
      "        [ 0.1498,  0.1929,  0.1074,  0.3992,  0.6223],\n",
      "        [ 0.1943,  0.9091,  0.5034, -0.1935,  0.0034]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9161, -0.0915, -0.1090,  1.3027,  1.2185],\n",
      "        [ 0.0759,  1.1306, -0.0004,  0.6278, -1.5187],\n",
      "        [-0.6355, -0.1000, -0.6859,  0.0824,  2.3768],\n",
      "        [-0.0406, -0.6543, -0.0596,  0.6498, -1.9000]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1065],\n",
      "        [-0.0802],\n",
      "        [ 1.3237],\n",
      "        [-0.7649]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1319,  1.9252,  0.9864, -0.5816,  0.9085],\n",
      "        [-0.3762,  0.5720,  2.0944,  0.8505, -0.2750],\n",
      "        [-1.2213, -0.0186,  0.2537,  0.6501, -0.4305],\n",
      "        [ 0.3084,  1.3893, -0.7836, -0.5651,  0.6097]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7481, -0.0269,  0.4421,  0.5189,  0.6036],\n",
      "        [ 1.0130,  2.0075,  1.5914,  0.6720,  1.6493],\n",
      "        [ 0.2507, -0.0017, -0.9045, -0.2568, -1.1445],\n",
      "        [ 0.3163,  0.2790,  0.7172, -0.2557,  0.5867]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1319,  1.9252,  0.9864, -0.5816,  0.9085],\n",
      "        [-0.3762,  0.5720,  2.0944,  0.8505, -0.2750],\n",
      "        [-1.2213, -0.0186,  0.2537,  0.6501, -0.4305],\n",
      "        [ 0.3084,  1.3893, -0.7836, -0.5651,  0.6097]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5321],\n",
      "        [ 4.2183],\n",
      "        [-0.2099],\n",
      "        [ 0.4254]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5574, -0.5358, -0.1991,  0.0115,  0.3448],\n",
      "        [-1.3461,  1.9767,  1.3565,  0.5601, -0.5423],\n",
      "        [-0.8213,  0.7341, -0.3389, -0.0508,  1.3178],\n",
      "        [ 0.3784, -0.4272,  2.0610,  0.9493, -0.0407]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2732, -0.3808,  0.6316, -0.3042,  0.9572],\n",
      "        [-0.2788, -0.4132, -0.5030, -0.2826, -0.9069],\n",
      "        [ 0.2100,  0.4162, -0.3205, -0.3164, -0.1580],\n",
      "        [-0.0035, -0.2048, -0.8566, -0.4292, -0.0828]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5574, -0.5358, -0.1991,  0.0115,  0.3448],\n",
      "        [-1.3461,  1.9767,  1.3565,  0.5601, -0.5423],\n",
      "        [-0.8213,  0.7341, -0.3389, -0.0508,  1.3178],\n",
      "        [ 0.3784, -0.4272,  2.0610,  0.9493, -0.0407]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2525],\n",
      "        [-0.7902],\n",
      "        [ 0.0497],\n",
      "        [-2.0834]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2021,  1.0361, -1.3415, -0.5340, -0.6131],\n",
      "        [ 0.8868, -1.5379,  0.9392,  0.0525, -0.0814],\n",
      "        [ 0.1033,  0.9351,  2.0702,  2.5050,  1.1872],\n",
      "        [-1.6156, -0.1987,  2.5904,  0.0446,  0.1793]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4839, -0.3001,  0.5381,  0.2907,  0.4104],\n",
      "        [ 0.1302,  0.1665,  0.2762,  0.0860,  0.0107],\n",
      "        [ 0.2109,  0.2589,  0.3462,  0.6800, -0.0773],\n",
      "        [ 1.1164,  1.1236,  0.3458,  0.1443,  1.0363]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2021,  1.0361, -1.3415, -0.5340, -0.6131],\n",
      "        [ 0.8868, -1.5379,  0.9392,  0.0525, -0.0814],\n",
      "        [ 0.1033,  0.9351,  2.0702,  2.5050,  1.1872],\n",
      "        [-1.6156, -0.1987,  2.5904,  0.0446,  0.1793]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0215],\n",
      "        [ 0.1225],\n",
      "        [ 2.5923],\n",
      "        [-0.9390]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1821, -0.2299,  1.1379,  1.8242,  0.1462],\n",
      "        [-2.2165, -0.3959, -0.5878, -0.0804,  0.5728],\n",
      "        [-0.6378,  0.2001,  1.4197,  0.1197,  2.0945],\n",
      "        [ 0.8650,  0.6236, -1.6237, -1.2624, -0.3130]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7461,  1.5167,  0.5715,  0.2832,  1.4763],\n",
      "        [ 0.4963,  1.0413,  0.2243,  0.1684,  0.5574],\n",
      "        [-0.2198, -1.0828, -0.8095, -1.0625, -2.1623],\n",
      "        [ 1.0133,  1.6020,  0.8160,  0.9430,  1.5141]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1821, -0.2299,  1.1379,  1.8242,  0.1462],\n",
      "        [-2.2165, -0.3959, -0.5878, -0.0804,  0.5728],\n",
      "        [-0.6378,  0.2001,  1.4197,  0.1197,  2.0945],\n",
      "        [ 0.8650,  0.6236, -1.6237, -1.2624, -0.3130]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1701],\n",
      "        [-1.3385],\n",
      "        [-5.8817],\n",
      "        [-1.1139]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5049, -0.1387,  0.8475, -0.2180, -0.5666],\n",
      "        [-0.7562,  0.1863, -0.7564, -0.7196, -1.2299],\n",
      "        [-1.1013, -0.6695,  0.5617, -0.1901,  0.3844],\n",
      "        [ 1.0049,  1.5422,  0.9810, -0.5286, -0.2938]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9166,  0.6354, -0.0190,  0.6356,  0.0337],\n",
      "        [ 0.4677,  0.8723,  1.0752,  0.5684,  1.2145],\n",
      "        [ 1.8942,  1.9230,  2.0654,  1.0404,  2.4715],\n",
      "        [ 0.9997,  1.6679,  1.5082,  1.1309,  1.6755]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5049, -0.1387,  0.8475, -0.2180, -0.5666],\n",
      "        [-0.7562,  0.1863, -0.7564, -0.7196, -1.2299],\n",
      "        [-1.1013, -0.6695,  0.5617, -0.1901,  0.3844],\n",
      "        [ 1.0049,  1.5422,  0.9810, -0.5286, -0.2938]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6413],\n",
      "        [-2.9072],\n",
      "        [-1.4611],\n",
      "        [ 3.9663]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5300,  1.4803,  0.8419,  0.9691, -0.5510],\n",
      "        [-2.7225,  0.4539,  1.3523,  0.4160, -0.5082],\n",
      "        [-0.3317,  0.3452,  0.8208,  1.4481,  1.6327],\n",
      "        [-0.1521,  0.7369,  0.7434,  1.6565,  0.8432]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5912,  1.1307,  1.7272,  0.7461,  1.0455],\n",
      "        [ 1.6203,  1.6351,  1.9096,  1.2281,  2.5792],\n",
      "        [ 1.0702,  2.1112,  1.0757,  1.4323,  2.4827],\n",
      "        [-0.3316,  0.2959,  1.4463,  0.0282,  0.2160]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5300,  1.4803,  0.8419,  0.9691, -0.5510],\n",
      "        [-2.7225,  0.4539,  1.3523,  0.4160, -0.5082],\n",
      "        [-0.3317,  0.3452,  0.8208,  1.4481,  1.6327],\n",
      "        [-0.1521,  0.7369,  0.7434,  1.6565,  0.8432]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.9615],\n",
      "        [-1.8865],\n",
      "        [ 7.3844],\n",
      "        [ 1.5725]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1376,  0.2654,  0.2520, -0.0034,  1.3685],\n",
      "        [-0.6818,  0.7397,  0.4771,  0.5480,  0.4143],\n",
      "        [ 0.9570, -0.1751,  0.8812,  0.3474, -0.6752],\n",
      "        [-0.7150, -0.6023,  0.5902,  0.6290,  0.3758]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1958, -0.6540, -0.3628, -0.4835, -1.0891],\n",
      "        [ 0.0875, -0.3614, -0.7262, -0.3885, -0.0628],\n",
      "        [-0.8182, -0.7259, -0.6618, -1.2285, -1.4699],\n",
      "        [-0.1454, -0.0626,  0.4811, -0.1928, -1.0150]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1376,  0.2654,  0.2520, -0.0034,  1.3685],\n",
      "        [-0.6818,  0.7397,  0.4771,  0.5480,  0.4143],\n",
      "        [ 0.9570, -0.1751,  0.8812,  0.3474, -0.6752],\n",
      "        [-0.7150, -0.6023,  0.5902,  0.6290,  0.3758]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7269],\n",
      "        [-0.9124],\n",
      "        [-0.6735],\n",
      "        [-0.0771]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8326,  0.8074, -0.2936,  0.5814,  1.1125],\n",
      "        [-0.7082,  0.7438,  0.2990,  0.5008,  1.2217],\n",
      "        [-0.4728,  1.1263,  2.7628,  0.7713,  0.9614],\n",
      "        [-0.4124, -0.7259,  0.1867, -0.7002,  0.8356]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4278,  0.9593,  0.9683,  0.2670,  1.5237],\n",
      "        [-0.0255,  0.8491,  0.2733,  0.1181,  0.7585],\n",
      "        [-0.8085, -1.3567,  0.1449, -0.1133, -0.1407],\n",
      "        [-0.4320,  0.0031,  0.4185,  0.5758, -0.1667]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8326,  0.8074, -0.2936,  0.5814,  1.1125],\n",
      "        [-0.7082,  0.7438,  0.2990,  0.5008,  1.2217],\n",
      "        [-0.4728,  1.1263,  2.7628,  0.7713,  0.9614],\n",
      "        [-0.4124, -0.7259,  0.1867, -0.7002,  0.8356]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.9846],\n",
      "        [ 1.7172],\n",
      "        [-0.9680],\n",
      "        [-0.2884]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8985, -0.6212,  1.0832,  0.1594,  0.0616],\n",
      "        [ 0.1244, -1.2260,  1.2155,  0.2734,  0.5572],\n",
      "        [-0.2968,  0.3557,  2.0004, -0.3989,  0.5811],\n",
      "        [-2.2165, -0.1805, -0.5245, -0.7807, -0.1284]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6020, -0.0358, -0.0705, -0.6785, -1.7307],\n",
      "        [-0.9381, -0.6554, -0.3763, -1.1491, -0.9099],\n",
      "        [ 0.7309,  1.0406,  0.8694,  1.3653,  1.1862],\n",
      "        [ 0.2914,  1.1259,  0.6078,  0.4454,  1.4555]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8985, -0.6212,  1.0832,  0.1594,  0.0616],\n",
      "        [ 0.1244, -1.2260,  1.2155,  0.2734,  0.5572],\n",
      "        [-0.2968,  0.3557,  2.0004, -0.3989,  0.5811],\n",
      "        [-2.2165, -0.1805, -0.5245, -0.7807, -0.1284]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2720],\n",
      "        [-0.5917],\n",
      "        [ 2.0371],\n",
      "        [-1.7023]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2516,  0.2682, -0.1209, -0.2788,  1.3903],\n",
      "        [ 1.7332, -0.6122,  0.8824, -0.4696,  0.3662],\n",
      "        [-1.9427, -3.0422, -0.1193, -0.3638,  1.8594],\n",
      "        [-0.2966,  1.3713, -0.3476,  0.6422,  0.8894]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0605,  0.2795, -0.4874,  0.2396,  0.2046],\n",
      "        [-0.0644, -0.6486, -0.1892, -0.6541, -0.3682],\n",
      "        [-0.7599, -0.0514, -0.1734,  0.0979, -1.0745],\n",
      "        [ 0.9019,  0.6830,  0.5737,  1.2225,  1.4483]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2516,  0.2682, -0.1209, -0.2788,  1.3903],\n",
      "        [ 1.7332, -0.6122,  0.8824, -0.4696,  0.3662],\n",
      "        [-1.9427, -3.0422, -0.1193, -0.3638,  1.8594],\n",
      "        [-0.2966,  1.3713, -0.3476,  0.6422,  0.8894]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0847],\n",
      "        [ 0.2908],\n",
      "        [-0.3803],\n",
      "        [ 2.5428]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8465,  1.2363, -0.1953,  0.6882,  1.3332],\n",
      "        [-1.1904,  0.2161, -0.5970,  0.5309, -0.4716],\n",
      "        [ 0.9384, -0.0240,  1.1525,  0.4881,  1.5568],\n",
      "        [-2.1242,  1.1978,  1.6676,  0.1312,  1.3456]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4269,  0.5033,  0.7970,  0.2279,  0.2777],\n",
      "        [-0.0489, -0.6893, -0.0387,  0.4397,  0.0047],\n",
      "        [-0.0463,  1.0075,  0.9598,  0.4059,  0.6733],\n",
      "        [-0.2816,  0.0500,  0.5828,  1.4226, -0.0277]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8465,  1.2363, -0.1953,  0.6882,  1.3332],\n",
      "        [-1.1904,  0.2161, -0.5970,  0.5309, -0.4716],\n",
      "        [ 0.9384, -0.0240,  1.1525,  0.4881,  1.5568],\n",
      "        [-2.1242,  1.1978,  1.6676,  0.1312,  1.3456]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3551],\n",
      "        [ 0.1637],\n",
      "        [ 2.2848],\n",
      "        [ 1.7792]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3886,  0.4240,  0.9821,  0.5459,  0.3269],\n",
      "        [ 0.5950,  0.1676,  0.4561,  1.5429, -0.0491],\n",
      "        [ 0.8158,  0.3154,  0.0488,  0.4890,  0.1381],\n",
      "        [ 0.6926,  0.3716,  0.3062,  1.9493,  0.6475]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1821, -0.1979, -0.7206,  0.4151, -0.8296],\n",
      "        [-0.1181,  0.1559, -0.3679, -0.0591, -0.3384],\n",
      "        [-0.6579, -0.6222, -0.9012, -0.4949, -1.3635],\n",
      "        [ 0.1349,  0.3661, -0.2253, -0.1618, -0.9322]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3886,  0.4240,  0.9821,  0.5459,  0.3269],\n",
      "        [ 0.5950,  0.1676,  0.4561,  1.5429, -0.0491],\n",
      "        [ 0.8158,  0.3154,  0.0488,  0.4890,  0.1381],\n",
      "        [ 0.6926,  0.3716,  0.3062,  1.9493,  0.6475]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9069],\n",
      "        [-0.2865],\n",
      "        [-1.2073],\n",
      "        [-0.7585]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8488,  0.5664,  0.6955, -0.2086, -0.0044],\n",
      "        [-1.0067,  0.8189, -0.1831, -0.5674, -3.0471],\n",
      "        [ 2.0532,  0.3390, -0.8999, -0.0998, -0.7047],\n",
      "        [ 0.1830, -0.1235,  0.0832,  1.3556,  1.0616]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0124,  0.3629,  0.2859,  0.3456,  1.2832],\n",
      "        [ 0.2883, -1.0019,  0.3728,  0.3696, -0.1266],\n",
      "        [ 0.0942,  0.3335,  0.4058,  0.4902,  0.3373],\n",
      "        [-0.2385,  0.9989,  1.1906,  0.4469,  0.7121]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8488,  0.5664,  0.6955, -0.2086, -0.0044],\n",
      "        [-1.0067,  0.8189, -0.1831, -0.5674, -3.0471],\n",
      "        [ 2.0532,  0.3390, -0.8999, -0.0998, -0.7047],\n",
      "        [ 0.1830, -0.1235,  0.0832,  1.3556,  1.0616]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3039],\n",
      "        [-1.0027],\n",
      "        [-0.3452],\n",
      "        [ 1.2938]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8309,  0.1040,  0.4517, -0.5652, -0.9686],\n",
      "        [ 0.1678,  0.6363,  0.6334, -0.3825,  0.5185],\n",
      "        [ 0.1763,  0.2749,  0.1948, -0.5360,  1.7994],\n",
      "        [-0.3309,  1.6501,  1.2511, -0.6255,  0.7372]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0090,  0.6765,  0.2770,  0.8251,  0.3790],\n",
      "        [ 0.4107,  0.1865,  0.2702,  0.0991,  0.4718],\n",
      "        [-0.1693,  0.2892,  0.8033,  1.0660,  1.1932],\n",
      "        [-0.5940,  0.0493,  0.7535, -0.0848,  0.9264]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8309,  0.1040,  0.4517, -0.5652, -0.9686],\n",
      "        [ 0.1678,  0.6363,  0.6334, -0.3825,  0.5185],\n",
      "        [ 0.1763,  0.2749,  0.1948, -0.5360,  1.7994],\n",
      "        [-0.3309,  1.6501,  1.2511, -0.6255,  0.7372]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6454],\n",
      "        [ 0.5654],\n",
      "        [ 1.7817],\n",
      "        [ 1.9566]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7640,  0.1538, -1.2356,  0.4668, -0.1670],\n",
      "        [ 1.7271,  0.4605,  0.9459,  0.6211,  1.8613],\n",
      "        [ 1.9899,  1.4866, -0.2671,  1.4907, -0.8873],\n",
      "        [ 0.8747, -0.4544, -0.6895, -1.0796, -0.9146]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3276,  1.0772,  0.3889,  0.8065,  1.3799],\n",
      "        [ 0.0129,  0.3293, -0.3826, -0.1902, -0.3545],\n",
      "        [-0.7704, -0.1886, -1.0498, -0.4438, -1.3542],\n",
      "        [-0.8076, -0.3193, -0.1600, -0.7347, -1.1707]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7640,  0.1538, -1.2356,  0.4668, -0.1670],\n",
      "        [ 1.7271,  0.4605,  0.9459,  0.6211,  1.8613],\n",
      "        [ 1.9899,  1.4866, -0.2671,  1.4907, -0.8873],\n",
      "        [ 0.8747, -0.4544, -0.6895, -1.0796, -0.9146]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4190],\n",
      "        [-0.9659],\n",
      "        [-0.9930],\n",
      "        [ 1.4128]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5477, -0.9274,  0.4134,  0.4187,  0.6548],\n",
      "        [-0.1855, -0.7387,  1.9216,  1.0634,  0.8331],\n",
      "        [ 0.4818,  0.6440, -0.1572,  1.3691,  0.0632],\n",
      "        [-1.1435, -0.9020,  1.0675, -0.4439, -0.1736]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5902, -0.0369,  1.3555,  0.5369,  1.0684],\n",
      "        [ 0.4937, -0.2839, -0.3725, -0.0341,  0.5108],\n",
      "        [-0.4074,  0.0987,  0.3083, -0.4125,  0.5258],\n",
      "        [-0.8721, -0.4563, -1.0427, -0.6627, -1.7086]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5477, -0.9274,  0.4134,  0.4187,  0.6548],\n",
      "        [-0.1855, -0.7387,  1.9216,  1.0634,  0.8331],\n",
      "        [ 0.4818,  0.6440, -0.1572,  1.3691,  0.0632],\n",
      "        [-1.1435, -0.9020,  1.0675, -0.4439, -0.1736]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8423],\n",
      "        [-0.2084],\n",
      "        [-0.7127],\n",
      "        [ 0.8865]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.3455, -0.2438,  0.4721, -0.1544,  0.5848],\n",
      "        [-1.3691,  0.6758,  0.2145,  1.4870,  1.7341],\n",
      "        [-0.6872,  1.4117, -0.8706,  0.8935, -0.0531],\n",
      "        [-0.9537, -0.1125,  1.0283, -1.3305, -1.8092]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3276, -0.6338, -0.2608, -0.7887, -0.8951],\n",
      "        [ 0.4275,  0.4953, -0.3536, -0.7883, -0.8309],\n",
      "        [-0.1146,  1.0923,  0.0700,  0.4895,  0.4023],\n",
      "        [-0.8613, -0.7367, -1.1971, -1.3255, -2.5482]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.3455, -0.2438,  0.4721, -0.1544,  0.5848],\n",
      "        [-1.3691,  0.6758,  0.2145,  1.4870,  1.7341],\n",
      "        [-0.6872,  1.4117, -0.8706,  0.8935, -0.0531],\n",
      "        [-0.9537, -0.1125,  1.0283, -1.3305, -1.8092]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1387],\n",
      "        [-2.9395],\n",
      "        [ 1.9757],\n",
      "        [ 6.0469]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2802,  0.0682, -0.4702, -0.9370,  0.7462],\n",
      "        [-0.5810,  1.0740,  0.3728,  0.2908,  0.9745],\n",
      "        [-1.2363,  0.5741, -1.5953,  0.5404,  0.2720],\n",
      "        [-0.7919, -0.1439,  0.8072,  0.4895,  0.3107]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6202,  1.1045,  0.9663,  0.4286,  1.0085],\n",
      "        [ 1.4732,  1.5877,  0.7975,  0.7117,  1.4461],\n",
      "        [-0.5034, -0.5587, -0.1452, -0.2458, -0.4834],\n",
      "        [ 0.5309,  0.4241, -0.3304, -0.5106, -0.3025]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2802,  0.0682, -0.4702, -0.9370,  0.7462],\n",
      "        [-0.5810,  1.0740,  0.3728,  0.2908,  0.9745],\n",
      "        [-1.2363,  0.5741, -1.5953,  0.5404,  0.2720],\n",
      "        [-0.7919, -0.1439,  0.8072,  0.4895,  0.3107]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2018],\n",
      "        [ 2.7627],\n",
      "        [ 0.2689],\n",
      "        [-1.0921]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4778,  0.0789, -0.2572, -0.6607,  0.0627],\n",
      "        [-1.7290,  0.8924, -0.3327, -0.2545,  0.2672],\n",
      "        [-1.0021,  0.4670,  2.2990,  2.2086,  1.5624],\n",
      "        [-0.8789,  1.8606, -0.1549,  0.1143, -0.5293]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1805,  0.3259,  0.8319,  0.3567,  0.6853],\n",
      "        [-0.0835, -0.0488,  0.7841,  0.2030, -0.3589],\n",
      "        [-0.5110, -0.1845,  0.0153, -0.1322, -0.0451],\n",
      "        [ 0.8838,  0.6871,  0.4759, -0.2719,  1.0095]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4778,  0.0789, -0.2572, -0.6607,  0.0627],\n",
      "        [-1.7290,  0.8924, -0.3327, -0.2545,  0.2672],\n",
      "        [-1.0021,  0.4670,  2.2990,  2.2086,  1.5624],\n",
      "        [-0.8789,  1.8606, -0.1549,  0.1143, -0.5293]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4672],\n",
      "        [-0.3077],\n",
      "        [ 0.0988],\n",
      "        [-0.1375]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6479,  1.0854,  0.1944,  1.7382,  0.0836],\n",
      "        [ 0.0086, -0.5290,  1.1877,  1.4330, -0.6713],\n",
      "        [ 0.0756, -0.9291, -1.0326,  0.1688, -0.3898],\n",
      "        [ 0.6084,  0.1872, -1.0570,  0.2895, -0.5108]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3263,  0.8281,  0.2573,  0.3726,  0.4797],\n",
      "        [ 0.3483, -0.1312, -0.0742, -0.5014,  0.0505],\n",
      "        [-0.0020,  0.6504,  0.3092, -0.2739,  0.1103],\n",
      "        [ 0.5871, -0.3126, -0.0479, -0.1467,  0.4106]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6479,  1.0854,  0.1944,  1.7382,  0.0836],\n",
      "        [ 0.0086, -0.5290,  1.1877,  1.4330, -0.6713],\n",
      "        [ 0.0756, -0.9291, -1.0326,  0.1688, -0.3898],\n",
      "        [ 0.6084,  0.1872, -1.0570,  0.2895, -0.5108]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0988],\n",
      "        [-0.7681],\n",
      "        [-1.0130],\n",
      "        [ 0.0970]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3252,  0.3246, -0.2980, -0.5945,  0.0453],\n",
      "        [ 0.8894,  0.6105,  1.6791, -1.7268, -0.5170],\n",
      "        [-0.6978, -0.4065, -0.3714,  2.8885,  1.2319],\n",
      "        [ 0.8251, -1.2406,  0.5241,  0.0845,  1.0340]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3635, -1.0633,  0.2855,  0.4101, -0.4103],\n",
      "        [ 0.2558,  0.0042,  0.6145,  0.5240,  0.5528],\n",
      "        [-0.3847,  0.4972,  0.8977,  1.0770,  1.7200],\n",
      "        [ 0.1186,  0.0116, -0.0081,  0.1791, -0.1410]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3252,  0.3246, -0.2980, -0.5945,  0.0453],\n",
      "        [ 0.8894,  0.6105,  1.6791, -1.7268, -0.5170],\n",
      "        [-0.6978, -0.4065, -0.3714,  2.8885,  1.2319],\n",
      "        [ 0.8251, -1.2406,  0.5241,  0.0845,  1.0340]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8109],\n",
      "        [ 0.0713],\n",
      "        [ 4.9627],\n",
      "        [-0.0515]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9136, -0.1366, -0.0189, -1.2738,  0.6037],\n",
      "        [-0.4079,  0.3698, -0.5808,  0.7904, -0.6968],\n",
      "        [ 0.6362, -0.2721, -0.5632, -0.3633, -0.4666],\n",
      "        [-0.8345,  1.4311,  0.8341, -1.5428,  0.8185]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0509,  0.2470,  0.0997,  0.0533,  0.2312],\n",
      "        [ 0.6796, -0.2779, -0.3272, -0.2357, -0.2761],\n",
      "        [-1.1864, -2.1314, -1.8571, -2.0243, -3.3732],\n",
      "        [ 0.2796,  0.3025, -0.3802, -0.2456, -0.5705]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9136, -0.1366, -0.0189, -1.2738,  0.6037],\n",
      "        [-0.4079,  0.3698, -0.5808,  0.7904, -0.6968],\n",
      "        [ 0.6362, -0.2721, -0.5632, -0.3633, -0.4666],\n",
      "        [-0.8345,  1.4311,  0.8341, -1.5428,  0.8185]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0105],\n",
      "        [-0.1839],\n",
      "        [ 3.1803],\n",
      "        [-0.2055]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2138,  0.2471,  0.8316,  0.9982,  1.4364],\n",
      "        [ 2.5780,  0.0413, -0.6805, -0.9869, -0.5699],\n",
      "        [ 0.8505, -0.8215, -0.8562, -2.0576,  0.3225],\n",
      "        [-0.2565,  0.5170, -0.6047, -0.8860, -0.1619]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3275, -0.1813,  0.5065,  0.5535,  0.3425],\n",
      "        [ 0.6148,  0.6867,  0.6204,  0.5522,  0.5718],\n",
      "        [ 0.3671,  0.2855, -0.6017,  0.0973, -0.5854],\n",
      "        [ 0.1325,  0.9407, -0.1830, -0.1200, -0.5252]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2138,  0.2471,  0.8316,  0.9982,  1.4364],\n",
      "        [ 2.5780,  0.0413, -0.6805, -0.9869, -0.5699],\n",
      "        [ 0.8505, -0.8215, -0.8562, -2.0576,  0.3225],\n",
      "        [-0.2565,  0.5170, -0.6047, -0.8860, -0.1619]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4910],\n",
      "        [ 0.3204],\n",
      "        [ 0.2039],\n",
      "        [ 0.7544]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4582,  1.0873, -2.0113,  0.5064,  1.0094],\n",
      "        [-1.2493,  1.3513, -0.3561, -0.4259,  1.1228],\n",
      "        [ 0.6999,  0.9944,  0.5094, -0.2158,  0.5981],\n",
      "        [ 0.6092,  1.8903,  1.5444,  0.6815, -0.4532]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4286,  0.1299, -0.0326, -0.2807, -1.0984],\n",
      "        [ 0.5482, -0.3452,  0.4102,  0.2031,  0.1754],\n",
      "        [ 0.6672, -0.3265, -0.6777, -0.4467, -0.5987],\n",
      "        [ 0.0220, -0.1304, -0.1542, -0.3175, -0.0932]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4582,  1.0873, -2.0113,  0.5064,  1.0094],\n",
      "        [-1.2493,  1.3513, -0.3561, -0.4259,  1.1228],\n",
      "        [ 0.6999,  0.9944,  0.5094, -0.2158,  0.5981],\n",
      "        [ 0.6092,  1.8903,  1.5444,  0.6815, -0.4532]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2404],\n",
      "        [-1.1870],\n",
      "        [-0.4647],\n",
      "        [-0.6454]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0600,  0.6726, -1.3437, -0.1083, -0.3777],\n",
      "        [-0.2417, -0.8602,  0.4615, -0.3409, -1.0544],\n",
      "        [ 0.6125, -0.0078,  0.4734,  0.0608,  0.1377],\n",
      "        [-0.0411, -0.4518,  0.7997,  1.5723, -0.9738]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1500,  0.7508, -0.1679,  0.8001,  0.2726],\n",
      "        [ 0.4580,  1.0626,  0.8640,  0.9250,  1.6108],\n",
      "        [ 0.3118, -0.6500, -0.3480,  0.1453, -0.2489],\n",
      "        [ 0.3182,  0.5339,  0.0087,  0.0167, -0.1505]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0600,  0.6726, -1.3437, -0.1083, -0.3777],\n",
      "        [-0.2417, -0.8602,  0.4615, -0.3409, -1.0544],\n",
      "        [ 0.6125, -0.0078,  0.4734,  0.0608,  0.1377],\n",
      "        [-0.0411, -0.4518,  0.7997,  1.5723, -0.9738]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6999],\n",
      "        [-2.6398],\n",
      "        [ 0.0058],\n",
      "        [-0.0746]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3368, -2.5912, -0.7932,  0.0692,  1.0262],\n",
      "        [ 0.1532, -0.2894, -0.3639,  0.4197, -0.4618],\n",
      "        [-0.1816,  0.3454,  0.4278,  0.7280,  0.4220],\n",
      "        [ 1.3618, -0.7706,  0.9279,  0.0568,  0.3311]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1908,  0.3835, -0.2458,  0.8189,  0.4998],\n",
      "        [ 1.3833,  1.3411,  1.5763,  1.4260,  1.7855],\n",
      "        [ 0.1080,  0.3062, -0.1853, -0.6395, -0.0876],\n",
      "        [ 0.7570,  0.0834,  0.4602,  0.0446,  0.4852]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3368, -2.5912, -0.7932,  0.0692,  1.0262],\n",
      "        [ 0.1532, -0.2894, -0.3639,  0.4197, -0.4618],\n",
      "        [-0.1816,  0.3454,  0.4278,  0.7280,  0.4220],\n",
      "        [ 1.3618, -0.7706,  0.9279,  0.0568,  0.3311]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4841],\n",
      "        [-0.9758],\n",
      "        [-0.4957],\n",
      "        [ 1.5567]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6157,  0.7926,  1.8484,  0.9625, -1.2125],\n",
      "        [-0.7372, -2.1209,  0.8939,  1.3951,  1.6553],\n",
      "        [-0.1414, -1.2808, -0.3775, -0.1905, -0.7854],\n",
      "        [-1.3304,  0.0546, -1.0742,  1.0008, -0.8971]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1958,  0.2020,  0.6577,  0.5194,  0.7123],\n",
      "        [ 0.1992,  0.0984,  0.0403,  0.1139,  0.8156],\n",
      "        [ 0.4917,  0.2467,  0.0417, -0.5081, -0.0592],\n",
      "        [-0.8097, -1.2673, -0.5847, -0.5520, -1.0268]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6157,  0.7926,  1.8484,  0.9625, -1.2125],\n",
      "        [-0.7372, -2.1209,  0.8939,  1.3951,  1.6553],\n",
      "        [-0.1414, -1.2808, -0.3775, -0.1905, -0.7854],\n",
      "        [-1.3304,  0.0546, -1.0742,  1.0008, -0.8971]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1326],\n",
      "        [ 1.1893],\n",
      "        [-0.2579],\n",
      "        [ 2.0048]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0700,  0.7331,  0.6977,  0.5471,  2.0395],\n",
      "        [-1.1923,  0.3107, -1.9620, -0.2395, -1.2393],\n",
      "        [ 0.2465, -0.4306,  1.2165, -0.7427,  0.1090],\n",
      "        [ 0.4163,  0.1256, -0.3298,  0.4786, -1.9209]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1054, -0.2584,  0.5903, -0.1941, -0.2379],\n",
      "        [-0.4331,  0.0629, -0.9630, -0.1924, -0.9963],\n",
      "        [ 0.3483, -0.6964, -0.0969, -0.4119, -0.4581],\n",
      "        [-0.6897, -0.7219, -2.0217, -1.5834, -2.3562]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0700,  0.7331,  0.6977,  0.5471,  2.0395],\n",
      "        [-1.1923,  0.3107, -1.9620, -0.2395, -1.2393],\n",
      "        [ 0.2465, -0.4306,  1.2165, -0.7427,  0.1090],\n",
      "        [ 0.4163,  0.1256, -0.3298,  0.4786, -1.9209]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3763],\n",
      "        [ 3.7062],\n",
      "        [ 0.5239],\n",
      "        [ 4.0572]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3011, -1.4243, -0.5711, -0.3720, -1.0813],\n",
      "        [-1.1274, -0.7736, -0.2024, -0.0253, -0.0614],\n",
      "        [-1.0500, -0.3860,  1.2594, -2.2081,  1.7161],\n",
      "        [-0.7654, -0.0335,  0.1437, -0.3282,  2.5146]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3689, -0.1585,  0.2516,  0.5762,  0.7798],\n",
      "        [-1.0273, -1.6944, -1.9931, -1.5850, -3.9458],\n",
      "        [ 1.4747, -0.1462, -0.7161, -0.2597,  0.0409],\n",
      "        [-1.6293, -2.8457, -1.8296, -2.2482, -4.9252]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3011, -1.4243, -0.5711, -0.3720, -1.0813],\n",
      "        [-1.1274, -0.7736, -0.2024, -0.0253, -0.0614],\n",
      "        [-1.0500, -0.3860,  1.2594, -2.2081,  1.7161],\n",
      "        [-0.7654, -0.0335,  0.1437, -0.3282,  2.5146]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ -0.4953],\n",
      "        [  3.1549],\n",
      "        [ -1.7503],\n",
      "        [-10.5675]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.2431,  0.7842,  0.5547,  2.1229, -0.0308],\n",
      "        [-0.6128, -0.4054,  0.4522, -1.5758, -1.7087],\n",
      "        [-0.1858,  0.4077, -1.0562,  0.5636,  0.6534],\n",
      "        [-1.4641, -0.0282, -1.1074, -0.8897, -0.2079]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1050,  0.7971,  0.5418,  0.5950,  0.7186],\n",
      "        [ 0.5454, -0.2116,  0.0898, -0.2695,  0.1890],\n",
      "        [ 0.5318,  1.1726,  1.1777,  0.7783,  1.2187],\n",
      "        [ 0.0716,  0.4111, -0.9827, -0.5968, -0.4387]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.2431,  0.7842,  0.5547,  2.1229, -0.0308],\n",
      "        [-0.6128, -0.4054,  0.4522, -1.5758, -1.7087],\n",
      "        [-0.1858,  0.4077, -1.0562,  0.5636,  0.6534],\n",
      "        [-1.4641, -0.0282, -1.1074, -0.8897, -0.2079]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.9310],\n",
      "        [-0.1061],\n",
      "        [ 0.3704],\n",
      "        [ 1.5940]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2168,  0.5674, -0.5595,  0.0899, -1.7465],\n",
      "        [-0.0950,  0.3513, -2.4415,  0.3190, -0.6225],\n",
      "        [ 1.1924, -0.5215,  0.0472,  1.6932,  1.2607],\n",
      "        [-0.9686, -1.4502, -0.2023,  1.2112,  1.0654]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7516,  0.0715, -0.6712, -0.6331, -0.9207],\n",
      "        [-0.4878, -0.4849,  0.6620, -0.0595, -0.1098],\n",
      "        [ 0.5120,  0.3432,  0.1867, -0.3158,  0.8908],\n",
      "        [-0.4289, -0.1749, -1.0869, -0.8109, -1.1875]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2168,  0.5674, -0.5595,  0.0899, -1.7465],\n",
      "        [-0.0950,  0.3513, -2.4415,  0.3190, -0.6225],\n",
      "        [ 1.1924, -0.5215,  0.0472,  1.6932,  1.2607],\n",
      "        [-0.9686, -1.4502, -0.2023,  1.2112,  1.0654]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8042],\n",
      "        [-1.6910],\n",
      "        [ 1.0288],\n",
      "        [-1.3583]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.2658,  2.0082, -0.1322,  0.7535, -0.1819],\n",
      "        [ 0.5367,  0.4126, -0.2266,  2.2318,  1.6105],\n",
      "        [-1.2969,  1.0781,  0.2116, -0.1334,  0.0066],\n",
      "        [-0.0777,  1.3684, -0.1777, -0.0225,  1.7638]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0087, -1.2750, -0.9702, -1.1581, -2.3135],\n",
      "        [ 0.7863,  0.5509,  0.6048,  0.7955,  0.6815],\n",
      "        [ 0.0263, -0.3859,  0.4522,  0.2422, -0.7316],\n",
      "        [ 0.4548,  0.1059, -0.2503, -0.3890, -0.0575]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.2658,  2.0082, -0.1322,  0.7535, -0.1819],\n",
      "        [ 0.5367,  0.4126, -0.2266,  2.2318,  1.6105],\n",
      "        [-1.2969,  1.0781,  0.2116, -0.1334,  0.0066],\n",
      "        [-0.0777,  1.3684, -0.1777, -0.0225,  1.7638]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-5.1696],\n",
      "        [ 3.3853],\n",
      "        [-0.3916],\n",
      "        [ 0.0614]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1475,  1.8331, -0.2519,  0.6864, -0.2853],\n",
      "        [-1.6667,  0.1803, -0.1927, -0.8537, -1.1414],\n",
      "        [ 1.2012,  0.4409,  0.0008, -0.0940, -1.2319],\n",
      "        [-0.8799, -0.7427,  0.6710,  0.3182, -0.2277]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4234,  1.0450,  0.5980,  1.1923,  1.9570],\n",
      "        [-0.4304, -0.7617, -1.0828, -0.8599, -1.2840],\n",
      "        [ 0.6260,  0.4880, -0.0314,  0.4923,  0.4707],\n",
      "        [ 0.0445, -0.0264, -0.0004, -0.1651, -0.2449]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1475,  1.8331, -0.2519,  0.6864, -0.2853],\n",
      "        [-1.6667,  0.1803, -0.1927, -0.8537, -1.1414],\n",
      "        [ 1.2012,  0.4409,  0.0008, -0.0940, -1.2319],\n",
      "        [-0.8799, -0.7427,  0.6710,  0.3182, -0.2277]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8151],\n",
      "        [ 2.9884],\n",
      "        [ 0.3409],\n",
      "        [-0.0167]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7359, -0.4355, -0.7415,  1.7454, -1.5340],\n",
      "        [-1.6285, -0.6705,  0.1543,  0.9454, -1.8034],\n",
      "        [ 0.0967, -1.0036, -0.4767,  1.4745,  1.3778],\n",
      "        [-0.2045, -0.6173,  0.1388,  2.1936,  0.5124]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1343,  0.3945,  0.3020, -0.0909,  0.2734],\n",
      "        [-1.4120, -1.4179, -2.1818, -2.1770, -3.7987],\n",
      "        [ 0.2299, -0.0075, -0.0094,  0.0305, -0.2420],\n",
      "        [-0.1716, -0.4859,  0.0374, -0.8173, -0.0385]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7359, -0.4355, -0.7415,  1.7454, -1.5340],\n",
      "        [-1.6285, -0.6705,  0.1543,  0.9454, -1.8034],\n",
      "        [ 0.0967, -1.0036, -0.4767,  1.4745,  1.3778],\n",
      "        [-0.2045, -0.6173,  0.1388,  2.1936,  0.5124]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8750],\n",
      "        [ 7.7060],\n",
      "        [-0.2542],\n",
      "        [-1.4723]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0369, -0.5195,  2.5757,  0.1336,  0.7284],\n",
      "        [-1.1957, -0.3772, -0.1009,  0.7114, -0.9401],\n",
      "        [-0.2978,  0.2141,  0.3103,  0.3184,  0.0160],\n",
      "        [-0.1695, -0.2067, -0.6297,  0.4446, -1.1838]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3805,  0.1974, -0.3086,  0.3085,  0.7932],\n",
      "        [ 0.0973,  0.0732, -0.3429, -0.3792, -0.3249],\n",
      "        [ 0.6302,  0.3043,  1.0756, -0.3754,  0.3988],\n",
      "        [ 0.8603,  1.0798, -0.2373,  0.3136,  1.2483]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0369, -0.5195,  2.5757,  0.1336,  0.7284],\n",
      "        [-1.1957, -0.3772, -0.1009,  0.7114, -0.9401],\n",
      "        [-0.2978,  0.2141,  0.3103,  0.3184,  0.0160],\n",
      "        [-0.1695, -0.2067, -0.6297,  0.4446, -1.1838]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2925],\n",
      "        [-0.0737],\n",
      "        [ 0.0981],\n",
      "        [-1.5579]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7364,  1.6398,  0.0216,  0.5765,  1.6984],\n",
      "        [ 1.3280, -0.4485,  1.2458, -0.4777,  1.2631],\n",
      "        [-0.8248,  0.9076, -2.1462, -0.1307, -0.2534],\n",
      "        [-0.2200,  1.0547,  0.9083, -0.5008, -0.2847]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1304, -0.0409, -0.1733, -0.1057, -0.2839],\n",
      "        [ 0.5166, -0.1763, -0.0608, -0.1000,  0.4034],\n",
      "        [ 0.0650,  0.3906, -0.4075, -0.4988, -0.1059],\n",
      "        [ 0.7978,  0.4560,  0.5442,  1.1878,  0.9413]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7364,  1.6398,  0.0216,  0.5765,  1.6984],\n",
      "        [ 1.3280, -0.4485,  1.2458, -0.4777,  1.2631],\n",
      "        [-0.8248,  0.9076, -2.1462, -0.1307, -0.2534],\n",
      "        [-0.2200,  1.0547,  0.9083, -0.5008, -0.2847]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3876],\n",
      "        [ 1.2467],\n",
      "        [ 1.2673],\n",
      "        [-0.0631]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1709,  0.5615,  0.7011,  0.0043, -0.4156],\n",
      "        [-0.5820, -0.2916, -0.1705,  1.4622, -1.2898],\n",
      "        [ 0.1125, -2.0456,  0.6224, -0.5961, -1.0136],\n",
      "        [-1.2308,  0.9409, -0.5064,  2.0482, -2.3279]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6680, -0.8468,  0.2769, -0.7089, -0.1569],\n",
      "        [-0.0373, -0.1700,  0.0119, -0.6933, -0.9103],\n",
      "        [-0.0975, -0.1338, -0.4517,  0.1113, -0.9751],\n",
      "        [ 0.4788,  0.8952,  1.0767,  0.8541,  1.4808]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1709,  0.5615,  0.7011,  0.0043, -0.4156],\n",
      "        [-0.5820, -0.2916, -0.1705,  1.4622, -1.2898],\n",
      "        [ 0.1125, -2.0456,  0.6224, -0.5961, -1.0136],\n",
      "        [-1.2308,  0.9409, -0.5064,  2.0482, -2.3279]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6693],\n",
      "        [ 0.2297],\n",
      "        [ 0.9035],\n",
      "        [-1.9899]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8660,  0.8943, -1.4731,  0.8688, -0.8312],\n",
      "        [ 1.1017,  1.3001,  0.7264,  0.6475, -1.0453],\n",
      "        [ 0.3421,  0.9251, -0.0367,  0.2789,  1.7530],\n",
      "        [-0.2336,  0.5207,  0.3425,  2.1046, -0.6519]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9217,  0.6811,  0.2672,  0.7708,  0.8458],\n",
      "        [ 0.3267,  0.1693, -0.7195, -0.2078, -0.3267],\n",
      "        [-0.4395, -0.3879, -0.3307, -0.4275, -0.7185],\n",
      "        [ 1.0568,  1.9770,  1.4764,  1.9533,  2.2208]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8660,  0.8943, -1.4731,  0.8688, -0.8312],\n",
      "        [ 1.1017,  1.3001,  0.7264,  0.6475, -1.0453],\n",
      "        [ 0.3421,  0.9251, -0.0367,  0.2789,  1.7530],\n",
      "        [-0.2336,  0.5207,  0.3425,  2.1046, -0.6519]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9804],\n",
      "        [ 0.2644],\n",
      "        [-1.8758],\n",
      "        [ 3.9512]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0222,  1.9621, -0.0677,  1.4427, -0.2238],\n",
      "        [ 1.4500, -1.2322, -1.3821, -0.0117, -0.0474],\n",
      "        [-0.3046,  1.2167,  1.2332,  0.2478,  1.8336],\n",
      "        [-2.4430, -1.4364, -0.7105, -0.0230,  1.7243]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4801, -0.5985, -0.3524, -0.1813, -0.2172],\n",
      "        [ 0.2121,  0.3065,  0.3249, -0.3115,  0.2849],\n",
      "        [ 0.3438,  0.7102,  0.5967,  0.6785,  0.3929],\n",
      "        [ 0.1327,  0.1159, -0.1751,  0.9306, -0.2015]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0222,  1.9621, -0.0677,  1.4427, -0.2238],\n",
      "        [ 1.4500, -1.2322, -1.3821, -0.0117, -0.0474],\n",
      "        [-0.3046,  1.2167,  1.2332,  0.2478,  1.8336],\n",
      "        [-2.4430, -1.4364, -0.7105, -0.0230,  1.7243]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3741],\n",
      "        [-0.5289],\n",
      "        [ 2.3838],\n",
      "        [-0.7352]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5914,  0.6769, -0.1928, -0.5671, -0.2475],\n",
      "        [-0.6312,  0.2370, -0.2306,  0.5630, -0.7166],\n",
      "        [-0.8821, -0.6113,  0.9782, -0.3259,  0.7538],\n",
      "        [-0.6685,  0.9429,  0.8341,  1.2472,  0.3691]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6821,  0.7702,  1.0934,  0.2255,  1.1494],\n",
      "        [ 0.0781,  0.6252, -0.3289,  0.5302,  0.8805],\n",
      "        [-0.5666, -0.2030, -0.1723, -0.8561, -2.2718],\n",
      "        [ 1.0619,  0.1735,  1.2925,  0.3324,  0.8895]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5914,  0.6769, -0.1928, -0.5671, -0.2475],\n",
      "        [-0.6312,  0.2370, -0.2306,  0.5630, -0.7166],\n",
      "        [-0.8821, -0.6113,  0.9782, -0.3259,  0.7538],\n",
      "        [-0.6685,  0.9429,  0.8341,  1.2472,  0.3691]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5052],\n",
      "        [-0.1578],\n",
      "        [-0.9780],\n",
      "        [ 1.2746]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8548,  0.9504, -0.3028,  2.6917, -0.9416],\n",
      "        [-0.6788,  0.7979, -0.5143,  0.9479,  2.2632],\n",
      "        [-0.3724, -1.5161,  0.8533,  0.7773, -0.3170],\n",
      "        [ 1.7715,  0.6249, -1.7311, -0.9860,  0.3228]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9280,  0.9220,  0.9169, -0.2025,  1.6901],\n",
      "        [ 0.2994,  0.3827, -0.1965, -0.2145,  0.1694],\n",
      "        [-0.0559, -0.7067,  0.1366,  0.0595,  0.0969],\n",
      "        [ 0.3379,  0.4238,  1.2494,  0.7067,  0.6146]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8548,  0.9504, -0.3028,  2.6917, -0.9416],\n",
      "        [-0.6788,  0.7979, -0.5143,  0.9479,  2.2632],\n",
      "        [-0.3724, -1.5161,  0.8533,  0.7773, -0.3170],\n",
      "        [ 1.7715,  0.6249, -1.7311, -0.9860,  0.3228]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.3310],\n",
      "        [ 0.3833],\n",
      "        [ 1.2245],\n",
      "        [-1.7979]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1828, -0.0789,  2.3639,  0.2118, -0.0582],\n",
      "        [ 1.2570, -0.5573,  1.3610,  3.5594,  0.9242],\n",
      "        [-0.6088,  1.4072, -1.0964,  2.3639,  0.6537],\n",
      "        [-0.5445, -0.2548,  0.0223,  1.8034, -1.5862]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1781,  1.3647,  1.1152,  0.9903,  1.9650],\n",
      "        [ 0.2980, -0.1108,  0.5499, -0.2931,  0.2773],\n",
      "        [-0.8010, -0.5742, -0.2638, -0.1093, -1.0351],\n",
      "        [ 0.9896,  0.9219,  0.8341,  1.7800,  1.4520]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1828, -0.0789,  2.3639,  0.2118, -0.0582],\n",
      "        [ 1.2570, -0.5573,  1.3610,  3.5594,  0.9242],\n",
      "        [-0.6088,  1.4072, -1.0964,  2.3639,  0.6537],\n",
      "        [-0.5445, -0.2548,  0.0223,  1.8034, -1.5862]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2305],\n",
      "        [ 0.3975],\n",
      "        [-0.9660],\n",
      "        [ 0.1517]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.4605,  0.9573,  1.1514,  0.3643,  2.3074],\n",
      "        [-0.5238,  0.7320, -2.5718, -0.6574,  0.4217],\n",
      "        [ 0.8150, -1.4083,  0.5282,  1.4448, -1.8475],\n",
      "        [-1.5017,  1.4342, -1.1030,  0.5358,  1.4120]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9770,  0.8178,  0.3125,  0.7405,  0.4829],\n",
      "        [ 0.3796,  0.2611, -0.3844,  0.4538,  0.8729],\n",
      "        [ 0.1208, -0.8201, -0.1739, -0.2668, -0.5330],\n",
      "        [ 1.0375,  1.4376,  1.3832,  0.7705,  0.5780]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.4605,  0.9573,  1.1514,  0.3643,  2.3074],\n",
      "        [-0.5238,  0.7320, -2.5718, -0.6574,  0.4217],\n",
      "        [ 0.8150, -1.4083,  0.5282,  1.4448, -1.8475],\n",
      "        [-1.5017,  1.4342, -1.1030,  0.5358,  1.4120]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1228],\n",
      "        [ 1.0507],\n",
      "        [ 1.7608],\n",
      "        [ 0.2070]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6090,  1.1041, -0.4589,  2.2295, -0.8343],\n",
      "        [-1.0746, -1.6918, -0.1670,  0.7812, -0.1673],\n",
      "        [-2.4285,  0.1133,  0.4591, -0.0639,  2.3662],\n",
      "        [ 0.9702,  1.0910,  0.8874, -0.0082,  1.1664]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1753,  0.3924,  1.1821,  0.5464,  1.1852],\n",
      "        [-0.4303, -0.4162, -0.7339, -0.3168, -0.7721],\n",
      "        [-0.2610, -0.4007, -1.0479, -0.2107, -1.8897],\n",
      "        [-0.0419,  0.1596, -0.2940, -0.3712,  0.1699]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6090,  1.1041, -0.4589,  2.2295, -0.8343],\n",
      "        [-1.0746, -1.6918, -0.1670,  0.7812, -0.1673],\n",
      "        [-2.4285,  0.1133,  0.4591, -0.0639,  2.3662],\n",
      "        [ 0.9702,  1.0910,  0.8874, -0.0082,  1.1664]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1618],\n",
      "        [ 1.1708],\n",
      "        [-4.3505],\n",
      "        [ 0.0738]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.7519,  1.4498,  1.3904,  0.3379,  0.1401],\n",
      "        [-0.9908, -0.5987, -0.0638, -0.7007,  1.6199],\n",
      "        [-1.1480,  0.0635, -0.1133, -0.5272,  0.3626],\n",
      "        [-0.4254,  0.9028,  0.8743,  1.9627,  0.9267]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3147,  0.4055,  1.3728,  1.5807,  0.7345],\n",
      "        [-0.2437,  0.0920, -1.1319, -0.7723, -0.9235],\n",
      "        [ 0.9411,  0.7112,  0.6800,  0.9842,  1.3403],\n",
      "        [ 0.3802,  0.1227,  0.9766, -0.0815,  0.3970]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.7519,  1.4498,  1.3904,  0.3379,  0.1401],\n",
      "        [-0.9908, -0.5987, -0.0638, -0.7007,  1.6199],\n",
      "        [-1.1480,  0.0635, -0.1133, -0.5272,  0.3626],\n",
      "        [-0.4254,  0.9028,  0.8743,  1.9627,  0.9267]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2677],\n",
      "        [-0.6962],\n",
      "        [-1.1451],\n",
      "        [ 1.0109]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6968,  0.2831,  0.9273,  1.9784, -0.8670],\n",
      "        [ 0.7742, -0.1807,  1.4658, -2.0355,  0.4916],\n",
      "        [-0.1129,  0.8748, -0.7061,  2.8313,  1.0462],\n",
      "        [ 0.6426,  0.6480,  0.8948, -0.1012,  1.0357]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2251, -1.3467, -0.8919, -1.0107, -0.6212],\n",
      "        [-0.4893, -0.3687, -0.0853, -0.7176, -0.0390],\n",
      "        [ 1.4578,  1.6561,  1.0436,  1.4940,  2.3886],\n",
      "        [-0.5191, -0.5201, -0.1627,  0.1690, -1.0702]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6968,  0.2831,  0.9273,  1.9784, -0.8670],\n",
      "        [ 0.7742, -0.1807,  1.4658, -2.0355,  0.4916],\n",
      "        [-0.1129,  0.8748, -0.7061,  2.8313,  1.0462],\n",
      "        [ 0.6426,  0.6480,  0.8948, -0.1012,  1.0357]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5125],\n",
      "        [ 1.0043],\n",
      "        [ 7.2761],\n",
      "        [-1.9417]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3732,  0.1166,  2.7990,  1.9818, -0.1729],\n",
      "        [-1.4182,  0.2580, -0.4692,  0.6326, -0.2750],\n",
      "        [ 1.1114,  1.8274,  0.8407,  0.2781,  0.6550],\n",
      "        [-0.8445,  0.4453,  0.8043, -0.1597, -1.9885]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7156,  1.2066,  1.3851,  0.6841,  1.2978],\n",
      "        [-0.4146, -0.7298, -1.2099, -0.6100, -0.9103],\n",
      "        [-0.9970, -1.3048, -1.5638, -1.7015, -3.0970],\n",
      "        [ 0.5148,  0.6475,  1.6359,  0.2799,  0.6475]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3732,  0.1166,  2.7990,  1.9818, -0.1729],\n",
      "        [-1.4182,  0.2580, -0.4692,  0.6326, -0.2750],\n",
      "        [ 1.1114,  1.8274,  0.8407,  0.2781,  0.6550],\n",
      "        [-0.8445,  0.4453,  0.8043, -0.1597, -1.9885]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.1663],\n",
      "        [ 0.8319],\n",
      "        [-7.3088],\n",
      "        [-0.1628]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3752,  0.7797,  1.3932, -0.7631,  0.4789],\n",
      "        [-0.6799,  1.0729,  0.9957, -0.7146,  0.5748],\n",
      "        [-1.1644, -0.0137, -0.8126, -0.2504,  0.0339],\n",
      "        [ 0.2696,  0.4334, -0.0180, -1.0848, -0.3469]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7177, -0.6009, -1.4489, -1.0303, -1.9140],\n",
      "        [-0.7883, -0.5165, -1.2824, -0.1282, -0.7919],\n",
      "        [ 1.3198,  1.2105,  1.0263,  1.1405,  1.8670],\n",
      "        [ 0.1663,  0.6150,  0.4204,  0.0967,  0.8870]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3752,  0.7797,  1.3932, -0.7631,  0.4789],\n",
      "        [-0.6799,  1.0729,  0.9957, -0.7146,  0.5748],\n",
      "        [-1.1644, -0.0137, -0.8126, -0.2504,  0.0339],\n",
      "        [ 0.2696,  0.4334, -0.0180, -1.0848, -0.3469]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8868],\n",
      "        [-1.6586],\n",
      "        [-2.6098],\n",
      "        [-0.1088]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2679, -0.3594, -0.1895,  0.9073,  1.3768],\n",
      "        [-1.5535,  0.9339, -0.9900,  0.0519, -0.9905],\n",
      "        [ 1.3959, -1.1579, -0.0553,  2.4318,  0.3886],\n",
      "        [ 1.1655,  0.4583,  1.1820,  0.4032,  0.4389]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8670,  0.6045, -0.0705,  0.6630,  1.1495],\n",
      "        [ 0.1628, -0.5014, -0.0633,  0.0660,  0.0804],\n",
      "        [ 1.9547,  2.5000,  2.6853,  2.2144,  2.6967],\n",
      "        [ 0.3098, -0.1401,  0.2123, -0.5008,  0.4010]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2679, -0.3594, -0.1895,  0.9073,  1.3768],\n",
      "        [-1.5535,  0.9339, -0.9900,  0.0519, -0.9905],\n",
      "        [ 1.3959, -1.1579, -0.0553,  2.4318,  0.3886],\n",
      "        [ 1.1655,  0.4583,  1.1820,  0.4032,  0.4389]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7481],\n",
      "        [-0.7348],\n",
      "        [ 6.1183],\n",
      "        [ 0.5219]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7780,  0.0539, -0.3690, -0.6196, -0.7529],\n",
      "        [ 0.5953,  0.0056,  1.4431, -2.2617,  0.3454],\n",
      "        [ 0.3999,  0.2920, -1.2612, -1.8759,  0.0223],\n",
      "        [-0.3165, -0.3535,  0.7086, -0.3778, -0.7394]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0052, -0.6688, -0.1695,  0.0827, -0.8598],\n",
      "        [ 0.0778, -0.2336,  0.3850, -0.0479, -0.6858],\n",
      "        [ 0.2564,  0.8836,  0.5250,  0.0464, -0.5864],\n",
      "        [ 0.5254,  0.7108,  0.1777,  0.5268, -0.8302]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7780,  0.0539, -0.3690, -0.6196, -0.7529],\n",
      "        [ 0.5953,  0.0056,  1.4431, -2.2617,  0.3454],\n",
      "        [ 0.3999,  0.2920, -1.2612, -1.8759,  0.0223],\n",
      "        [-0.3165, -0.3535,  0.7086, -0.3778, -0.7394]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6186],\n",
      "        [ 0.4720],\n",
      "        [-0.4016],\n",
      "        [ 0.1233]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0778,  1.2691, -0.4830, -1.7006,  0.0910],\n",
      "        [-1.0301,  0.9729,  0.7999, -1.5314, -1.5005],\n",
      "        [-1.1173, -0.6071, -0.3422,  0.9237,  0.3264],\n",
      "        [ 0.3258, -0.0097,  0.1307,  1.4990, -0.1006]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1828,  0.5979, -0.5962, -0.3427, -0.0162],\n",
      "        [-0.2369, -0.5428, -0.5780, -0.5081, -0.6016],\n",
      "        [ 0.1077,  1.0905,  0.3179,  0.5736,  0.7962],\n",
      "        [-0.0517,  0.0379, -0.1512,  0.1879,  0.3709]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0778,  1.2691, -0.4830, -1.7006,  0.0910],\n",
      "        [-1.0301,  0.9729,  0.7999, -1.5314, -1.5005],\n",
      "        [-1.1173, -0.6071, -0.3422,  0.9237,  0.3264],\n",
      "        [ 0.3258, -0.0097,  0.1307,  1.4990, -0.1006]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6424],\n",
      "        [ 0.9343],\n",
      "        [-0.1014],\n",
      "        [ 0.2073]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8511, -0.1576,  1.4891, -0.0583,  1.2770],\n",
      "        [-0.2245,  0.6610, -0.5254, -0.1279,  2.4619],\n",
      "        [-0.8203,  0.0652,  1.0358,  0.3576, -0.8095],\n",
      "        [ 1.9284,  0.3695,  0.0278, -1.3459,  0.4733]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8487, -0.9489, -0.9090, -0.7114, -1.5359],\n",
      "        [-0.3494, -0.1660, -1.2981, -0.5928, -1.5772],\n",
      "        [ 0.3056,  1.3995,  0.8694,  0.9460,  0.7379],\n",
      "        [ 0.2966, -0.3845, -0.3465,  0.4425,  0.1014]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8511, -0.1576,  1.4891, -0.0583,  1.2770],\n",
      "        [-0.2245,  0.6610, -0.5254, -0.1279,  2.4619],\n",
      "        [-0.8203,  0.0652,  1.0358,  0.3576, -0.8095],\n",
      "        [ 1.9284,  0.3695,  0.0278, -1.3459,  0.4733]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.8461],\n",
      "        [-3.1564],\n",
      "        [ 0.4820],\n",
      "        [-0.1274]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6662,  1.4710, -0.6191, -0.8059, -1.2150],\n",
      "        [-0.2431, -1.1209,  0.7548,  0.8618,  0.0650],\n",
      "        [ 0.8360, -1.7523,  0.0040, -0.3066,  0.2057],\n",
      "        [-0.3826,  0.9716,  1.9187, -0.4909, -0.5048]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2202,  1.0469,  1.8594,  1.2337,  0.9952],\n",
      "        [ 0.6561,  0.6488,  0.9489,  0.9510,  1.4386],\n",
      "        [ 0.0482,  1.1652,  1.1532,  1.2265,  1.0764],\n",
      "        [-0.3259,  0.2503,  0.1451,  0.1973,  0.6716]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6662,  1.4710, -0.6191, -0.8059, -1.2150],\n",
      "        [-0.2431, -1.1209,  0.7548,  0.8618,  0.0650],\n",
      "        [ 0.8360, -1.7523,  0.0040, -0.3066,  0.2057],\n",
      "        [-0.3826,  0.9716,  1.9187, -0.4909, -0.5048]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.6275],\n",
      "        [ 0.7425],\n",
      "        [-2.1515],\n",
      "        [ 0.2105]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6852,  2.1235, -1.6978,  0.9433,  0.1137],\n",
      "        [ 1.4449,  0.0131,  2.1362, -0.9229, -0.6428],\n",
      "        [-0.7751,  0.1833, -1.4563,  0.5729,  1.5648],\n",
      "        [ 0.6251,  0.5544, -2.3462,  1.1305, -2.2119]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.7827,  2.4516,  2.2444,  2.1350,  3.0567],\n",
      "        [ 0.3605,  0.2201,  0.8012,  0.2404, -0.2873],\n",
      "        [ 0.6040,  1.9606,  1.2708,  1.7609,  1.6755],\n",
      "        [-0.0856,  0.2524, -0.3703,  0.1276,  0.4859]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6852,  2.1235, -1.6978,  0.9433,  0.1137],\n",
      "        [ 1.4449,  0.0131,  2.1362, -0.9229, -0.6428],\n",
      "        [-0.7751,  0.1833, -1.4563,  0.5729,  1.5648],\n",
      "        [ 0.6251,  0.5544, -2.3462,  1.1305, -2.2119]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5356],\n",
      "        [ 2.1980],\n",
      "        [ 1.6713],\n",
      "        [ 0.0248]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7474,  0.9522,  1.0854,  0.0993,  1.1319],\n",
      "        [-0.8972, -0.9552,  0.0214, -0.0804, -0.5423],\n",
      "        [-1.1195, -1.3154, -0.0213,  0.9946, -1.2427],\n",
      "        [-0.7974, -0.7653,  1.9782,  0.0630, -0.5896]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3550,  0.3368,  0.7581,  0.6527,  0.2979],\n",
      "        [-0.5226, -0.5258, -0.7491, -0.8811, -1.5827],\n",
      "        [ 0.8758,  1.7151,  1.7570,  1.3900,  0.9434],\n",
      "        [-0.0920,  1.1009, -0.4235,  0.0670,  1.0913]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7474,  0.9522,  1.0854,  0.0993,  1.1319],\n",
      "        [-0.8972, -0.9552,  0.0214, -0.0804, -0.5423],\n",
      "        [-1.1195, -1.3154, -0.0213,  0.9946, -1.2427],\n",
      "        [-0.7974, -0.7653,  1.9782,  0.0630, -0.5896]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2802],\n",
      "        [ 1.8842],\n",
      "        [-3.0639],\n",
      "        [-2.2461]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2163,  0.9544, -1.0722,  0.7961, -0.4189],\n",
      "        [-0.3254,  1.5673,  1.7742,  1.0820, -0.0860],\n",
      "        [ 0.9643,  0.9643, -1.6943,  0.5144,  0.6851],\n",
      "        [ 1.3572, -1.3932,  1.4121, -2.0277,  0.5362]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5402, -1.0525, -0.7491, -0.5182, -1.0570],\n",
      "        [-0.2823, -1.7129, -1.6907, -1.7530, -3.0543],\n",
      "        [-0.4081,  0.1579,  0.2044,  0.2420, -0.0920],\n",
      "        [ 0.8501,  2.4966,  1.1178,  1.2662,  1.8826]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2163,  0.9544, -1.0722,  0.7961, -0.4189],\n",
      "        [-0.3254,  1.5673,  1.7742,  1.0820, -0.0860],\n",
      "        [ 0.9643,  0.9643, -1.6943,  0.5144,  0.6851],\n",
      "        [ 1.3572, -1.3932,  1.4121, -2.0277,  0.5362]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8282],\n",
      "        [-7.2263],\n",
      "        [-0.5262],\n",
      "        [-2.3042]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2108, -0.1313, -0.1733,  0.0754, -0.8491],\n",
      "        [-1.8541,  1.6491,  0.7838, -0.4350, -0.5061],\n",
      "        [ 0.1073,  1.2817,  0.6510, -0.6587,  1.0259],\n",
      "        [ 0.8686,  0.9785,  0.8028, -0.1093, -0.5383]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0424, -0.3828, -0.4006,  0.0336,  0.0792],\n",
      "        [ 1.3381,  1.5276,  0.7173,  0.9703,  2.1150],\n",
      "        [ 0.2660,  0.5456,  0.2179, -0.8066, -0.5658],\n",
      "        [ 1.3950,  1.2444,  1.3138,  1.8742,  2.2071]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2108, -0.1313, -0.1733,  0.0754, -0.8491],\n",
      "        [-1.8541,  1.6491,  0.7838, -0.4350, -0.5061],\n",
      "        [ 0.1073,  1.2817,  0.6510, -0.6587,  1.0259],\n",
      "        [ 0.8686,  0.9785,  0.8028, -0.1093, -0.5383]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0639],\n",
      "        [-0.8922],\n",
      "        [ 0.8205],\n",
      "        [ 2.0912]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8214,  0.9061, -1.3976, -1.3109, -0.2606],\n",
      "        [ 0.2213,  0.1757,  1.5013,  0.1131,  1.6710],\n",
      "        [ 0.2153,  0.4737,  0.8175,  0.8434, -0.3558],\n",
      "        [-2.1380, -1.4875,  1.1480, -1.8449,  0.9299]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4665, -0.0520, -0.7088,  0.0095, -0.3831],\n",
      "        [ 1.2297,  1.4582,  1.7655,  1.9081,  2.0248],\n",
      "        [ 0.3292, -0.2138, -1.1303,  0.0897, -0.1748],\n",
      "        [ 0.2416,  0.1428,  0.1897, -0.0684,  0.3286]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8214,  0.9061, -1.3976, -1.3109, -0.2606],\n",
      "        [ 0.2213,  0.1757,  1.5013,  0.1131,  1.6710],\n",
      "        [ 0.2153,  0.4737,  0.8175,  0.8434, -0.3558],\n",
      "        [-2.1380, -1.4875,  1.1480, -1.8449,  0.9299]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4140],\n",
      "        [ 6.7781],\n",
      "        [-0.8166],\n",
      "        [-0.0793]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0737,  0.0527, -0.4799,  0.5301, -0.7219],\n",
      "        [ 0.7004, -1.7306,  1.0699,  1.7855,  0.1424],\n",
      "        [ 0.1167,  0.6879, -0.4848,  2.0893, -0.6324],\n",
      "        [-0.6424,  0.6704, -0.8801,  0.3067, -1.1465]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5986, -0.8558, -0.4769, -1.0536, -0.8624],\n",
      "        [-0.9235, -1.7004, -0.8350, -1.5209, -2.6323],\n",
      "        [ 0.5186,  0.2333, -0.6861,  0.5700,  0.6972],\n",
      "        [ 0.4959, -0.0694,  0.5148, -0.5786,  0.1179]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0737,  0.0527, -0.4799,  0.5301, -0.7219],\n",
      "        [ 0.7004, -1.7306,  1.0699,  1.7855,  0.1424],\n",
      "        [ 0.1167,  0.6879, -0.4848,  2.0893, -0.6324],\n",
      "        [-0.6424,  0.6704, -0.8801,  0.3067, -1.1465]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2037],\n",
      "        [-1.6879],\n",
      "        [ 1.3037],\n",
      "        [-1.1307]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0216, -0.0090, -1.7036, -2.9012, -0.0962],\n",
      "        [ 0.3598,  0.6363,  0.8526,  2.5431, -1.3721],\n",
      "        [-0.7365,  0.1245,  0.0603, -1.2715,  1.7475],\n",
      "        [-1.9654,  0.0174, -0.6447, -0.2451, -0.8295]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2254, -0.0198, -0.3664, -0.4133, -0.7302],\n",
      "        [ 0.2567, -0.8476, -0.9845, -0.3446, -1.8411],\n",
      "        [-0.6066, -0.2877, -0.5638, -0.8931, -0.5907],\n",
      "        [ 0.7051,  0.3689,  1.0140,  0.7940,  0.6249]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0216, -0.0090, -1.7036, -2.9012, -0.0962],\n",
      "        [ 0.3598,  0.6363,  0.8526,  2.5431, -1.3721],\n",
      "        [-0.7365,  0.1245,  0.0603, -1.2715,  1.7475],\n",
      "        [-1.9654,  0.0174, -0.6447, -0.2451, -0.8295]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8986],\n",
      "        [ 0.3635],\n",
      "        [ 0.4804],\n",
      "        [-2.7461]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8119,  0.3110,  0.5103,  0.3281, -0.2655],\n",
      "        [-0.2588,  0.4163,  0.5589, -0.2322, -1.4787],\n",
      "        [-0.9427, -0.2822, -1.2642, -0.0769, -0.9163],\n",
      "        [-0.8035, -0.5475, -1.6705,  1.1879,  0.3328]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7790, -0.6535, -1.0375, -0.7221, -1.8297],\n",
      "        [-0.5795, -0.7128, -0.8807, -0.3573, -1.3185],\n",
      "        [-0.6131, -0.4960, -0.7169, -0.8652, -0.3098],\n",
      "        [ 1.6453,  1.2918,  1.2331,  0.9175,  1.6941]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8119,  0.3110,  0.5103,  0.3281, -0.2655],\n",
      "        [-0.2588,  0.4163,  0.5589, -0.2322, -1.4787],\n",
      "        [-0.9427, -0.2822, -1.2642, -0.0769, -0.9163],\n",
      "        [-0.8035, -0.5475, -1.6705,  1.1879,  0.3328]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8953],\n",
      "        [ 1.3936],\n",
      "        [ 1.9747],\n",
      "        [-2.4355]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5688,  2.0260,  1.4808, -2.4085,  0.3368],\n",
      "        [-0.4748,  0.1279,  1.9214,  0.4097, -1.1943],\n",
      "        [ 0.5961, -1.7491,  0.8841, -0.0148, -0.1703],\n",
      "        [-0.3425, -0.4794,  0.2523, -0.7901, -1.2896]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1332,  0.1775, -0.2668, -0.2149, -0.3985],\n",
      "        [-1.1654, -0.5269, -1.4901, -1.0530, -2.0625],\n",
      "        [-0.4446, -1.1073, -1.4935, -0.9902, -2.0050],\n",
      "        [ 2.0278,  2.0977,  2.3712,  1.8676,  3.1077]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5688,  2.0260,  1.4808, -2.4085,  0.3368],\n",
      "        [-0.4748,  0.1279,  1.9214,  0.4097, -1.1943],\n",
      "        [ 0.5961, -1.7491,  0.8841, -0.0148, -0.1703],\n",
      "        [-0.3425, -0.4794,  0.2523, -0.7901, -1.2896]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5570],\n",
      "        [-0.3453],\n",
      "        [ 0.7078],\n",
      "        [-6.5851]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0018,  2.8376, -1.3566,  0.6956, -0.3283],\n",
      "        [ 0.4575,  0.7452,  0.0759, -2.8631, -0.1424],\n",
      "        [-1.4972, -0.8575, -0.1852, -0.5910, -0.2786],\n",
      "        [-0.1733, -0.3263, -0.0015, -0.1502, -0.7511]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5437, -0.4677, -0.5594, -0.2480, -1.0005],\n",
      "        [-0.4308, -0.4972, -0.7989, -0.8190, -0.5623],\n",
      "        [-1.0074, -1.6387, -1.2771, -1.4757, -1.8556],\n",
      "        [ 0.0979, -0.5343,  0.1364, -0.2451,  0.0369]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0018,  2.8376, -1.3566,  0.6956, -0.3283],\n",
      "        [ 0.4575,  0.7452,  0.0759, -2.8631, -0.1424],\n",
      "        [-1.4972, -0.8575, -0.1852, -0.5910, -0.2786],\n",
      "        [-0.1733, -0.3263, -0.0015, -0.1502, -0.7511]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1323],\n",
      "        [ 1.7967],\n",
      "        [ 4.5391],\n",
      "        [ 0.1663]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1119,  0.2468, -0.7633,  0.7321,  1.2098],\n",
      "        [-0.4739, -1.4356,  1.0651,  0.9665,  1.0240],\n",
      "        [ 0.5455, -0.0849, -2.2495,  1.5090, -1.0230],\n",
      "        [-1.2421,  0.4893,  0.5745, -1.5693, -0.2035]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1831, -0.0174, -0.1286, -0.3024, -0.4969],\n",
      "        [-0.7976, -0.8264, -1.1411, -1.6595, -2.4795],\n",
      "        [-0.4191,  0.4604, -0.4672,  0.2381, -0.1192],\n",
      "        [ 0.4592, -0.0992,  0.0322, -0.4533, -0.3345]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1119,  0.2468, -0.7633,  0.7321,  1.2098],\n",
      "        [-0.4739, -1.4356,  1.0651,  0.9665,  1.0240],\n",
      "        [ 0.5455, -0.0849, -2.2495,  1.5090, -1.0230],\n",
      "        [-1.2421,  0.4893,  0.5745, -1.5693, -0.2035]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7082],\n",
      "        [-3.7939],\n",
      "        [ 1.2645],\n",
      "        [ 0.1791]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5941,  0.5383,  0.5532, -0.3302, -0.7498],\n",
      "        [ 0.3177,  0.8919,  0.7907, -0.8050, -0.0072],\n",
      "        [ 1.3849,  1.2648, -0.6319,  0.2639,  1.9121],\n",
      "        [ 0.3667,  1.2331, -0.1068,  0.8467,  1.8092]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0575, -0.3810, -0.4878,  0.0600, -0.5097],\n",
      "        [ 0.7175,  0.0921, -0.1227, -0.2439,  1.1903],\n",
      "        [-0.3841, -0.3929, -0.7901, -1.1680, -1.1399],\n",
      "        [ 0.4676, -0.1737,  0.4045,  0.2770, -0.1972]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5941,  0.5383,  0.5532, -0.3302, -0.7498],\n",
      "        [ 0.3177,  0.8919,  0.7907, -0.8050, -0.0072],\n",
      "        [ 1.3849,  1.2648, -0.6319,  0.2639,  1.9121],\n",
      "        [ 0.3667,  1.2331, -0.1068,  0.8467,  1.8092]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2042],\n",
      "        [ 0.4009],\n",
      "        [-3.0174],\n",
      "        [-0.2081]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7874, -0.1421,  1.5980, -0.2868, -1.0045],\n",
      "        [-0.5094,  0.6613, -0.5004, -0.1801, -0.6445],\n",
      "        [ 0.9970,  0.1438, -0.6071,  3.7528,  0.9294],\n",
      "        [-1.3656,  0.1592,  0.0056,  1.5116, -0.7153]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5610, -0.3497, -0.8624, -0.5734, -1.3225],\n",
      "        [ 0.6517, -0.5637, -0.9077, -1.0071, -1.1998],\n",
      "        [ 1.0492,  0.6898,  0.7751,  0.9835,  2.1149],\n",
      "        [ 0.3260, -0.1563, -0.0109,  0.0762,  0.2089]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7874, -0.1421,  1.5980, -0.2868, -1.0045],\n",
      "        [-0.5094,  0.6613, -0.5004, -0.1801, -0.6445],\n",
      "        [ 0.9970,  0.1438, -0.6071,  3.7528,  0.9294],\n",
      "        [-1.3656,  0.1592,  0.0056,  1.5116, -0.7153]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2772],\n",
      "        [ 0.7042],\n",
      "        [ 6.3312],\n",
      "        [-0.5044]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0626, -0.2186, -0.0409,  0.0553, -0.7048],\n",
      "        [-0.3554,  1.4196, -0.1194,  0.5550,  0.0160],\n",
      "        [-1.4243,  0.8615,  1.0185,  1.4430, -1.8894],\n",
      "        [-0.3084,  0.0559, -0.4301,  0.0685,  1.4226]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1150, -0.0647, -0.7698, -0.9862, -1.3451],\n",
      "        [ 0.0800, -0.6062, -1.1650, -0.9633, -1.7514],\n",
      "        [-1.2291, -1.4040, -1.7676, -1.8384, -3.1469],\n",
      "        [-0.3811, -0.6953, -0.6946,  0.2358,  0.6271]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0626, -0.2186, -0.0409,  0.0553, -0.7048],\n",
      "        [-0.3554,  1.4196, -0.1194,  0.5550,  0.0160],\n",
      "        [-1.4243,  0.8615,  1.0185,  1.4430, -1.8894],\n",
      "        [-0.3084,  0.0559, -0.4301,  0.0685,  1.4226]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9320],\n",
      "        [-1.3126],\n",
      "        [ 2.0339],\n",
      "        [ 1.2857]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0366,  0.6315, -0.1919,  0.9862,  0.7550],\n",
      "        [ 0.2416,  0.8083,  0.7332,  1.3839,  0.5622],\n",
      "        [-0.4643, -1.5146, -0.4509,  0.8200,  0.5959],\n",
      "        [-0.8465, -0.5485,  0.7770, -0.4115,  1.3897]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2162, -0.8239, -0.5673, -0.1485, -1.4114],\n",
      "        [ 0.1619, -0.4995, -0.5190, -0.9188, -1.2588],\n",
      "        [-2.1127, -1.2196, -2.3712, -2.0401, -5.0211],\n",
      "        [-0.5943, -1.2537, -1.0038, -0.3423, -0.4998]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0366,  0.6315, -0.1919,  0.9862,  0.7550],\n",
      "        [ 0.2416,  0.8083,  0.7332,  1.3839,  0.5622],\n",
      "        [-0.4643, -1.5146, -0.4509,  0.8200,  0.5959],\n",
      "        [-0.8465, -0.5485,  0.7770, -0.4115,  1.3897]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6314],\n",
      "        [-2.7244],\n",
      "        [-0.7676],\n",
      "        [-0.1431]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9189, -0.4527,  1.1472, -0.8371, -0.3018],\n",
      "        [ 1.0069,  2.8423, -0.5295, -0.5745, -0.0494],\n",
      "        [ 1.8741,  0.9235,  1.8699, -0.5573, -0.0900],\n",
      "        [-0.4022,  1.0604,  0.7510,  1.4182,  0.4910]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0351, -0.8698, -0.5515,  0.1363, -0.0378],\n",
      "        [ 1.1520,  0.5688,  0.5282,  0.7342,  1.3773],\n",
      "        [-0.0067,  0.4762,  0.3444,  0.1063, -0.1510],\n",
      "        [ 0.0979, -0.0894,  0.0712, -0.4525, -0.5542]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9189, -0.4527,  1.1472, -0.8371, -0.3018],\n",
      "        [ 1.0069,  2.8423, -0.5295, -0.5745, -0.0494],\n",
      "        [ 1.8741,  0.9235,  1.8699, -0.5573, -0.0900],\n",
      "        [-0.4022,  1.0604,  0.7510,  1.4182,  0.4910]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4090],\n",
      "        [ 2.0070],\n",
      "        [ 1.0257],\n",
      "        [-0.9946]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6338,  1.4285, -1.9349, -1.1022,  0.1451],\n",
      "        [-0.1383, -0.1477,  1.8097, -1.4509, -1.0223],\n",
      "        [ 1.1158,  0.5431,  0.3048, -0.7384, -0.6518],\n",
      "        [ 1.7738, -0.2668,  0.4715,  0.7521,  1.4070]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1147, -0.7906, -0.8072, -1.4765, -0.6134],\n",
      "        [ 0.5468, -0.8994, -0.5859, -0.7316, -1.0460],\n",
      "        [-0.0010, -0.6426, -0.2391,  0.0603, -0.4032],\n",
      "        [ 0.2169,  0.0785, -0.3316,  0.2139,  0.0456]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6338,  1.4285, -1.9349, -1.1022,  0.1451],\n",
      "        [-0.1383, -0.1477,  1.8097, -1.4509, -1.0223],\n",
      "        [ 1.1158,  0.5431,  0.3048, -0.7384, -0.6518],\n",
      "        [ 1.7738, -0.2668,  0.4715,  0.7521,  1.4070]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0436],\n",
      "        [ 1.1277],\n",
      "        [-0.2047],\n",
      "        [ 0.4324]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.2405,  0.5928,  0.1204, -0.7125,  1.6402],\n",
      "        [-0.9397, -1.9302,  2.1296,  1.0633, -1.9866],\n",
      "        [ 0.3105,  0.7492, -0.5436,  1.0098,  2.0702],\n",
      "        [ 1.3010,  1.3305, -0.1034, -1.6788, -1.7587]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4927, -2.0695, -0.8440, -1.5920, -2.7517],\n",
      "        [-0.2196, -0.9081, -0.7564, -1.6124, -2.0113],\n",
      "        [-0.0256, -0.2121,  0.1609,  0.3509, -0.0755],\n",
      "        [ 0.3330,  0.1845,  0.3117, -0.4293, -0.0326]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.2405,  0.5928,  0.1204, -0.7125,  1.6402],\n",
      "        [-0.9397, -1.9302,  2.1296,  1.0633, -1.9866],\n",
      "        [ 0.3105,  0.7492, -0.5436,  1.0098,  2.0702],\n",
      "        [ 1.3010,  1.3305, -0.1034, -1.6788, -1.7587]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.6037],\n",
      "        [ 2.6295],\n",
      "        [-0.0562],\n",
      "        [ 1.4246]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2390,  1.0342,  0.9710, -0.6697,  0.3266],\n",
      "        [ 1.6199,  1.3265, -0.2402,  0.0367,  1.8872],\n",
      "        [-0.1902,  0.0495,  0.3852, -0.2580, -0.0749],\n",
      "        [-0.6961,  0.5770, -0.1392,  0.9172,  0.0605]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9895, -0.0100,  0.1827, -0.3391,  1.1937],\n",
      "        [-0.7647, -1.4608, -2.0910, -2.4498, -3.5349],\n",
      "        [-0.0058, -0.1543, -0.3096,  0.2094,  0.0229],\n",
      "        [-0.6014, -0.7182, -0.3724, -1.3464, -1.2977]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2390,  1.0342,  0.9710, -0.6697,  0.3266],\n",
      "        [ 1.6199,  1.3265, -0.2402,  0.0367,  1.8872],\n",
      "        [-0.1902,  0.0495,  0.3852, -0.2580, -0.0749],\n",
      "        [-0.6961,  0.5770, -0.1392,  0.9172,  0.0605]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0204],\n",
      "        [-9.4349],\n",
      "        [-0.1815],\n",
      "        [-1.2572]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3720, -0.2828,  0.6700, -0.6032, -0.1686],\n",
      "        [-1.6143,  0.2997, -0.1215, -1.4446,  0.7003],\n",
      "        [-0.0944, -0.0323,  0.8481,  0.1868, -0.8221],\n",
      "        [-0.0840,  1.4189, -0.9632, -1.3408, -2.5559]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1099, -0.9284, -0.3525, -0.8697, -0.8594],\n",
      "        [ 1.7717,  2.0614,  1.9992,  0.9145,  2.9860],\n",
      "        [-0.0445, -0.1673, -0.3370,  0.0242, -0.4963],\n",
      "        [ 0.3825, -0.6510, -0.2550, -0.3217, -0.4777]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3720, -0.2828,  0.6700, -0.6032, -0.1686],\n",
      "        [-1.6143,  0.2997, -0.1215, -1.4446,  0.7003],\n",
      "        [-0.0944, -0.0323,  0.8481,  0.1868, -0.8221],\n",
      "        [-0.0840,  1.4189, -0.9632, -1.3408, -2.5559]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5451],\n",
      "        [-1.7150],\n",
      "        [ 0.1363],\n",
      "        [ 0.9421]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2789,  0.7322,  1.4541, -0.3419,  0.0265],\n",
      "        [-0.9876,  1.2881, -1.5810,  0.3555, -0.7996],\n",
      "        [-0.0867,  0.2420, -0.1654,  0.8767,  1.4973],\n",
      "        [ 1.0188,  0.4806, -0.8418, -0.0681, -1.0416]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2695, -1.6549, -0.8002, -1.0486, -1.6538],\n",
      "        [ 2.0355,  2.4218,  2.5767,  1.4088,  3.7252],\n",
      "        [-0.9621, -0.0785, -0.1128, -0.2614, -0.2248],\n",
      "        [-0.4407, -0.5180, -1.1262, -0.5381, -0.7340]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2789,  0.7322,  1.4541, -0.3419,  0.0265],\n",
      "        [-0.9876,  1.2881, -1.5810,  0.3555, -0.7996],\n",
      "        [-0.0867,  0.2420, -0.1654,  0.8767,  1.4973],\n",
      "        [ 1.0188,  0.4806, -0.8418, -0.0681, -1.0416]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9856],\n",
      "        [-5.4423],\n",
      "        [-0.4827],\n",
      "        [ 1.0513]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7739,  1.6838,  2.2671,  0.2691, -0.8634],\n",
      "        [ 0.5993, -0.1262, -0.6176,  0.5335, -0.8651],\n",
      "        [-1.9552,  0.2261,  0.5590, -0.7494, -1.1335],\n",
      "        [-0.9566,  1.1582, -0.8569,  0.3882,  0.8351]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7255, -0.8900, -0.2975, -0.6745, -0.7274],\n",
      "        [-0.0214,  1.0855, -0.1367, -0.2500,  0.8362],\n",
      "        [-0.2221, -0.4518,  0.1330, -0.1947,  0.1392],\n",
      "        [-1.1018, -0.8594,  0.0767, -0.2810, -1.7472]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7739,  1.6838,  2.2671,  0.2691, -0.8634],\n",
      "        [ 0.5993, -0.1262, -0.6176,  0.5335, -0.8651],\n",
      "        [-1.9552,  0.2261,  0.5590, -0.7494, -1.1335],\n",
      "        [-0.9566,  1.1582, -0.8569,  0.3882,  0.8351]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1650],\n",
      "        [-0.9222],\n",
      "        [ 0.3946],\n",
      "        [-1.5753]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.0207, -0.8965,  1.3101, -0.4615, -0.1231],\n",
      "        [-0.3692,  1.0422,  1.5059,  1.4823,  2.3210],\n",
      "        [-1.2854,  1.0461,  0.0644,  0.9221,  1.3225],\n",
      "        [ 0.4322, -0.8372, -0.0984,  0.0482,  0.3042]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9195, -0.1286, -0.7181,  0.0442,  0.7476],\n",
      "        [ 0.6477,  1.1937, -0.1173, -0.1896,  0.7772],\n",
      "        [-0.1568, -0.0125, -0.0259, -0.2370,  0.1376],\n",
      "        [ 0.4183, -0.2509, -0.2892, -0.3867,  0.3176]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.0207, -0.8965,  1.3101, -0.4615, -0.1231],\n",
      "        [-0.3692,  1.0422,  1.5059,  1.4823,  2.3210],\n",
      "        [-1.2854,  1.0461,  0.0644,  0.9221,  1.3225],\n",
      "        [ 0.4322, -0.8372, -0.0984,  0.0482,  0.3042]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9202],\n",
      "        [ 2.3511],\n",
      "        [ 0.1503],\n",
      "        [ 0.4973]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4176,  0.6777, -0.0107,  0.0763,  0.1305],\n",
      "        [-1.3450, -1.6645, -1.3968,  0.7479,  0.4009],\n",
      "        [-0.8004, -0.2921,  2.3411,  0.5122, -0.3613],\n",
      "        [ 0.7615, -2.4177, -1.7151,  1.3873, -0.1193]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8371, -1.2612, -1.0851, -0.7080, -1.5157],\n",
      "        [-0.8267, -1.3302, -1.2515, -0.6638, -1.5578],\n",
      "        [ 0.0929, -0.0534,  1.1147, -0.2431, -0.0027],\n",
      "        [-0.1783, -0.7519, -0.1389,  0.0226, -0.6092]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4176,  0.6777, -0.0107,  0.0763,  0.1305],\n",
      "        [-1.3450, -1.6645, -1.3968,  0.7479,  0.4009],\n",
      "        [-0.8004, -0.2921,  2.3411,  0.5122, -0.3613],\n",
      "        [ 0.7615, -2.4177, -1.7151,  1.3873, -0.1193]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4446],\n",
      "        [ 3.9532],\n",
      "        [ 2.4273],\n",
      "        [ 2.0244]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6092,  2.4508, -0.4590,  0.8961, -0.4524],\n",
      "        [-1.0898,  0.4550, -0.4778,  1.2392, -0.7831],\n",
      "        [-0.7010, -1.7309, -1.1417, -0.6983,  0.2697],\n",
      "        [-1.0075, -1.0850, -0.9451, -2.2506, -0.0948]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8561, -0.6414, -0.9549, -0.4783, -0.0659],\n",
      "        [-1.1786, -1.6941, -1.6596, -2.6317, -4.7482],\n",
      "        [-0.3978, -1.1228, -0.5786, -0.6772, -2.0246],\n",
      "        [-0.3515, -0.7926, -1.1825, -0.9891, -1.5102]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6092,  2.4508, -0.4590,  0.8961, -0.4524],\n",
      "        [-1.0898,  0.4550, -0.4778,  1.2392, -0.7831],\n",
      "        [-0.7010, -1.7309, -1.1417, -0.6983,  0.2697],\n",
      "        [-1.0075, -1.0850, -0.9451, -2.2506, -0.0948]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0541],\n",
      "        [ 1.7636],\n",
      "        [ 2.8097],\n",
      "        [ 4.7009]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2042, -0.2846, -0.2852, -0.8509, -0.4839],\n",
      "        [-0.5347, -0.1559,  1.0154,  2.5920, -0.0264],\n",
      "        [-0.2308, -0.2895, -0.8245, -0.3654, -0.1561],\n",
      "        [ 1.0621, -0.4379,  1.2132,  1.0363, -0.5899]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6059,  0.7248,  0.2869, -0.2231,  1.9957],\n",
      "        [-0.2142,  0.5574, -0.2817,  0.1651,  0.8618],\n",
      "        [-0.8997, -1.7013, -1.7589, -2.1198, -3.5828],\n",
      "        [ 0.2585, -0.0104, -0.3049,  0.0930, -0.4158]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2042, -0.2846, -0.2852, -0.8509, -0.4839],\n",
      "        [-0.5347, -0.1559,  1.0154,  2.5920, -0.0264],\n",
      "        [-0.2308, -0.2895, -0.8245, -0.3654, -0.1561],\n",
      "        [ 1.0621, -0.4379,  1.2132,  1.0363, -0.5899]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9977],\n",
      "        [ 0.1468],\n",
      "        [ 3.4843],\n",
      "        [ 0.2509]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8572,  0.1396, -0.2039,  0.2410,  1.6207],\n",
      "        [-0.2821, -0.5428,  1.4170,  0.5393,  1.6998],\n",
      "        [ 0.2675, -0.6619, -0.0428, -0.2241,  0.1230],\n",
      "        [-0.1652,  0.3559,  0.1631, -0.9376, -0.2643]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5327,  2.7579,  2.2909,  0.9529,  2.4115],\n",
      "        [-0.1215,  0.9040,  0.1272,  0.7041,  0.4438],\n",
      "        [ 0.3316,  0.6393,  0.0907,  0.2862,  0.4469],\n",
      "        [-0.0368, -0.0412, -0.4083,  0.5952,  0.1216]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8572,  0.1396, -0.2039,  0.2410,  1.6207],\n",
      "        [-0.2821, -0.5428,  1.4170,  0.5393,  1.6998],\n",
      "        [ 0.2675, -0.6619, -0.0428, -0.2241,  0.1230],\n",
      "        [-0.1652,  0.3559,  0.1631, -0.9376, -0.2643]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.3696],\n",
      "        [ 0.8579],\n",
      "        [-0.3475],\n",
      "        [-0.6653]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5176, -0.3956,  1.9752, -0.0188, -0.5674],\n",
      "        [ 1.4747,  0.0887, -1.4944,  0.9921, -0.4782],\n",
      "        [ 0.1338,  0.3679, -1.3924,  0.8598, -1.2199],\n",
      "        [-1.0807, -0.2139,  1.6724,  0.1329, -0.2310]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3399, -0.6737, -1.7254, -0.2508, -1.9236],\n",
      "        [-0.0306,  0.0204, -0.1376,  0.1775, -0.4551],\n",
      "        [ 0.7815, -0.2712,  0.0972, -0.0432, -0.0480],\n",
      "        [ 0.3332, -0.1578,  0.3534, -0.6174,  0.4142]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5176, -0.3956,  1.9752, -0.0188, -0.5674],\n",
      "        [ 1.4747,  0.0887, -1.4944,  0.9921, -0.4782],\n",
      "        [ 0.1338,  0.3679, -1.3924,  0.8598, -1.2199],\n",
      "        [-1.0807, -0.2139,  1.6724,  0.1329, -0.2310]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2212],\n",
      "        [ 0.5560],\n",
      "        [-0.1091],\n",
      "        [ 0.0870]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3862, -0.6981,  0.6624, -0.3572,  0.0139],\n",
      "        [-0.1890,  0.2284,  1.4954, -1.5384,  0.5923],\n",
      "        [-0.4399,  0.3261, -0.5410,  0.3750,  1.3877],\n",
      "        [ 1.4640,  0.5145, -0.9287, -1.7413, -0.5482]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2764,  0.5472,  0.3116,  0.0397,  0.4249],\n",
      "        [-0.2772,  0.0838,  0.2197, -0.4855, -0.0595],\n",
      "        [-0.0957, -0.4384, -0.5351, -0.1689,  0.0122],\n",
      "        [ 0.6954, -0.3151, -0.1143, -0.3737,  0.0510]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3862, -0.6981,  0.6624, -0.3572,  0.0139],\n",
      "        [-0.1890,  0.2284,  1.4954, -1.5384,  0.5923],\n",
      "        [-0.4399,  0.3261, -0.5410,  0.3750,  1.3877],\n",
      "        [ 1.4640,  0.5145, -0.9287, -1.7413, -0.5482]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6769],\n",
      "        [ 1.1117],\n",
      "        [ 0.1422],\n",
      "        [ 1.5849]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9734, -1.2056, -0.3246, -0.1102, -2.3584],\n",
      "        [-0.2924,  0.3269, -0.4186, -1.2763,  0.7688],\n",
      "        [-1.2788, -0.2578,  1.3735, -0.2764,  0.3509],\n",
      "        [ 1.1953,  0.9281,  0.4531, -2.1296, -0.4499]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9581,  0.0685,  0.4138,  0.7471,  0.9160],\n",
      "        [-0.4895,  0.1352, -0.5014, -0.1881, -1.3534],\n",
      "        [ 0.2336, -0.0925,  0.3770, -0.4200, -0.0783],\n",
      "        [-0.2773, -0.7045, -0.6130, -0.5262, -1.0473]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9734, -1.2056, -0.3246, -0.1102, -2.3584],\n",
      "        [-0.2924,  0.3269, -0.4186, -1.2763,  0.7688],\n",
      "        [-1.2788, -0.2578,  1.3735, -0.2764,  0.3509],\n",
      "        [ 1.1953,  0.9281,  0.4531, -2.1296, -0.4499]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.3921],\n",
      "        [-0.4033],\n",
      "        [ 0.3316],\n",
      "        [ 0.3288]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0505, -0.2819, -0.4077, -0.7250, -1.3556],\n",
      "        [-1.2571,  0.4140,  1.1824,  0.1833,  0.7705],\n",
      "        [-0.4571,  0.7214, -0.4256, -0.6850, -0.7572],\n",
      "        [ 1.4777, -0.0536,  1.5962,  1.2208, -0.0711]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3536, -0.2650,  0.5999, -0.3837,  0.0789],\n",
      "        [ 0.1346,  0.1565,  0.8140,  0.0499,  0.1109],\n",
      "        [ 0.6010, -0.4612, -0.4733, -0.0768, -0.5258],\n",
      "        [-0.3497, -0.9543, -0.7762, -0.6176, -1.2265]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0505, -0.2819, -0.4077, -0.7250, -1.3556],\n",
      "        [-1.2571,  0.4140,  1.1824,  0.1833,  0.7705],\n",
      "        [-0.4571,  0.7214, -0.4256, -0.6850, -0.7572],\n",
      "        [ 1.4777, -0.0536,  1.5962,  1.2208, -0.0711]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3700],\n",
      "        [ 0.9526],\n",
      "        [ 0.0449],\n",
      "        [-2.3714]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4742,  1.2283,  2.3026, -1.6369,  0.8455],\n",
      "        [-0.0208, -1.6544,  1.5827, -0.7253,  1.8844],\n",
      "        [-1.0232,  0.0250, -0.8417,  0.4018,  0.4538],\n",
      "        [-0.7447,  1.3763,  0.6330, -0.4012, -1.8378]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0933, -0.0406, -0.6757, -0.1446, -0.8511],\n",
      "        [-0.1256, -0.3263, -0.2705,  0.0702, -0.8450],\n",
      "        [-0.1052,  0.3668,  0.0210, -0.3200, -0.3543],\n",
      "        [ 1.3665,  0.9090,  1.5671,  0.3953,  0.3256]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4742,  1.2283,  2.3026, -1.6369,  0.8455],\n",
      "        [-0.0208, -1.6544,  1.5827, -0.7253,  1.8844],\n",
      "        [-1.0232,  0.0250, -0.8417,  0.4018,  0.4538],\n",
      "        [-0.7447,  1.3763,  0.6330, -0.4012, -1.8378]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0445],\n",
      "        [-1.5289],\n",
      "        [-0.1903],\n",
      "        [ 0.4683]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0226, -0.5151,  1.3538,  1.5124,  0.2453],\n",
      "        [-0.0764,  1.2165,  0.4491, -1.2993,  1.3041],\n",
      "        [-0.0224, -0.0561, -0.1048, -0.6713,  1.5467],\n",
      "        [ 0.0733,  0.6561,  1.3227, -0.8247, -0.8965]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7079,  1.3318,  0.7123,  0.5355,  1.6206],\n",
      "        [ 0.9826,  0.8353,  0.6099,  1.1348,  1.1978],\n",
      "        [ 0.0643, -0.2184,  0.8915,  0.3816, -0.3206],\n",
      "        [ 0.1246, -0.1262, -0.9305, -0.0507,  0.0697]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0226, -0.5151,  1.3538,  1.5124,  0.2453],\n",
      "        [-0.0764,  1.2165,  0.4491, -1.2993,  1.3041],\n",
      "        [-0.0224, -0.0561, -0.1048, -0.6713,  1.5467],\n",
      "        [ 0.0733,  0.6561,  1.3227, -0.8247, -0.8965]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5016],\n",
      "        [ 1.3026],\n",
      "        [-0.8346],\n",
      "        [-1.3251]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2662, -0.5126,  0.7865,  0.3940,  1.0991],\n",
      "        [ 0.7427,  0.5609, -0.4694, -0.7350,  0.3320],\n",
      "        [ 0.0271, -0.8807,  0.2621, -0.1449,  1.4280],\n",
      "        [-2.1505, -0.6275,  0.2217,  1.9234,  2.1122]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1874, -0.2597, -0.3874,  0.4630,  0.8150],\n",
      "        [-0.2637,  0.2849, -0.0655, -0.5519, -0.1264],\n",
      "        [ 0.4867, -0.0258, -0.3925, -0.0280,  0.9603],\n",
      "        [ 0.2693,  0.4543,  1.4281,  0.6722,  1.0604]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2662, -0.5126,  0.7865,  0.3940,  1.0991],\n",
      "        [ 0.7427,  0.5609, -0.4694, -0.7350,  0.3320],\n",
      "        [ 0.0271, -0.8807,  0.2621, -0.1449,  1.4280],\n",
      "        [-2.1505, -0.6275,  0.2217,  1.9234,  2.1122]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8567],\n",
      "        [ 0.3584],\n",
      "        [ 1.3085],\n",
      "        [ 2.9851]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8388, -1.3862,  0.6851, -1.7950,  1.1141],\n",
      "        [-0.3681, -1.2039, -0.3733, -0.7641,  0.2609],\n",
      "        [ 0.1846,  0.8768,  0.9944, -0.6967,  1.4615],\n",
      "        [-0.1103,  0.6327,  0.9861, -0.0532, -1.7271]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3379,  0.3951, -0.0899,  0.3820, -0.9893],\n",
      "        [ 0.0910,  0.2449, -0.6490,  0.1672,  0.6341],\n",
      "        [-0.2583, -0.1828,  0.2160, -0.4300, -0.3779],\n",
      "        [-0.5638, -0.6734, -1.1768, -1.3585, -2.1937]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8388, -1.3862,  0.6851, -1.7950,  1.1141],\n",
      "        [-0.3681, -1.2039, -0.3733, -0.7641,  0.2609],\n",
      "        [ 0.1846,  0.8768,  0.9944, -0.6967,  1.4615],\n",
      "        [-0.1103,  0.6327,  0.9861, -0.0532, -1.7271]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.6806],\n",
      "        [-0.0483],\n",
      "        [-0.2460],\n",
      "        [ 2.3369]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-4.0535, -1.1149,  0.6535,  1.3629,  2.6212],\n",
      "        [-1.1888, -0.2134, -1.4474, -1.1799, -1.0708],\n",
      "        [ 1.8921, -0.1122, -0.3944,  1.2070,  0.4943],\n",
      "        [-0.0244,  0.7527,  0.3930, -1.3554, -0.5938]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7289,  1.7503,  1.7602,  0.9381,  1.7684],\n",
      "        [ 0.0351,  0.3203,  0.8313,  0.7542,  0.2714],\n",
      "        [-0.0793,  0.0064, -0.3328,  0.2062, -0.0777],\n",
      "        [-1.1738, -1.2837, -1.3220, -2.1003, -2.6782]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-4.0535, -1.1149,  0.6535,  1.3629,  2.6212],\n",
      "        [-1.1888, -0.2134, -1.4474, -1.1799, -1.0708],\n",
      "        [ 1.8921, -0.1122, -0.3944,  1.2070,  0.4943],\n",
      "        [-0.0244,  0.7527,  0.3930, -1.3554, -0.5938]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1581],\n",
      "        [-2.4938],\n",
      "        [ 0.1909],\n",
      "        [ 2.9801]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1294, -0.0689,  0.0425,  0.0431,  0.1460],\n",
      "        [ 0.3771, -0.5978,  0.0311, -0.5050,  1.5577],\n",
      "        [-0.6061, -0.1566,  1.7098, -0.4023,  0.7696],\n",
      "        [-0.3799,  0.0099, -1.1724,  0.2692, -0.4110]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0926,  0.0522,  0.6354,  0.6291,  0.6994],\n",
      "        [ 0.9895,  1.1685,  1.5904,  1.2166,  1.9062],\n",
      "        [ 0.0465,  0.2943,  0.1913,  0.5679,  0.3360],\n",
      "        [-0.0873,  0.3804, -0.3725,  0.2132,  0.2679]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1294, -0.0689,  0.0425,  0.0431,  0.1460],\n",
      "        [ 0.3771, -0.5978,  0.0311, -0.5050,  1.5577],\n",
      "        [-0.6061, -0.1566,  1.7098, -0.4023,  0.7696],\n",
      "        [-0.3799,  0.0099, -1.1724,  0.2692, -0.4110]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2572],\n",
      "        [ 2.0789],\n",
      "        [ 0.2830],\n",
      "        [ 0.4209]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2097,  0.7255,  0.2494,  1.4983,  0.2556],\n",
      "        [-0.5326,  0.9457,  0.9719, -0.6251, -0.0997],\n",
      "        [-0.9788,  2.1310, -0.5593, -0.9449, -0.5334],\n",
      "        [ 1.1217,  2.2835,  0.2931,  0.7110, -0.3359]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8731,  1.1240,  0.6960,  0.5564,  0.7762],\n",
      "        [ 0.2983,  0.6640,  1.1232,  0.6953,  1.5980],\n",
      "        [-0.0809, -0.0286,  0.3245, -0.0171,  0.2404],\n",
      "        [ 0.0906,  0.0369,  0.3154,  0.0764,  0.2127]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2097,  0.7255,  0.2494,  1.4983,  0.2556],\n",
      "        [-0.5326,  0.9457,  0.9719, -0.6251, -0.0997],\n",
      "        [-0.9788,  2.1310, -0.5593, -0.9449, -0.5334],\n",
      "        [ 1.1217,  2.2835,  0.2931,  0.7110, -0.3359]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9650],\n",
      "        [ 0.9667],\n",
      "        [-0.2753],\n",
      "        [ 0.2613]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7182,  1.0106, -1.6037,  0.8534,  0.6179],\n",
      "        [ 0.7872, -0.8135, -0.4674,  2.5862,  0.6961],\n",
      "        [ 0.9727, -0.1430,  1.6931,  1.1427,  1.3057],\n",
      "        [ 1.1679, -0.0179,  1.6376, -0.7771, -0.8769]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0741,  0.7046,  0.8027,  0.9736,  0.6904],\n",
      "        [-0.3455, -0.1144, -0.3539,  0.1692,  0.0272],\n",
      "        [-0.1846,  0.0440, -0.0908, -0.4080,  0.0056],\n",
      "        [-0.7073,  0.1489,  0.8875,  0.5175,  0.2134]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7182,  1.0106, -1.6037,  0.8534,  0.6179],\n",
      "        [ 0.7872, -0.8135, -0.4674,  2.5862,  0.6961],\n",
      "        [ 0.9727, -0.1430,  1.6931,  1.1427,  1.3057],\n",
      "        [ 1.1679, -0.0179,  1.6376, -0.7771, -0.8769]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7354],\n",
      "        [ 0.4431],\n",
      "        [-0.7985],\n",
      "        [ 0.0352]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2600, -0.3978, -1.7093,  0.1481,  1.8146],\n",
      "        [-1.2695,  0.4375, -0.2138, -0.4930, -0.1344],\n",
      "        [-0.2329, -0.6667, -1.0618,  0.0804,  0.5010],\n",
      "        [ 0.4721,  1.1098,  0.4650, -1.1852,  0.6565]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0249,  1.2371,  1.2024,  0.7321,  0.6593],\n",
      "        [ 0.0897,  0.0111, -0.1914,  0.0794, -0.0365],\n",
      "        [ 0.8473,  0.1397,  0.2781,  0.5503,  0.1030],\n",
      "        [ 0.1671,  0.4692,  0.2409, -0.0701, -0.3088]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2600, -0.3978, -1.7093,  0.1481,  1.8146],\n",
      "        [-1.2695,  0.4375, -0.2138, -0.4930, -0.1344],\n",
      "        [-0.2329, -0.6667, -1.0618,  0.0804,  0.5010],\n",
      "        [ 0.4721,  1.1098,  0.4650, -1.1852,  0.6565]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2491],\n",
      "        [-0.1023],\n",
      "        [-0.4899],\n",
      "        [ 0.5921]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8875, -1.1970,  0.8552,  0.1586,  0.2087],\n",
      "        [ 1.6958, -0.5783,  0.1129,  0.4524,  0.3791],\n",
      "        [-0.5427, -0.4289,  0.0572, -0.7684,  0.4818],\n",
      "        [-0.2214,  1.3881, -0.8737, -0.1294,  1.6061]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3237,  1.7830,  0.9745,  1.3896,  1.5363],\n",
      "        [-0.0746,  1.3309,  0.3026,  0.5286,  0.1403],\n",
      "        [ 0.9107,  0.0161, -0.1440, -0.0553, -0.1554],\n",
      "        [-0.0549,  0.5531, -0.2704,  0.1485,  0.2911]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8875, -1.1970,  0.8552,  0.1586,  0.2087],\n",
      "        [ 1.6958, -0.5783,  0.1129,  0.4524,  0.3791],\n",
      "        [-0.5427, -0.4289,  0.0572, -0.7684,  0.4818],\n",
      "        [-0.2214,  1.3881, -0.8737, -0.1294,  1.6061]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3709],\n",
      "        [-0.5698],\n",
      "        [-0.5418],\n",
      "        [ 1.4646]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.4573,  0.2601, -0.2577,  0.3606,  1.0888],\n",
      "        [ 0.1962,  0.3937,  1.7798,  1.3504, -0.7546],\n",
      "        [-2.2501,  0.7638,  1.1183,  0.4792, -0.0625],\n",
      "        [-0.7862,  1.2728, -0.9157, -1.1830,  1.0951]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2575,  1.3947,  2.0346,  1.9460,  1.6448],\n",
      "        [-0.0679,  1.0492,  0.7091,  0.5740,  0.3044],\n",
      "        [ 0.6240, -0.0503, -0.1082, -0.9379, -0.3330],\n",
      "        [ 0.0132,  0.0049, -1.5167, -0.6767, -1.0844]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.4573,  0.2601, -0.2577,  0.3606,  1.0888],\n",
      "        [ 0.1962,  0.3937,  1.7798,  1.3504, -0.7546],\n",
      "        [-2.2501,  0.7638,  1.1183,  0.4792, -0.0625],\n",
      "        [-0.7862,  1.2728, -0.9157, -1.1830,  1.0951]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7590],\n",
      "        [ 2.2073],\n",
      "        [-1.9921],\n",
      "        [ 0.9978]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9623, -0.2656,  1.3134, -0.0759, -1.1125],\n",
      "        [-1.2443,  0.3068,  1.8334, -0.1423,  1.3287],\n",
      "        [-1.0767, -0.4714,  0.6037,  1.1942,  0.4888],\n",
      "        [-2.3031,  1.7386,  0.6541, -0.5795, -1.4712]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2173,  1.9617,  2.1190,  1.9747,  1.5721],\n",
      "        [-0.2411,  0.2552, -0.5376, -0.6470, -1.6380],\n",
      "        [ 1.0206,  1.6978,  1.0582,  0.1087,  1.1825],\n",
      "        [-0.9418, -0.2459, -0.5940, -0.6752, -1.6403]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9623, -0.2656,  1.3134, -0.0759, -1.1125],\n",
      "        [-1.2443,  0.3068,  1.8334, -0.1423,  1.3287],\n",
      "        [-1.0767, -0.4714,  0.6037,  1.1942,  0.4888],\n",
      "        [-2.3031,  1.7386,  0.6541, -0.5795, -1.4712]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0257],\n",
      "        [-2.6918],\n",
      "        [-0.5525],\n",
      "        [ 4.1575]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4379, -0.2128,  1.1421, -0.8890,  0.0699],\n",
      "        [-0.2290,  1.2388, -0.5777, -1.0915,  0.5257],\n",
      "        [-1.0816,  1.9683,  1.3313,  0.1800,  0.9922],\n",
      "        [-0.0481,  0.7784, -0.0004,  0.6202, -0.7963]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2831,  0.0996, -1.2000, -0.1664, -0.0933],\n",
      "        [ 0.3641,  1.4022,  1.0859,  0.3224,  0.4588],\n",
      "        [ 0.9448,  0.8698,  0.8119,  0.5469,  0.7506],\n",
      "        [-1.4304, -1.3166, -1.3124, -2.7147, -4.5409]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4379, -0.2128,  1.1421, -0.8890,  0.0699],\n",
      "        [-0.2290,  1.2388, -0.5777, -1.0915,  0.5257],\n",
      "        [-1.0816,  1.9683,  1.3313,  0.1800,  0.9922],\n",
      "        [-0.0481,  0.7784, -0.0004,  0.6202, -0.7963]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3743],\n",
      "        [ 0.9156],\n",
      "        [ 2.6144],\n",
      "        [ 0.9767]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0398, -0.3510,  0.9148,  0.0205,  0.1123],\n",
      "        [-1.1742,  0.1413, -0.4802, -1.5890, -0.6527],\n",
      "        [ 2.0665, -0.0974, -0.9304,  1.2985, -0.9985],\n",
      "        [-0.3097, -0.4835,  0.8984,  0.1349, -1.0546]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9870,  0.8055,  1.0471,  0.9944,  0.7603],\n",
      "        [-0.6973,  1.2103,  0.8288,  0.7147,  1.2703],\n",
      "        [-0.0700, -0.0098, -1.1437,  0.1792, -1.1667],\n",
      "        [-0.1141,  0.4497, -0.0271, -0.4342,  0.2293]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0398, -0.3510,  0.9148,  0.0205,  0.1123],\n",
      "        [-1.1742,  0.1413, -0.4802, -1.5890, -0.6527],\n",
      "        [ 2.0665, -0.0974, -0.9304,  1.2985, -0.9985],\n",
      "        [-0.3097, -0.4835,  0.8984,  0.1349, -1.0546]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2454],\n",
      "        [-1.3730],\n",
      "        [ 2.3179],\n",
      "        [-0.5069]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7139,  0.1912,  0.0339, -0.1861,  0.4580],\n",
      "        [-2.1928,  1.3367, -0.5755,  1.5277,  0.2143],\n",
      "        [ 0.2714, -0.2067,  0.5506,  0.5169, -0.2430],\n",
      "        [-0.1564, -1.3523, -0.4631, -2.1019,  0.8691]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5326,  0.0481,  0.5803,  0.7094,  0.1916],\n",
      "        [ 0.3608,  1.2077,  1.4435,  1.1928,  1.4214],\n",
      "        [-0.6806, -1.0918, -1.0635, -1.1498, -2.4822],\n",
      "        [-0.1639, -0.2655, -1.0111, -0.1253, -0.6226]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7139,  0.1912,  0.0339, -0.1861,  0.4580],\n",
      "        [-2.1928,  1.3367, -0.5755,  1.5277,  0.2143],\n",
      "        [ 0.2714, -0.2067,  0.5506,  0.5169, -0.2430],\n",
      "        [-0.1564, -1.3523, -0.4631, -2.1019,  0.8691]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9282],\n",
      "        [ 2.1192],\n",
      "        [-0.5357],\n",
      "        [ 0.5752]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2684,  2.8099,  0.5144, -0.0297,  0.3281],\n",
      "        [-0.7389,  1.4409,  0.1329,  1.3265, -0.8561],\n",
      "        [-1.0427,  0.0603,  0.6126,  0.5681,  0.1006],\n",
      "        [ 0.2889,  0.1881,  0.8801,  0.8344,  1.3957]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5640,  0.4933,  0.7076,  0.1618,  0.7558],\n",
      "        [ 0.0659,  0.0279,  0.4883, -0.1717, -0.3116],\n",
      "        [-0.3589, -0.9030, -0.3902, -1.0429, -1.5553],\n",
      "        [ 0.0673,  0.2939, -0.0853, -0.1862, -0.0090]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2684,  2.8099,  0.5144, -0.0297,  0.3281],\n",
      "        [-0.7389,  1.4409,  0.1329,  1.3265, -0.8561],\n",
      "        [-1.0427,  0.0603,  0.6126,  0.5681,  0.1006],\n",
      "        [ 0.2889,  0.1881,  0.8801,  0.8344,  1.3957]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8420],\n",
      "        [ 0.0953],\n",
      "        [-0.6682],\n",
      "        [-0.1682]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4867,  0.5732,  1.3252,  0.5784, -0.0597],\n",
      "        [ 0.5587,  0.7461,  0.9700,  0.6120,  0.9706],\n",
      "        [ 1.4608,  1.0478,  1.8693, -0.0449, -0.3646],\n",
      "        [ 1.1110, -2.1037,  0.8127,  0.3387,  1.5023]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2641, -0.8709, -0.7770, -0.2913, -0.8945],\n",
      "        [ 0.1011,  0.6602,  0.7794,  0.3891,  0.4446],\n",
      "        [ 0.2395, -0.5767,  0.0134, -0.9242, -1.2347],\n",
      "        [-0.1955,  0.0789,  0.3575,  0.1304, -0.6954]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4867,  0.5732,  1.3252,  0.5784, -0.0597],\n",
      "        [ 0.5587,  0.7461,  0.9700,  0.6120,  0.9706],\n",
      "        [ 1.4608,  1.0478,  1.8693, -0.0449, -0.3646],\n",
      "        [ 1.1110, -2.1037,  0.8127,  0.3387,  1.5023]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7724],\n",
      "        [ 1.9747],\n",
      "        [ 0.2625],\n",
      "        [-1.0931]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8956,  0.4145, -0.2234,  0.3086, -0.2301],\n",
      "        [-0.6917, -0.9580,  0.6504,  0.1402,  1.5006],\n",
      "        [-0.2418,  0.7565,  0.0179, -0.7762,  1.9446],\n",
      "        [ 2.8874,  0.4049,  0.9389, -0.5375,  0.9208]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0247, -0.0365,  0.1782,  0.1283,  1.5937],\n",
      "        [-0.8509, -0.3742,  0.2830, -0.5283, -0.8698],\n",
      "        [ 0.0964, -0.1684, -0.0109, -0.1500, -0.4554],\n",
      "        [ 0.3275,  0.0717,  0.9609, -0.0939,  0.3128]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8956,  0.4145, -0.2234,  0.3086, -0.2301],\n",
      "        [-0.6917, -0.9580,  0.6504,  0.1402,  1.5006],\n",
      "        [-0.2418,  0.7565,  0.0179, -0.7762,  1.9446],\n",
      "        [ 2.8874,  0.4049,  0.9389, -0.5375,  0.9208]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5356],\n",
      "        [-0.2483],\n",
      "        [-0.9201],\n",
      "        [ 2.2154]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6466, -0.8567,  0.7537, -0.5668, -0.1977],\n",
      "        [ 1.0256,  1.5488,  1.1802, -0.8470,  1.0629],\n",
      "        [ 1.7286,  0.1307,  0.4933,  0.1418, -0.1132],\n",
      "        [ 0.3281,  0.4386,  0.8174,  1.2446, -0.8049]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6616,  0.4817,  0.6666,  0.7647, -0.1974],\n",
      "        [-0.1728, -0.2443,  0.1358, -0.1688, -0.2019],\n",
      "        [-0.2785, -0.1579, -0.7984,  0.6597, -0.0486],\n",
      "        [-0.1606, -0.9941, -0.4775, -1.0363, -1.4944]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6466, -0.8567,  0.7537, -0.5668, -0.1977],\n",
      "        [ 1.0256,  1.5488,  1.1802, -0.8470,  1.0629],\n",
      "        [ 1.7286,  0.1307,  0.4933,  0.1418, -0.1132],\n",
      "        [ 0.3281,  0.4386,  0.8174,  1.2446, -0.8049]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3940],\n",
      "        [-0.4669],\n",
      "        [-0.7968],\n",
      "        [-0.9659]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1563, -0.6362,  1.5208, -0.2659, -0.6184],\n",
      "        [-1.0107, -0.9280,  0.9368, -0.6903, -0.6167],\n",
      "        [ 0.0256, -0.2357, -0.0215, -2.2080,  0.6261],\n",
      "        [-1.3356,  1.3125,  1.9025, -0.1715, -0.9362]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8967,  0.9842,  0.6946,  0.3208,  1.3238],\n",
      "        [-1.0493,  1.2933,  0.3591,  0.7160,  0.1359],\n",
      "        [ 0.4077, -0.7364, -0.4521, -0.4086,  0.0473],\n",
      "        [-0.0092, -0.5184, -0.6330, -0.6625, -0.3658]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1563, -0.6362,  1.5208, -0.2659, -0.6184],\n",
      "        [-1.0107, -0.9280,  0.9368, -0.6903, -0.6167],\n",
      "        [ 0.0256, -0.2357, -0.0215, -2.2080,  0.6261],\n",
      "        [-1.3356,  1.3125,  1.9025, -0.1715, -0.9362]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5106],\n",
      "        [-0.3814],\n",
      "        [ 1.1255],\n",
      "        [-1.4163]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0615, -1.0341, -0.0781,  0.1920,  0.7422],\n",
      "        [ 0.0980,  0.0539,  1.4367,  0.1700,  0.2071],\n",
      "        [ 1.0648,  0.4742,  0.0135,  0.1247, -0.5709],\n",
      "        [ 0.0201, -1.1591,  0.5912, -0.6211, -0.1905]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7945,  1.1645,  1.6914,  0.6624,  1.9114],\n",
      "        [-0.1479,  0.2062,  0.6896,  1.0242,  0.7560],\n",
      "        [-0.3522, -0.7169, -0.1996, -0.3500, -0.9985],\n",
      "        [ 0.3331,  0.2679, -0.0272,  1.2977,  0.8179]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0615, -1.0341, -0.0781,  0.1920,  0.7422],\n",
      "        [ 0.0980,  0.0539,  1.4367,  0.1700,  0.2071],\n",
      "        [ 1.0648,  0.4742,  0.0135,  0.1247, -0.5709],\n",
      "        [ 0.0201, -1.1591,  0.5912, -0.6211, -0.1905]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2584],\n",
      "        [ 1.3181],\n",
      "        [-0.1913],\n",
      "        [-1.2817]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.7827, -1.3800,  1.9040, -0.7145, -0.0524],\n",
      "        [-1.2742,  0.6145, -0.0329,  0.1721, -0.4305],\n",
      "        [-0.6080,  0.5648, -0.2126, -1.9634,  1.8452],\n",
      "        [-0.6087,  2.6082, -0.2664,  2.0255, -1.5685]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3228,  1.3914,  1.5278,  1.5111,  1.6652],\n",
      "        [-0.5587, -0.3645, -0.7322,  0.3404, -0.6287],\n",
      "        [ 0.3102, -0.0658, -0.3292, -0.3851, -1.2675],\n",
      "        [ 1.0706,  0.6763,  1.1497,  1.0612,  0.4617]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7827, -1.3800,  1.9040, -0.7145, -0.0524],\n",
      "        [-1.2742,  0.6145, -0.0329,  0.1721, -0.4305],\n",
      "        [-0.6080,  0.5648, -0.2126, -1.9634,  1.8452],\n",
      "        [-0.6087,  2.6082, -0.2664,  2.0255, -1.5685]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2135],\n",
      "        [ 0.8411],\n",
      "        [-1.7384],\n",
      "        [ 2.2312]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0948,  3.0060,  0.0869,  1.3257,  0.1964],\n",
      "        [-1.5797, -1.7845,  0.9345,  1.3217, -1.7359],\n",
      "        [-0.4146, -0.7577,  0.8258,  1.3625, -0.5474],\n",
      "        [ 1.0999, -1.6921,  0.4856,  2.2225, -0.7261]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3957, -0.0274, -0.4421,  0.1883,  0.4277],\n",
      "        [-0.3473, -0.1828, -0.4484, -0.1776, -0.2753],\n",
      "        [ 0.8692,  0.3288,  0.9724,  0.3122,  0.9478],\n",
      "        [ 0.1127, -0.5524, -0.8225,  0.3708, -1.1960]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0948,  3.0060,  0.0869,  1.3257,  0.1964],\n",
      "        [-1.5797, -1.7845,  0.9345,  1.3217, -1.7359],\n",
      "        [-0.4146, -0.7577,  0.8258,  1.3625, -0.5474],\n",
      "        [ 1.0999, -1.6921,  0.4856,  2.2225, -0.7261]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1754],\n",
      "        [ 0.6989],\n",
      "        [ 0.1000],\n",
      "        [ 2.3518]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2863,  0.2825,  0.0462, -0.1262, -0.3106],\n",
      "        [-1.5533,  0.7048, -1.6199,  0.8442, -0.2067],\n",
      "        [-0.9028, -0.5185,  2.2520, -0.1010,  2.1190],\n",
      "        [ 0.0733, -0.9626,  1.2908,  0.5189,  0.3339]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4738, -0.1439, -0.4960,  0.0290,  0.5583],\n",
      "        [-0.4730, -0.1045, -1.0459, -0.3395, -1.2113],\n",
      "        [ 0.3974,  0.3232,  0.3537,  0.6715,  0.0287],\n",
      "        [-1.0860, -1.1008, -1.6245, -1.0484, -2.3451]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2863,  0.2825,  0.0462, -0.1262, -0.3106],\n",
      "        [-1.5533,  0.7048, -1.6199,  0.8442, -0.2067],\n",
      "        [-0.9028, -0.5185,  2.2520, -0.1010,  2.1190],\n",
      "        [ 0.0733, -0.9626,  1.2908,  0.5189,  0.3339]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3689],\n",
      "        [ 2.3191],\n",
      "        [ 0.2632],\n",
      "        [-2.4440]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0883, -0.4254, -0.3361,  0.2216, -0.1926],\n",
      "        [-0.8933, -0.6122, -0.1158, -0.3179,  1.8244],\n",
      "        [-1.2909,  0.8103, -0.0453, -0.9210, -0.4334],\n",
      "        [ 0.6105, -0.7644, -0.5219,  1.0311,  1.0280]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4272, -0.2491, -0.0321,  0.1260, -0.5215],\n",
      "        [-0.2570,  0.1193,  0.2006,  0.0215,  0.0852],\n",
      "        [ 0.2142,  0.0388, -0.2407, -0.1254, -0.6355],\n",
      "        [-0.0510,  0.0856, -0.2520,  0.0788,  0.0840]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0883, -0.4254, -0.3361,  0.2216, -0.1926],\n",
      "        [-0.8933, -0.6122, -0.1158, -0.3179,  1.8244],\n",
      "        [-1.2909,  0.8103, -0.0453, -0.9210, -0.4334],\n",
      "        [ 0.6105, -0.7644, -0.5219,  1.0311,  1.0280]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2198],\n",
      "        [ 0.2819],\n",
      "        [ 0.1567],\n",
      "        [ 0.2026]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6663, -0.0983,  0.9279, -1.8903, -1.1579],\n",
      "        [ 0.1610, -1.0531,  1.7877,  0.3424,  0.5994],\n",
      "        [-1.1926,  0.7065, -0.2872,  1.0417, -0.5426],\n",
      "        [-0.9456,  0.4075,  0.8219, -1.0913,  0.3245]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1205, -0.3407, -0.3575,  0.6694, -0.1768],\n",
      "        [ 0.2153,  0.2939,  0.4574, -0.5680,  0.5173],\n",
      "        [ 0.8410, -0.5550,  0.1028,  0.0594, -0.0079],\n",
      "        [ 0.3857, -0.7591, -0.0926, -0.3660, -0.3948]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6663, -0.0983,  0.9279, -1.8903, -1.1579],\n",
      "        [ 0.1610, -1.0531,  1.7877,  0.3424,  0.5994],\n",
      "        [-1.1926,  0.7065, -0.2872,  1.0417, -0.5426],\n",
      "        [-0.9456,  0.4075,  0.8219, -1.0913,  0.3245]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4392],\n",
      "        [ 0.6584],\n",
      "        [-1.3584],\n",
      "        [-0.4788]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0058, -1.0045, -0.2278,  3.5462, -0.2270],\n",
      "        [-0.6195,  0.1802,  1.2539, -0.0065, -0.0752],\n",
      "        [-0.0212,  1.2159, -0.3868,  0.9903, -1.1836],\n",
      "        [ 0.4982,  0.3840,  0.1794,  1.5984,  0.3289]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6245,  1.0070,  0.9724,  0.1261,  0.8884],\n",
      "        [-0.3004,  0.0637,  0.8703,  0.4030, -0.6293],\n",
      "        [ 0.2745,  0.4891,  0.0506,  0.6692,  1.3232],\n",
      "        [-0.4150,  0.0414, -0.3175, -1.2735,  0.0939]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0058, -1.0045, -0.2278,  3.5462, -0.2270],\n",
      "        [-0.6195,  0.1802,  1.2539, -0.0065, -0.0752],\n",
      "        [-0.0212,  1.2159, -0.3868,  0.9903, -1.1836],\n",
      "        [ 0.4982,  0.3840,  0.1794,  1.5984,  0.3289]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6159],\n",
      "        [ 1.3335],\n",
      "        [-0.3341],\n",
      "        [-2.2526]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1917, -0.2439,  0.9541,  0.4325, -0.7427],\n",
      "        [-0.9860, -0.3356, -0.3990,  0.2203,  0.9261],\n",
      "        [ 0.0946,  0.9733,  1.1552, -0.7598,  2.5008],\n",
      "        [ 0.9185,  0.5184,  0.3280,  1.7415,  0.4568]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6599,  0.3588,  1.1580,  1.2178,  1.9549],\n",
      "        [ 0.0299, -0.3953,  0.0181, -0.2685, -1.0015],\n",
      "        [ 1.0018, -0.3762,  0.4494,  0.2005,  0.7397],\n",
      "        [ 0.9326,  0.7364,  1.1105,  0.8449,  1.0250]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1917, -0.2439,  0.9541,  0.4325, -0.7427],\n",
      "        [-0.9860, -0.3356, -0.3990,  0.2203,  0.9261],\n",
      "        [ 0.0946,  0.9733,  1.1552, -0.7598,  2.5008],\n",
      "        [ 0.9185,  0.5184,  0.3280,  1.7415,  0.4568]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8785],\n",
      "        [-0.8907],\n",
      "        [ 1.9452],\n",
      "        [ 3.5424]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4922, -1.0476,  0.2586,  1.1262, -0.8566],\n",
      "        [-0.0730,  0.3041,  0.7742, -1.3176, -1.3785],\n",
      "        [-0.5786,  0.1620,  1.5052, -0.6327,  0.7520],\n",
      "        [ 0.2145,  0.3913,  1.3512,  1.4591, -1.8685]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1808,  0.9787,  0.5862,  0.4798,  0.9698],\n",
      "        [-0.0011, -0.3517,  0.5564,  0.6667,  0.1529],\n",
      "        [ 0.0682, -0.5700, -0.3423,  0.1581, -0.5197],\n",
      "        [-0.6424, -1.5472, -1.4069, -0.9329, -1.6951]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4922, -1.0476,  0.2586,  1.1262, -0.8566],\n",
      "        [-0.0730,  0.3041,  0.7742, -1.3176, -1.3785],\n",
      "        [-0.5786,  0.1620,  1.5052, -0.6327,  0.7520],\n",
      "        [ 0.2145,  0.3913,  1.3512,  1.4591, -1.8685]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5829],\n",
      "        [-0.7654],\n",
      "        [-1.1379],\n",
      "        [-0.8382]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6291,  0.7136, -0.3292, -2.4764,  1.8232],\n",
      "        [-0.0650,  1.0296, -0.6741,  0.5262,  0.5801],\n",
      "        [-0.8504, -0.5178, -0.2996, -1.4487,  1.4807],\n",
      "        [-0.6748,  0.6639, -0.8786,  1.8595, -0.4547]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6852,  1.0890,  1.1677,  1.0610,  1.3319],\n",
      "        [ 0.0073,  0.2038,  0.0979, -0.3613,  0.7887],\n",
      "        [ 0.4437, -0.2618, -0.4346, -0.3298, -0.8138],\n",
      "        [ 0.2371, -0.4274,  0.3328, -0.7769, -1.2856]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6291,  0.7136, -0.3292, -2.4764,  1.8232],\n",
      "        [-0.0650,  1.0296, -0.6741,  0.5262,  0.5801],\n",
      "        [-0.8504, -0.5178, -0.2996, -1.4487,  1.4807],\n",
      "        [-0.6748,  0.6639, -0.8786,  1.8595, -0.4547]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2378],\n",
      "        [ 0.4108],\n",
      "        [-0.8386],\n",
      "        [-1.5962]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2633, -0.8271,  1.6869,  0.7350, -1.3354],\n",
      "        [ 0.0039,  1.6939, -0.1988,  0.9468,  1.4574],\n",
      "        [-0.8961, -0.6707, -0.0130,  0.6959,  1.5461],\n",
      "        [ 0.8335, -0.1400,  2.5366, -0.2708, -0.2898]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0875,  0.1328,  0.9461,  0.6164,  1.6558],\n",
      "        [ 0.0568,  0.2896, -0.0403,  0.6505,  0.4950],\n",
      "        [ 0.3185, -0.5098, -0.2564,  0.0912, -0.1502],\n",
      "        [ 0.7047,  0.1187,  0.0772,  0.7785,  0.4589]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2633, -0.8271,  1.6869,  0.7350, -1.3354],\n",
      "        [ 0.0039,  1.6939, -0.1988,  0.9468,  1.4574],\n",
      "        [-0.8961, -0.6707, -0.0130,  0.6959,  1.5461],\n",
      "        [ 0.8335, -0.1400,  2.5366, -0.2708, -0.2898]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6458],\n",
      "        [ 1.8361],\n",
      "        [-0.1090],\n",
      "        [ 0.4227]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9142,  1.8326, -0.8323,  0.6236,  0.7565],\n",
      "        [-0.1239,  0.5696, -0.1119,  0.3850, -0.5385],\n",
      "        [ 0.4962,  0.6986, -1.1698, -0.8278, -0.0111],\n",
      "        [-1.0958,  0.1323,  0.6549,  1.6645,  0.2455]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0768,  1.2535,  1.1063,  1.2871,  1.6084],\n",
      "        [-0.5841, -0.6226, -1.2887, -0.3518, -1.6073],\n",
      "        [ 0.0818, -0.8942, -0.0528, -0.0770,  0.3992],\n",
      "        [ 0.2766, -0.6650, -0.5343,  0.0933,  0.3589]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9142,  1.8326, -0.8323,  0.6236,  0.7565],\n",
      "        [-0.1239,  0.5696, -0.1119,  0.3850, -0.5385],\n",
      "        [ 0.4962,  0.6986, -1.1698, -0.8278, -0.0111],\n",
      "        [-1.0958,  0.1323,  0.6549,  1.6645,  0.2455]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.4570],\n",
      "        [ 0.5921],\n",
      "        [-0.4630],\n",
      "        [-0.4976]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4470, -2.2610,  0.9660, -0.4149, -0.9630],\n",
      "        [ 2.0691,  0.4141, -0.0990, -1.6984,  1.7754],\n",
      "        [-0.4107, -0.2582,  1.9575,  1.7681, -0.9865],\n",
      "        [-0.4720, -0.9521, -0.0873, -1.6438, -1.5158]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4991, -1.2069, -1.6409, -1.1263, -1.7023],\n",
      "        [-0.7574, -1.1341, -0.8842, -1.4447, -1.4327],\n",
      "        [ 0.4292, -0.5309, -0.5857, -0.1905,  0.0004],\n",
      "        [-0.2752, -0.5596, -0.7360, -0.3479, -0.0030]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4470, -2.2610,  0.9660, -0.4149, -0.9630],\n",
      "        [ 2.0691,  0.4141, -0.0990, -1.6984,  1.7754],\n",
      "        [-0.4107, -0.2582,  1.9575,  1.7681, -0.9865],\n",
      "        [-0.4720, -0.9521, -0.0873, -1.6438, -1.5158]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.4733],\n",
      "        [-2.0391],\n",
      "        [-1.5230],\n",
      "        [ 1.3035]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5949,  0.2969,  0.9334,  0.1520, -0.7669],\n",
      "        [-0.7268,  0.3643,  0.6474, -0.0071, -0.9208],\n",
      "        [-1.2401, -0.0673, -0.3961,  0.9359,  0.5146],\n",
      "        [-0.0089,  0.4254, -0.2649,  1.6978, -1.1818]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.7260, -2.0499, -3.1382, -3.1008, -4.5057],\n",
      "        [ 0.1839, -0.5069,  0.7021,  0.2643,  0.1220],\n",
      "        [ 0.6507, -0.3863,  0.8673,  0.6834,  1.2093],\n",
      "        [-0.3535, -1.1490, -1.0494, -0.4447, -1.4715]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5949,  0.2969,  0.9334,  0.1520, -0.7669],\n",
      "        [-0.7268,  0.3643,  0.6474, -0.0071, -0.9208],\n",
      "        [-1.2401, -0.0673, -0.3961,  0.9359,  0.5146],\n",
      "        [-0.0089,  0.4254, -0.2649,  1.6978, -1.1818]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4732],\n",
      "        [ 0.0221],\n",
      "        [ 0.1374],\n",
      "        [ 0.7765]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2643, -0.6131,  1.2440,  0.9211, -0.1445],\n",
      "        [-0.3788,  0.0343, -1.0697,  0.4645, -1.2524],\n",
      "        [-1.8279, -0.8002, -0.3862,  1.2941,  0.6609],\n",
      "        [ 3.1169,  0.2634,  0.3628,  0.7970,  0.0693]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.3554, -1.7316, -2.2211, -3.0162, -5.5234],\n",
      "        [-0.0302, -0.5074, -0.1192, -0.5939, -1.2998],\n",
      "        [ 0.1927, -0.0818,  0.1950, -0.1191, -0.5215],\n",
      "        [-0.2497, -1.1503, -0.7286, -0.8114, -1.0892]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2643, -0.6131,  1.2440,  0.9211, -0.1445],\n",
      "        [-0.3788,  0.0343, -1.0697,  0.4645, -1.2524],\n",
      "        [-1.8279, -0.8002, -0.3862,  1.2941,  0.6609],\n",
      "        [ 3.1169,  0.2634,  0.3628,  0.7970,  0.0693]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.3232],\n",
      "        [ 1.4736],\n",
      "        [-0.8608],\n",
      "        [-2.0679]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4543, -0.0528,  1.0200,  0.3246,  1.2342],\n",
      "        [-2.6405,  0.0049,  2.0530,  1.0365,  0.9333],\n",
      "        [-0.8292, -0.0138,  1.4872, -1.0861, -1.3060],\n",
      "        [ 0.1966, -1.4238,  0.5316,  2.4509, -0.3379]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2474, -0.2556,  0.2101,  0.9681, -0.4235],\n",
      "        [-0.0845, -1.4216, -1.6338, -0.8516, -1.8448],\n",
      "        [ 0.8424,  0.5352,  0.6588,  0.0630,  0.4224],\n",
      "        [ 0.6379,  0.0936, -1.3078, -0.8557,  0.1667]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4543, -0.0528,  1.0200,  0.3246,  1.2342],\n",
      "        [-2.6405,  0.0049,  2.0530,  1.0365,  0.9333],\n",
      "        [-0.8292, -0.0138,  1.4872, -1.0861, -1.3060],\n",
      "        [ 0.1966, -1.4238,  0.5316,  2.4509, -0.3379]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1317],\n",
      "        [-5.7425],\n",
      "        [-0.3463],\n",
      "        [-2.8567]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3199,  0.4274,  0.1873,  0.0831,  1.2177],\n",
      "        [ 0.6673,  1.1026,  0.8451, -1.5378,  2.5422],\n",
      "        [-0.8242,  1.4766,  1.1298, -0.7714, -0.5145],\n",
      "        [-1.6824, -0.1277, -0.8106,  0.6925,  0.2731]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3880,  0.2848, -0.0390, -0.1976, -0.7699],\n",
      "        [ 1.7899,  1.5548,  1.9253,  1.6550,  2.0018],\n",
      "        [ 0.6090, -0.2661, -0.1245, -0.7041, -0.1523],\n",
      "        [ 1.3365,  1.1103,  1.3215,  1.2647,  2.0033]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3199,  0.4274,  0.1873,  0.0831,  1.2177],\n",
      "        [ 0.6673,  1.1026,  0.8451, -1.5378,  2.5422],\n",
      "        [-0.8242,  1.4766,  1.1298, -0.7714, -0.5145],\n",
      "        [-1.6824, -0.1277, -0.8106,  0.6925,  0.2731]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3274],\n",
      "        [ 7.0798],\n",
      "        [-0.4140],\n",
      "        [-2.0387]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0450, -0.1100, -0.4114, -1.2864, -0.7450],\n",
      "        [-0.3703,  0.4018,  2.3952,  0.4385, -0.6048],\n",
      "        [ 0.6260, -1.3968,  1.7435,  0.2956,  0.2916],\n",
      "        [ 1.7133, -0.2266,  0.4195,  0.0143,  1.1801]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1824,  0.5413,  0.5382, -0.1082, -0.0689],\n",
      "        [-1.0675, -2.0018, -1.5675, -1.1036, -2.7128],\n",
      "        [ 0.4547,  0.1879, -0.1761,  0.4668,  0.3522],\n",
      "        [ 2.0600,  1.5160,  1.6147,  1.3848,  1.8593]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0450, -0.1100, -0.4114, -1.2864, -0.7450],\n",
      "        [-0.3703,  0.4018,  2.3952,  0.4385, -0.6048],\n",
      "        [ 0.6260, -1.3968,  1.7435,  0.2956,  0.2916],\n",
      "        [ 1.7133, -0.2266,  0.4195,  0.0143,  1.1801]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0823],\n",
      "        [-3.0067],\n",
      "        [-0.0442],\n",
      "        [ 6.0772]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2048,  0.6210,  1.3033,  0.3176,  1.9324],\n",
      "        [-0.6554,  0.8099,  0.0580, -1.1875, -0.6299],\n",
      "        [ 0.9128,  0.9266, -1.0085,  2.2346,  1.4349],\n",
      "        [-0.8732,  1.3005,  0.0218,  1.4356,  1.4397]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0628, -0.3456, -0.8732,  0.0474, -0.3922],\n",
      "        [ 0.5381,  0.1489, -0.4913, -0.2274,  0.5449],\n",
      "        [ 0.6407,  0.2388,  0.0990, -0.3214,  0.2791],\n",
      "        [-0.7923, -0.9458, -1.0047, -0.8611, -1.5186]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2048,  0.6210,  1.3033,  0.3176,  1.9324],\n",
      "        [-0.6554,  0.8099,  0.0580, -1.1875, -0.6299],\n",
      "        [ 0.9128,  0.9266, -1.0085,  2.2346,  1.4349],\n",
      "        [-0.8732,  1.3005,  0.0218,  1.4356,  1.4397]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0825],\n",
      "        [-0.3338],\n",
      "        [ 0.3887],\n",
      "        [-3.9825]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4616,  0.6787, -0.6626,  0.7760,  0.0114],\n",
      "        [ 1.3440, -0.2344, -0.1602, -1.5205, -0.2772],\n",
      "        [ 1.0684,  0.3606,  0.7603,  0.3433,  0.7583],\n",
      "        [-0.5220, -0.4306,  0.4046, -0.0640, -0.6905]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9753,  0.7798,  0.5461,  0.8207,  1.8417],\n",
      "        [ 0.0819, -0.9036,  0.2335, -0.4490, -0.3141],\n",
      "        [ 0.4430,  0.0266,  0.1100, -0.0714,  0.9057],\n",
      "        [ 1.0266,  1.3933,  1.5546,  0.4498,  2.3208]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4616,  0.6787, -0.6626,  0.7760,  0.0114],\n",
      "        [ 1.3440, -0.2344, -0.1602, -1.5205, -0.2772],\n",
      "        [ 1.0684,  0.3606,  0.7603,  0.3433,  0.7583],\n",
      "        [-0.5220, -0.4306,  0.4046, -0.0640, -0.6905]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2755],\n",
      "        [ 1.0542],\n",
      "        [ 1.2288],\n",
      "        [-2.1382]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3253,  0.2081, -1.4154, -0.1232, -0.3203],\n",
      "        [-0.2549,  0.2059, -0.5331,  1.1603, -2.1212],\n",
      "        [-1.1759, -0.7875,  0.5954, -0.0442,  1.5628],\n",
      "        [-0.8087,  0.7339, -2.0082,  1.6984,  1.4534]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0468,  0.7710,  0.1001,  0.8814,  0.8821],\n",
      "        [ 0.0563, -0.4580, -0.9724, -0.5587, -0.9796],\n",
      "        [-0.0288, -0.4663,  0.1579, -0.8458, -0.7781],\n",
      "        [ 1.3971,  1.7055,  2.1516,  0.6981,  2.0634]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3253,  0.2081, -1.4154, -0.1232, -0.3203],\n",
      "        [-0.2549,  0.2059, -0.5331,  1.1603, -2.1212],\n",
      "        [-1.1759, -0.7875,  0.5954, -0.0442,  1.5628],\n",
      "        [-0.8087,  0.7339, -2.0082,  1.6984,  1.4534]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4344],\n",
      "        [ 1.8394],\n",
      "        [-0.6835],\n",
      "        [-0.0143]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7462, -0.4427,  0.3139,  0.7336,  1.6943],\n",
      "        [-0.9685,  0.4880,  0.8555, -2.0621, -0.3520],\n",
      "        [ 0.5985, -1.1503,  1.1913,  0.6282, -0.9611],\n",
      "        [ 0.6996,  0.0882, -0.2986, -1.9294, -0.4118]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1069, -0.1568, -0.3814, -0.1617, -0.5659],\n",
      "        [-0.7353, -0.7948, -1.0624, -1.4705, -1.9929],\n",
      "        [ 0.6629, -0.4486, -0.3710, -0.0809,  0.3072],\n",
      "        [ 0.1987,  0.1314, -0.0235,  0.2621, -0.2071]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7462, -0.4427,  0.3139,  0.7336,  1.6943],\n",
      "        [-0.9685,  0.4880,  0.8555, -2.0621, -0.3520],\n",
      "        [ 0.5985, -1.1503,  1.1913,  0.6282, -0.9611],\n",
      "        [ 0.6996,  0.0882, -0.2986, -1.9294, -0.4118]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0479],\n",
      "        [ 3.1493],\n",
      "        [ 0.1248],\n",
      "        [-0.2629]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7874, -0.1460,  0.6144, -0.0066,  1.5519],\n",
      "        [-1.1671, -0.5018,  0.7308,  1.8254, -1.0561],\n",
      "        [ 1.3909,  2.0249,  0.2441,  2.3870,  1.0728],\n",
      "        [-0.7751, -1.0302, -0.6359, -0.3400, -0.7170]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1845,  0.0264,  0.9971, -0.1148,  0.8208],\n",
      "        [-1.6299, -2.0352, -2.5113, -2.1736, -4.0149],\n",
      "        [ 1.0088,  0.4468, -0.2017, -0.1185, -0.2785],\n",
      "        [ 0.2144,  0.6474, -0.4115, -0.0752,  0.3785]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7874, -0.1460,  0.6144, -0.0066,  1.5519],\n",
      "        [-1.1671, -0.5018,  0.7308,  1.8254, -1.0561],\n",
      "        [ 1.3909,  2.0249,  0.2441,  2.3870,  1.0728],\n",
      "        [-0.7751, -1.0302, -0.6359, -0.3400, -0.7170]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0286],\n",
      "        [ 1.3605],\n",
      "        [ 1.6772],\n",
      "        [-0.8172]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3622, -1.5246,  1.5682, -1.3237,  0.5826],\n",
      "        [ 0.7190, -0.1426,  0.3801,  1.3211, -0.5194],\n",
      "        [-0.0205, -1.8589,  1.0048, -0.0952,  0.9627],\n",
      "        [ 0.0088, -1.2492, -0.6727, -0.4172,  0.0155]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4802, -0.0721, -0.0966, -0.9627, -0.5267],\n",
      "        [ 0.5089,  0.2500, -0.2177,  0.0798, -0.3045],\n",
      "        [-0.5228, -0.6765, -1.1856, -0.7062, -0.8637],\n",
      "        [ 0.6468,  0.0266,  0.5903, -0.4609,  0.2713]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3622, -1.5246,  1.5682, -1.3237,  0.5826],\n",
      "        [ 0.7190, -0.1426,  0.3801,  1.3211, -0.5194],\n",
      "        [-0.0205, -1.8589,  1.0048, -0.0952,  0.9627],\n",
      "        [ 0.0088, -1.2492, -0.6727, -0.4172,  0.0155]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7520],\n",
      "        [ 0.5110],\n",
      "        [-0.6874],\n",
      "        [-0.2281]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2262, -0.7198,  0.2068, -1.1593, -1.3363],\n",
      "        [-1.2731, -0.9871,  2.2915,  0.0546,  1.6172],\n",
      "        [-1.0105, -0.0808,  0.6417,  1.0002, -1.6732],\n",
      "        [ 0.2159,  1.0038, -1.1256, -0.9486, -0.1177]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3247, -0.7115, -0.2749, -0.7935, -1.0300],\n",
      "        [ 0.7929, -0.2402,  0.0191,  0.0342, -0.1488],\n",
      "        [-0.2376, -0.0145, -0.4100,  0.4204, -0.9265],\n",
      "        [ 0.1062,  0.0307, -0.5980,  0.2472, -0.6273]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2262, -0.7198,  0.2068, -1.1593, -1.3363],\n",
      "        [-1.2731, -0.9871,  2.2915,  0.0546,  1.6172],\n",
      "        [-1.0105, -0.0808,  0.6417,  1.0002, -1.6732],\n",
      "        [ 0.2159,  1.0038, -1.1256, -0.9486, -0.1177]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3534],\n",
      "        [-0.9674],\n",
      "        [ 1.9489],\n",
      "        [ 0.5663]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8619, -0.1878,  1.1371, -0.3638,  0.4779],\n",
      "        [-2.5451, -0.3731,  0.2590, -0.4189, -0.9169],\n",
      "        [-0.3041, -1.5880, -0.7344,  0.8415,  0.1555],\n",
      "        [ 0.6905, -1.9190,  0.1364,  1.1125,  2.1836]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.3872, -1.6655, -0.7685, -1.8213, -2.9045],\n",
      "        [ 0.1121,  0.3365,  0.9340,  0.2343,  0.9548],\n",
      "        [-0.0092, -0.6679, -0.6112,  0.1841, -1.2183],\n",
      "        [ 0.0432, -0.3229, -0.4441, -0.3460,  0.4170]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8619, -0.1878,  1.1371, -0.3638,  0.4779],\n",
      "        [-2.5451, -0.3731,  0.2590, -0.4189, -0.9169],\n",
      "        [-0.3041, -1.5880, -0.7344,  0.8415,  0.1555],\n",
      "        [ 0.6905, -1.9190,  0.1364,  1.1125,  2.1836]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4822],\n",
      "        [-1.1425],\n",
      "        [ 1.4778],\n",
      "        [ 1.1144]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4587, -1.0332,  1.9394,  2.6804,  1.5934],\n",
      "        [-0.1094, -0.3805, -1.3122,  0.0221,  0.1331],\n",
      "        [-0.4443,  1.0332, -0.0128, -1.4326,  0.4616],\n",
      "        [ 1.8584,  0.0392, -0.3814, -1.8125,  1.4468]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1003, -1.5988, -0.7733, -0.0792, -1.1949],\n",
      "        [ 0.6791,  0.5936,  0.5896,  1.1433,  1.1270],\n",
      "        [-0.9791, -1.3766, -2.1012, -1.0007, -2.1430],\n",
      "        [-0.3327, -0.3248, -0.5537, -0.9601, -0.8625]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4587, -1.0332,  1.9394,  2.6804,  1.5934],\n",
      "        [-0.1094, -0.3805, -1.3122,  0.0221,  0.1331],\n",
      "        [-0.4443,  1.0332, -0.0128, -1.4326,  0.4616],\n",
      "        [ 1.8584,  0.0392, -0.3814, -1.8125,  1.4468]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0101],\n",
      "        [-0.8986],\n",
      "        [-0.5160],\n",
      "        [ 0.0724]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3386, -1.0089,  0.5931,  0.4978,  0.8288],\n",
      "        [ 0.0087, -1.0327,  0.5124, -2.2705,  0.0729],\n",
      "        [ 0.9940, -0.7950, -1.9132, -1.1597, -1.0209],\n",
      "        [ 0.9078,  0.1394, -0.3859, -0.1923,  0.1846]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6358,  0.2299,  0.1365,  0.4533,  1.0720],\n",
      "        [ 1.0614,  1.1310,  1.3541,  0.8768,  0.8134],\n",
      "        [-0.5803, -2.0716, -2.4090, -1.1177, -1.7687],\n",
      "        [-0.1673, -0.2475,  0.3583, -0.8123, -0.4064]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3386, -1.0089,  0.5931,  0.4978,  0.8288],\n",
      "        [ 0.0087, -1.0327,  0.5124, -2.2705,  0.0729],\n",
      "        [ 0.9940, -0.7950, -1.9132, -1.1597, -1.0209],\n",
      "        [ 0.9078,  0.1394, -0.3859, -0.1923,  0.1846]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8142],\n",
      "        [-2.3965],\n",
      "        [ 8.7809],\n",
      "        [-0.2435]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8070,  0.7833,  0.8100, -1.2616,  0.6605],\n",
      "        [ 0.5204,  0.4193, -0.2186,  1.3737,  1.1913],\n",
      "        [-1.0339,  0.7130,  0.4174, -1.4445,  0.8108],\n",
      "        [-0.6100,  1.0943,  1.1025, -0.5006, -0.6907]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4250, -1.0156, -1.1136, -0.9342, -1.5385],\n",
      "        [ 1.2648,  1.4583,  2.1008,  1.5082,  1.9438],\n",
      "        [ 0.2189, -0.8957, -0.0821, -0.2639,  0.2202],\n",
      "        [-0.0074, -0.3494,  0.0635,  0.2128, -0.1217]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8070,  0.7833,  0.8100, -1.2616,  0.6605],\n",
      "        [ 0.5204,  0.4193, -0.2186,  1.3737,  1.1913],\n",
      "        [-1.0339,  0.7130,  0.4174, -1.4445,  0.8108],\n",
      "        [-0.6100,  1.0943,  1.1025, -0.5006, -0.6907]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1923],\n",
      "        [ 5.1979],\n",
      "        [-0.3394],\n",
      "        [-0.3304]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5822, -0.3727, -0.7378,  1.0046,  0.3651],\n",
      "        [-0.1381, -0.9177,  0.7142, -1.5069,  0.8076],\n",
      "        [-0.9376, -1.3310,  1.7953, -0.6116,  0.7347],\n",
      "        [-2.4791, -1.0365,  0.8212, -0.7547, -1.1147]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0616, -0.6906, -1.2829, -0.5745, -0.8548],\n",
      "        [-0.3521, -0.2153, -0.1957, -0.3775, -1.2846],\n",
      "        [ 0.1625,  0.1825, -0.2669,  0.1581, -0.1541],\n",
      "        [ 0.1829,  0.0423, -0.1304, -0.4219, -0.5327]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5822, -0.3727, -0.7378,  1.0046,  0.3651],\n",
      "        [-0.1381, -0.9177,  0.7142, -1.5069,  0.8076],\n",
      "        [-0.9376, -1.3310,  1.7953, -0.6116,  0.7347],\n",
      "        [-2.4791, -1.0365,  0.8212, -0.7547, -1.1147]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2789],\n",
      "        [-0.3621],\n",
      "        [-1.0843],\n",
      "        [ 0.3077]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9251, -0.7154, -0.0039,  0.2528,  0.1121],\n",
      "        [ 0.0473,  1.3584,  0.8561, -0.7944,  0.8934],\n",
      "        [-0.7261, -0.7719,  0.0713, -0.3458, -1.4264],\n",
      "        [-0.5418,  0.2249,  0.8716, -2.0680, -0.7475]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5628, -1.0333, -0.4725,  0.0286, -0.9374],\n",
      "        [ 0.1258,  0.6275,  0.8278,  0.7529,  0.2210],\n",
      "        [ 1.2013,  0.1337,  0.3526,  0.4705,  0.6093],\n",
      "        [-0.0276, -0.8181,  0.2046, -0.0871, -0.3491]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9251, -0.7154, -0.0039,  0.2528,  0.1121],\n",
      "        [ 0.0473,  1.3584,  0.8561, -0.7944,  0.8934],\n",
      "        [-0.7261, -0.7719,  0.0713, -0.3458, -1.4264],\n",
      "        [-0.5418,  0.2249,  0.8716, -2.0680, -0.7475]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1227],\n",
      "        [ 1.1663],\n",
      "        [-1.9822],\n",
      "        [ 0.4504]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0345,  2.3342, -0.5704,  0.8269, -0.1887],\n",
      "        [ 1.8924,  0.2432, -2.0863,  0.9649,  1.2455],\n",
      "        [-0.4891, -0.6824,  1.2402, -1.0040,  0.3089],\n",
      "        [-1.2142,  0.1886, -0.2772,  1.2619, -0.8489]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0237, -1.2441, -0.6683, -0.4126, -0.2927],\n",
      "        [-0.5918,  1.1774, -0.1150,  0.9548,  0.2149],\n",
      "        [ 1.1465,  0.7007,  0.5026,  1.3765,  1.4808],\n",
      "        [-0.0485, -0.2414, -0.0218, -0.6244, -0.1957]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0345,  2.3342, -0.5704,  0.8269, -0.1887],\n",
      "        [ 1.8924,  0.2432, -2.0863,  0.9649,  1.2455],\n",
      "        [-0.4891, -0.6824,  1.2402, -1.0040,  0.3089],\n",
      "        [-1.2142,  0.1886, -0.2772,  1.2619, -0.8489]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8332],\n",
      "        [ 0.5952],\n",
      "        [-1.3400],\n",
      "        [-0.6025]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4305, -0.4958,  1.1867, -2.2628,  1.1510],\n",
      "        [-1.0989, -0.5346, -0.4163, -1.7239, -1.3542],\n",
      "        [-2.0804,  0.4133,  0.6122,  0.7710,  1.6074],\n",
      "        [ 1.7076,  1.0649,  0.1738, -0.2897, -0.0854]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2194,  1.0689,  0.5422,  0.8174,  1.8624],\n",
      "        [ 0.0043,  0.0242, -0.0172, -0.3106, -0.3354],\n",
      "        [ 1.7975,  1.4213,  1.9348,  1.5992,  2.2407],\n",
      "        [-0.2341,  0.1541, -0.5650,  0.1190, -0.1713]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4305, -0.4958,  1.1867, -2.2628,  1.1510],\n",
      "        [-1.0989, -0.5346, -0.4163, -1.7239, -1.3542],\n",
      "        [-2.0804,  0.4133,  0.6122,  0.7710,  1.6074],\n",
      "        [ 1.7076,  1.0649,  0.1738, -0.2897, -0.0854]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9324],\n",
      "        [ 0.9791],\n",
      "        [ 2.8672],\n",
      "        [-0.3537]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0843,  0.8161,  0.4168, -1.5967,  0.5378],\n",
      "        [ 0.1895, -1.4602,  0.0251,  0.4983, -0.5436],\n",
      "        [ 0.0097,  0.2906,  1.2966, -1.2730, -0.5731],\n",
      "        [-0.0539,  0.3663,  1.4932,  1.2455,  0.0924]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7213, -0.0201,  0.0880,  0.8352,  0.3482],\n",
      "        [-0.2412,  0.5037, -0.4086, -0.2019, -0.2178],\n",
      "        [ 0.8004,  0.6405,  0.7797,  0.4877,  1.2396],\n",
      "        [-0.4157,  0.4253,  0.1335, -0.1803, -0.8642]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0843,  0.8161,  0.4168, -1.5967,  0.5378],\n",
      "        [ 0.1895, -1.4602,  0.0251,  0.4983, -0.5436],\n",
      "        [ 0.0097,  0.2906,  1.2966, -1.2730, -0.5731],\n",
      "        [-0.0539,  0.3663,  1.4932,  1.2455,  0.0924]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9081],\n",
      "        [-0.7737],\n",
      "        [-0.1264],\n",
      "        [ 0.0732]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2945, -0.4823, -0.8002, -0.3925,  0.6965],\n",
      "        [-0.6426,  1.6608, -0.1677,  0.3447,  0.8733],\n",
      "        [ 1.0563, -0.0642,  0.6852,  0.2884, -0.7469],\n",
      "        [-2.7127, -1.0236,  0.5779, -0.4676,  0.8106]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6631,  1.0822,  1.2099,  0.6193,  1.8172],\n",
      "        [-0.0723,  1.0095,  0.9631,  0.4824,  1.2613],\n",
      "        [ 0.8287,  0.9184,  0.3619,  1.1113,  1.2322],\n",
      "        [-0.0658, -0.4150,  0.2688, -0.1975, -0.9154]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2945, -0.4823, -0.8002, -0.3925,  0.6965],\n",
      "        [-0.6426,  1.6608, -0.1677,  0.3447,  0.8733],\n",
      "        [ 1.0563, -0.0642,  0.6852,  0.2884, -0.7469],\n",
      "        [-2.7127, -1.0236,  0.5779, -0.4676,  0.8106]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3259],\n",
      "        [ 2.8294],\n",
      "        [ 0.4646],\n",
      "        [ 0.1090]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6441,  2.1528, -0.2388, -0.7744,  2.0243],\n",
      "        [-0.1352,  0.5506,  0.4691,  1.0271, -0.2623],\n",
      "        [ 1.9128, -0.6993,  0.7406,  0.9723,  0.6887],\n",
      "        [-0.3107, -1.4447,  1.4396,  0.0002, -0.8853]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3264,  1.8555,  1.2083,  0.7750,  2.4448],\n",
      "        [-0.8482, -0.3942, -0.3494, -0.2669, -2.0181],\n",
      "        [ 0.4414,  0.8786,  1.3501,  1.0340,  0.3596],\n",
      "        [-0.0167, -0.2849,  0.8626,  0.0948,  0.1247]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6441,  2.1528, -0.2388, -0.7744,  2.0243],\n",
      "        [-0.1352,  0.5506,  0.4691,  1.0271, -0.2623],\n",
      "        [ 1.9128, -0.6993,  0.7406,  0.9723,  0.6887],\n",
      "        [-0.3107, -1.4447,  1.4396,  0.0002, -0.8853]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.8742],\n",
      "        [-0.0111],\n",
      "        [ 2.4828],\n",
      "        [ 1.5482]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4082,  0.1494, -1.1374,  0.3935, -0.0271],\n",
      "        [-0.9604,  0.1992, -0.7005, -1.2268,  2.2144],\n",
      "        [-0.0228,  0.6394, -1.2116, -0.8640,  0.3717],\n",
      "        [-0.7460,  0.1125,  1.8285, -1.2910, -0.7905]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4947, -0.9462, -1.1777, -0.9664, -1.6881],\n",
      "        [-0.3437, -0.0143, -0.7132,  0.0258, -0.7046],\n",
      "        [-0.0526, -0.3032, -0.7131, -0.5207, -0.9650],\n",
      "        [-0.7117, -1.1717, -0.7207, -0.8208, -0.5274]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4082,  0.1494, -1.1374,  0.3935, -0.0271],\n",
      "        [-0.9604,  0.1992, -0.7005, -1.2268,  2.2144],\n",
      "        [-0.0228,  0.6394, -1.2116, -0.8640,  0.3717],\n",
      "        [-0.7460,  0.1125,  1.8285, -1.2910, -0.7905]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0656],\n",
      "        [-0.7651],\n",
      "        [ 0.7625],\n",
      "        [ 0.5578]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1979, -0.0064,  1.0200, -0.3598,  1.2095],\n",
      "        [ 0.9483, -0.9743,  0.2398, -0.4756,  0.6005],\n",
      "        [-0.3249,  0.1334,  1.3608, -0.0219,  1.6443],\n",
      "        [-1.9749,  0.5274, -1.3505,  0.8552,  0.1473]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1746, -1.5386, -1.0811, -0.7616, -2.6731],\n",
      "        [-0.5599,  0.4884, -0.2083,  0.4467,  0.2127],\n",
      "        [-0.8574, -0.8104, -0.7249, -0.3798, -1.1837],\n",
      "        [-0.1650, -0.9389, -0.6939, -0.9930, -1.6776]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1979, -0.0064,  1.0200, -0.3598,  1.2095],\n",
      "        [ 0.9483, -0.9743,  0.2398, -0.4756,  0.6005],\n",
      "        [-0.3249,  0.1334,  1.3608, -0.0219,  1.6443],\n",
      "        [-1.9749,  0.5274, -1.3505,  0.8552,  0.1473]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.2841],\n",
      "        [-1.1414],\n",
      "        [-2.7540],\n",
      "        [-0.3284]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0262,  1.2833, -0.7710, -1.3243,  0.0778],\n",
      "        [ 1.1863,  2.5081, -1.1250,  1.4321,  0.4868],\n",
      "        [ 0.0790,  1.2337, -0.2228,  1.1234,  1.1060],\n",
      "        [-0.6904,  0.4858,  0.4393,  0.1196, -0.4231]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4267,  1.0236,  1.4312,  0.6589,  1.0613],\n",
      "        [-0.1136,  1.2882,  1.1628,  0.9910,  1.0793],\n",
      "        [ 0.3042,  1.5260,  1.3823,  0.9528,  1.9344],\n",
      "        [ 0.0940,  0.0745,  0.0697, -0.3917, -1.1156]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0262,  1.2833, -0.7710, -1.3243,  0.0778],\n",
      "        [ 1.1863,  2.5081, -1.1250,  1.4321,  0.4868],\n",
      "        [ 0.0790,  1.2337, -0.2228,  1.1234,  1.1060],\n",
      "        [-0.6904,  0.4858,  0.4393,  0.1196, -0.4231]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6174],\n",
      "        [ 3.7325],\n",
      "        [ 4.8086],\n",
      "        [ 0.4272]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3592, -0.6536, -0.8148, -1.0292,  1.2074],\n",
      "        [-0.2452,  0.8154,  1.0221, -0.1596, -0.3877],\n",
      "        [ 0.4302,  0.6360,  0.8599,  1.8784,  0.7933],\n",
      "        [-0.9938,  0.8197, -0.8080, -0.4323, -1.1136]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7565,  0.6333,  0.1017,  0.7175,  1.4969],\n",
      "        [-1.1749, -0.7565, -0.6452, -1.3031, -1.6379],\n",
      "        [-1.0584, -0.7091, -0.9813, -0.9889, -2.1853],\n",
      "        [ 0.2312,  0.4733,  0.1433,  0.0498, -0.3552]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3592, -0.6536, -0.8148, -1.0292,  1.2074],\n",
      "        [-0.2452,  0.8154,  1.0221, -0.1596, -0.3877],\n",
      "        [ 0.4302,  0.6360,  0.8599,  1.8784,  0.7933],\n",
      "        [-0.9938,  0.8197, -0.8080, -0.4323, -1.1136]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3004],\n",
      "        [-0.1453],\n",
      "        [-5.3412],\n",
      "        [ 0.4164]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6955,  1.5758, -0.1938, -0.4922, -0.7004],\n",
      "        [-0.3600,  0.3261, -0.6021, -1.1528, -0.8349],\n",
      "        [ 1.0848, -0.3682, -1.4313,  0.9725, -0.2628],\n",
      "        [ 0.1746, -0.8624,  0.1119, -0.0491, -0.1875]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6480,  1.2400, -0.3715,  0.6381,  0.9144],\n",
      "        [-0.6166, -0.6452, -0.9939, -1.2627, -2.3176],\n",
      "        [ 0.8623,  1.4958,  0.5050,  1.0589,  2.3395],\n",
      "        [-0.2369, -0.3851, -0.1685,  0.0165, -0.7224]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6955,  1.5758, -0.1938, -0.4922, -0.7004],\n",
      "        [-0.3600,  0.3261, -0.6021, -1.1528, -0.8349],\n",
      "        [ 1.0848, -0.3682, -1.4313,  0.9725, -0.2628],\n",
      "        [ 0.1746, -0.8624,  0.1119, -0.0491, -0.1875]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5221],\n",
      "        [ 4.0007],\n",
      "        [ 0.0769],\n",
      "        [ 0.4065]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5284,  1.2417,  1.1333, -0.4702, -0.0820],\n",
      "        [ 0.6669, -3.0074, -1.1662,  1.9761, -0.4740],\n",
      "        [ 0.2501, -0.5354,  0.2011, -0.5624,  0.2747],\n",
      "        [ 0.5905, -0.1733,  0.4539,  0.8327,  0.4755]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2401, -0.2556,  0.1193, -0.3643, -0.7536],\n",
      "        [ 0.2766,  0.3619,  0.5126,  0.1864, -0.5986],\n",
      "        [ 0.9067,  1.5407,  1.4856,  0.4650,  1.6852],\n",
      "        [-0.1980, -0.0050, -0.2407,  0.0220, -0.2833]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5284,  1.2417,  1.1333, -0.4702, -0.0820],\n",
      "        [ 0.6669, -3.0074, -1.1662,  1.9761, -0.4740],\n",
      "        [ 0.2501, -0.5354,  0.2011, -0.5624,  0.2747],\n",
      "        [ 0.5905, -0.1733,  0.4539,  0.8327,  0.4755]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3160],\n",
      "        [-0.8497],\n",
      "        [-0.0979],\n",
      "        [-0.3417]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1372,  0.6078, -1.8276,  0.3772,  2.2739],\n",
      "        [-0.6669, -0.3240,  0.2456,  0.5245,  1.2931],\n",
      "        [-0.1534,  0.3894,  1.0519,  1.6581,  2.2640],\n",
      "        [ 0.4263,  2.2589,  0.1044, -1.1882,  0.0885]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5633, -0.2971, -0.2833,  0.2164,  0.4575],\n",
      "        [-0.0433,  0.4729,  0.3572,  0.0335,  0.0123],\n",
      "        [ 0.6418,  0.6403,  2.0540,  1.2817,  1.3159],\n",
      "        [-0.1165, -0.1774,  0.1962, -0.0716, -0.3246]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1372,  0.6078, -1.8276,  0.3772,  2.2739],\n",
      "        [-0.6669, -0.3240,  0.2456,  0.5245,  1.2931],\n",
      "        [-0.1534,  0.3894,  1.0519,  1.6581,  2.2640],\n",
      "        [ 0.4263,  2.2589,  0.1044, -1.1882,  0.0885]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8185],\n",
      "        [-0.0031],\n",
      "        [ 7.4162],\n",
      "        [-0.3735]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4154,  0.6553,  0.0813,  0.4538,  1.1241],\n",
      "        [ 1.6515, -0.8974,  1.1927, -0.4026,  0.9442],\n",
      "        [ 0.0828, -0.2759,  1.0900,  0.9123,  1.1190],\n",
      "        [ 0.3047,  0.1181,  0.2864,  0.7148, -0.7504]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4490, -0.5237,  0.1810,  0.7653,  0.6381],\n",
      "        [ 0.2806,  0.6772, -0.1609,  0.3310,  0.0472],\n",
      "        [-1.3482, -1.8458, -1.7923, -1.9587, -4.3684],\n",
      "        [-0.1933,  0.7306, -0.0646, -0.2189, -0.7616]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4154,  0.6553,  0.0813,  0.4538,  1.1241],\n",
      "        [ 1.6515, -0.8974,  1.1927, -0.4026,  0.9442],\n",
      "        [ 0.0828, -0.2759,  1.0900,  0.9123,  1.1190],\n",
      "        [ 0.3047,  0.1181,  0.2864,  0.7148, -0.7504]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5495],\n",
      "        [-0.4249],\n",
      "        [-8.2312],\n",
      "        [ 0.4239]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.1161,  0.6032, -0.5182, -1.4979,  0.3525],\n",
      "        [ 0.0339,  0.9422, -0.6614,  1.1226, -0.4968],\n",
      "        [ 0.8048, -0.0339, -0.1776, -0.4494,  0.9910],\n",
      "        [-0.5309,  1.2995,  1.6315,  0.8102, -0.5573]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1982,  1.4099,  0.1173,  0.2849,  0.5877],\n",
      "        [ 0.4412,  0.4106, -0.4392, -0.2951, -0.0590],\n",
      "        [ 0.9190,  1.0302,  0.9339,  1.3351,  1.3163],\n",
      "        [-0.2215,  0.1266, -0.2433, -0.5309, -0.4257]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.1161,  0.6032, -0.5182, -1.4979,  0.3525],\n",
      "        [ 0.0339,  0.9422, -0.6614,  1.1226, -0.4968],\n",
      "        [ 0.8048, -0.0339, -0.1776, -0.4494,  0.9910],\n",
      "        [-0.5309,  1.2995,  1.6315,  0.8102, -0.5573]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1508],\n",
      "        [ 0.3903],\n",
      "        [ 1.2434],\n",
      "        [-0.3078]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0822, -0.4607, -2.0945, -1.7749, -0.8504],\n",
      "        [ 0.5264,  0.7724, -1.5556, -0.8445,  1.9828],\n",
      "        [-0.3112, -0.0596, -0.6073, -0.2845,  0.2582],\n",
      "        [-1.5981,  0.0461, -0.5376,  0.6425,  0.9789]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1101,  0.4324,  0.9166,  0.8214,  0.2606],\n",
      "        [ 0.0477, -0.2781, -0.2809, -0.3027, -0.3827],\n",
      "        [ 0.7856,  0.7755,  0.3638,  0.4788,  0.5173],\n",
      "        [-0.0415, -0.4574, -0.3746, -0.1932, -0.3768]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0822, -0.4607, -2.0945, -1.7749, -0.8504],\n",
      "        [ 0.5264,  0.7724, -1.5556, -0.8445,  1.9828],\n",
      "        [-0.3112, -0.0596, -0.6073, -0.2845,  0.2582],\n",
      "        [-1.5981,  0.0461, -0.5376,  0.6425,  0.9789]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.7894],\n",
      "        [-0.2558],\n",
      "        [-0.5143],\n",
      "        [-0.2463]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0103, -1.0472,  2.2002, -0.2339,  1.6583],\n",
      "        [-0.9768,  0.9592, -1.0815,  0.3894,  2.0922],\n",
      "        [-0.3197, -0.5969,  0.2894, -0.2843, -0.0133],\n",
      "        [-1.3310,  0.7032, -1.8988, -1.4731, -0.2251]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.8156,  2.0414,  1.8687,  1.9258,  2.5937],\n",
      "        [-0.0831, -0.7011,  0.5295, -0.7729, -0.2764],\n",
      "        [ 0.3185,  0.5089,  0.1206, -0.2353,  0.7074],\n",
      "        [-0.2534, -0.4522, -0.1248, -0.2060, -0.0858]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0103, -1.0472,  2.2002, -0.2339,  1.6583],\n",
      "        [-0.9768,  0.9592, -1.0815,  0.3894,  2.0922],\n",
      "        [-0.3197, -0.5969,  0.2894, -0.2843, -0.0133],\n",
      "        [-1.3310,  0.7032, -1.8988, -1.4731, -0.2251]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.8431],\n",
      "        [-2.0432],\n",
      "        [-0.3131],\n",
      "        [ 0.5789]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4268, -1.8617, -0.4657,  0.4649,  1.4019],\n",
      "        [-0.1904, -1.6526,  0.1817,  0.2421, -1.7010],\n",
      "        [ 0.2823,  0.8054,  0.1246,  1.0118,  1.6654],\n",
      "        [ 0.8435,  0.2602,  0.2422, -0.8819,  0.6680]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0347, -0.5285, -0.6521, -0.6487, -0.6858],\n",
      "        [ 0.4531,  1.2160,  0.5821,  0.5882,  1.3298],\n",
      "        [ 0.5262, -0.0002,  0.2291, -0.0823, -0.2920],\n",
      "        [-0.0077, -0.2200, -0.0917, -0.7748, -1.1618]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4268, -1.8617, -0.4657,  0.4649,  1.4019],\n",
      "        [-0.1904, -1.6526,  0.1817,  0.2421, -1.7010],\n",
      "        [ 0.2823,  0.8054,  0.1246,  1.0118,  1.6654],\n",
      "        [ 0.8435,  0.2602,  0.2422, -0.8819,  0.6680]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0395],\n",
      "        [-4.1096],\n",
      "        [-0.3927],\n",
      "        [-0.1788]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1702, -2.0997,  1.0021,  0.7693,  0.0828],\n",
      "        [-1.6183, -0.1173, -0.0187,  1.4809,  0.9774],\n",
      "        [ 0.4198,  0.7139, -1.3210, -0.4141, -0.7106],\n",
      "        [ 1.4223,  0.2270, -1.9406,  1.9696, -0.1427]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6873, -0.3472,  0.0708, -0.3141, -0.6366],\n",
      "        [ 1.7247,  2.6837,  2.3169,  1.5749,  3.2777],\n",
      "        [ 0.4527, -0.1946,  0.5798,  0.5459, -0.7287],\n",
      "        [ 0.2122, -0.3654, -0.3672, -0.5874, -0.6440]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1702, -2.0997,  1.0021,  0.7693,  0.0828],\n",
      "        [-1.6183, -0.1173, -0.0187,  1.4809,  0.9774],\n",
      "        [ 0.4198,  0.7139, -1.3210, -0.4141, -0.7106],\n",
      "        [ 1.4223,  0.2270, -1.9406,  1.9696, -0.1427]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2988],\n",
      "        [ 2.3871],\n",
      "        [-0.4231],\n",
      "        [-0.1335]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1080,  1.5185, -1.3302,  0.7369,  0.4649],\n",
      "        [ 0.5772,  0.2527, -0.2254, -0.8968, -0.2658],\n",
      "        [-0.0145, -1.7261, -0.6672,  0.0485,  0.3571],\n",
      "        [ 0.7766,  1.2150,  1.1313,  1.1685,  1.9667]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4022,  0.9537,  0.8644,  0.4242,  0.5734],\n",
      "        [-0.0537,  0.3308, -0.0117, -0.0670,  0.7603],\n",
      "        [ 0.6223,  0.2128, -0.2697, -0.0023, -0.0951],\n",
      "        [-0.2652, -1.1262, -0.3068, -0.8457, -0.9679]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1080,  1.5185, -1.3302,  0.7369,  0.4649],\n",
      "        [ 0.5772,  0.2527, -0.2254, -0.8968, -0.2658],\n",
      "        [-0.0145, -1.7261, -0.6672,  0.0485,  0.3571],\n",
      "        [ 0.7766,  1.2150,  1.1313,  1.1685,  1.9667]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8342],\n",
      "        [-0.0868],\n",
      "        [-0.2305],\n",
      "        [-4.8130]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3276, -0.1287,  1.3329, -0.2611, -0.0574],\n",
      "        [-1.8538, -0.7196, -0.2020,  1.0477, -1.1691],\n",
      "        [-1.2778,  0.9056,  0.1212,  1.2258, -0.7304],\n",
      "        [-0.1903,  0.3869,  0.6307,  0.3433,  1.5506]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1444,  0.5292,  0.3659,  1.1137,  1.3731],\n",
      "        [ 0.3713, -0.0612, -0.0842,  0.4448,  0.3278],\n",
      "        [ 0.4790, -0.9982,  0.3084, -0.0346, -0.1718],\n",
      "        [ 1.1916,  1.7796,  1.5211,  1.1416,  2.4330]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3276, -0.1287,  1.3329, -0.2611, -0.0574],\n",
      "        [-1.8538, -0.7196, -0.2020,  1.0477, -1.1691],\n",
      "        [-1.2778,  0.9056,  0.1212,  1.2258, -0.7304],\n",
      "        [-0.1903,  0.3869,  0.6307,  0.3433,  1.5506]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0973],\n",
      "        [-0.5445],\n",
      "        [-1.3956],\n",
      "        [ 5.5855]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1049,  1.6273,  0.7943, -0.9283, -0.2814],\n",
      "        [ 0.1366,  1.5296, -1.8991,  0.0549, -0.1528],\n",
      "        [ 0.4417, -0.4586, -0.9621,  0.3498, -0.2010],\n",
      "        [ 1.0758,  0.2938, -0.3169,  1.7524,  0.7210]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1641,  0.2669,  1.4503,  0.4057,  0.8234],\n",
      "        [ 0.1520,  0.0693,  0.1044, -0.3619, -0.2665],\n",
      "        [ 1.0311,  0.9946,  0.0333,  1.0006,  1.4485],\n",
      "        [-0.4727, -0.4633, -0.4027, -1.2511, -1.3394]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1049,  1.6273,  0.7943, -0.9283, -0.2814],\n",
      "        [ 0.1366,  1.5296, -1.8991,  0.0549, -0.1528],\n",
      "        [ 0.4417, -0.4586, -0.9621,  0.3498, -0.2010],\n",
      "        [ 1.0758,  0.2938, -0.3169,  1.7524,  0.7210]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9952],\n",
      "        [-0.0507],\n",
      "        [ 0.0263],\n",
      "        [-3.6751]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0659,  1.2414, -0.5337, -0.4148,  0.5498],\n",
      "        [-1.0995,  0.3973,  1.4232,  0.4632,  0.2090],\n",
      "        [ 0.7264, -0.5315,  0.1147, -1.4592,  1.2142],\n",
      "        [ 0.6986,  1.7043,  0.3958,  0.5889,  1.3440]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5889,  0.8502,  0.7314,  0.8214,  0.9731],\n",
      "        [ 0.3323, -0.3472, -0.0377, -0.5725,  0.0305],\n",
      "        [ 0.1018, -0.1514, -0.0884, -0.1677, -0.0198],\n",
      "        [ 1.0282,  1.7093,  1.0469,  1.4703,  1.0817]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0659,  1.2414, -0.5337, -0.4148,  0.5498],\n",
      "        [-1.0995,  0.3973,  1.4232,  0.4632,  0.2090],\n",
      "        [ 0.7264, -0.5315,  0.1147, -1.4592,  1.2142],\n",
      "        [ 0.6986,  1.7043,  0.3958,  0.5889,  1.3440]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4871],\n",
      "        [-0.8158],\n",
      "        [ 0.3649],\n",
      "        [ 6.3655]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0203,  0.0394, -0.2011, -1.8675,  1.4720],\n",
      "        [-0.3368,  0.9950,  1.2519, -0.0668,  0.6251],\n",
      "        [ 0.1402, -2.4175,  0.7686,  0.8870, -0.0980],\n",
      "        [-1.3797, -1.2601,  1.4317,  1.2931, -0.3256]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4115, -0.3436,  0.5427,  0.4381,  0.1588],\n",
      "        [ 0.9213,  0.4663, -0.2078,  0.0629, -0.4317],\n",
      "        [ 0.1666,  0.6200, -0.0341, -0.1034, -0.4446],\n",
      "        [-1.0850, -1.2541, -1.9485, -1.4933, -3.2875]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0203,  0.0394, -0.2011, -1.8675,  1.4720],\n",
      "        [-0.3368,  0.9950,  1.2519, -0.0668,  0.6251],\n",
      "        [ 0.1402, -2.4175,  0.7686,  0.8870, -0.0980],\n",
      "        [-1.3797, -1.2601,  1.4317,  1.2931, -0.3256]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6987],\n",
      "        [-0.3804],\n",
      "        [-1.5498],\n",
      "        [-0.5729]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4101, -0.0458,  1.3585, -0.1293, -2.6594],\n",
      "        [-0.3820,  0.8927,  0.0752,  1.4430, -1.1054],\n",
      "        [-0.1396,  0.9387, -0.0768,  0.5763,  0.9091],\n",
      "        [-0.3262,  0.8456,  0.0480,  1.3885,  0.2989]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1870,  0.8874,  0.7867,  1.0590,  0.1344],\n",
      "        [ 0.6051, -0.3699, -0.3939, -0.9779, -0.4951],\n",
      "        [ 0.8970,  0.4455,  0.6010, -0.0267,  0.7218],\n",
      "        [-0.5963, -0.8722, -2.0605, -2.3041, -3.0404]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4101, -0.0458,  1.3585, -0.1293, -2.6594],\n",
      "        [-0.3820,  0.8927,  0.0752,  1.4430, -1.1054],\n",
      "        [-0.1396,  0.9387, -0.0768,  0.5763,  0.9091],\n",
      "        [-0.3262,  0.8456,  0.0480,  1.3885,  0.2989]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7973],\n",
      "        [-1.4547],\n",
      "        [ 0.8875],\n",
      "        [-4.7500]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1735,  0.5045, -2.1169,  0.0069, -0.6013],\n",
      "        [ 0.3553, -0.6858,  0.0308,  0.0138,  1.1062],\n",
      "        [-1.8279,  0.2880,  0.4493, -0.8986, -0.2585],\n",
      "        [-1.3901,  0.9912, -0.4976, -0.6123,  0.2056]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4368,  0.0731,  0.8242,  1.0129,  1.0383],\n",
      "        [ 0.6738, -0.2801,  0.8385,  1.1049,  1.1409],\n",
      "        [ 0.5465,  0.0900,  0.2630,  0.3728,  0.9091],\n",
      "        [ 0.5211, -0.2990, -0.2026,  0.3104,  0.7295]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1735,  0.5045, -2.1169,  0.0069, -0.6013],\n",
      "        [ 0.3553, -0.6858,  0.0308,  0.0138,  1.1062],\n",
      "        [-1.8279,  0.2880,  0.4493, -0.8986, -0.2585],\n",
      "        [-1.3901,  0.9912, -0.4976, -0.6123,  0.2056]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8126],\n",
      "        [ 1.7347],\n",
      "        [-1.4248],\n",
      "        [-0.9599]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5725, -0.1535,  0.8534,  0.6192, -1.2414],\n",
      "        [ 0.4216,  1.2206, -0.3890, -0.8547, -1.8058],\n",
      "        [ 0.2394, -0.5489,  1.0322,  1.9990,  1.0919],\n",
      "        [ 0.4140, -0.1480, -2.1818,  1.4166,  0.3925]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2313,  1.2176,  2.0123,  1.3348,  1.3826],\n",
      "        [ 0.5070, -0.5415, -0.8232, -0.6360, -1.6456],\n",
      "        [ 0.9229,  0.3989,  1.7041,  1.0153,  1.4079],\n",
      "        [ 0.8357, -0.2092,  0.3879, -0.0448,  0.0463]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5725, -0.1535,  0.8534,  0.6192, -1.2414],\n",
      "        [ 0.4216,  1.2206, -0.3890, -0.8547, -1.8058],\n",
      "        [ 0.2394, -0.5489,  1.0322,  1.9990,  1.0919],\n",
      "        [ 0.4140, -0.1480, -2.1818,  1.4166,  0.3925]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5083],\n",
      "        [ 3.3881],\n",
      "        [ 5.3278],\n",
      "        [-0.5148]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8192,  1.4811, -0.4580,  1.3930,  1.0666],\n",
      "        [ 0.3465,  0.5847,  0.8088, -0.3880,  0.2711],\n",
      "        [-0.1672, -0.9672,  0.8210, -0.8233,  0.3340],\n",
      "        [-0.2601,  0.6585, -1.7310, -0.1992,  2.0241]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1461,  2.3426,  2.0033,  1.7611,  1.3136],\n",
      "        [-0.7057, -1.3700, -1.0205, -2.3499, -2.7213],\n",
      "        [-0.9328, -1.3741, -1.4151, -2.0505, -2.4615],\n",
      "        [ 0.3773, -0.3191,  0.2633, -0.5597,  0.6360]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8192,  1.4811, -0.4580,  1.3930,  1.0666],\n",
      "        [ 0.3465,  0.5847,  0.8088, -0.3880,  0.2711],\n",
      "        [-0.1672, -0.9672,  0.8210, -0.8233,  0.3340],\n",
      "        [-0.2601,  0.6585, -1.7310, -0.1992,  2.0241]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 6.2869],\n",
      "        [-1.6968],\n",
      "        [ 1.1892],\n",
      "        [ 0.6346]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4313,  1.3865, -0.8537, -0.0027, -1.5517],\n",
      "        [-1.8667,  0.0074, -0.1206,  0.1360,  0.1811],\n",
      "        [ 1.1421, -0.2052,  0.8162, -1.7408, -0.8518],\n",
      "        [-1.7741,  0.4221, -0.2339,  0.0940,  1.9736]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.3320, -0.8787, -2.0817, -2.3934, -3.3648],\n",
      "        [-1.0075, -1.0552, -1.2724, -0.5178, -1.8622],\n",
      "        [-1.2257, -1.5427, -2.2906, -1.7615, -2.8664],\n",
      "        [ 0.0483, -0.9047, -0.8141, -0.9557, -1.0984]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4313,  1.3865, -0.8537, -0.0027, -1.5517],\n",
      "        [-1.8667,  0.0074, -0.1206,  0.1360,  0.1811],\n",
      "        [ 1.1421, -0.2052,  0.8162, -1.7408, -0.8518],\n",
      "        [-1.7741,  0.4221, -0.2339,  0.0940,  1.9736]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 6.3610],\n",
      "        [ 1.6188],\n",
      "        [ 2.5550],\n",
      "        [-2.5350]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3013, -0.0586, -1.2494, -0.4373,  1.8176],\n",
      "        [ 0.7653,  1.5211,  0.9498, -0.1470, -0.0396],\n",
      "        [ 0.0553, -0.7192, -0.9511, -1.3492, -0.3822],\n",
      "        [ 0.1196, -0.1912,  1.3744, -0.0156, -0.1992]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3392, -0.3785,  0.1562, -0.5970,  0.4788],\n",
      "        [-1.1792, -0.9169, -1.6462, -1.4889, -2.5966],\n",
      "        [-2.0459, -2.2187, -2.4857, -2.5012, -4.5655],\n",
      "        [ 1.0428,  1.1450,  0.4743,  0.5170,  1.3981]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3013, -0.0586, -1.2494, -0.4373,  1.8176],\n",
      "        [ 0.7653,  1.5211,  0.9498, -0.1470, -0.0396],\n",
      "        [ 0.0553, -0.7192, -0.9511, -1.3492, -0.3822],\n",
      "        [ 0.1196, -0.1912,  1.3744, -0.0156, -0.1992]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0606],\n",
      "        [-3.5390],\n",
      "        [ 8.9662],\n",
      "        [ 0.2710]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5021,  0.6969, -0.2721,  0.6548, -0.4019],\n",
      "        [-0.7110,  1.1641, -0.6804, -0.1147, -0.2631],\n",
      "        [-2.2995,  1.7148,  1.1117,  0.3042,  0.7211],\n",
      "        [-0.0411, -0.1327,  1.1542,  1.8292,  0.1643]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3260,  0.2140, -0.2253, -0.1872, -1.3495],\n",
      "        [ 0.2649, -0.0411, -1.1050, -0.8394, -0.1925],\n",
      "        [-0.1981, -0.1054,  0.3041, -0.3415,  0.1108],\n",
      "        [ 0.7204,  0.0464,  1.1463, -0.2166,  0.2888]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5021,  0.6969, -0.2721,  0.6548, -0.4019],\n",
      "        [-0.7110,  1.1641, -0.6804, -0.1147, -0.2631],\n",
      "        [-2.2995,  1.7148,  1.1117,  0.3042,  0.7211],\n",
      "        [-0.0411, -0.1327,  1.1542,  1.8292,  0.1643]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4666],\n",
      "        [ 0.6626],\n",
      "        [ 0.5889],\n",
      "        [ 0.9384]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7153, -1.3425,  1.0146,  0.3793, -0.7564],\n",
      "        [ 1.0181,  0.1282,  0.2108, -0.7302, -0.2474],\n",
      "        [ 0.1773,  0.7462,  0.3182,  0.8103,  0.3280],\n",
      "        [ 0.4758, -0.7733,  0.6594, -0.1861, -0.2441]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1251,  0.3736, -0.1490, -0.1423, -0.8334],\n",
      "        [ 0.3532, -1.2676, -0.9303, -0.6325, -1.9842],\n",
      "        [ 0.1123,  0.7121, -0.7022,  0.1788,  0.0049],\n",
      "        [ 0.0958, -0.0755,  0.4261, -0.7037, -1.0488]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7153, -1.3425,  1.0146,  0.3793, -0.7564],\n",
      "        [ 1.0181,  0.1282,  0.2108, -0.7302, -0.2474],\n",
      "        [ 0.1773,  0.7462,  0.3182,  0.8103,  0.3280],\n",
      "        [ 0.4758, -0.7733,  0.6594, -0.1861, -0.2441]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0130],\n",
      "        [ 0.9536],\n",
      "        [ 0.4744],\n",
      "        [ 0.7720]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1530, -0.1356,  1.2344, -0.0297,  0.2705],\n",
      "        [ 1.2022, -0.0292,  0.2291,  0.3439,  0.4892],\n",
      "        [ 0.0836,  1.0927,  1.7157, -0.9850, -2.0996],\n",
      "        [ 0.8044,  0.1270,  0.2091,  1.4205, -1.3222]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4968,  0.4549, -0.2550,  0.2826, -0.3326],\n",
      "        [-0.5703, -1.6637, -0.8652, -1.5023, -2.0393],\n",
      "        [-0.0059, -0.1322,  0.7703,  0.9865,  0.2066],\n",
      "        [ 0.3244, -0.6273, -1.1258, -1.0616, -1.0973]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1530, -0.1356,  1.2344, -0.0297,  0.2705],\n",
      "        [ 1.2022, -0.0292,  0.2291,  0.3439,  0.4892],\n",
      "        [ 0.0836,  1.0927,  1.7157, -0.9850, -2.0996],\n",
      "        [ 0.8044,  0.1270,  0.2091,  1.4205, -1.3222]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3988],\n",
      "        [-2.3496],\n",
      "        [-0.2288],\n",
      "        [-0.1113]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2998,  1.8908,  1.3912, -0.9136,  1.9736],\n",
      "        [-1.0244, -0.1957, -0.2458,  1.3368, -0.4381],\n",
      "        [-0.6654, -0.1252,  1.3421, -0.9964, -1.1133],\n",
      "        [-0.9020,  0.1243,  1.2986,  1.6845,  0.5953]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0185,  0.1024, -0.0432,  0.8347, -0.2791],\n",
      "        [ 0.6671, -0.4366,  0.3091, -0.2685,  0.7592],\n",
      "        [ 0.2764,  0.1088,  0.3915, -0.0315, -0.1002],\n",
      "        [ 0.6966, -1.6533, -0.3993, -0.8957, -1.2179]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2998,  1.8908,  1.3912, -0.9136,  1.9736],\n",
      "        [-1.0244, -0.1957, -0.2458,  1.3368, -0.4381],\n",
      "        [-0.6654, -0.1252,  1.3421, -0.9964, -1.1133],\n",
      "        [-0.9020,  0.1243,  1.2986,  1.6845,  0.5953]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1854],\n",
      "        [-1.3655],\n",
      "        [ 0.4707],\n",
      "        [-3.5863]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7517, -0.2438, -1.0481, -1.8185, -1.3938],\n",
      "        [ 0.1506, -1.2859, -0.4207, -0.8369, -0.2989],\n",
      "        [ 0.7617, -0.9311,  0.9515, -0.1088, -0.0391],\n",
      "        [-0.7367,  0.5178, -0.4006, -1.0219, -0.2427]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1093,  0.5493,  1.1343,  0.1645,  0.2153],\n",
      "        [ 0.6076,  0.3335, -0.2292, -0.0393,  0.4835],\n",
      "        [ 0.4247, -0.1100,  0.0589, -0.3023,  0.4257],\n",
      "        [ 1.2316,  1.2609,  0.8982,  0.8054,  1.8476]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7517, -0.2438, -1.0481, -1.8185, -1.3938],\n",
      "        [ 0.1506, -1.2859, -0.4207, -0.8369, -0.2989],\n",
      "        [ 0.7617, -0.9311,  0.9515, -0.1088, -0.0391],\n",
      "        [-0.7367,  0.5178, -0.4006, -1.0219, -0.2427]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0041],\n",
      "        [-0.3527],\n",
      "        [ 0.4982],\n",
      "        [-1.8857]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6982,  0.2558, -0.8097,  1.1847,  0.4189],\n",
      "        [-1.3530, -1.6996,  0.5796,  1.8394,  0.1300],\n",
      "        [ 0.7346, -1.0132,  0.7186,  0.1299,  0.0070],\n",
      "        [-1.2512,  0.9873,  2.9764, -1.0511,  0.6807]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8854,  1.7785,  0.8608,  1.4933,  1.4861],\n",
      "        [ 0.5419, -0.5195, -0.8871, -0.4887,  0.6804],\n",
      "        [ 0.1059,  0.4714, -0.0974, -0.2881,  0.0408],\n",
      "        [ 1.4824,  1.5885,  2.4375,  1.6979,  2.4056]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6982,  0.2558, -0.8097,  1.1847,  0.4189],\n",
      "        [-1.3530, -1.6996,  0.5796,  1.8394,  0.1300],\n",
      "        [ 0.7346, -1.0132,  0.7186,  0.1299,  0.0070],\n",
      "        [-1.2512,  0.9873,  2.9764, -1.0511,  0.6807]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.7677],\n",
      "        [-1.1748],\n",
      "        [-0.5069],\n",
      "        [ 6.8213]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1418, -1.3395,  0.6560,  0.6573,  1.5215],\n",
      "        [ 1.2941,  0.0100, -0.6464,  0.7177, -0.2187],\n",
      "        [-1.1706, -1.3998,  1.1138,  0.3122,  1.9658],\n",
      "        [-0.5152,  2.1822,  0.5169,  1.5031, -0.2420]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1846,  0.8324, -0.2241, -0.7097, -0.7310],\n",
      "        [ 1.2140,  0.2496, -0.0379, -0.4297,  0.5705],\n",
      "        [ 0.3808,  0.6799, -0.0025,  0.6882,  0.5855],\n",
      "        [-0.6042, -0.6528, -0.7129, -1.1221, -0.9719]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1418, -1.3395,  0.6560,  0.6573,  1.5215],\n",
      "        [ 1.2941,  0.0100, -0.6464,  0.7177, -0.2187],\n",
      "        [-1.1706, -1.3998,  1.1138,  0.3122,  1.9658],\n",
      "        [-0.5152,  2.1822,  0.5169,  1.5031, -0.2420]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8146],\n",
      "        [ 1.1649],\n",
      "        [-0.0344],\n",
      "        [-2.9332]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6931,  1.0263,  1.2964, -0.5233,  0.8637],\n",
      "        [-1.1961,  0.7804,  1.8413,  1.1449, -0.1744],\n",
      "        [-0.6682,  0.0021, -0.0090,  0.1016,  0.7681],\n",
      "        [-0.5126,  0.8531,  1.7882, -0.7139,  1.2531]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7001,  0.9533,  1.4758,  0.8309,  2.0013],\n",
      "        [ 0.8110, -0.9980, -1.1723, -1.4227, -1.0753],\n",
      "        [ 0.4499,  0.1652, -0.0325,  0.3710, -0.4761],\n",
      "        [ 1.4138,  1.2521,  1.5894,  0.0299,  1.3851]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6931,  1.0263,  1.2964, -0.5233,  0.8637],\n",
      "        [-1.1961,  0.7804,  1.8413,  1.1449, -0.1744],\n",
      "        [-0.6682,  0.0021, -0.0090,  0.1016,  0.7681],\n",
      "        [-0.5126,  0.8531,  1.7882, -0.7139,  1.2531]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.7001],\n",
      "        [-5.3488],\n",
      "        [-0.6280],\n",
      "        [ 4.8998]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5039,  0.2784,  0.5819, -0.9063,  1.3740],\n",
      "        [-2.7905,  0.4367,  2.2330, -1.9053, -0.1120],\n",
      "        [-0.9377, -0.4758,  0.5712, -0.6642,  2.1226],\n",
      "        [-0.5608,  0.4602,  1.0802, -0.6669, -1.3798]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2814, -0.3424, -0.4863, -0.3631, -1.1586],\n",
      "        [ 1.9825,  2.0212,  1.5048,  1.1225,  2.1406],\n",
      "        [ 0.4957, -0.6457, -0.6595, -0.0111, -0.1005],\n",
      "        [-0.6376, -1.8041, -0.6345, -1.1545, -2.4138]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5039,  0.2784,  0.5819, -0.9063,  1.3740],\n",
      "        [-2.7905,  0.4367,  2.2330, -1.9053, -0.1120],\n",
      "        [-0.9377, -0.4758,  0.5712, -0.6642,  2.1226],\n",
      "        [-0.5608,  0.4602,  1.0802, -0.6669, -1.3798]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4994],\n",
      "        [-3.6677],\n",
      "        [-0.7402],\n",
      "        [ 2.9424]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3138,  0.7761,  0.5210, -0.0716,  0.8108],\n",
      "        [-0.8777, -0.7768, -0.3481,  0.6563, -0.3038],\n",
      "        [ 1.5186, -0.8682, -0.3452,  0.8479, -0.0877],\n",
      "        [-1.2944,  1.4263, -0.3196,  0.2394, -1.0857]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1482,  0.6772,  0.7805,  1.6282,  0.4773],\n",
      "        [ 0.3048,  0.2469, -0.3770,  0.2661,  0.1492],\n",
      "        [ 0.1751,  0.4898, -0.2727, -0.0543,  0.2436],\n",
      "        [-1.4788, -1.8667, -1.9660, -2.2239, -4.1560]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3138,  0.7761,  0.5210, -0.0716,  0.8108],\n",
      "        [-0.8777, -0.7768, -0.3481,  0.6563, -0.3038],\n",
      "        [ 1.5186, -0.8682, -0.3452,  0.8479, -0.0877],\n",
      "        [-1.2944,  1.4263, -0.3196,  0.2394, -1.0857]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1561],\n",
      "        [-0.1988],\n",
      "        [-0.1325],\n",
      "        [ 3.8596]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.3193,  0.0106,  0.3610,  0.3196, -0.5289],\n",
      "        [-0.8175, -0.9302,  0.4323, -0.6645, -0.1332],\n",
      "        [-1.2648, -1.3434,  0.4664,  0.7756, -1.3427],\n",
      "        [-0.5868,  0.6485, -0.1272,  0.1197,  1.5260]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0211, -0.0638,  0.3681,  0.7593,  0.2261],\n",
      "        [ 0.0520, -0.6767,  0.3692, -0.2683,  0.2659],\n",
      "        [ 0.2064, -0.1232, -0.1877, -0.3284, -0.2349],\n",
      "        [ 0.0089, -0.1250, -0.0786, -0.2934,  0.2217]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.3193,  0.0106,  0.3610,  0.3196, -0.5289],\n",
      "        [-0.8175, -0.9302,  0.4323, -0.6645, -0.1332],\n",
      "        [-1.2648, -1.3434,  0.4664,  0.7756, -1.3427],\n",
      "        [-0.5868,  0.6485, -0.1272,  0.1197,  1.5260]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2064],\n",
      "        [ 0.8894],\n",
      "        [-0.1223],\n",
      "        [ 0.2270]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9420, -0.6185,  0.2903, -0.5154,  1.8958],\n",
      "        [ 0.5805, -0.3164,  0.0398,  1.0577, -0.1891],\n",
      "        [-0.5481, -0.1874,  0.3714,  0.1797, -0.2275],\n",
      "        [ 0.3837,  0.0095,  1.4774,  0.6143,  0.1576]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2896,  1.1256,  1.2003,  0.4213,  0.5361],\n",
      "        [-0.5326, -0.2063,  0.2315, -0.2679, -0.0663],\n",
      "        [-0.0466,  0.1446, -0.6732, -0.1919, -0.9672],\n",
      "        [-0.1740, -0.5415, -0.0196, -0.1249, -0.3040]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9420, -0.6185,  0.2903, -0.5154,  1.8958],\n",
      "        [ 0.5805, -0.3164,  0.0398,  1.0577, -0.1891],\n",
      "        [-0.5481, -0.1874,  0.3714,  0.1797, -0.2275],\n",
      "        [ 0.3837,  0.0095,  1.4774,  0.6143,  0.1576]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7242],\n",
      "        [-0.5056],\n",
      "        [-0.0660],\n",
      "        [-0.2255]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1807, -1.1888, -1.4105, -0.9879,  0.4589],\n",
      "        [ 0.5119,  0.8375,  0.6709, -0.0396, -0.4602],\n",
      "        [-0.5392, -0.0350,  0.5984,  0.6105,  0.4774],\n",
      "        [-0.1253, -0.6238, -0.3884,  0.4673, -1.0435]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0529,  1.2663,  0.4583,  0.2803,  0.2291],\n",
      "        [ 0.1279,  0.2363,  0.1726, -0.0036,  0.1021],\n",
      "        [ 0.0178, -0.2366, -0.5316, -0.3977, -0.2996],\n",
      "        [ 0.1644, -0.1359,  0.3958,  0.2419,  0.2999]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1807, -1.1888, -1.4105, -0.9879,  0.4589],\n",
      "        [ 0.5119,  0.8375,  0.6709, -0.0396, -0.4602],\n",
      "        [-0.5392, -0.0350,  0.5984,  0.6105,  0.4774],\n",
      "        [-0.1253, -0.6238, -0.3884,  0.4673, -1.0435]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2612],\n",
      "        [ 0.3323],\n",
      "        [-0.7053],\n",
      "        [-0.2895]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0498,  1.6116,  1.6952, -0.8573,  0.2360],\n",
      "        [-0.5005,  0.4172,  0.1108, -0.1605, -0.8820],\n",
      "        [-0.5427,  0.3955,  0.9161, -0.9264,  0.5875],\n",
      "        [ 0.2090, -0.9913,  0.6614,  0.5492, -0.4916]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9139,  1.1713,  1.4800,  0.6562,  1.7159],\n",
      "        [-0.0409, -0.5658, -0.0434,  0.4585, -0.0960],\n",
      "        [-0.0574, -0.6055,  0.0718, -0.4264,  0.5356],\n",
      "        [ 0.2356,  0.4286, -0.3422,  0.6361,  0.3116]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0498,  1.6116,  1.6952, -0.8573,  0.2360],\n",
      "        [-0.5005,  0.4172,  0.1108, -0.1605, -0.8820],\n",
      "        [-0.5427,  0.3955,  0.9161, -0.9264,  0.5875],\n",
      "        [ 0.2090, -0.9913,  0.6614,  0.5492, -0.4916]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.2795],\n",
      "        [-0.2093],\n",
      "        [ 0.5671],\n",
      "        [-0.4058]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4599, -2.5668, -1.7222,  1.4470, -1.1403],\n",
      "        [ 0.5349,  0.4821,  0.1848,  0.1225,  0.8238],\n",
      "        [-0.0746, -0.8318, -0.2898,  1.1645,  0.6305],\n",
      "        [-0.3539, -1.1943,  1.1203, -0.1308, -0.0369]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6513,  0.4580, -0.0039,  0.0365, -0.2551],\n",
      "        [ 0.4755, -0.5117, -0.3070, -0.1803, -0.6435],\n",
      "        [ 0.5365, -0.9876, -0.1855, -0.5005, -0.9872],\n",
      "        [-0.0103, -0.2731, -0.6512, -0.3646, -0.0358]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4599, -2.5668, -1.7222,  1.4470, -1.1403],\n",
      "        [ 0.5349,  0.4821,  0.1848,  0.1225,  0.8238],\n",
      "        [-0.0746, -0.8318, -0.2898,  1.1645,  0.6305],\n",
      "        [-0.3539, -1.1943,  1.1203, -0.1308, -0.0369]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1250],\n",
      "        [-0.6013],\n",
      "        [-0.3701],\n",
      "        [-0.3507]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0131,  0.7485, -2.4396,  1.7251, -0.8508],\n",
      "        [-0.1939, -0.6623,  0.7065,  0.2510,  0.4797],\n",
      "        [ 0.8243,  1.9142,  0.7726, -0.9046, -1.1874],\n",
      "        [-1.2520,  0.8183, -1.2813, -0.6888, -0.8398]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2491,  1.1174,  0.9390,  0.9531,  1.0611],\n",
      "        [ 0.3681,  0.1187, -0.1638, -0.6155, -0.1760],\n",
      "        [ 0.1114,  0.1866,  0.1026, -0.5015, -0.5042],\n",
      "        [ 0.1803, -0.0964, -0.6641, -0.4138, -0.5432]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0131,  0.7485, -2.4396,  1.7251, -0.8508],\n",
      "        [-0.1939, -0.6623,  0.7065,  0.2510,  0.4797],\n",
      "        [ 0.8243,  1.9142,  0.7726, -0.9046, -1.1874],\n",
      "        [-1.2520,  0.8183, -1.2813, -0.6888, -0.8398]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4608],\n",
      "        [-0.5047],\n",
      "        [ 1.5807],\n",
      "        [ 1.2876]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4587,  0.4505, -0.1448,  1.7683,  0.3447],\n",
      "        [-0.1458,  2.3566,  0.8005,  0.9000, -0.2425],\n",
      "        [ 0.1728,  0.7453, -0.6598, -1.2396, -1.2568],\n",
      "        [ 1.0779,  0.4025, -0.3925,  0.9596, -0.6542]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4697,  0.9957,  1.3527,  0.9633,  1.7618],\n",
      "        [ 0.2063,  0.1073, -0.1395,  0.0252, -0.2599],\n",
      "        [-0.4873, -0.9890, -0.7376, -1.4741, -1.6454],\n",
      "        [-0.3967, -0.9039, -0.9837, -0.9191, -1.3859]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4587,  0.4505, -0.1448,  1.7683,  0.3447],\n",
      "        [-0.1458,  2.3566,  0.8005,  0.9000, -0.2425],\n",
      "        [ 0.1728,  0.7453, -0.6598, -1.2396, -1.2568],\n",
      "        [ 1.0779,  0.4025, -0.3925,  0.9596, -0.6542]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3479],\n",
      "        [ 0.1968],\n",
      "        [ 3.5605],\n",
      "        [-0.3807]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4291,  0.2615, -0.4304, -0.8314, -0.6819],\n",
      "        [-0.4355,  0.2598, -1.2959, -0.3266,  1.5080],\n",
      "        [-0.4736, -0.0033,  0.0616, -1.2380, -0.8632],\n",
      "        [ 0.1733,  0.5717,  1.0324,  1.2716,  0.6797]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2068,  0.3124,  0.5162,  0.4282,  0.1257],\n",
      "        [ 0.2213, -0.1539, -1.0419,  0.4689, -0.6518],\n",
      "        [-1.9114, -1.3697, -1.7287, -1.6821, -2.8182],\n",
      "        [ 0.1950, -0.9576,  0.4535, -0.1924, -1.0067]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4291,  0.2615, -0.4304, -0.8314, -0.6819],\n",
      "        [-0.4355,  0.2598, -1.2959, -0.3266,  1.5080],\n",
      "        [-0.4736, -0.0033,  0.0616, -1.2380, -0.8632],\n",
      "        [ 0.1733,  0.5717,  1.0324,  1.2716,  0.6797]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6710],\n",
      "        [ 0.0778],\n",
      "        [ 5.3184],\n",
      "        [-0.9743]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3886,  0.5653, -1.2970, -0.6554,  0.7726],\n",
      "        [-0.0502,  2.0225, -0.0771, -1.9088, -0.9367],\n",
      "        [ 0.1404, -0.1337,  0.0894, -1.1556,  1.3039],\n",
      "        [-0.0868,  0.5074, -0.6020,  0.1719, -0.3204]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1938,  1.5271,  0.2782,  0.7384,  0.0853],\n",
      "        [ 0.3231, -0.1584, -0.0778, -0.4797, -0.9619],\n",
      "        [-0.6280, -0.2227, -0.5278, -0.3667, -0.1247],\n",
      "        [ 0.7209,  0.1722, -1.1895, -0.3522,  0.4852]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3886,  0.5653, -1.2970, -0.6554,  0.7726],\n",
      "        [-0.0502,  2.0225, -0.0771, -1.9088, -0.9367],\n",
      "        [ 0.1404, -0.1337,  0.0894, -1.1556,  1.3039],\n",
      "        [-0.0868,  0.5074, -0.6020,  0.1719, -0.3204]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1598],\n",
      "        [ 1.4861],\n",
      "        [ 0.1556],\n",
      "        [ 0.5249]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5039, -0.4489,  1.4261, -0.0441,  0.5997],\n",
      "        [-0.5333, -0.9405,  1.4455,  2.1251, -1.5905],\n",
      "        [ 0.1972,  0.2959,  0.4886,  1.0190, -0.1155],\n",
      "        [-0.1940, -0.0833,  0.6052, -0.9665, -1.2188]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0015,  0.5960,  0.3821,  0.3146,  0.3707],\n",
      "        [-0.7839, -0.7338, -0.6746, -1.1268, -1.6850],\n",
      "        [ 0.0658,  0.1615,  0.2234, -0.0047, -0.5896],\n",
      "        [ 0.6419, -0.3058, -0.1606, -0.5605, -0.1891]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5039, -0.4489,  1.4261, -0.0441,  0.5997],\n",
      "        [-0.5333, -0.9405,  1.4455,  2.1251, -1.5905],\n",
      "        [ 0.1972,  0.2959,  0.4886,  1.0190, -0.1155],\n",
      "        [-0.1940, -0.0833,  0.6052, -0.9665, -1.2188]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4851],\n",
      "        [ 0.4186],\n",
      "        [ 0.2332],\n",
      "        [ 0.5759]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5995,  1.6617, -0.0025,  1.7148, -1.7489],\n",
      "        [ 0.2854,  0.3040, -1.1866,  1.7282,  1.8891],\n",
      "        [-1.0183,  1.8182, -1.4870,  0.3687,  0.4556],\n",
      "        [-0.4610, -0.3471,  0.3649,  0.1317,  1.1810]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2734,  0.8165,  1.3320,  0.1874,  1.1352],\n",
      "        [-0.9667, -0.8315, -0.2038, -1.4043, -1.0702],\n",
      "        [-0.1805,  0.6235,  0.1167, -0.3096, -0.1658],\n",
      "        [ 0.2956,  0.0720, -0.2353, -0.9250,  0.3983]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5995,  1.6617, -0.0025,  1.7148, -1.7489],\n",
      "        [ 0.2854,  0.3040, -1.1866,  1.7282,  1.8891],\n",
      "        [-1.0183,  1.8182, -1.4870,  0.3687,  0.4556],\n",
      "        [-0.4610, -0.3471,  0.3649,  0.1317,  1.1810]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1466],\n",
      "        [-4.7356],\n",
      "        [ 0.9541],\n",
      "        [ 0.1014]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0467,  1.9357, -0.0353,  2.0484, -0.6317],\n",
      "        [-1.3887,  0.8717, -1.1278,  1.0453, -0.2296],\n",
      "        [ 0.1084,  1.4590,  1.2874,  0.6693, -3.0480],\n",
      "        [ 1.7368, -0.9660,  0.3913,  0.2474, -1.1470]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1585,  0.9956,  0.8142,  1.1719,  0.8737],\n",
      "        [ 1.4346,  1.6629,  1.2900,  1.4633,  2.1264],\n",
      "        [ 0.0523, -0.0355, -0.3420, -0.1447, -0.9856],\n",
      "        [-0.2157, -0.5343,  0.0371, -0.1075, -0.2596]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0467,  1.9357, -0.0353,  2.0484, -0.6317],\n",
      "        [-1.3887,  0.8717, -1.1278,  1.0453, -0.2296],\n",
      "        [ 0.1084,  1.4590,  1.2874,  0.6693, -3.0480],\n",
      "        [ 1.7368, -0.9660,  0.3913,  0.2474, -1.1470]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.7398],\n",
      "        [-0.9564],\n",
      "        [ 2.4208],\n",
      "        [ 0.4271]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6598,  0.2163,  1.0522,  0.7119, -0.8362],\n",
      "        [ 0.1986,  0.9618,  2.3612,  3.3469,  0.7169],\n",
      "        [-0.5502,  0.7197, -0.3824, -1.1808,  1.2389],\n",
      "        [ 0.8423,  1.5046, -1.5239, -1.1841, -2.9156]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7986, -0.6979, -0.7832, -1.4728, -1.8016],\n",
      "        [ 1.5324,  1.0859,  2.1604,  1.6494,  2.8963],\n",
      "        [-0.7139, -0.5086, -1.1963, -1.0827, -2.3243],\n",
      "        [ 0.1179,  0.5669,  0.2615,  0.3564, -0.1069]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6598,  0.2163,  1.0522,  0.7119, -0.8362],\n",
      "        [ 0.1986,  0.9618,  2.3612,  3.3469,  0.7169],\n",
      "        [-0.5502,  0.7197, -0.3824, -1.1808,  1.2389],\n",
      "        [ 0.8423,  1.5046, -1.5239, -1.1841, -2.9156]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[  0.8085],\n",
      "        [ 14.0462],\n",
      "        [ -1.1168],\n",
      "        [  0.4434]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7693,  1.0427,  0.7145,  1.4318,  1.6877],\n",
      "        [ 1.3596,  0.4511, -1.3041,  1.2237, -2.2530],\n",
      "        [-0.7018,  0.5110,  0.6950,  0.1824,  1.7474],\n",
      "        [ 1.2062,  0.3854,  1.1156,  0.8737, -1.4057]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6609, -0.6477, -1.3815, -0.6889, -2.4674],\n",
      "        [-2.3406, -2.9857, -3.3253, -3.7794, -7.3573],\n",
      "        [-0.0996, -0.8367, -0.3979, -0.8405, -0.4635],\n",
      "        [-0.4105,  0.2143,  0.3242, -0.0532,  0.9216]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7693,  1.0427,  0.7145,  1.4318,  1.6877],\n",
      "        [ 1.3596,  0.4511, -1.3041,  1.2237, -2.2530],\n",
      "        [-0.7018,  0.5110,  0.6950,  0.1824,  1.7474],\n",
      "        [ 1.2062,  0.3854,  1.1156,  0.8737, -1.4057]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ -7.3213],\n",
      "        [ 11.7587],\n",
      "        [ -1.5975],\n",
      "        [ -1.3928]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7005,  0.3180,  0.7416, -1.0851,  1.0596],\n",
      "        [ 0.4009,  0.4257,  0.0133,  0.5420,  0.1725],\n",
      "        [ 0.0394, -0.0235,  2.3635,  0.2930, -0.7317],\n",
      "        [-0.4197,  0.5043, -0.6895,  0.2612,  0.5111]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9139,  2.0327,  0.9561,  1.3293,  2.9572],\n",
      "        [-0.0233, -0.4044,  0.1896, -0.2316,  0.1084],\n",
      "        [ 0.2985, -0.0460, -0.4562, -0.2642,  0.7286],\n",
      "        [ 0.6348,  0.8306,  0.6212,  1.1749,  0.7442]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7005,  0.3180,  0.7416, -1.0851,  1.0596],\n",
      "        [ 0.4009,  0.4257,  0.0133,  0.5420,  0.1725],\n",
      "        [ 0.0394, -0.0235,  2.3635,  0.2930, -0.7317],\n",
      "        [-0.4197,  0.5043, -0.6895,  0.2612,  0.5111]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.6869],\n",
      "        [-0.2858],\n",
      "        [-1.6759],\n",
      "        [ 0.4114]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3792,  1.7741,  0.2929, -0.7984, -1.6562],\n",
      "        [ 0.3264, -0.3282,  1.2404, -0.1315,  1.0590],\n",
      "        [ 0.0993,  1.7352, -1.8093,  0.8856, -0.6860],\n",
      "        [-1.5370,  0.4216, -0.3419, -1.1933,  0.7799]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0951,  1.3746,  0.2906,  1.0712,  0.7750],\n",
      "        [ 0.2902,  0.0054, -0.0154,  0.3744,  0.1222],\n",
      "        [ 0.9380,  0.9259,  1.7074, -0.1740,  1.7501],\n",
      "        [-0.1700,  0.1227,  0.3554, -0.0771,  1.0960]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3792,  1.7741,  0.2929, -0.7984, -1.6562],\n",
      "        [ 0.3264, -0.3282,  1.2404, -0.1315,  1.0590],\n",
      "        [ 0.0993,  1.7352, -1.8093,  0.8856, -0.6860],\n",
      "        [-1.5370,  0.4216, -0.3419, -1.1933,  0.7799]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4211],\n",
      "        [ 0.1540],\n",
      "        [-2.7442],\n",
      "        [ 1.1384]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1373,  0.7556, -0.9551, -0.1086, -2.1938],\n",
      "        [-1.7342,  0.3742, -1.5230, -0.2384,  1.8553],\n",
      "        [-2.1504,  0.1993, -0.8092,  1.6257,  0.9144],\n",
      "        [ 1.8385, -0.7059,  0.4340,  0.6460,  3.4186]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2400,  0.5691,  0.2917,  0.3397,  0.4843],\n",
      "        [ 0.1169,  0.4558,  0.3911, -0.2144, -0.2392],\n",
      "        [ 1.6107,  1.5386,  1.2712,  1.0661,  1.5292],\n",
      "        [-0.1148,  0.0575, -0.0999,  0.2743, -0.3004]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1373,  0.7556, -0.9551, -0.1086, -2.1938],\n",
      "        [-1.7342,  0.3742, -1.5230, -0.2384,  1.8553],\n",
      "        [-2.1504,  0.1993, -0.8092,  1.6257,  0.9144],\n",
      "        [ 1.8385, -0.7059,  0.4340,  0.6460,  3.4186]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9810],\n",
      "        [-1.0204],\n",
      "        [-1.0544],\n",
      "        [-1.1448]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0435,  1.2726, -0.5005, -2.1671,  0.5370],\n",
      "        [-1.3233,  0.0697,  0.6686,  0.1015, -0.4719],\n",
      "        [-1.3694,  0.0075,  1.5675,  0.8376,  1.0247],\n",
      "        [ 0.1756,  0.2347, -0.0151,  0.6622,  1.7328]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0566,  1.5592,  0.8619,  1.2109,  1.3901],\n",
      "        [ 0.4463,  0.9202,  0.3370,  0.8133,  1.3011],\n",
      "        [ 1.5248,  2.3857,  1.9771,  2.0388,  3.0666],\n",
      "        [ 0.2785,  0.0684, -0.0817,  0.6986,  0.0598]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0435,  1.2726, -0.5005, -2.1671,  0.5370],\n",
      "        [-1.3233,  0.0697,  0.6686,  0.1015, -0.4719],\n",
      "        [-1.3694,  0.0075,  1.5675,  0.8376,  1.0247],\n",
      "        [ 0.1756,  0.2347, -0.0151,  0.6622,  1.7328]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3271],\n",
      "        [-0.8327],\n",
      "        [ 5.8788],\n",
      "        [ 0.6324]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6860,  0.6518,  1.0485,  0.2662, -0.3483],\n",
      "        [-1.6908,  0.3924,  0.6759,  2.3038, -0.9044],\n",
      "        [ 0.0687, -0.2291,  0.6215, -0.3189,  1.2593],\n",
      "        [-0.8075, -0.0797,  0.4804, -0.8068, -0.8207]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7309,  1.7248,  1.2500,  1.4373,  1.4442],\n",
      "        [ 1.1478,  0.6209,  0.8181,  0.5776,  0.3926],\n",
      "        [ 0.0755, -0.0782,  0.4298, -1.0799, -1.4236],\n",
      "        [-0.1932, -0.0359, -0.2180,  0.3031,  0.3773]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6860,  0.6518,  1.0485,  0.2662, -0.3483],\n",
      "        [-1.6908,  0.3924,  0.6759,  2.3038, -0.9044],\n",
      "        [ 0.0687, -0.2291,  0.6215, -0.3189,  1.2593],\n",
      "        [-0.8075, -0.0797,  0.4804, -0.8068, -0.8207]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8132],\n",
      "        [-0.1686],\n",
      "        [-1.1581],\n",
      "        [-0.5001]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3916, -0.1741, -2.5023, -0.8008, -1.6474],\n",
      "        [-0.4711, -0.9708,  0.7768,  1.3710,  0.9556],\n",
      "        [-1.0811,  0.0361, -0.9906, -0.7244,  0.9880],\n",
      "        [ 1.0765,  1.2752, -0.5811, -2.0157,  0.8806]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0417,  1.1082, -0.3160,  0.2166,  0.1139],\n",
      "        [ 0.7737,  0.4524,  0.3069, -0.1561,  0.3897],\n",
      "        [ 0.2601,  1.3062,  1.5163,  0.5861,  0.9221],\n",
      "        [ 0.2364, -0.0744, -1.1930,  0.2923, -0.2043]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3916, -0.1741, -2.5023, -0.8008, -1.6474],\n",
      "        [-0.4711, -0.9708,  0.7768,  1.3710,  0.9556],\n",
      "        [-1.0811,  0.0361, -0.9906, -0.7244,  0.9880],\n",
      "        [ 1.0765,  1.2752, -0.5811, -2.0157,  0.8806]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2530],\n",
      "        [-0.4068],\n",
      "        [-1.2496],\n",
      "        [ 0.0837]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1216, -0.9846,  0.8108, -1.9135, -1.0275],\n",
      "        [ 0.8290, -0.8133,  1.2329, -1.3951,  1.0575],\n",
      "        [-0.2981,  0.1219, -0.1787,  1.2522,  1.4477],\n",
      "        [ 2.2080, -0.6483, -1.9763, -0.3424, -1.2066]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1988,  1.0223,  0.5548,  1.3002,  0.3561],\n",
      "        [ 0.7385,  0.4788,  0.0602,  0.4427,  0.5346],\n",
      "        [ 0.7465,  0.8718,  1.1265,  1.1145,  1.5710],\n",
      "        [ 0.2314, -0.2358, -0.6248,  0.6778,  0.7635]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1216, -0.9846,  0.8108, -1.9135, -1.0275],\n",
      "        [ 0.8290, -0.8133,  1.2329, -1.3951,  1.0575],\n",
      "        [-0.2981,  0.1219, -0.1787,  1.2522,  1.4477],\n",
      "        [ 2.2080, -0.6483, -1.9763, -0.3424, -1.2066]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9890],\n",
      "        [ 0.2447],\n",
      "        [ 3.3521],\n",
      "        [ 0.7453]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3505, -0.5564, -0.4531, -0.2062,  1.2254],\n",
      "        [ 0.0786,  1.0056,  0.5578,  0.4135, -0.3630],\n",
      "        [ 0.8249, -0.9933, -0.2704,  0.8394, -0.5759],\n",
      "        [ 1.2105, -0.9393, -1.0471, -1.1108, -0.4021]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3505,  1.5389,  1.5654,  2.0346,  2.2605],\n",
      "        [ 0.2436,  0.3665,  0.2164, -0.2903,  0.1098],\n",
      "        [-0.5074, -0.2712, -0.5230, -0.4506, -1.9015],\n",
      "        [-0.1939, -0.5116, -0.5208,  0.1643,  0.1210]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3505, -0.5564, -0.4531, -0.2062,  1.2254],\n",
      "        [ 0.0786,  1.0056,  0.5578,  0.4135, -0.3630],\n",
      "        [ 0.8249, -0.9933, -0.2704,  0.8394, -0.5759],\n",
      "        [ 1.2105, -0.9393, -1.0471, -1.1108, -0.4021]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.6088],\n",
      "        [ 0.3485],\n",
      "        [ 0.7091],\n",
      "        [ 0.5600]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1317,  1.3192,  0.3603,  0.6794, -1.0755],\n",
      "        [-1.7999, -0.2720,  1.9365, -0.5498, -0.2705],\n",
      "        [-0.7144,  2.4492,  1.1585,  0.1868,  0.4520],\n",
      "        [-0.4608,  0.3384,  0.3108, -2.4292, -0.5601]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5374,  1.1565,  0.8590,  1.3406,  1.2921],\n",
      "        [ 0.8258, -0.1646,  0.1111,  0.1338,  0.2462],\n",
      "        [-0.0748, -1.2763, -0.1941, -0.6937, -1.2509],\n",
      "        [ 0.5864, -0.0240,  0.5504, -0.3333, -0.0560]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1317,  1.3192,  0.3603,  0.6794, -1.0755],\n",
      "        [-1.7999, -0.2720,  1.9365, -0.5498, -0.2705],\n",
      "        [-0.7144,  2.4492,  1.1585,  0.1868,  0.4520],\n",
      "        [-0.4608,  0.3384,  0.3108, -2.4292, -0.5601]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7482],\n",
      "        [-1.3666],\n",
      "        [-3.9922],\n",
      "        [ 0.7337]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1966,  1.5099,  2.0631,  0.1014, -0.0067],\n",
      "        [ 0.0667, -1.5027, -0.5328,  0.6200,  0.8790],\n",
      "        [ 0.8664, -0.8013, -0.1685,  0.5622,  0.4823],\n",
      "        [-0.0068,  0.2605, -1.1363,  1.6179, -0.1930]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0889,  0.7755,  1.3933,  1.6226,  0.9056],\n",
      "        [ 0.6878,  0.3516,  0.0427,  0.9473,  1.3808],\n",
      "        [ 1.2367,  1.2180,  1.6417,  1.3330,  1.6844],\n",
      "        [-0.4120, -0.2122,  0.1036, -0.1247, -0.3163]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1966,  1.5099,  2.0631,  0.1014, -0.0067],\n",
      "        [ 0.0667, -1.5027, -0.5328,  0.6200,  0.8790],\n",
      "        [ 0.8664, -0.8013, -0.1685,  0.5622,  0.4823],\n",
      "        [-0.0068,  0.2605, -1.1363,  1.6179, -0.1930]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.2214],\n",
      "        [ 1.2957],\n",
      "        [ 1.3805],\n",
      "        [-0.3109]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5079, -0.5273,  1.8463,  0.7436, -1.0194],\n",
      "        [ 0.5260, -0.5367,  2.1346,  1.2744,  0.7447],\n",
      "        [-0.0178,  0.7193,  0.9351, -0.0009,  0.9576],\n",
      "        [ 1.6338,  0.5463,  0.4450, -0.0118,  1.8175]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7326, -0.1894, -0.1942, -0.5491, -2.4477],\n",
      "        [-0.0566,  0.0822,  0.5960, -0.2255, -0.0869],\n",
      "        [ 0.4858,  0.7645,  1.6365,  1.1381,  1.3076],\n",
      "        [ 0.2570,  0.5093,  0.4271,  0.2498, -0.0209]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5079, -0.5273,  1.8463,  0.7436, -1.0194],\n",
      "        [ 0.5260, -0.5367,  2.1346,  1.2744,  0.7447],\n",
      "        [-0.0178,  0.7193,  0.9351, -0.0009,  0.9576],\n",
      "        [ 1.6338,  0.5463,  0.4450, -0.0118,  1.8175]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2001],\n",
      "        [ 0.8461],\n",
      "        [ 3.3227],\n",
      "        [ 0.8472]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6953, -1.3322,  0.2022, -1.1506,  0.3737],\n",
      "        [-1.1033, -0.3177,  0.5377,  0.1375,  0.5299],\n",
      "        [ 0.2075, -0.3084,  0.4799, -0.1730, -0.0256],\n",
      "        [ 0.2139, -1.0247, -2.1688,  1.1740, -0.1495]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.6000, -0.9317, -0.9738, -1.5589, -3.2202],\n",
      "        [ 0.4675,  0.1997,  0.3227,  0.0632,  0.4112],\n",
      "        [-0.6430, -0.8483, -0.3007, -0.7142, -1.6813],\n",
      "        [ 0.0307,  0.0874, -1.0059, -0.2762,  0.6504]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6953, -1.3322,  0.2022, -1.1506,  0.3737],\n",
      "        [-1.1033, -0.3177,  0.5377,  0.1375,  0.5299],\n",
      "        [ 0.2075, -0.3084,  0.4799, -0.1730, -0.0256],\n",
      "        [ 0.2139, -1.0247, -2.1688,  1.1740, -0.1495]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5221],\n",
      "        [-0.1791],\n",
      "        [ 0.1505],\n",
      "        [ 1.6772]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6685,  0.7920, -0.7678,  0.3106,  0.3053],\n",
      "        [-0.5657, -0.6381,  0.3114, -0.3149,  1.7180],\n",
      "        [-1.0788, -1.1259,  0.8101, -2.3818, -1.2909],\n",
      "        [-0.5882, -0.1862, -0.8738,  0.1308, -1.8157]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.5156, -1.2409, -2.1786, -2.0039, -4.1574],\n",
      "        [ 0.2133,  0.3405,  0.7005,  0.7762,  0.6810],\n",
      "        [ 0.1491, -0.2366,  0.4227, -0.3696, -1.1796],\n",
      "        [-0.5427, -0.3830, -1.0900, -0.1115, -1.0919]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6685,  0.7920, -0.7678,  0.3106,  0.3053],\n",
      "        [-0.5657, -0.6381,  0.3114, -0.3149,  1.7180],\n",
      "        [-1.0788, -1.1259,  0.8101, -2.3818, -1.2909],\n",
      "        [-0.5882, -0.1862, -0.8738,  0.1308, -1.8157]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1887],\n",
      "        [ 0.8058],\n",
      "        [ 2.8510],\n",
      "        [ 3.3110]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3090,  0.9145,  0.2000, -1.5814, -0.7869],\n",
      "        [ 0.2482,  1.4584,  1.8680, -0.2147,  0.9830],\n",
      "        [ 0.4017, -0.5523, -1.1423, -0.1642,  0.4007],\n",
      "        [-1.6831, -0.7336,  1.0777, -0.6156,  0.2725]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2739,  0.0737, -0.0161, -0.5307, -0.4076],\n",
      "        [-0.2343,  0.0880, -0.0469, -0.2102,  0.4181],\n",
      "        [-1.6456, -0.9832, -1.1413, -1.9128, -2.9349],\n",
      "        [-1.0799, -1.2478, -1.0736, -2.1327, -3.9655]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3090,  0.9145,  0.2000, -1.5814, -0.7869],\n",
      "        [ 0.2482,  1.4584,  1.8680, -0.2147,  0.9830],\n",
      "        [ 0.4017, -0.5523, -1.1423, -0.1642,  0.4007],\n",
      "        [-1.6831, -0.7336,  1.0777, -0.6156,  0.2725]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3088],\n",
      "        [ 0.4387],\n",
      "        [ 0.3238],\n",
      "        [ 1.8084]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5714,  0.1673,  2.9994, -0.6008, -0.7399],\n",
      "        [ 0.2715,  1.1385, -0.3700,  1.5326, -0.1179],\n",
      "        [-0.0362, -2.0210,  0.1329, -0.6348,  1.0751],\n",
      "        [-1.7330,  0.4884,  1.9321,  1.3211,  0.0057]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1923, -0.3247, -1.1407, -0.6429, -1.3597],\n",
      "        [-0.3664,  0.5682,  1.0540,  0.8442,  0.3094],\n",
      "        [-1.4366, -1.2583, -1.7884, -1.3079, -3.0840],\n",
      "        [-0.1077, -0.0404,  0.3453, -0.1201, -0.2652]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5714,  0.1673,  2.9994, -0.6008, -0.7399],\n",
      "        [ 0.2715,  1.1385, -0.3700,  1.5326, -0.1179],\n",
      "        [-0.0362, -2.0210,  0.1329, -0.6348,  1.0751],\n",
      "        [-1.7330,  0.4884,  1.9321,  1.3211,  0.0057]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7812],\n",
      "        [ 1.4149],\n",
      "        [-0.1280],\n",
      "        [ 0.6741]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0070, -1.0615, -0.6976,  0.3817,  0.0054],\n",
      "        [ 1.2694,  0.3809, -0.8610,  0.0503,  0.5211],\n",
      "        [-1.2960,  0.2571,  1.2482,  0.3853,  0.2429],\n",
      "        [-0.4183,  2.7476,  1.4786, -1.2005, -1.2229]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6907,  0.1785,  0.2053,  0.6857,  1.2098],\n",
      "        [-0.2702,  0.8407, -0.1606,  0.2076, -0.0582],\n",
      "        [-0.1483,  0.6356,  0.3498,  0.4016, -0.3589],\n",
      "        [ 0.2091,  0.3600,  0.3964,  0.0965,  0.6738]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0070, -1.0615, -0.6976,  0.3817,  0.0054],\n",
      "        [ 1.2694,  0.3809, -0.8610,  0.0503,  0.5211],\n",
      "        [-1.2960,  0.2571,  1.2482,  0.3853,  0.2429],\n",
      "        [-0.4183,  2.7476,  1.4786, -1.2005, -1.2229]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7600],\n",
      "        [ 0.0956],\n",
      "        [ 0.8598],\n",
      "        [ 0.5479]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6144,  1.1924,  0.9113, -0.3301, -0.9299],\n",
      "        [ 0.9446,  0.5879,  0.5060, -0.5999, -0.0941],\n",
      "        [-0.2840,  0.9445, -0.6432, -0.6799,  1.9562],\n",
      "        [ 2.1393, -0.2035, -0.1586,  0.2909, -1.5539]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9794,  1.0445, -0.2365,  0.3297,  0.9854],\n",
      "        [ 0.0969,  0.8622,  0.7527,  0.8262, -0.0850],\n",
      "        [-0.3960, -0.5657,  0.2514,  0.5227, -0.6286],\n",
      "        [-0.4560,  0.3604,  0.1078, -0.0248,  0.0051]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6144,  1.1924,  0.9113, -0.3301, -0.9299],\n",
      "        [ 0.9446,  0.5879,  0.5060, -0.5999, -0.0941],\n",
      "        [-0.2840,  0.9445, -0.6432, -0.6799,  1.9562],\n",
      "        [ 2.1393, -0.2035, -0.1586,  0.2909, -1.5539]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6065],\n",
      "        [ 0.4916],\n",
      "        [-2.1687],\n",
      "        [-1.0812]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 3.4270e-01,  4.2374e-01,  1.6156e+00, -1.1960e+00,  8.0044e-01],\n",
      "        [-8.5935e-01,  4.7451e-01,  1.7168e-01, -1.2587e+00, -7.5353e-01],\n",
      "        [-4.3370e-01,  8.0567e-01,  7.6754e-01, -2.0125e-01,  1.2641e+00],\n",
      "        [-1.5235e-01, -8.4463e-02, -4.3006e-01,  4.9412e-05, -2.2626e-01]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0691,  0.1016, -0.1735, -0.0431,  0.0892],\n",
      "        [ 0.3363,  0.6216,  0.7812,  0.7180,  0.5664],\n",
      "        [ 0.8807,  1.1089,  1.1265,  0.6923,  1.2686],\n",
      "        [ 0.9103,  0.4879,  0.8349,  0.5568,  0.8984]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 3.4270e-01,  4.2374e-01,  1.6156e+00, -1.1960e+00,  8.0044e-01],\n",
      "        [-8.5935e-01,  4.7451e-01,  1.7168e-01, -1.2587e+00, -7.5353e-01],\n",
      "        [-4.3370e-01,  8.0567e-01,  7.6754e-01, -2.0125e-01,  1.2641e+00],\n",
      "        [-1.5235e-01, -8.4463e-02, -4.3006e-01,  4.9412e-05, -2.2626e-01]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1380],\n",
      "        [-1.1905],\n",
      "        [ 2.8404],\n",
      "        [-0.7422]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8353,  0.2140,  0.0026, -0.6691, -0.3748],\n",
      "        [ 0.0691,  0.2499,  0.4103,  0.9894, -1.4961],\n",
      "        [-1.4025,  0.4285, -1.7055,  0.8064,  1.2414],\n",
      "        [ 0.0713,  0.0135,  1.1330,  0.7365,  0.3273]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1543, -0.5737, -0.4803,  0.2222,  0.2744],\n",
      "        [ 0.2944,  1.1629,  1.8669,  1.0800,  1.8092],\n",
      "        [-0.6724, -0.8463, -0.7062, -0.7923, -1.1787],\n",
      "        [ 0.7621,  0.4384,  0.1519,  1.6075,  0.8446]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8353,  0.2140,  0.0026, -0.6691, -0.3748],\n",
      "        [ 0.0691,  0.2499,  0.4103,  0.9894, -1.4961],\n",
      "        [-1.4025,  0.4285, -1.7055,  0.8064,  1.2414],\n",
      "        [ 0.0713,  0.0135,  1.1330,  0.7365,  0.3273]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5045],\n",
      "        [-0.5613],\n",
      "        [-0.3171],\n",
      "        [ 1.6926]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8346,  0.4357,  1.8186,  0.7935,  1.2279],\n",
      "        [-1.8271, -0.7431,  0.7831,  0.6130,  1.0472],\n",
      "        [-0.7264,  0.4819, -0.5773, -0.2656, -0.2281],\n",
      "        [-1.4439, -1.4576,  0.7828,  1.7315, -0.0920]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2636, -0.2518,  0.1872, -0.3416,  0.0618],\n",
      "        [ 0.8332,  1.0013,  1.6075,  1.1058,  1.7430],\n",
      "        [ 0.1071,  0.5040, -0.3655,  0.3685, -0.7940],\n",
      "        [-0.4079, -0.7910,  0.3161,  0.4295, -0.8415]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8346,  0.4357,  1.8186,  0.7935,  1.2279],\n",
      "        [-1.8271, -0.7431,  0.7831,  0.6130,  1.0472],\n",
      "        [-0.7264,  0.4819, -0.5773, -0.2656, -0.2281],\n",
      "        [-1.4439, -1.4576,  0.7828,  1.7315, -0.0920]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5193],\n",
      "        [ 1.4955],\n",
      "        [ 0.4594],\n",
      "        [ 2.8105]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.7064, -2.3227, -0.5433, -0.8604,  1.7397],\n",
      "        [-2.0530, -0.3684, -0.5027,  0.7722,  2.1680],\n",
      "        [-1.2910, -0.4456,  1.4793, -0.8625, -1.3654],\n",
      "        [-1.3077, -0.9079,  0.7737,  0.7774,  1.4661]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3146, -0.1592, -0.0118,  0.3594,  0.0843],\n",
      "        [-0.7394,  0.7873,  0.6567,  0.6340,  0.9033],\n",
      "        [ 0.1317,  0.0735,  0.1170,  0.1836, -0.4481],\n",
      "        [-1.1907, -0.6658, -0.9233, -2.0238, -2.7095]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7064, -2.3227, -0.5433, -0.8604,  1.7397],\n",
      "        [-2.0530, -0.3684, -0.5027,  0.7722,  2.1680],\n",
      "        [-1.2910, -0.4456,  1.4793, -0.8625, -1.3654],\n",
      "        [-1.3077, -0.9079,  0.7737,  0.7774,  1.4661]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4359],\n",
      "        [ 3.3457],\n",
      "        [ 0.4237],\n",
      "        [-4.0984]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4961,  0.1510,  0.0900, -0.1551,  0.2748],\n",
      "        [ 0.4529,  0.5293,  1.5799, -0.4299,  0.9217],\n",
      "        [-1.3491, -0.7273, -1.0641,  1.1619,  1.9202],\n",
      "        [ 0.2667, -0.0824, -0.1256,  0.4365,  0.2565]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1693,  0.2013,  0.5909,  0.3433,  0.3327],\n",
      "        [-1.0042, -0.0495, -1.0213, -1.5210, -1.5128],\n",
      "        [-0.2044,  0.2044,  0.1368,  0.7423,  0.3553],\n",
      "        [ 0.5020,  0.6423, -0.0333,  0.6299,  1.3014]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4961,  0.1510,  0.0900, -0.1551,  0.2748],\n",
      "        [ 0.4529,  0.5293,  1.5799, -0.4299,  0.9217],\n",
      "        [-1.3491, -0.7273, -1.0641,  1.1619,  1.9202],\n",
      "        [ 0.2667, -0.0824, -0.1256,  0.4365,  0.2565]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0378],\n",
      "        [-2.8350],\n",
      "        [ 1.5263],\n",
      "        [ 0.6939]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5315,  1.3601, -0.7386, -0.1836,  0.5493],\n",
      "        [ 1.5056,  0.7644, -0.3545,  1.2849, -0.3370],\n",
      "        [ 0.2666,  1.0465,  0.2970,  0.9961,  1.9537],\n",
      "        [ 0.9085, -1.0697,  0.4328, -0.0820,  1.9814]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4272, -0.0093, -0.0105, -0.0477,  0.1297],\n",
      "        [ 0.3147, -0.0400,  0.4574,  0.9926,  1.3584],\n",
      "        [-0.5853, -0.3134, -0.5607, -0.5092, -0.8758],\n",
      "        [-0.0963,  0.4473,  0.0095,  0.3808,  0.8399]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5315,  1.3601, -0.7386, -0.1836,  0.5493],\n",
      "        [ 1.5056,  0.7644, -0.3545,  1.2849, -0.3370],\n",
      "        [ 0.2666,  1.0465,  0.2970,  0.9961,  1.9537],\n",
      "        [ 0.9085, -1.0697,  0.4328, -0.0820,  1.9814]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3021],\n",
      "        [ 1.0987],\n",
      "        [-2.8689],\n",
      "        [ 1.0710]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1905,  0.5037, -1.5907, -1.5362,  2.2027],\n",
      "        [-1.3915, -0.5395,  1.1613,  0.4410, -0.5786],\n",
      "        [-1.1061,  0.7077, -1.1938, -1.2461, -0.7255],\n",
      "        [ 0.7175,  0.4797,  0.8570, -0.9543, -0.1053]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2703, -0.4231, -0.1879,  0.0252,  0.3157],\n",
      "        [-0.4244,  0.8645,  0.0971,  0.1093,  0.4961],\n",
      "        [ 0.6184,  0.7388,  1.2433,  1.5277,  1.2865],\n",
      "        [ 0.0085, -0.3015,  0.2037, -0.4438, -0.6313]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1905,  0.5037, -1.5907, -1.5362,  2.2027],\n",
      "        [-1.3915, -0.5395,  1.1613,  0.4410, -0.5786],\n",
      "        [-1.1061,  0.7077, -1.1938, -1.2461, -0.7255],\n",
      "        [ 0.7175,  0.4797,  0.8570, -0.9543, -0.1053]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7939],\n",
      "        [-0.0019],\n",
      "        [-4.4823],\n",
      "        [ 0.5260]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6689,  0.6639,  0.3432, -1.0490, -0.6951],\n",
      "        [ 0.3536, -0.2877,  0.9127, -0.9892, -0.0767],\n",
      "        [-1.0566, -0.0001,  0.0835,  0.2169, -0.5876],\n",
      "        [-0.3616,  1.1863, -0.5145,  0.0056, -0.9751]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0185,  0.1060, -0.1374,  0.3342,  0.2041],\n",
      "        [-0.3247,  0.9763,  0.3088, -0.0521,  0.7658],\n",
      "        [ 2.1223,  2.2999,  2.0985,  2.3375,  3.7512],\n",
      "        [-0.2240, -1.3179,  0.2146,  0.3993, -0.6078]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6689,  0.6639,  0.3432, -1.0490, -0.6951],\n",
      "        [ 0.3536, -0.2877,  0.9127, -0.9892, -0.0767],\n",
      "        [-1.0566, -0.0001,  0.0835,  0.2169, -0.5876],\n",
      "        [-0.3616,  1.1863, -0.5145,  0.0056, -0.9751]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4569],\n",
      "        [-0.1211],\n",
      "        [-3.7649],\n",
      "        [-0.9979]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5315,  1.5796,  1.0389, -1.7483,  0.7316],\n",
      "        [ 1.2153,  1.0012, -1.3662,  0.7041, -0.6478],\n",
      "        [ 1.4490,  0.3967,  0.0898,  0.8005,  1.5251],\n",
      "        [ 1.5464,  0.7493,  0.7265,  0.6255,  1.0012]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0044, -0.1428,  1.1080,  0.0566,  0.1697],\n",
      "        [-0.3970,  0.9892,  1.1852,  0.9065,  1.0240],\n",
      "        [ 0.4381,  0.2834, -0.5625, -0.3817, -0.0474],\n",
      "        [-0.0163, -0.6973,  0.2118,  0.0403,  0.5464]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5315,  1.5796,  1.0389, -1.7483,  0.7316],\n",
      "        [ 1.2153,  1.0012, -1.3662,  0.7041, -0.6478],\n",
      "        [ 1.4490,  0.3967,  0.0898,  0.8005,  1.5251],\n",
      "        [ 1.5464,  0.7493,  0.7265,  0.6255,  1.0012]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9531],\n",
      "        [-1.1364],\n",
      "        [ 0.3188],\n",
      "        [ 0.1784]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5512, -0.5242,  1.7949,  0.5860, -1.9081],\n",
      "        [-0.0799,  0.1376,  1.1590,  1.2082, -0.9544],\n",
      "        [-0.1929,  0.9696,  1.4842,  0.8739,  1.7155],\n",
      "        [-1.4368,  0.6421, -0.1197,  0.4258,  1.2945]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2159,  0.1309, -0.5012, -0.6624, -0.3551],\n",
      "        [ 0.3426,  1.2270,  1.3707,  1.8612,  1.7788],\n",
      "        [ 0.6915,  0.3442,  0.0767, -0.3683,  0.5297],\n",
      "        [ 0.0181,  0.5902,  0.7723,  0.2737, -0.3203]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5512, -0.5242,  1.7949,  0.5860, -1.9081],\n",
      "        [-0.0799,  0.1376,  1.1590,  1.2082, -0.9544],\n",
      "        [-0.1929,  0.9696,  1.4842,  0.8739,  1.7155],\n",
      "        [-1.4368,  0.6421, -0.1197,  0.4258,  1.2945]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7978],\n",
      "        [ 2.2810],\n",
      "        [ 0.9010],\n",
      "        [-0.0376]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0102, -0.8113, -0.6373,  0.5534,  0.4345],\n",
      "        [ 0.0390,  0.5442, -1.0667,  0.0434,  0.2075],\n",
      "        [ 0.1864,  0.3943,  0.3277, -0.5865, -0.2638],\n",
      "        [-0.3334, -1.2919,  1.6366, -0.8069,  1.7705]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2519,  0.0652, -0.1659,  0.2128,  0.1507],\n",
      "        [-0.3380,  0.3653,  0.3095,  0.9162,  0.2946],\n",
      "        [-0.1894,  0.4117,  0.2086,  0.4276, -0.0870],\n",
      "        [-0.1347, -0.2218, -0.1480,  0.4564, -0.2412]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0102, -0.8113, -0.6373,  0.5534,  0.4345],\n",
      "        [ 0.0390,  0.5442, -1.0667,  0.0434,  0.2075],\n",
      "        [ 0.1864,  0.3943,  0.3277, -0.5865, -0.2638],\n",
      "        [-0.3334, -1.2919,  1.6366, -0.8069,  1.7705]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0184],\n",
      "        [-0.0437],\n",
      "        [-0.0325],\n",
      "        [-0.7063]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7726, -0.9940,  2.2708,  1.3821, -0.5890],\n",
      "        [-0.4109,  0.1249,  0.5463,  0.3728,  0.9413],\n",
      "        [ 0.4656,  1.0665,  1.2461, -1.1497, -0.0509],\n",
      "        [ 0.6106,  1.5545,  1.7065, -0.3834,  2.2375]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0157,  0.8018, -0.0898, -0.6123,  0.5749],\n",
      "        [-0.0589,  0.6207, -0.2685,  0.6904,  0.9753],\n",
      "        [-0.1080,  0.5690, -0.0032, -0.2288,  0.0368],\n",
      "        [-0.1679, -0.0759, -0.5190, -0.4461, -0.1945]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7726, -0.9940,  2.2708,  1.3821, -0.5890],\n",
      "        [-0.4109,  0.1249,  0.5463,  0.3728,  0.9413],\n",
      "        [ 0.4656,  1.0665,  1.2461, -1.1497, -0.0509],\n",
      "        [ 0.6106,  1.5545,  1.7065, -0.3834,  2.2375]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1579],\n",
      "        [ 1.1305],\n",
      "        [ 0.8138],\n",
      "        [-1.3704]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2460,  0.2759,  0.3032, -1.3627, -0.4499],\n",
      "        [-1.7837,  1.6200, -1.8421,  0.1856,  2.2616],\n",
      "        [-1.5319, -0.0771, -1.2520, -0.2741, -0.3180],\n",
      "        [ 0.1596,  1.7823, -0.2142, -1.0928,  0.6042]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2507,  0.6337,  0.5262,  1.0256,  1.2780],\n",
      "        [-0.5324,  0.5672,  0.3212,  0.6306,  0.5768],\n",
      "        [-0.6679,  0.1004,  0.4164,  0.2405, -0.3028],\n",
      "        [ 0.3748, -0.1279, -0.3150,  0.2988,  1.3516]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2460,  0.2759,  0.3032, -1.3627, -0.4499],\n",
      "        [-1.7837,  1.6200, -1.8421,  0.1856,  2.2616],\n",
      "        [-1.5319, -0.0771, -1.2520, -0.2741, -0.3180],\n",
      "        [ 0.1596,  1.7823, -0.2142, -1.0928,  0.6042]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3306],\n",
      "        [ 2.6983],\n",
      "        [ 0.5243],\n",
      "        [ 0.3895]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6385, -0.0512, -1.2522,  0.7522,  0.0248],\n",
      "        [-0.8889,  0.3964, -0.0108,  0.6938,  1.8139],\n",
      "        [-0.4211, -0.5448, -1.7982,  1.5916,  0.6999],\n",
      "        [ 1.4117,  1.1057, -0.2114, -0.6167,  1.2685]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1863,  1.8838,  1.7822,  0.7033,  1.6659],\n",
      "        [-0.6081, -0.6766, -0.8873, -1.1545, -2.2824],\n",
      "        [-0.4972,  0.3029,  0.3663,  0.1603, -0.2564],\n",
      "        [ 0.4119, -0.2953, -0.2339,  0.0180, -0.0179]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6385, -0.0512, -1.2522,  0.7522,  0.0248],\n",
      "        [-0.8889,  0.3964, -0.0108,  0.6938,  1.8139],\n",
      "        [-0.4211, -0.5448, -1.7982,  1.5916,  0.6999],\n",
      "        [ 1.4117,  1.1057, -0.2114, -0.6167,  1.2685]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5153],\n",
      "        [-4.6591],\n",
      "        [-0.5386],\n",
      "        [ 0.2705]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2138, -0.1207, -0.8402, -0.4315, -0.0365],\n",
      "        [-0.6641,  0.7429, -1.6028, -1.0993,  0.6180],\n",
      "        [-0.8985,  1.6922, -0.3254, -1.1140,  1.5574],\n",
      "        [-1.0507, -0.2790,  1.2650,  1.2045,  2.0092]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6600,  1.6737,  1.9392,  1.2003,  2.7406],\n",
      "        [-0.1210,  1.3331,  1.4535,  1.3985,  1.0781],\n",
      "        [-0.1761,  0.0514,  0.6269,  0.7524,  0.9479],\n",
      "        [ 0.3575, -0.2470,  0.0516, -0.0712, -0.0911]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2138, -0.1207, -0.8402, -0.4315, -0.0365],\n",
      "        [-0.6641,  0.7429, -1.6028, -1.0993,  0.6180],\n",
      "        [-0.8985,  1.6922, -0.3254, -1.1140,  1.5574],\n",
      "        [-1.0507, -0.2790,  1.2650,  1.2045,  2.0092]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.4644],\n",
      "        [-2.1300],\n",
      "        [ 0.6791],\n",
      "        [-0.5103]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7380, -0.2564,  0.0074,  1.7301, -0.5288],\n",
      "        [-0.9326,  1.2471, -0.6717, -0.3798,  1.2636],\n",
      "        [ 0.9405, -0.2991,  0.0869, -0.3555,  0.9489],\n",
      "        [ 0.6904,  0.8272,  0.9672, -0.5278, -0.0876]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4481,  0.3407,  0.3852,  0.3057,  0.0376],\n",
      "        [ 0.7886,  2.2917,  2.2971,  1.7304,  2.2905],\n",
      "        [-0.0079,  0.3913, -0.0822, -0.1134,  0.5259],\n",
      "        [ 0.6216, -0.1508, -1.0497, -0.3481, -0.1397]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7380, -0.2564,  0.0074,  1.7301, -0.5288],\n",
      "        [-0.9326,  1.2471, -0.6717, -0.3798,  1.2636],\n",
      "        [ 0.9405, -0.2991,  0.0869, -0.3555,  0.9489],\n",
      "        [ 0.6904,  0.8272,  0.9672, -0.5278, -0.0876]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0939],\n",
      "        [ 2.8166],\n",
      "        [ 0.4078],\n",
      "        [-0.5150]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6952, -0.5609, -0.0583,  0.9616,  0.2368],\n",
      "        [ 0.4150,  0.9605, -0.0738,  1.5176, -0.9861],\n",
      "        [-0.2882, -1.4942,  0.6896, -0.8050, -0.6651],\n",
      "        [ 0.3333,  0.2275, -0.6823,  0.8783, -0.9710]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2057, -0.0683, -0.0604, -0.3748,  0.3075],\n",
      "        [ 0.1260,  0.6918,  1.1908,  0.7652,  0.7505],\n",
      "        [-0.3115,  0.3933,  0.5752, -0.1240, -0.1341],\n",
      "        [ 0.1254, -0.0622, -0.0987, -0.6228, -0.5927]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6952, -0.5609, -0.0583,  0.9616,  0.2368],\n",
      "        [ 0.4150,  0.9605, -0.0738,  1.5176, -0.9861],\n",
      "        [-0.2882, -1.4942,  0.6896, -0.8050, -0.6651],\n",
      "        [ 0.3333,  0.2275, -0.6823,  0.8783, -0.9710]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3888],\n",
      "        [ 1.0501],\n",
      "        [ 0.0878],\n",
      "        [ 0.1235]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8696,  1.4292,  0.7008, -0.8164,  0.4918],\n",
      "        [ 0.7478, -0.0012, -0.7956, -2.1269, -0.7595],\n",
      "        [-0.8183, -0.0036,  2.0857, -1.2575,  1.4013],\n",
      "        [-0.5363, -0.0968, -1.2520, -1.6714, -1.1342]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3491, -0.1776, -0.2291, -0.2210, -0.0588],\n",
      "        [ 0.0428, -0.0058,  0.9004,  1.2589,  1.0935],\n",
      "        [ 0.5024, -0.6459,  1.1800, -0.0862, -0.0628],\n",
      "        [ 0.3700, -0.3444, -0.1193, -0.0701, -0.2506]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8696,  1.4292,  0.7008, -0.8164,  0.4918],\n",
      "        [ 0.7478, -0.0012, -0.7956, -2.1269, -0.7595],\n",
      "        [-0.8183, -0.0036,  2.0857, -1.2575,  1.4013],\n",
      "        [-0.5363, -0.0968, -1.2520, -1.6714, -1.1342]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5664],\n",
      "        [-4.1924],\n",
      "        [ 2.0729],\n",
      "        [ 0.3856]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5250, -0.0002,  0.1781,  2.7801, -0.6243],\n",
      "        [-0.9532,  0.3159,  0.7401,  0.1458, -0.1014],\n",
      "        [-0.5942,  1.1451, -0.0881,  2.0847, -0.2924],\n",
      "        [-0.3066,  1.4626,  1.1790,  0.2725,  0.2815]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1273,  0.0292, -0.1752,  0.0657, -0.3756],\n",
      "        [ 1.1416,  1.7533,  2.5272,  1.9548,  3.1557],\n",
      "        [-0.1822, -0.4885,  0.0455, -0.7287, -1.5282],\n",
      "        [ 0.6130, -0.4748, -0.1161, -0.8626, -0.1828]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5250, -0.0002,  0.1781,  2.7801, -0.6243],\n",
      "        [-0.9532,  0.3159,  0.7401,  0.1458, -0.1014],\n",
      "        [-0.5942,  1.1451, -0.0881,  2.0847, -0.2924],\n",
      "        [-0.3066,  1.4626,  1.1790,  0.2725,  0.2815]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5800],\n",
      "        [ 1.3011],\n",
      "        [-1.5275],\n",
      "        [-1.3056]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1368,  0.9435, -1.4208, -0.6742, -1.6577],\n",
      "        [ 1.4663,  2.5196,  2.2488,  0.2350,  0.9850],\n",
      "        [-0.8795, -0.4269,  0.6975,  0.3662,  0.8511],\n",
      "        [ 0.5635,  1.5160,  0.2911,  0.2734,  0.4842]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0274, -0.1378,  0.0919,  0.2331, -0.1174],\n",
      "        [-0.0968, -0.0292, -0.1141,  0.7637,  0.2175],\n",
      "        [ 0.6138,  0.8674,  0.8254,  0.2751,  0.2625],\n",
      "        [ 0.4967,  0.0660,  0.2593,  0.3273,  0.2230]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1368,  0.9435, -1.4208, -0.6742, -1.6577],\n",
      "        [ 1.4663,  2.5196,  2.2488,  0.2350,  0.9850],\n",
      "        [-0.8795, -0.4269,  0.6975,  0.3662,  0.8511],\n",
      "        [ 0.5635,  1.5160,  0.2911,  0.2734,  0.4842]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1920],\n",
      "        [-0.0784],\n",
      "        [-0.0102],\n",
      "        [ 0.6529]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6400, -0.1433,  0.6983, -1.8045, -0.1081],\n",
      "        [ 0.3264,  0.2592,  0.6008,  1.5608, -0.2878],\n",
      "        [-0.0134, -1.1996,  2.1919,  0.8522, -0.5006],\n",
      "        [ 0.0848,  1.1976, -0.2540,  1.5100,  0.9273]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2498, -0.2870,  0.0912,  0.0348,  0.1392],\n",
      "        [ 0.7576, -0.3889,  0.4182,  0.0023, -0.2372],\n",
      "        [-0.4835, -0.2027, -0.0052,  0.2105, -0.0705],\n",
      "        [ 0.4572, -0.5164, -0.2764, -0.3372, -0.5525]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6400, -0.1433,  0.6983, -1.8045, -0.1081],\n",
      "        [ 0.3264,  0.2592,  0.6008,  1.5608, -0.2878],\n",
      "        [-0.0134, -1.1996,  2.1919,  0.8522, -0.5006],\n",
      "        [ 0.0848,  1.1976, -0.2540,  1.5100,  0.9273]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3828],\n",
      "        [ 0.4696],\n",
      "        [ 0.4529],\n",
      "        [-1.5309]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9850,  0.0904,  0.5597,  1.3894,  1.3155],\n",
      "        [-0.3556, -0.3132, -1.5203,  0.0619, -1.3533],\n",
      "        [-0.8751, -0.2809,  0.3934,  1.5399, -0.7410],\n",
      "        [ 0.6713, -0.1546, -1.5478, -1.0824,  0.6196]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0762,  0.3928,  0.3949, -0.0819, -0.2327],\n",
      "        [ 0.3619,  0.4543, -0.1331, -0.4120, -0.4175],\n",
      "        [-0.3222,  0.0393,  0.0839,  0.1578, -0.4054],\n",
      "        [ 0.8150,  0.1522,  0.1321,  0.2559,  1.2590]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9850,  0.0904,  0.5597,  1.3894,  1.3155],\n",
      "        [-0.3556, -0.3132, -1.5203,  0.0619, -1.3533],\n",
      "        [-0.8751, -0.2809,  0.3934,  1.5399, -0.7410],\n",
      "        [ 0.6713, -0.1546, -1.5478, -1.0824,  0.6196]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8966],\n",
      "        [ 0.4708],\n",
      "        [ 0.8473],\n",
      "        [ 0.8222]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7284, -0.3297,  0.0410,  1.6893, -0.2102],\n",
      "        [-0.1825,  1.4923, -1.1905, -0.7010, -0.7339],\n",
      "        [-0.0093,  0.6489, -0.1900, -1.2966,  0.6502],\n",
      "        [ 0.2114,  1.1134,  0.0222,  2.0036,  0.3984]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3850,  0.3872,  0.3135, -0.1186, -0.7919],\n",
      "        [ 0.6559,  0.6840, -0.1343,  0.1356,  0.2192],\n",
      "        [-0.3702, -0.1251, -0.7573,  0.2764,  0.1279],\n",
      "        [ 0.4840, -0.2526,  0.1194,  0.9279, -0.5057]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7284, -0.3297,  0.0410,  1.6893, -0.2102],\n",
      "        [-0.1825,  1.4923, -1.1905, -0.7010, -0.7339],\n",
      "        [-0.0093,  0.6489, -0.1900, -1.2966,  0.6502],\n",
      "        [ 0.2114,  1.1134,  0.0222,  2.0036,  0.3984]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4292],\n",
      "        [ 0.8051],\n",
      "        [-0.2090],\n",
      "        [ 1.4815]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6985,  0.2172, -1.7926, -0.1172,  0.8000],\n",
      "        [-0.0776, -0.0350,  1.8535,  0.0480,  0.1750],\n",
      "        [-0.1389, -0.0299,  2.1892,  1.7204,  1.0360],\n",
      "        [-0.9281,  0.1811,  1.4029,  0.1653, -0.5214]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1061, -0.1047,  0.2701, -0.2364, -0.5216],\n",
      "        [ 0.2655, -0.4565, -0.3410,  0.5575, -0.1558],\n",
      "        [-0.2070,  0.2790, -0.0325, -0.2744, -0.6116],\n",
      "        [ 0.1556, -0.0281, -0.3034, -0.6407, -1.2864]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6985,  0.2172, -1.7926, -0.1172,  0.8000],\n",
      "        [-0.0776, -0.0350,  1.8535,  0.0480,  0.1750],\n",
      "        [-0.1389, -0.0299,  2.1892,  1.7204,  1.0360],\n",
      "        [-0.9281,  0.1811,  1.4029,  0.1653, -0.5214]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8224],\n",
      "        [-0.6372],\n",
      "        [-1.1565],\n",
      "        [-0.0102]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0938,  1.3899,  1.3027,  0.6835, -0.9266],\n",
      "        [ 0.0533, -0.1497,  0.8218,  0.7953, -1.0537],\n",
      "        [-0.1068, -1.5333,  0.0695,  0.7811,  1.3568],\n",
      "        [-0.3520,  0.0799,  2.1287,  1.4166, -1.3732]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4823,  0.3347, -0.3504, -0.6371,  0.6620],\n",
      "        [ 0.2063,  0.5429,  0.6544,  0.4391,  0.8376],\n",
      "        [ 0.4062,  0.5738,  0.3152,  0.2066,  0.9269],\n",
      "        [ 0.1263, -0.1091, -0.4727,  0.0542,  0.3507]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0938,  1.3899,  1.3027,  0.6835, -0.9266],\n",
      "        [ 0.0533, -0.1497,  0.8218,  0.7953, -1.0537],\n",
      "        [-0.1068, -1.5333,  0.0695,  0.7811,  1.3568],\n",
      "        [-0.3520,  0.0799,  2.1287,  1.4166, -1.3732]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5125],\n",
      "        [-0.0658],\n",
      "        [ 0.5177],\n",
      "        [-1.4641]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5698, -0.1644,  1.3231, -0.2809, -0.6874],\n",
      "        [ 0.0524, -0.1156,  0.0208,  0.1781,  0.6669],\n",
      "        [ 0.1237,  0.2198, -0.0023, -0.3987,  0.3874],\n",
      "        [-0.7773, -1.2755, -0.1631,  0.2476,  0.7919]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2397, -0.0205, -0.6284, -0.3640,  0.1407],\n",
      "        [-0.0412,  0.3457, -0.0633, -0.0858,  0.8751],\n",
      "        [-0.0618, -0.0720, -0.3880,  0.6386,  0.1377],\n",
      "        [ 0.2035,  0.1891,  0.2604,  0.7347,  1.1305]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5698, -0.1644,  1.3231, -0.2809, -0.6874],\n",
      "        [ 0.0524, -0.1156,  0.0208,  0.1781,  0.6669],\n",
      "        [ 0.1237,  0.2198, -0.0023, -0.3987,  0.3874],\n",
      "        [-0.7773, -1.2755, -0.1631,  0.2476,  0.7919]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9590],\n",
      "        [ 0.5249],\n",
      "        [-0.2238],\n",
      "        [ 0.6353]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4280, -2.0737,  0.0347, -0.4682, -1.8559],\n",
      "        [-1.1049,  0.1714,  1.6962,  0.3763,  0.7248],\n",
      "        [ 0.2090,  2.0809,  0.9146, -0.4570,  1.1502],\n",
      "        [-0.5430,  0.5154,  0.0438,  0.9945,  0.3242]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7048, -0.1708, -0.0640,  0.0680,  0.5493],\n",
      "        [ 0.1852,  0.2720,  0.7167,  0.4213,  0.8638],\n",
      "        [-0.6357,  0.3404, -0.1941, -0.2758, -0.5350],\n",
      "        [ 0.6701,  0.1822,  0.0239, -0.6319,  0.5910]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4280, -2.0737,  0.0347, -0.4682, -1.8559],\n",
      "        [-1.1049,  0.1714,  1.6962,  0.3763,  0.7248],\n",
      "        [ 0.2090,  2.0809,  0.9146, -0.4570,  1.1502],\n",
      "        [-0.5430,  0.5154,  0.0438,  0.9945,  0.3242]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3978],\n",
      "        [ 1.8423],\n",
      "        [-0.0914],\n",
      "        [-0.7058]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1841, -0.2116, -1.3075,  0.5819, -0.4787],\n",
      "        [ 0.4985,  0.5803, -0.4284, -0.5882,  0.2872],\n",
      "        [-0.2711, -1.3764,  0.7857,  0.1944,  0.8243],\n",
      "        [-0.9885,  0.1567, -0.1521,  0.1492,  0.1803]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4425, -0.2896, -0.0019, -0.7668, -0.0071],\n",
      "        [-0.0577, -0.5455, -1.4252, -0.9036, -1.8772],\n",
      "        [-0.4465,  0.2792, -0.2681, -0.2173,  0.4448],\n",
      "        [ 0.2792,  1.3264, -0.6443, -0.0985,  0.9121]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1841, -0.2116, -1.3075,  0.5819, -0.4787],\n",
      "        [ 0.4985,  0.5803, -0.4284, -0.5882,  0.2872],\n",
      "        [-0.2711, -1.3764,  0.7857,  0.1944,  0.8243],\n",
      "        [-0.9885,  0.1567, -0.1521,  0.1492,  0.1803]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4605],\n",
      "        [ 0.2575],\n",
      "        [-0.1495],\n",
      "        [ 0.1797]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6874,  0.4282, -0.3109, -0.4564, -0.2924],\n",
      "        [-0.2042, -0.5011, -0.2680,  0.4645,  0.9037],\n",
      "        [ 0.0355,  1.2857,  0.8643, -0.6214,  1.7335],\n",
      "        [-1.3901,  0.0736, -0.8735,  1.5409,  0.8741]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7316, -0.4276, -0.0398, -0.1234,  0.1362],\n",
      "        [-0.6432, -0.6251, -0.1534, -0.6747, -1.0916],\n",
      "        [-0.1762, -0.0680,  0.0311, -0.5025,  0.0436],\n",
      "        [ 0.1675,  0.0500,  0.1427, -0.7621, -0.0806]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6874,  0.4282, -0.3109, -0.4564, -0.2924],\n",
      "        [-0.2042, -0.5011, -0.2680,  0.4645,  0.9037],\n",
      "        [ 0.0355,  1.2857,  0.8643, -0.6214,  1.7335],\n",
      "        [-1.3901,  0.0736, -0.8735,  1.5409,  0.8741]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6571],\n",
      "        [-0.8142],\n",
      "        [ 0.3209],\n",
      "        [-1.5986]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7928, -0.7083, -0.3884,  0.5529,  0.1474],\n",
      "        [-0.1205,  0.2794, -0.3874,  1.7178, -1.1480],\n",
      "        [ 0.8921, -0.3380, -1.4105, -1.2304,  1.1245],\n",
      "        [-0.7577,  1.7629, -0.1364,  1.1918,  0.5529]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5108, -0.3340, -0.3119, -1.3458,  0.1131],\n",
      "        [ 0.3227,  0.6859, -0.1188, -0.3515, -0.3548],\n",
      "        [-0.0497, -0.7834, -0.4923, -0.5270,  0.2056],\n",
      "        [ 0.4082,  0.5368,  1.1289,  0.8039,  1.5207]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7928, -0.7083, -0.3884,  0.5529,  0.1474],\n",
      "        [-0.1205,  0.2794, -0.3874,  1.7178, -1.1480],\n",
      "        [ 0.8921, -0.3380, -1.4105, -1.2304,  1.1245],\n",
      "        [-0.7577,  1.7629, -0.1364,  1.1918,  0.5529]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0353],\n",
      "        [ 0.0023],\n",
      "        [ 1.7943],\n",
      "        [ 2.2818]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1844,  0.1503,  0.0557,  0.1312, -0.2084],\n",
      "        [ 0.0506,  1.4252,  0.8036,  0.4149,  2.0375],\n",
      "        [ 0.3695,  1.8899, -0.8816,  0.3481,  1.7001],\n",
      "        [ 0.6194, -1.0802,  0.1324, -2.1423, -0.1797]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1612, -1.3201, -0.1948, -0.7370, -0.3071],\n",
      "        [-0.3977, -0.4215, -0.2103, -0.1229, -0.0468],\n",
      "        [-0.2310, -0.5292, -1.5258, -0.9299, -1.8527],\n",
      "        [-0.1473, -0.4752, -1.0444, -0.3425, -1.1152]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1844,  0.1503,  0.0557,  0.1312, -0.2084],\n",
      "        [ 0.0506,  1.4252,  0.8036,  0.4149,  2.0375],\n",
      "        [ 0.3695,  1.8899, -0.8816,  0.3481,  1.7001],\n",
      "        [ 0.6194, -1.0802,  0.1324, -2.1423, -0.1797]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2718],\n",
      "        [-0.9362],\n",
      "        [-3.2138],\n",
      "        [ 1.2179]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8729, -0.2651,  0.5206, -0.9181, -0.1417],\n",
      "        [-0.2914,  1.4131,  0.0303,  0.5142,  0.3788],\n",
      "        [-0.4552, -0.2133, -1.2129, -0.1099,  1.4713],\n",
      "        [ 0.2245,  0.2454,  1.2724, -0.8556, -0.2263]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6490, -1.1181, -0.9393, -0.9536, -0.8582],\n",
      "        [-0.0063,  0.3558, -0.3001,  0.3523,  0.8404],\n",
      "        [ 0.5783,  1.0051,  0.0285,  0.6279,  1.2301],\n",
      "        [-0.0305, -0.5041, -1.2223, -0.7494, -1.0514]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8729, -0.2651,  0.5206, -0.9181, -0.1417],\n",
      "        [-0.2914,  1.4131,  0.0303,  0.5142,  0.3788],\n",
      "        [-0.4552, -0.2133, -1.2129, -0.1099,  1.4713],\n",
      "        [ 0.2245,  0.2454,  1.2724, -0.8556, -0.2263]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0201],\n",
      "        [ 0.9949],\n",
      "        [ 1.2286],\n",
      "        [-0.8067]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0901,  1.5249,  0.5443,  0.0822, -0.6493],\n",
      "        [ 0.0890,  1.3331, -1.5704, -1.5021,  2.2384],\n",
      "        [ 0.9975, -0.2205,  1.3668, -1.8357, -0.4035],\n",
      "        [-1.8565, -0.2299,  0.0974,  1.0322,  0.6452]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8694, -1.3619, -0.5920, -1.4576, -2.3715],\n",
      "        [-0.5885, -0.5333,  0.0082,  0.2671, -0.0583],\n",
      "        [ 0.2262, -0.3614,  0.7320, -0.0695,  0.0081],\n",
      "        [ 0.0618, -0.8117, -0.5542, -0.4953, -0.5587]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0901,  1.5249,  0.5443,  0.0822, -0.6493],\n",
      "        [ 0.0890,  1.3331, -1.5704, -1.5021,  2.2384],\n",
      "        [ 0.9975, -0.2205,  1.3668, -1.8357, -0.4035],\n",
      "        [-1.8565, -0.2299,  0.0974,  1.0322,  0.6452]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0312],\n",
      "        [-1.3079],\n",
      "        [ 1.4302],\n",
      "        [-0.8537]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.2593, -1.2450, -0.0227,  1.6051, -0.3734],\n",
      "        [-0.6431,  1.3873,  0.4226,  0.9988,  1.7790],\n",
      "        [-0.6338, -1.0752, -0.0713, -0.5479,  1.8855],\n",
      "        [-0.9682,  0.6325,  0.1853, -0.4563,  1.1030]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6504, -1.3892, -1.8395, -0.6950, -1.6608],\n",
      "        [ 0.9755,  0.4622,  0.3104, -0.3829,  1.1190],\n",
      "        [-0.3056, -0.7759, -0.4492, -0.3620, -0.6344],\n",
      "        [ 0.1953, -0.1611,  0.7970, -0.1810,  0.6984]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.2593, -1.2450, -0.0227,  1.6051, -0.3734],\n",
      "        [-0.6431,  1.3873,  0.4226,  0.9988,  1.7790],\n",
      "        [-0.6338, -1.0752, -0.0713, -0.5479,  1.8855],\n",
      "        [-0.9682,  0.6325,  0.1853, -0.4563,  1.1030]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.7455],\n",
      "        [ 1.7531],\n",
      "        [ 0.0620],\n",
      "        [ 0.7097]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0963, -0.8693,  1.6148,  0.2301,  0.5335],\n",
      "        [-0.3011,  0.5534,  0.0012, -0.8150,  0.4540],\n",
      "        [-1.6598, -1.0014, -0.5431,  0.6255, -0.5760],\n",
      "        [-1.6287, -0.0171,  0.1230,  1.8540, -0.3817]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1186, -1.7187, -2.4164, -1.0226, -4.0659],\n",
      "        [-0.5267, -1.2924, -0.2409, -0.5621, -1.3293],\n",
      "        [-0.3962, -0.8810, -0.7229, -0.5140, -1.3558],\n",
      "        [ 0.2779, -0.0594, -0.2705,  0.1725, -0.2970]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0963, -0.8693,  1.6148,  0.2301,  0.5335],\n",
      "        [-0.3011,  0.5534,  0.0012, -0.8150,  0.4540],\n",
      "        [-1.6598, -1.0014, -0.5431,  0.6255, -0.5760],\n",
      "        [-1.6287, -0.0171,  0.1230,  1.8540, -0.3817]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.7046],\n",
      "        [-0.7024],\n",
      "        [ 2.3921],\n",
      "        [-0.0518]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4254,  1.5763,  1.9911,  1.4461,  0.1229],\n",
      "        [ 0.0908, -1.3654,  0.8360, -0.3186,  1.0438],\n",
      "        [-0.2222,  0.1624, -0.0686, -0.7296, -0.2473],\n",
      "        [-0.4674, -2.3690,  0.7269,  1.3482, -0.8633]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5242, -1.0626,  0.5405, -1.0805,  0.1325],\n",
      "        [ 0.1416, -0.0721, -0.4813, -0.4325, -0.5694],\n",
      "        [-0.7435, -0.5872, -1.6059, -1.5616, -2.5185],\n",
      "        [ 0.0193, -0.3851,  0.2215, -0.0849,  0.4521]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4254,  1.5763,  1.9911,  1.4461,  0.1229],\n",
      "        [ 0.0908, -1.3654,  0.8360, -0.3186,  1.0438],\n",
      "        [-0.2222,  0.1624, -0.0686, -0.7296, -0.2473],\n",
      "        [-0.4674, -2.3690,  0.7269,  1.3482, -0.8633]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.3681],\n",
      "        [-0.7475],\n",
      "        [ 1.9422],\n",
      "        [ 0.5594]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4723, -0.7238,  0.8891, -0.2629,  0.8706],\n",
      "        [-1.3853, -0.5686,  0.5990, -0.2805,  0.0506],\n",
      "        [-1.1769,  2.2864, -1.8514, -0.4267,  0.5715],\n",
      "        [-1.2684, -0.5982, -0.5047, -1.1401,  0.8563]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9358,  0.6176,  0.7996, -0.1188,  0.9559],\n",
      "        [-0.8460, -0.4025,  0.1644, -0.5345, -1.0410],\n",
      "        [-0.2383,  0.0588,  0.2897, -0.1763,  0.7027],\n",
      "        [-0.1455, -0.1898, -0.0770, -0.0195,  1.0033]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4723, -0.7238,  0.8891, -0.2629,  0.8706],\n",
      "        [-1.3853, -0.5686,  0.5990, -0.2805,  0.0506],\n",
      "        [-1.1769,  2.2864, -1.8514, -0.4267,  0.5715],\n",
      "        [-1.2684, -0.5982, -0.5047, -1.1401,  0.8563]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6853],\n",
      "        [ 1.5966],\n",
      "        [ 0.3553],\n",
      "        [ 1.2184]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.0561,  0.2166, -0.1179,  2.2445,  2.2104],\n",
      "        [ 0.2619, -0.2610,  0.2245, -1.2037,  0.6940],\n",
      "        [-0.7500,  0.1209,  0.3126,  0.3423, -0.2397],\n",
      "        [-1.5400, -0.0221, -0.2883, -0.1163,  2.6342]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1488, -0.2639,  0.0115, -0.9025,  0.6094],\n",
      "        [-0.5216, -1.0674, -1.0004, -1.6504, -1.7109],\n",
      "        [ 0.1206,  0.2747,  0.5658,  0.4186,  0.0851],\n",
      "        [-0.0305, -1.1891, -0.4755, -0.6814, -0.6422]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0561,  0.2166, -0.1179,  2.2445,  2.2104],\n",
      "        [ 0.2619, -0.2610,  0.2245, -1.2037,  0.6940],\n",
      "        [-0.7500,  0.1209,  0.3126,  0.3423, -0.2397],\n",
      "        [-1.5400, -0.0221, -0.2883, -0.1163,  2.6342]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7455],\n",
      "        [ 0.7168],\n",
      "        [ 0.2426],\n",
      "        [-1.4021]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2200,  0.4810,  0.4377,  0.4839,  1.3839],\n",
      "        [-1.7492,  0.6436, -0.3252, -0.1659, -0.6961],\n",
      "        [ 1.5864,  0.5701,  1.1102, -0.5593, -0.0161],\n",
      "        [ 2.8829,  1.2170,  1.0610, -0.1338,  2.7450]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2170, -0.0789,  0.6457, -0.1483, -0.2233],\n",
      "        [-0.5235, -0.3458, -0.7866, -0.9175, -1.4528],\n",
      "        [ 0.1884,  0.0061,  0.5798,  0.0961,  0.2143],\n",
      "        [-0.0469,  0.4469,  0.3654,  0.6840,  0.9823]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2200,  0.4810,  0.4377,  0.4839,  1.3839],\n",
      "        [-1.7492,  0.6436, -0.3252, -0.1659, -0.6961],\n",
      "        [ 1.5864,  0.5701,  1.1102, -0.5593, -0.0161],\n",
      "        [ 2.8829,  1.2170,  1.0610, -0.1338,  2.7450]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6209],\n",
      "        [ 2.1125],\n",
      "        [ 0.8888],\n",
      "        [ 3.4013]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7721,  0.8153, -0.6313,  1.8299,  0.6839],\n",
      "        [-1.3995, -0.4314,  0.1063, -0.7223, -0.0467],\n",
      "        [-1.3459,  0.4300,  0.3286, -0.7010, -0.6381],\n",
      "        [ 0.0882, -1.6616,  0.7500, -0.0020, -0.0787]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7752,  0.5308,  0.4080, -0.0778,  1.4636],\n",
      "        [-1.1743, -2.0487, -1.7613, -1.7267, -2.9043],\n",
      "        [-0.1892, -0.3704,  0.5355,  0.0433, -0.3078],\n",
      "        [-0.8562, -0.7711, -1.6871, -1.9067, -1.9851]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7721,  0.8153, -0.6313,  1.8299,  0.6839],\n",
      "        [-1.3995, -0.4314,  0.1063, -0.7223, -0.0467],\n",
      "        [-1.3459,  0.4300,  0.3286, -0.7010, -0.6381],\n",
      "        [ 0.0882, -1.6616,  0.7500, -0.0020, -0.0787]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3399],\n",
      "        [ 3.7226],\n",
      "        [ 0.4375],\n",
      "        [ 0.1006]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6527,  0.2611,  0.8501, -0.0972, -1.8329],\n",
      "        [ 0.6372, -0.3320, -0.7707,  0.4343,  0.2148],\n",
      "        [-1.2917,  0.3602,  0.0505,  1.6503,  0.0748],\n",
      "        [ 0.4588, -0.1229,  0.3427,  1.8775, -0.0560]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0660,  0.6254,  1.0132,  1.1302,  1.7558],\n",
      "        [ 0.0267,  0.3096,  0.1368, -0.0659, -0.3550],\n",
      "        [ 0.1846,  0.1884, -0.5883, -0.2110, -0.1758],\n",
      "        [-0.5229, -1.2507, -1.5668, -1.4627, -2.4467]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6527,  0.2611,  0.8501, -0.0972, -1.8329],\n",
      "        [ 0.6372, -0.3320, -0.7707,  0.4343,  0.2148],\n",
      "        [-1.2917,  0.3602,  0.0505,  1.6503,  0.0748],\n",
      "        [ 0.4588, -0.1229,  0.3427,  1.8775, -0.0560]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6077],\n",
      "        [-0.2961],\n",
      "        [-0.5618],\n",
      "        [-3.2323]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8749, -0.3007,  0.7876,  1.5452,  0.3529],\n",
      "        [-1.6105,  0.2359, -0.4699, -0.6001, -0.3710],\n",
      "        [-0.8176, -0.8270, -0.4842, -1.1303, -0.8304],\n",
      "        [-0.4079,  2.2532, -0.2017, -0.5978, -0.4515]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3723,  0.8439,  1.2933,  1.7883,  1.7378],\n",
      "        [-0.1364, -0.2273, -0.3978, -0.5609,  0.3204],\n",
      "        [-0.4503,  0.9071,  0.5143,  0.2807,  0.5768],\n",
      "        [ 0.1018, -0.6083, -0.3367, -0.1508,  0.5720]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8749, -0.3007,  0.7876,  1.5452,  0.3529],\n",
      "        [-1.6105,  0.2359, -0.4699, -0.6001, -0.3710],\n",
      "        [-0.8176, -0.8270, -0.4842, -1.1303, -0.8304],\n",
      "        [-0.4079,  2.2532, -0.2017, -0.5978, -0.4515]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.3420],\n",
      "        [ 0.5707],\n",
      "        [-1.4272],\n",
      "        [-1.5123]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0156, -0.7814,  0.2404, -0.0396, -1.9423],\n",
      "        [ 1.8621,  0.6592,  0.1418, -0.0417,  1.6823],\n",
      "        [-0.1639,  1.3335,  0.3951, -0.5970,  0.2402],\n",
      "        [ 2.5921,  1.9143, -0.1418,  0.1109,  0.5093]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0016, -1.2602, -0.8281, -0.7191, -1.9439],\n",
      "        [ 0.0677,  0.0537, -0.2994,  0.2701, -0.3370],\n",
      "        [ 0.3496,  1.0144,  0.7549,  0.4545,  0.9065],\n",
      "        [ 0.7470,  0.7668,  0.3496,  0.3242,  1.3206]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0156, -0.7814,  0.2404, -0.0396, -1.9423],\n",
      "        [ 1.8621,  0.6592,  0.1418, -0.0417,  1.6823],\n",
      "        [-0.1639,  1.3335,  0.3951, -0.5970,  0.2402],\n",
      "        [ 2.5921,  1.9143, -0.1418,  0.1109,  0.5093]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.5913],\n",
      "        [-0.4592],\n",
      "        [ 1.5402],\n",
      "        [ 4.0632]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5851,  1.9411,  0.9311,  1.4055,  0.2824],\n",
      "        [ 1.6168,  1.0694, -1.8916,  0.2705,  1.1674],\n",
      "        [-0.7556, -0.6746, -0.1985, -1.6618, -0.8133],\n",
      "        [-1.4013, -0.4096, -1.0761, -0.8507, -0.2253]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.5227, -1.7933, -2.8018, -2.1561, -4.2955],\n",
      "        [ 0.4901,  0.0435,  0.2984, -0.4779,  0.0459],\n",
      "        [-0.2970,  0.1393,  0.7504, -0.1910, -0.1330],\n",
      "        [-0.6449, -1.0772, -1.2285, -1.5175, -2.5200]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5851,  1.9411,  0.9311,  1.4055,  0.2824],\n",
      "        [ 1.6168,  1.0694, -1.8916,  0.2705,  1.1674],\n",
      "        [-0.7556, -0.6746, -0.1985, -1.6618, -0.8133],\n",
      "        [-1.4013, -0.4096, -1.0761, -0.8507, -0.2253]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-7.9193],\n",
      "        [ 0.1987],\n",
      "        [ 0.4071],\n",
      "        [ 4.5259]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4728,  0.1209, -0.6657,  1.6012,  1.2339],\n",
      "        [-0.9182, -0.2029,  0.4775,  2.3680, -0.7309],\n",
      "        [-0.5677,  0.8061, -2.0747,  0.7379,  0.5396],\n",
      "        [ 0.6827,  0.0111,  1.8022,  0.1962,  0.0123]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0996,  1.0542,  0.7598,  0.3380,  1.8334],\n",
      "        [ 0.3061,  0.2399, -0.1288, -0.6833,  0.2041],\n",
      "        [ 0.1815,  0.4121, -0.1687,  0.0962,  0.3334],\n",
      "        [ 0.4259, -0.1305, -0.0493,  0.2751,  0.4145]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4728,  0.1209, -0.6657,  1.6012,  1.2339],\n",
      "        [-0.9182, -0.2029,  0.4775,  2.3680, -0.7309],\n",
      "        [-0.5677,  0.8061, -2.0747,  0.7379,  0.5396],\n",
      "        [ 0.6827,  0.0111,  1.8022,  0.1962,  0.0123]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.9450],\n",
      "        [-2.1585],\n",
      "        [ 0.8302],\n",
      "        [ 0.2596]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2849, -0.3480,  0.1388,  0.4276, -1.7642],\n",
      "        [-1.1371, -1.8227, -0.7811,  0.8482,  1.3763],\n",
      "        [ 0.3633,  0.1000,  1.6389, -0.9577,  0.7289],\n",
      "        [-0.4452, -0.4206,  0.3650,  0.5646, -0.3401]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1886, -0.5621, -1.1395, -1.3738, -1.0935],\n",
      "        [ 1.1582,  1.1430,  1.5169,  1.0779,  1.5011],\n",
      "        [ 0.4217,  0.2393,  0.3789,  0.0087,  0.5738],\n",
      "        [ 0.5239, -0.2650,  0.2429,  0.1851,  0.2025]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2849, -0.3480,  0.1388,  0.4276, -1.7642],\n",
      "        [-1.1371, -1.8227, -0.7811,  0.8482,  1.3763],\n",
      "        [ 0.3633,  0.1000,  1.6389, -0.9577,  0.7289],\n",
      "        [-0.4452, -0.4206,  0.3650,  0.5646, -0.3401]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4328],\n",
      "        [-1.6049],\n",
      "        [ 1.2081],\n",
      "        [ 0.0025]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3298, -0.0987,  1.1978,  0.8540, -0.3635],\n",
      "        [ 1.4166, -0.5314, -0.9685, -0.2401, -0.1896],\n",
      "        [-0.5023,  1.1510,  0.0760, -0.0156, -0.0644],\n",
      "        [-1.3396,  0.3968,  0.7725,  0.0032, -0.0668]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4414, -1.0020, -1.8203, -0.7895, -1.6238],\n",
      "        [ 1.6914,  1.0462,  1.6664,  1.2659,  1.9142],\n",
      "        [-0.1318,  0.4127,  0.2684,  0.2337, -0.5984],\n",
      "        [-0.0438,  0.1023,  0.3550, -0.3797,  0.2919]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3298, -0.0987,  1.1978,  0.8540, -0.3635],\n",
      "        [ 1.4166, -0.5314, -0.9685, -0.2401, -0.1896],\n",
      "        [-0.5023,  1.1510,  0.0760, -0.0156, -0.0644],\n",
      "        [-1.3396,  0.3968,  0.7725,  0.0032, -0.0668]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.3110],\n",
      "        [-0.4406],\n",
      "        [ 0.5966],\n",
      "        [ 0.3528]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3441,  0.1971, -0.9103, -1.0184,  0.7462],\n",
      "        [-0.3092,  1.8964, -0.7609, -0.6632, -0.5630],\n",
      "        [-0.7560, -0.2033,  0.7020,  1.7445, -2.1394],\n",
      "        [ 1.3995, -0.7215, -1.0309,  2.6257,  0.8344]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7130, -0.4795, -0.4113, -0.3021,  1.0293],\n",
      "        [ 1.0449,  1.6020,  1.1335,  0.8600,  2.5865],\n",
      "        [-0.5568, -0.3123, -0.5031, -0.2355, -0.6630],\n",
      "        [ 0.7475,  0.0736,  0.1733,  0.3169, -0.1708]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3441,  0.1971, -0.9103, -1.0184,  0.7462],\n",
      "        [-0.3092,  1.8964, -0.7609, -0.6632, -0.5630],\n",
      "        [-0.7560, -0.2033,  0.7020,  1.7445, -2.1394],\n",
      "        [ 1.3995, -0.7215, -1.0309,  2.6257,  0.8344]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3972],\n",
      "        [-0.1741],\n",
      "        [ 1.1389],\n",
      "        [ 1.5041]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2163, -0.1430,  0.2595,  0.8495,  0.0879],\n",
      "        [ 1.0596, -0.7127,  1.1917, -0.2011,  1.0077],\n",
      "        [-0.6676,  0.5093, -0.5014, -1.5856, -0.3295],\n",
      "        [ 0.3998,  0.7102, -1.9570, -0.5629,  0.4157]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2280, -0.1866,  0.5939,  0.2709, -0.8266],\n",
      "        [ 0.1997,  0.0134, -0.2401,  0.0748,  0.1371],\n",
      "        [-0.9410,  0.3956, -0.7325, -0.4537, -1.6036],\n",
      "        [-0.3511, -1.1726,  0.0642, -0.4784, -0.7776]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2163, -0.1430,  0.2595,  0.8495,  0.0879],\n",
      "        [ 1.0596, -0.7127,  1.1917, -0.2011,  1.0077],\n",
      "        [-0.6676,  0.5093, -0.5014, -1.5856, -0.3295],\n",
      "        [ 0.3998,  0.7102, -1.9570, -0.5629,  0.4157]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3876],\n",
      "        [ 0.0390],\n",
      "        [ 2.4446],\n",
      "        [-1.1528]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4710,  0.5268, -0.2025,  0.8879,  0.2067],\n",
      "        [ 0.3515,  0.7244,  0.4068,  0.1949, -0.0226],\n",
      "        [-0.6847,  0.8432, -0.7927,  1.1229, -1.2174],\n",
      "        [-0.9710, -0.6390,  1.6545,  1.4759, -0.0765]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4127, -0.2674, -0.5824, -0.8715, -0.5192],\n",
      "        [ 0.2994, -0.5371, -0.4900,  0.0328, -0.2960],\n",
      "        [-1.0003, -1.0220, -1.2285, -1.7042, -2.9508],\n",
      "        [-0.2907, -0.1812, -0.1979, -0.0537, -0.3459]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4710,  0.5268, -0.2025,  0.8879,  0.2067],\n",
      "        [ 0.3515,  0.7244,  0.4068,  0.1949, -0.0226],\n",
      "        [-0.6847,  0.8432, -0.7927,  1.1229, -1.2174],\n",
      "        [-0.9710, -0.6390,  1.6545,  1.4759, -0.0765]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0984],\n",
      "        [-0.4701],\n",
      "        [ 2.4755],\n",
      "        [ 0.0178]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2220,  0.2750,  0.2119,  0.3918,  1.7448],\n",
      "        [ 0.9519, -1.2589,  1.3716, -0.1051, -0.1408],\n",
      "        [ 0.7342, -0.1734, -0.7752,  1.9899,  0.3748],\n",
      "        [-0.6298, -0.3864,  1.0853, -1.4596,  0.2245]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1310,  0.7575, -0.8331, -0.2201,  0.0068],\n",
      "        [ 0.1566, -0.9380,  0.1016, -0.0664, -0.0288],\n",
      "        [ 0.0550, -0.0954, -0.2098, -0.3408, -0.0846],\n",
      "        [ 0.5019, -0.5089, -0.3204, -0.3710, -0.1955]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2220,  0.2750,  0.2119,  0.3918,  1.7448],\n",
      "        [ 0.9519, -1.2589,  1.3716, -0.1051, -0.1408],\n",
      "        [ 0.7342, -0.1734, -0.7752,  1.9899,  0.3748],\n",
      "        [-0.6298, -0.3864,  1.0853, -1.4596,  0.2245]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2084],\n",
      "        [ 1.4802],\n",
      "        [-0.4903],\n",
      "        [ 0.0303]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1886, -1.3527,  0.0312,  1.5048,  0.3790],\n",
      "        [ 0.3258, -0.6021, -0.6889,  0.2912, -0.1265],\n",
      "        [-0.7115, -0.3786,  0.6188,  0.4510,  0.6968],\n",
      "        [ 0.6089, -0.9653,  0.7696, -0.1183,  0.7864]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3638, -0.6425, -0.8125, -1.0193, -0.7903],\n",
      "        [-0.3028, -0.8206, -0.5357, -0.7311, -1.9687],\n",
      "        [ 0.5713, -0.1017, -0.1955, -0.2898, -0.1557],\n",
      "        [ 0.2886, -0.1517, -0.6706, -0.7300, -0.1853]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1886, -1.3527,  0.0312,  1.5048,  0.3790],\n",
      "        [ 0.3258, -0.6021, -0.6889,  0.2912, -0.1265],\n",
      "        [-0.7115, -0.3786,  0.6188,  0.4510,  0.6968],\n",
      "        [ 0.6089, -0.9653,  0.7696, -0.1183,  0.7864]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0582],\n",
      "        [ 0.8007],\n",
      "        [-0.7282],\n",
      "        [-0.2533]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5258,  0.5799, -0.1615,  0.5036,  0.5625],\n",
      "        [-0.7224, -1.4147,  2.4874,  0.5268, -0.8668],\n",
      "        [ 0.8119,  0.6341,  0.4285, -0.0882,  1.3249],\n",
      "        [-1.6425,  0.0792, -0.6400,  0.0751, -0.7807]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7946,  0.0005, -0.4248,  0.1031,  0.5020],\n",
      "        [-0.4535, -0.2917, -0.0617, -1.3320, -1.6149],\n",
      "        [ 0.4257, -0.2978, -0.7098, -0.2695, -0.2054],\n",
      "        [ 0.2044, -1.0267,  0.2461, -0.4799, -0.2325]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5258,  0.5799, -0.1615,  0.5036,  0.5625],\n",
      "        [-0.7224, -1.4147,  2.4874,  0.5268, -0.8668],\n",
      "        [ 0.8119,  0.6341,  0.4285, -0.0882,  1.3249],\n",
      "        [-1.6425,  0.0792, -0.6400,  0.0751, -0.7807]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8210],\n",
      "        [ 1.2849],\n",
      "        [-0.3957],\n",
      "        [-0.4290]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4162, -1.6849, -1.1224, -0.2586, -0.1519],\n",
      "        [ 0.5888,  1.1781,  0.7213, -1.2546, -1.3175],\n",
      "        [ 0.2397,  0.2321,  1.2971,  1.0815, -1.1477],\n",
      "        [-1.0596,  0.8409,  2.1951, -0.5518,  0.1870]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1649, -0.2261, -0.4766,  0.2831, -0.4188],\n",
      "        [-0.5295, -0.9149,  0.0331, -1.3461, -2.6092],\n",
      "        [ 0.4232, -0.6686, -0.1811, -0.6639, -0.0734],\n",
      "        [ 0.2237, -0.0950, -0.0802, -0.4266, -0.4739]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4162, -1.6849, -1.1224, -0.2586, -0.1519],\n",
      "        [ 0.5888,  1.1781,  0.7213, -1.2546, -1.3175],\n",
      "        [ 0.2397,  0.2321,  1.2971,  1.0815, -1.1477],\n",
      "        [-1.0596,  0.8409,  2.1951, -0.5518,  0.1870]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8378],\n",
      "        [ 3.7608],\n",
      "        [-0.9224],\n",
      "        [-0.3461]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8499, -1.6178, -0.3649,  0.5139,  0.2324],\n",
      "        [-0.2949, -0.4188, -1.0005,  0.8663,  0.0173],\n",
      "        [ 1.6308,  0.7447,  0.7525, -0.4559, -0.1487],\n",
      "        [ 1.8546,  1.8191,  1.6947, -0.3638,  0.3147]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5304, -0.4507, -0.4425, -0.2937, -0.5479],\n",
      "        [ 0.1919,  0.1183,  1.1272, -0.1881,  0.3352],\n",
      "        [ 1.0296, -0.5475, -0.2565, -0.6239,  0.0713],\n",
      "        [ 0.1805, -0.6791, -0.0454, -0.0961, -0.1033]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8499, -1.6178, -0.3649,  0.5139,  0.2324],\n",
      "        [-0.2949, -0.4188, -1.0005,  0.8663,  0.0173],\n",
      "        [ 1.6308,  0.7447,  0.7525, -0.4559, -0.1487],\n",
      "        [ 1.8546,  1.8191,  1.6947, -0.3638,  0.3147]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5935],\n",
      "        [-1.3911],\n",
      "        [ 1.3522],\n",
      "        [-0.9751]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0116,  0.2629, -1.9897,  1.3195,  0.8086],\n",
      "        [-1.6883,  1.6364, -0.0012, -1.9530, -0.1687],\n",
      "        [-0.0604,  0.9121,  0.1450,  1.2516,  2.3416],\n",
      "        [ 0.2869,  0.3756,  0.7308,  0.4692, -1.0102]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8475, -1.1201, -1.3597, -0.7088, -1.6746],\n",
      "        [ 0.6440,  0.7778,  0.0952,  0.6021,  1.1124],\n",
      "        [-0.1603, -0.6164, -0.4137, -0.3392, -1.2171],\n",
      "        [ 0.1462,  0.3726, -0.0273,  0.0608,  0.6031]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0116,  0.2629, -1.9897,  1.3195,  0.8086],\n",
      "        [-1.6883,  1.6364, -0.0012, -1.9530, -0.1687],\n",
      "        [-0.0604,  0.9121,  0.1450,  1.2516,  2.3416],\n",
      "        [ 0.2869,  0.3756,  0.7308,  0.4692, -1.0102]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1116],\n",
      "        [-1.1780],\n",
      "        [-3.8872],\n",
      "        [-0.4187]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0979, -2.2476,  1.8893, -0.7187,  0.2277],\n",
      "        [ 0.2792, -0.0452,  1.0775,  0.8330,  1.9796],\n",
      "        [-1.4897,  0.7941,  1.4547,  0.5413, -0.1552],\n",
      "        [ 1.4371,  0.4239,  1.4152,  0.0739,  3.2734]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4312, -0.8011, -0.5272, -1.4514, -1.7257],\n",
      "        [ 0.7278,  1.2032,  0.9032,  0.7542,  1.3601],\n",
      "        [ 0.7783,  1.9354,  0.8001,  1.8018,  1.8163],\n",
      "        [ 0.3960, -1.0464, -0.6730, -0.9644, -0.9322]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0979, -2.2476,  1.8893, -0.7187,  0.2277],\n",
      "        [ 0.2792, -0.0452,  1.0775,  0.8330,  1.9796],\n",
      "        [-1.4897,  0.7941,  1.4547,  0.5413, -0.1552],\n",
      "        [ 1.4371,  0.4239,  1.4152,  0.0739,  3.2734]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.9281],\n",
      "        [ 4.4428],\n",
      "        [ 2.2345],\n",
      "        [-3.9497]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0719,  2.5276, -0.4421, -0.8898,  0.5979],\n",
      "        [ 1.5148,  1.0733,  1.8434,  0.0282,  1.1027],\n",
      "        [-0.1574, -0.9105, -0.7295,  2.1601,  1.0267],\n",
      "        [ 0.3339, -0.3551, -2.3824, -0.2046,  1.2547]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2227, -1.3600, -1.4831, -1.5354, -2.7629],\n",
      "        [-0.9046, -1.1307, -0.8029, -1.2875, -2.2515],\n",
      "        [ 0.6256,  0.8170,  0.9538,  0.0898,  0.5234],\n",
      "        [ 1.3217,  1.5143,  1.4166,  0.7437,  2.3767]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0719,  2.5276, -0.4421, -0.8898,  0.5979],\n",
      "        [ 1.5148,  1.0733,  1.8434,  0.0282,  1.1027],\n",
      "        [-0.1574, -0.9105, -0.7295,  2.1601,  1.0267],\n",
      "        [ 0.3339, -0.3551, -2.3824, -0.2046,  1.2547]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.1557],\n",
      "        [-6.5828],\n",
      "        [-0.8069],\n",
      "        [-0.6414]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2044,  0.2591,  0.3087,  1.1193, -0.6258],\n",
      "        [ 0.1165, -0.2315,  0.2625,  0.1579, -0.4950],\n",
      "        [-0.1443, -0.2289,  0.3582,  1.8548,  0.9243],\n",
      "        [ 1.7719, -0.2472,  1.1004, -0.5797, -1.9737]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6693, -0.7567, -0.7414, -0.4026, -0.3276],\n",
      "        [ 1.1863,  1.7379,  2.3533,  1.1109,  1.7818],\n",
      "        [ 0.7437,  0.7464,  1.0467,  0.7387,  0.7454],\n",
      "        [ 1.2662,  2.2253,  1.6131,  1.5330,  1.8918]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2044,  0.2591,  0.3087,  1.1193, -0.6258],\n",
      "        [ 0.1165, -0.2315,  0.2625,  0.1579, -0.4950],\n",
      "        [-0.1443, -0.2289,  0.3582,  1.8548,  0.9243],\n",
      "        [ 1.7719, -0.2472,  1.1004, -0.5797, -1.9737]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5338],\n",
      "        [-0.3530],\n",
      "        [ 2.1559],\n",
      "        [-1.1543]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5965, -0.4257, -1.0115, -0.6474,  0.0695],\n",
      "        [-1.2224,  2.0735,  0.2936,  0.1004,  0.2616],\n",
      "        [-1.9482, -0.5106, -0.1577,  0.8122,  0.2926],\n",
      "        [ 0.3937,  2.3820,  0.3123, -0.4207,  1.2939]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5494, -0.8557, -1.7509, -0.9143, -0.9967],\n",
      "        [ 1.3203,  2.0554,  2.3027,  1.6602,  2.3878],\n",
      "        [-0.3949, -0.7742, -0.5221, -0.6513, -1.1121],\n",
      "        [-0.0101, -0.2436, -0.5409, -0.3619, -0.1024]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5965, -0.4257, -1.0115, -0.6474,  0.0695],\n",
      "        [-1.2224,  2.0735,  0.2936,  0.1004,  0.2616],\n",
      "        [-1.9482, -0.5106, -0.1577,  0.8122,  0.2926],\n",
      "        [ 0.3937,  2.3820,  0.3123, -0.4207,  1.2939]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.9857],\n",
      "        [ 4.1154],\n",
      "        [ 0.3927],\n",
      "        [-0.7333]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3974,  0.5896,  2.5442, -1.7180,  0.4794],\n",
      "        [ 0.3186,  1.0765,  0.4502, -1.1868,  0.8980],\n",
      "        [-0.8102, -0.2986, -0.6024, -1.3370,  0.2638],\n",
      "        [ 0.8513,  1.0677, -0.3075,  0.5440, -0.0437]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4009, -1.5987, -1.8251, -1.5792, -2.7281],\n",
      "        [-0.1589,  0.7422,  0.1164, -0.7715, -0.1797],\n",
      "        [-0.1616, -0.6239,  0.6188, -0.3716, -1.2641],\n",
      "        [ 0.3485,  0.0317,  0.9647,  0.7240, -0.1038]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3974,  0.5896,  2.5442, -1.7180,  0.4794],\n",
      "        [ 0.3186,  1.0765,  0.4502, -1.1868,  0.8980],\n",
      "        [-0.8102, -0.2986, -0.6024, -1.3370,  0.2638],\n",
      "        [ 0.8513,  1.0677, -0.3075,  0.5440, -0.0437]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.3401],\n",
      "        [ 1.5550],\n",
      "        [ 0.1077],\n",
      "        [ 0.4322]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7403,  1.7000,  0.7457,  0.0577, -1.6708],\n",
      "        [ 0.4637, -0.7352, -0.6867, -0.2854,  0.6920],\n",
      "        [-0.6893, -1.8746,  0.1320, -0.0891,  0.2146],\n",
      "        [ 0.4145, -1.3066, -0.0803,  0.9299,  1.5516]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7379, -0.2759, -1.0469, -0.5608, -0.2038],\n",
      "        [-0.6467, -0.2796, -0.1657, -0.3556, -1.3437],\n",
      "        [ 0.1315,  0.2309, -0.0169, -0.4667,  0.8020],\n",
      "        [ 0.4286,  0.4048, -0.2695,  0.0114, -0.4721]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7403,  1.7000,  0.7457,  0.0577, -1.6708],\n",
      "        [ 0.4637, -0.7352, -0.6867, -0.2854,  0.6920],\n",
      "        [-0.6893, -1.8746,  0.1320, -0.0891,  0.2146],\n",
      "        [ 0.4145, -1.3066, -0.0803,  0.9299,  1.5516]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3952],\n",
      "        [-0.8089],\n",
      "        [-0.3119],\n",
      "        [-1.0515]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1761, -1.2272, -2.3422, -0.7381,  1.0144],\n",
      "        [-0.2326, -0.0489,  0.6375,  1.6868,  0.5531],\n",
      "        [ 0.0189,  0.3893,  0.4350,  0.2869,  0.2553],\n",
      "        [ 0.7885,  0.5128, -1.1007,  0.8785, -0.5997]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7584, -0.7497, -1.0575, -1.0780, -0.5176],\n",
      "        [ 0.0676, -0.3428, -0.9380,  0.1000, -0.3958],\n",
      "        [-0.1049,  0.2441,  0.2629,  0.0425,  0.7298],\n",
      "        [ 0.8440,  0.0538, -0.2718,  0.0905,  0.9035]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1761, -1.2272, -2.3422, -0.7381,  1.0144],\n",
      "        [-0.2326, -0.0489,  0.6375,  1.6868,  0.5531],\n",
      "        [ 0.0189,  0.3893,  0.4350,  0.2869,  0.2553],\n",
      "        [ 0.7885,  0.5128, -1.1007,  0.8785, -0.5997]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0170],\n",
      "        [-0.6472],\n",
      "        [ 0.4060],\n",
      "        [ 0.5299]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2111, -2.4229, -1.2645, -0.2802, -0.5284],\n",
      "        [ 0.1329, -1.0134,  0.5634, -1.5184,  0.2893],\n",
      "        [ 2.3153, -0.2487, -0.5930,  2.4981,  1.9036],\n",
      "        [-0.4977, -0.4524, -1.7189,  0.3026,  2.2752]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5701, -1.5949, -2.1404, -2.2078, -1.8765],\n",
      "        [-0.5263, -0.0447, -0.0593, -0.0540, -0.2592],\n",
      "        [-0.1505,  0.3438,  0.6688,  0.6263,  1.0614],\n",
      "        [ 0.8236, -0.1710, -0.0099, -0.0243, -0.0468]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2111, -2.4229, -1.2645, -0.2802, -0.5284],\n",
      "        [ 0.1329, -1.0134,  0.5634, -1.5184,  0.2893],\n",
      "        [ 2.3153, -0.2487, -0.5930,  2.4981,  1.9036],\n",
      "        [-0.4977, -0.4524, -1.7189,  0.3026,  2.2752]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 8.0606],\n",
      "        [-0.0510],\n",
      "        [ 2.7546],\n",
      "        [-0.4294]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8129, -0.4789,  0.6168,  0.1299, -1.7720],\n",
      "        [ 0.2269,  0.4922,  1.5009, -0.3871, -0.0073],\n",
      "        [-0.2902, -0.8610,  1.0803,  1.4213, -0.2793],\n",
      "        [-0.8594, -0.1115, -0.1783,  1.1472, -0.4101]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3283,  0.1776, -0.3057,  0.0494,  0.0524],\n",
      "        [ 0.4016, -0.2374, -0.0194, -0.3443, -0.5960],\n",
      "        [-0.7696, -0.6888, -1.3155, -1.3322, -2.4271],\n",
      "        [ 0.5793, -0.2660,  0.4725, -0.0073,  0.4312]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8129, -0.4789,  0.6168,  0.1299, -1.7720],\n",
      "        [ 0.2269,  0.4922,  1.5009, -0.3871, -0.0073],\n",
      "        [-0.2902, -0.8610,  1.0803,  1.4213, -0.2793],\n",
      "        [-0.8594, -0.1115, -0.1783,  1.1472, -0.4101]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0933],\n",
      "        [ 0.0828],\n",
      "        [-1.8204],\n",
      "        [-0.7376]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2443,  0.1733,  0.4495, -0.0250,  0.0099],\n",
      "        [-1.2497,  2.3701, -0.0714,  0.0110,  1.0283],\n",
      "        [-0.3823, -0.5127,  1.4079, -0.7088,  1.5118],\n",
      "        [ 0.2179,  0.3284, -0.3720,  0.5012,  0.0620]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5413,  0.0693,  0.0591, -0.7016, -0.5370],\n",
      "        [ 0.3667,  0.4658, -0.9368,  0.3947,  0.0046],\n",
      "        [ 0.1113, -0.2063,  0.5207,  0.1796, -0.2191],\n",
      "        [ 0.3100, -0.3167,  0.3400,  0.4072,  0.8589]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2443,  0.1733,  0.4495, -0.0250,  0.0099],\n",
      "        [-1.2497,  2.3701, -0.0714,  0.0110,  1.0283],\n",
      "        [-0.3823, -0.5127,  1.4079, -0.7088,  1.5118],\n",
      "        [ 0.2179,  0.3284, -0.3720,  0.5012,  0.0620]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1831],\n",
      "        [ 0.7217],\n",
      "        [ 0.3377],\n",
      "        [ 0.0945]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0143,  0.6248,  0.7453,  0.2516,  0.0446],\n",
      "        [ 0.8734,  0.8484, -0.7847, -0.1521,  1.2007],\n",
      "        [-0.8359, -1.2449,  1.0412,  0.3806, -0.5182],\n",
      "        [-0.2361, -1.6555,  0.1252, -0.8960,  1.8192]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2655, -0.0657,  0.4259, -0.1372,  0.0266],\n",
      "        [-0.1816,  0.2602, -0.2434, -0.0881, -0.3745],\n",
      "        [ 0.2545, -0.3943,  0.0369,  0.1768, -0.0013],\n",
      "        [ 0.5578,  0.3279, -0.3615,  0.5389,  0.3465]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0143,  0.6248,  0.7453,  0.2516,  0.0446],\n",
      "        [ 0.8734,  0.8484, -0.7847, -0.1521,  1.2007],\n",
      "        [-0.8359, -1.2449,  1.0412,  0.3806, -0.5182],\n",
      "        [-0.2361, -1.6555,  0.1252, -0.8960,  1.8192]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0262],\n",
      "        [-0.1832],\n",
      "        [ 0.3844],\n",
      "        [-0.5723]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5181, -1.4331, -0.5487,  0.4901,  1.9360],\n",
      "        [ 1.2967,  2.2621, -0.4297,  0.5252,  1.0211],\n",
      "        [ 1.4092, -1.8239,  0.8340,  1.0886, -0.1110],\n",
      "        [ 2.1309, -1.0130,  2.3939,  2.2123, -0.3098]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1255, -0.2662, -0.0754,  0.1092, -0.4476],\n",
      "        [ 0.0079, -0.5039,  0.3222, -0.4416,  0.3750],\n",
      "        [-0.3716,  0.7670,  0.1107,  0.1753,  0.1312],\n",
      "        [ 0.1508, -0.6589,  0.6959, -0.2631,  0.4538]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5181, -1.4331, -0.5487,  0.4901,  1.9360],\n",
      "        [ 1.2967,  2.2621, -0.4297,  0.5252,  1.0211],\n",
      "        [ 1.4092, -1.8239,  0.8340,  1.0886, -0.1110],\n",
      "        [ 2.1309, -1.0130,  2.3939,  2.2123, -0.3098]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4552],\n",
      "        [-1.1171],\n",
      "        [-1.6541],\n",
      "        [ 1.9321]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1255,  0.4714, -0.4143, -0.5999,  1.9149],\n",
      "        [-1.5119,  0.1207,  0.6086, -0.9453, -0.3548],\n",
      "        [-1.8561,  0.7534,  0.3902,  1.4494, -0.9734],\n",
      "        [ 0.6844, -1.2189, -1.0835,  1.2680,  1.0063]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1396, -0.2011, -0.0085, -0.3510, -0.6432],\n",
      "        [ 0.2934,  0.1276,  0.0690,  0.4309,  0.5376],\n",
      "        [ 0.7547,  1.0387,  1.4337,  1.0109,  0.9493],\n",
      "        [-1.1136, -0.7700, -0.6764, -1.1682, -1.5159]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1255,  0.4714, -0.4143, -0.5999,  1.9149],\n",
      "        [-1.5119,  0.1207,  0.6086, -0.9453, -0.3548],\n",
      "        [-1.8561,  0.7534,  0.3902,  1.4494, -0.9734],\n",
      "        [ 0.6844, -1.2189, -1.0835,  1.2680,  1.0063]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0949],\n",
      "        [-0.9842],\n",
      "        [ 0.4821],\n",
      "        [-2.0975]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6558, -1.2844,  1.8654, -0.4098,  2.9519],\n",
      "        [ 0.4460, -0.5139, -0.0841,  0.5449, -0.7093],\n",
      "        [-1.1069,  0.4193,  1.8034, -0.1132,  0.8493],\n",
      "        [-1.7368,  0.3659,  0.8395,  1.2857, -0.7487]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1101,  0.4324,  0.7029,  0.5901,  0.9172],\n",
      "        [ 0.8558,  0.5196,  1.1058,  1.0043,  0.4447],\n",
      "        [ 0.1328,  0.9910,  0.4559,  0.6094,  0.6343],\n",
      "        [ 1.0493,  0.0192, -0.5401, -0.2164,  0.0786]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6558, -1.2844,  1.8654, -0.4098,  2.9519],\n",
      "        [ 0.4460, -0.5139, -0.0841,  0.5449, -0.7093],\n",
      "        [-1.1069,  0.4193,  1.8034, -0.1132,  0.8493],\n",
      "        [-1.7368,  0.3659,  0.8395,  1.2857, -0.7487]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4934],\n",
      "        [ 0.2536],\n",
      "        [ 1.5605],\n",
      "        [-2.6059]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.0235, -0.3264,  0.1382,  1.3252,  0.0687],\n",
      "        [-0.6609,  0.0321,  1.2981,  0.6057, -0.3176],\n",
      "        [-0.0331,  1.5136,  0.2312,  2.2791,  1.3753],\n",
      "        [ 0.7010,  0.4833, -0.0338,  0.7059, -0.1715]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6363, -0.6357, -1.4203, -1.1270, -1.6007],\n",
      "        [ 0.4544,  0.1705, -0.2615,  0.3562,  0.3353],\n",
      "        [-0.7391, -0.0985, -0.3970, -0.3921, -0.6896],\n",
      "        [ 1.6800,  0.7997,  1.5981,  1.1784,  0.9048]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.0235, -0.3264,  0.1382,  1.3252,  0.0687],\n",
      "        [-0.6609,  0.0321,  1.2981,  0.6057, -0.3176],\n",
      "        [-0.0331,  1.5136,  0.2312,  2.2791,  1.3753],\n",
      "        [ 0.7010,  0.4833, -0.0338,  0.7059, -0.1715]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8797],\n",
      "        [-0.5250],\n",
      "        [-2.0585],\n",
      "        [ 2.1869]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9965,  0.8967, -0.2165, -0.2536, -0.6563],\n",
      "        [-1.3720,  0.3686,  0.8515,  0.7143,  1.5551],\n",
      "        [-0.9862,  0.4982,  1.5320, -0.8049, -0.0432],\n",
      "        [-0.3658, -0.0005, -0.1801,  0.0592,  0.4548]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.1909,  0.4536,  0.4990,  0.0525,  1.3934],\n",
      "        [ 0.1084, -0.5400, -0.6392, -0.6195, -0.2431],\n",
      "        [ 0.0467,  0.7843,  1.9884,  0.4812,  0.9449],\n",
      "        [ 0.2887,  0.0958,  0.4956,  0.4582,  0.3358]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9965,  0.8967, -0.2165, -0.2536, -0.6563],\n",
      "        [-1.3720,  0.3686,  0.8515,  0.7143,  1.5551],\n",
      "        [-0.9862,  0.4982,  1.5320, -0.8049, -0.0432],\n",
      "        [-0.3658, -0.0005, -0.1801,  0.0592,  0.4548]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8159],\n",
      "        [-1.7126],\n",
      "        [ 2.9628],\n",
      "        [-0.0150]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.2541, -1.1740,  1.5487,  0.2389,  0.7183],\n",
      "        [ 0.4291,  0.9561,  0.1631,  0.2870, -0.0973],\n",
      "        [ 1.8712,  0.0450,  0.0285,  0.0989,  0.7192],\n",
      "        [ 0.9386, -0.3806,  1.8722, -0.4059, -0.2939]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8616,  0.5255,  0.8195,  0.8081,  0.8144],\n",
      "        [ 0.4457,  0.5101,  0.3562,  0.2544,  1.4075],\n",
      "        [-0.5853,  0.0492, -0.8678, -1.0834, -1.2582],\n",
      "        [ 0.2653,  0.7646, -0.3561,  0.2906, -0.0991]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.2541, -1.1740,  1.5487,  0.2389,  0.7183],\n",
      "        [ 0.4291,  0.9561,  0.1631,  0.2870, -0.0973],\n",
      "        [ 1.8712,  0.0450,  0.0285,  0.0989,  0.7192],\n",
      "        [ 0.9386, -0.3806,  1.8722, -0.4059, -0.2939]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.3723],\n",
      "        [ 0.6731],\n",
      "        [-2.1299],\n",
      "        [-0.7974]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0281,  0.3683,  0.1572, -0.0569,  0.8689],\n",
      "        [-1.4312, -0.8802, -0.7464, -0.0334, -0.4623],\n",
      "        [ 0.0485,  0.5647,  1.1621, -0.7160,  1.8801],\n",
      "        [-0.8133, -0.2990,  0.3705,  0.8452,  0.4502]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6721, -0.6517, -0.3653, -0.4079, -0.9010],\n",
      "        [ 0.4318,  0.2122,  0.2381, -0.4239,  0.4722],\n",
      "        [ 0.1303,  0.8270,  0.3593,  0.1208,  0.4358],\n",
      "        [ 0.9806,  0.1528,  0.8024,  0.4804,  0.4517]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0281,  0.3683,  0.1572, -0.0569,  0.8689],\n",
      "        [-1.4312, -0.8802, -0.7464, -0.0334, -0.4623],\n",
      "        [ 0.0485,  0.5647,  1.1621, -0.7160,  1.8801],\n",
      "        [-0.8133, -0.2990,  0.3705,  0.8452,  0.4502]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0759],\n",
      "        [-1.1865],\n",
      "        [ 1.6238],\n",
      "        [ 0.0634]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4924,  2.4646, -0.6982,  1.2291, -0.4065],\n",
      "        [ 0.5502,  0.9060,  1.9765,  0.3890,  0.3542],\n",
      "        [ 0.1401,  0.0445,  2.9172,  1.1421, -1.3597],\n",
      "        [-1.2910, -1.6128, -0.4023,  0.6645,  0.1502]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5406, -0.5573,  0.7421, -0.1521,  0.2255],\n",
      "        [ 1.1571,  0.2214,  0.8832,  0.3034,  0.5318],\n",
      "        [ 0.4343, -0.5121, -0.3354, -0.7222, -1.2458],\n",
      "        [ 0.1506,  0.2535,  0.6537,  0.7387,  0.6567]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4924,  2.4646, -0.6982,  1.2291, -0.4065],\n",
      "        [ 0.5502,  0.9060,  1.9765,  0.3890,  0.3542],\n",
      "        [ 0.1401,  0.0445,  2.9172,  1.1421, -1.3597],\n",
      "        [-1.2910, -1.6128, -0.4023,  0.6645,  0.1502]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9041],\n",
      "        [ 2.8893],\n",
      "        [-0.0713],\n",
      "        [-0.2768]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5912, -1.0597,  0.6353,  1.6714,  1.2699],\n",
      "        [-1.2936,  0.8802,  0.7196,  1.0243, -0.5534],\n",
      "        [ 1.3597, -0.7028,  1.9663,  1.4712, -1.0415],\n",
      "        [-2.5838,  0.9201,  0.0569, -0.9237, -1.0411]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6673,  1.8195,  0.6590,  1.0415,  1.2388],\n",
      "        [-0.2118, -0.7551, -0.2956, -1.0019, -1.5795],\n",
      "        [-0.0921, -0.0792, -0.1176, -0.2389, -0.2328],\n",
      "        [ 0.6667, -0.0305,  0.5676,  0.1074,  0.8135]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5912, -1.0597,  0.6353,  1.6714,  1.2699],\n",
      "        [-1.2936,  0.8802,  0.7196,  1.0243, -0.5534],\n",
      "        [ 1.3597, -0.7028,  1.9663,  1.4712, -1.0415],\n",
      "        [-2.5838,  0.9201,  0.0569, -0.9237, -1.0411]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7428],\n",
      "        [-0.7556],\n",
      "        [-0.4099],\n",
      "        [-2.6645]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3729,  0.6341,  0.3793,  0.2974, -0.3945],\n",
      "        [ 0.5778,  0.7180,  2.4124,  0.6453,  1.0686],\n",
      "        [ 0.4365, -0.0947, -0.2098,  0.0689,  0.5802],\n",
      "        [ 1.4784,  0.6193,  0.5071,  2.0777, -0.8458]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7743,  0.3656,  0.7333,  0.7970,  0.6195],\n",
      "        [ 0.2192, -0.8423, -0.0533,  0.0470, -0.6125],\n",
      "        [ 0.0545, -0.0106,  0.5447,  0.6034,  0.4470],\n",
      "        [ 1.3157,  1.0485,  0.8017,  1.2852,  2.6780]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3729,  0.6341,  0.3793,  0.2974, -0.3945],\n",
      "        [ 0.5778,  0.7180,  2.4124,  0.6453,  1.0686],\n",
      "        [ 0.4365, -0.0947, -0.2098,  0.0689,  0.5802],\n",
      "        [ 1.4784,  0.6193,  0.5071,  2.0777, -0.8458]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2138],\n",
      "        [-1.2308],\n",
      "        [ 0.2115],\n",
      "        [ 3.4064]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4312, -1.7061, -0.0033,  0.7280, -0.3111],\n",
      "        [ 0.2902,  0.0008,  1.1642, -0.8374,  1.1356],\n",
      "        [-0.4460,  0.3422, -0.3173, -0.1930, -1.7553],\n",
      "        [-1.7162,  0.1401,  0.1864,  1.6675,  0.6086]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2543,  0.0033,  0.5747, -0.4351,  0.1261],\n",
      "        [ 0.2250,  0.4742, -0.4689, -0.3316,  0.2147],\n",
      "        [-0.3467, -0.3808, -0.2006,  0.1934, -0.4098],\n",
      "        [-0.3577, -0.3647,  0.0429, -0.3949, -0.2151]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4312, -1.7061, -0.0033,  0.7280, -0.3111],\n",
      "        [ 0.2902,  0.0008,  1.1642, -0.8374,  1.1356],\n",
      "        [-0.4460,  0.3422, -0.3173, -0.1930, -1.7553],\n",
      "        [-1.7162,  0.1401,  0.1864,  1.6675,  0.6086]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4732],\n",
      "        [ 0.0412],\n",
      "        [ 0.7699],\n",
      "        [-0.2186]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2385, -0.4812,  0.4709, -0.5333,  0.4069],\n",
      "        [-1.0149,  1.0865, -0.7109,  0.1042,  1.3407],\n",
      "        [ 1.0150, -0.6255, -0.5972, -0.9366,  0.4103],\n",
      "        [-0.8448, -0.3784,  0.9943, -0.4362, -0.1540]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6363,  0.1677, -0.0705, -0.0545, -0.2895],\n",
      "        [ 0.1433, -0.3877, -0.6386, -0.3446,  0.4015],\n",
      "        [ 0.0266, -0.4478, -0.5462, -0.0143, -0.7084],\n",
      "        [ 0.2283,  0.1974,  0.5402,  0.9665,  0.4683]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2385, -0.4812,  0.4709, -0.5333,  0.4069],\n",
      "        [-1.0149,  1.0865, -0.7109,  0.1042,  1.3407],\n",
      "        [ 1.0150, -0.6255, -0.5972, -0.9366,  0.4103],\n",
      "        [-0.8448, -0.3784,  0.9943, -0.4362, -0.1540]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3544],\n",
      "        [ 0.3897],\n",
      "        [ 0.3560],\n",
      "        [-0.2241]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5903,  1.7528, -0.1661, -0.6469,  1.5893],\n",
      "        [ 1.2682,  0.1385, -0.0863,  0.6949, -0.7895],\n",
      "        [ 1.2282,  2.1725,  0.4162, -0.0874,  1.1982],\n",
      "        [-0.2606, -0.5163, -0.7447, -1.6007,  1.6325]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2034, -0.3493,  0.0773, -0.1765,  0.2261],\n",
      "        [-0.1450, -0.0026, -0.0016, -1.1802, -0.7786],\n",
      "        [-0.1410, -0.0153,  0.2137,  0.0664, -0.4341],\n",
      "        [ 0.5285,  0.5319,  0.4368,  0.0792,  1.3341]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5903,  1.7528, -0.1661, -0.6469,  1.5893],\n",
      "        [ 1.2682,  0.1385, -0.0863,  0.6949, -0.7895],\n",
      "        [ 1.2282,  2.1725,  0.4162, -0.0874,  1.1982],\n",
      "        [-0.2606, -0.5163, -0.7447, -1.6007,  1.6325]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2716],\n",
      "        [-0.3895],\n",
      "        [-0.6434],\n",
      "        [ 1.3134]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1575, -0.4058, -0.0110,  1.9638, -0.3191],\n",
      "        [ 1.8541, -1.3280, -0.7935,  0.3344, -0.5280],\n",
      "        [-0.2613,  1.4371,  0.0628, -0.0329, -0.3959],\n",
      "        [ 1.0201,  1.1922, -0.5470,  1.1691, -0.4746]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1280, -0.0761,  0.4807, -0.1620, -0.0201],\n",
      "        [ 0.2535,  0.1978,  0.2146,  0.2419, -0.0336],\n",
      "        [ 0.4285, -0.2678, -0.5633,  0.5591,  0.0577],\n",
      "        [ 0.2994,  0.2763,  0.1652,  0.3161,  0.1838]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1575, -0.4058, -0.0110,  1.9638, -0.3191],\n",
      "        [ 1.8541, -1.3280, -0.7935,  0.3344, -0.5280],\n",
      "        [-0.2613,  1.4371,  0.0628, -0.0329, -0.3959],\n",
      "        [ 1.0201,  1.1922, -0.5470,  1.1691, -0.4746]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4342],\n",
      "        [ 0.1358],\n",
      "        [-0.5735],\n",
      "        [ 0.8268]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0889, -0.6665,  1.1421, -0.7640,  1.1736],\n",
      "        [ 0.5481, -0.8720, -0.3016, -0.1472, -0.0738],\n",
      "        [ 0.0793,  0.6464, -0.3078,  0.5781, -1.3176],\n",
      "        [ 0.7815, -0.0371, -2.5043, -0.2264, -0.6427]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4705,  0.0353,  0.3582, -0.3847,  0.0078],\n",
      "        [ 0.5365, -0.3015,  0.1479, -0.4478,  0.5618],\n",
      "        [ 0.5473,  0.6146, -0.2089,  0.0269,  0.6393],\n",
      "        [-0.3188,  0.7367,  0.7664,  0.2543,  0.0876]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0889, -0.6665,  1.1421, -0.7640,  1.1736],\n",
      "        [ 0.5481, -0.8720, -0.3016, -0.1472, -0.0738],\n",
      "        [ 0.0793,  0.6464, -0.3078,  0.5781, -1.3176],\n",
      "        [ 0.7815, -0.0371, -2.5043, -0.2264, -0.6427]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2010],\n",
      "        [ 0.5368],\n",
      "        [-0.3218],\n",
      "        [-2.3097]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6824, -1.3956,  0.9629,  0.2369,  1.2749],\n",
      "        [-1.4791, -0.3198, -0.2320,  0.9186,  0.5459],\n",
      "        [-1.1418, -0.0367,  0.0642, -1.1475,  0.4159],\n",
      "        [-0.4260, -1.8399, -0.2043,  1.2920,  0.4516]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1359, -0.1911, -0.8031, -0.5189, -1.0812],\n",
      "        [ 0.4354, -0.1932,  0.1195, -0.4932, -0.2472],\n",
      "        [ 0.2024, -0.4628, -0.3313,  0.0355, -0.3608],\n",
      "        [ 0.6667,  1.3904,  1.5960,  1.6867,  2.0734]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6824, -1.3956,  0.9629,  0.2369,  1.2749],\n",
      "        [-1.4791, -0.3198, -0.2320,  0.9186,  0.5459],\n",
      "        [-1.1418, -0.0367,  0.0642, -1.1475,  0.4159],\n",
      "        [-0.4260, -1.8399, -0.2043,  1.2920,  0.4516]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1007],\n",
      "        [-1.1979],\n",
      "        [-0.4261],\n",
      "        [-0.0526]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9994,  1.0552,  0.4573,  2.2505, -0.4437],\n",
      "        [-1.9931, -0.1771,  0.9086,  3.1037, -0.5037],\n",
      "        [-0.3747,  1.2644,  0.5580, -0.5895,  0.0464],\n",
      "        [-0.3382,  0.7966,  0.1130,  1.1028, -1.2258]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2606,  0.9006, -0.3701,  0.6159,  1.4267],\n",
      "        [ 0.6080,  0.3457,  0.8746, -0.2155,  0.4014],\n",
      "        [ 0.5127, -0.4485, -0.2958,  0.0627, -0.1255],\n",
      "        [ 1.0684,  1.1867,  1.2363,  1.2697,  1.6033]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9994,  1.0552,  0.4573,  2.2505, -0.4437],\n",
      "        [-1.9931, -0.1771,  0.9086,  3.1037, -0.5037],\n",
      "        [-0.3747,  1.2644,  0.5580, -0.5895,  0.0464],\n",
      "        [-0.3382,  0.7966,  0.1130,  1.1028, -1.2258]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0551],\n",
      "        [-1.3495],\n",
      "        [-0.9670],\n",
      "        [ 0.1586]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1559,  0.4314,  1.5358,  0.2744,  0.3418],\n",
      "        [-1.3678,  0.6618, -0.9891, -0.7017,  0.5853],\n",
      "        [ 1.4110, -3.0341,  0.0505, -0.7226,  0.7381],\n",
      "        [ 1.3591,  1.0030,  0.6198, -0.6014, -2.0527]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5343, -0.2201, -0.5274,  0.1682, -0.8094],\n",
      "        [ 1.2177,  0.5271,  0.5375,  0.6917,  1.7985],\n",
      "        [ 0.3484, -0.0350, -0.7385,  0.0400,  0.5921],\n",
      "        [ 0.1309,  0.2707, -0.1398,  0.1294,  0.3923]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1559,  0.4314,  1.5358,  0.2744,  0.3418],\n",
      "        [-1.3678,  0.6618, -0.9891, -0.7017,  0.5853],\n",
      "        [ 1.4110, -3.0341,  0.0505, -0.7226,  0.7381],\n",
      "        [ 1.3591,  1.0030,  0.6198, -0.6014, -2.0527]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0521],\n",
      "        [-1.2811],\n",
      "        [ 0.9688],\n",
      "        [-0.5202]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4528, -0.4904, -0.7847,  1.3755,  1.2200],\n",
      "        [-0.5699, -1.2436,  2.1300,  1.8936, -0.3812],\n",
      "        [-1.7495, -1.3297,  0.6154, -1.1453,  0.0033],\n",
      "        [ 0.6354,  1.0804,  1.5754, -0.0171, -0.7690]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1112,  0.4121,  0.0139,  0.1441,  0.1943],\n",
      "        [ 1.7511,  1.0020,  1.4643,  1.1819,  1.4121],\n",
      "        [ 0.2074, -1.0643, -0.9067, -0.6568, -0.2899],\n",
      "        [ 0.3156, -0.1344, -0.3292, -0.1251, -0.0665]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4528, -0.4904, -0.7847,  1.3755,  1.2200],\n",
      "        [-0.5699, -1.2436,  2.1300,  1.8936, -0.3812],\n",
      "        [-1.7495, -1.3297,  0.6154, -1.1453,  0.0033],\n",
      "        [ 0.6354,  1.0804,  1.5754, -0.0171, -0.7690]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1719],\n",
      "        [ 2.5748],\n",
      "        [ 1.2454],\n",
      "        [-0.4101]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4853, -0.0177, -0.3001,  1.4199,  0.4470],\n",
      "        [ 0.3218, -0.0628,  0.9095, -1.0796, -0.2702],\n",
      "        [-0.8388,  1.5407,  1.8945, -0.1567, -0.4349],\n",
      "        [ 1.4072,  0.7578,  0.6885, -1.3706,  0.5659]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3889,  0.6809,  0.4988,  0.4433,  0.1082],\n",
      "        [ 0.6839, -0.1593,  0.6098,  0.6535,  0.6170],\n",
      "        [-0.8373, -0.9357, -0.5813, -1.0243, -1.0496],\n",
      "        [ 0.2048, -0.2987,  0.5787,  0.3869,  0.0234]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4853, -0.0177, -0.3001,  1.4199,  0.4470],\n",
      "        [ 0.3218, -0.0628,  0.9095, -1.0796, -0.2702],\n",
      "        [-0.8388,  1.5407,  1.8945, -0.1567, -0.4349],\n",
      "        [ 1.4072,  0.7578,  0.6885, -1.3706,  0.5659]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3274],\n",
      "        [-0.0875],\n",
      "        [-1.2236],\n",
      "        [-0.0569]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8753,  1.0471, -1.1284, -1.6190, -0.2687],\n",
      "        [ 0.2702,  0.1649, -1.3913, -0.7139,  0.2896],\n",
      "        [-1.9677, -2.3877, -1.6522,  0.7070, -0.1961],\n",
      "        [ 0.9491,  1.4561,  2.7583, -0.6310,  1.2444]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5657, -0.6708,  0.2301,  0.1284,  0.5736],\n",
      "        [ 0.4225,  0.9168,  0.1730,  0.4747,  0.6449],\n",
      "        [-0.2404, -0.6667, -0.1503, -0.5178, -0.8002],\n",
      "        [-0.0269, -0.4896, -0.3233, -0.8106,  0.8600]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8753,  1.0471, -1.1284, -1.6190, -0.2687],\n",
      "        [ 0.2702,  0.1649, -1.3913, -0.7139,  0.2896],\n",
      "        [-1.9677, -2.3877, -1.6522,  0.7070, -0.1961],\n",
      "        [ 0.9491,  1.4561,  2.7583, -0.6310,  1.2444]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8288],\n",
      "        [-0.1275],\n",
      "        [ 2.1043],\n",
      "        [-0.0486]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8548, -0.6148,  0.1321,  0.2160, -0.1160],\n",
      "        [ 2.1370, -0.2791,  1.0320, -0.8263, -1.2089],\n",
      "        [-0.2266, -0.2310, -1.6900,  0.7935,  0.7732],\n",
      "        [-1.3450, -0.2179,  0.7006,  0.4240, -0.2070]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7663,  0.7272,  0.4987,  0.6353,  0.6417],\n",
      "        [ 0.2174,  0.7365,  1.1841,  0.8806,  1.2484],\n",
      "        [-0.3110, -0.5463, -1.2060, -1.1339, -1.8030],\n",
      "        [ 0.4285,  0.0046,  0.0383, -0.0140, -0.1152]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8548, -0.6148,  0.1321,  0.2160, -0.1160],\n",
      "        [ 2.1370, -0.2791,  1.0320, -0.8263, -1.2089],\n",
      "        [-0.2266, -0.2310, -1.6900,  0.7935,  0.7732],\n",
      "        [-1.3450, -0.2179,  0.7006,  0.4240, -0.2070]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3367],\n",
      "        [-0.7558],\n",
      "        [-0.0589],\n",
      "        [-0.5325]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9255, -0.8981,  0.0056, -0.5876,  0.9592],\n",
      "        [ 0.0532,  0.2108, -1.1061, -1.2098, -0.1691],\n",
      "        [-0.2407, -1.5763, -0.4709,  0.9578, -1.3802],\n",
      "        [ 0.5657,  1.3985, -1.8520, -1.4943, -0.6947]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3843,  0.3565, -0.0971,  0.5797,  0.2299],\n",
      "        [ 0.4570,  0.8898,  1.0991,  0.9533,  0.8519],\n",
      "        [-0.8217, -1.4543, -0.7301, -1.2234, -1.9684],\n",
      "        [-0.4165, -0.0297, -0.2927,  0.2530,  0.1803]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9255, -0.8981,  0.0056, -0.5876,  0.9592],\n",
      "        [ 0.0532,  0.2108, -1.1061, -1.2098, -0.1691],\n",
      "        [-0.2407, -1.5763, -0.4709,  0.9578, -1.3802],\n",
      "        [ 0.5657,  1.3985, -1.8520, -1.4943, -0.6947]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0852],\n",
      "        [-2.3011],\n",
      "        [ 4.3792],\n",
      "        [-0.2385]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0736, -0.9850, -1.5845, -0.0477,  0.5122],\n",
      "        [-0.6213, -1.0672,  0.3988, -0.8310, -0.0007],\n",
      "        [ 0.0312,  0.4449,  1.6123,  1.0727,  0.3631],\n",
      "        [ 0.1848, -0.9800,  1.2501, -0.6278,  0.8660]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8611,  0.3452,  0.4183, -0.0372,  0.1451],\n",
      "        [ 0.2455,  0.2524, -0.2074, -0.1757, -0.1963],\n",
      "        [ 0.2602,  0.7411, -0.3960, -0.1200, -0.2160],\n",
      "        [ 0.5278,  0.2470, -0.8552, -0.6121, -0.0373]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0736, -0.9850, -1.5845, -0.0477,  0.5122],\n",
      "        [-0.6213, -1.0672,  0.3988, -0.8310, -0.0007],\n",
      "        [ 0.0312,  0.4449,  1.6123,  1.0727,  0.3631],\n",
      "        [ 0.1848, -0.9800,  1.2501, -0.6278,  0.8660]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9900],\n",
      "        [-0.3585],\n",
      "        [-0.5079],\n",
      "        [-0.8617]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5924, -1.4513,  0.0770, -0.7043, -0.4907],\n",
      "        [ 1.0678, -0.2241,  0.4793,  0.7657, -0.7479],\n",
      "        [ 1.7563,  1.5153,  0.1304,  0.1650, -0.8853],\n",
      "        [-0.4307,  1.1712, -1.0521,  0.0255, -0.2811]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7162,  0.3195,  0.8419,  0.3703,  1.4036],\n",
      "        [ 0.2604, -0.4696, -0.1933, -0.4070, -0.1282],\n",
      "        [ 0.2370,  0.2453, -0.0365, -0.3869, -0.5923],\n",
      "        [ 0.4948,  0.1039,  0.3710, -0.1121,  0.2641]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5924, -1.4513,  0.0770, -0.7043, -0.4907],\n",
      "        [ 1.0678, -0.2241,  0.4793,  0.7657, -0.7479],\n",
      "        [ 1.7563,  1.5153,  0.1304,  0.1650, -0.8853],\n",
      "        [-0.4307,  1.1712, -1.0521,  0.0255, -0.2811]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7727],\n",
      "        [ 0.0749],\n",
      "        [ 1.2437],\n",
      "        [-0.5587]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4316,  1.4848,  1.0709, -1.5497, -1.0236],\n",
      "        [-0.5573, -1.9400,  0.2597, -0.5844, -0.1952],\n",
      "        [ 0.2990, -0.1969,  0.4446,  0.9267,  0.5182],\n",
      "        [-1.2934, -1.5847,  0.1357, -1.2986,  0.9880]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7580,  1.5826,  1.8872,  1.0233,  1.3167],\n",
      "        [ 0.0884, -0.0363, -0.4094, -0.4877, -0.2725],\n",
      "        [-0.1910, -0.0281, -0.3807, -0.4422, -1.3927],\n",
      "        [ 0.3754, -0.1809,  0.1384, -0.4130,  0.1436]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4316,  1.4848,  1.0709, -1.5497, -1.0236],\n",
      "        [-0.5573, -1.9400,  0.2597, -0.5844, -0.1952],\n",
      "        [ 0.2990, -0.1969,  0.4446,  0.9267,  0.5182],\n",
      "        [-1.2934, -1.5847,  0.1357, -1.2986,  0.9880]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5226],\n",
      "        [ 0.2530],\n",
      "        [-1.3524],\n",
      "        [ 0.4983]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6480,  0.8176,  0.8903, -1.9012,  0.6529],\n",
      "        [-0.5942,  1.8755, -2.1038, -0.0950,  2.1474],\n",
      "        [-1.1193, -1.6739, -0.4777, -0.2620,  0.5835],\n",
      "        [-0.5008, -0.2477,  1.8927,  1.1595,  0.2145]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3582,  1.0501,  0.4837,  0.8134,  0.3600],\n",
      "        [ 0.1508,  0.0652, -0.4541,  0.0845,  0.2992],\n",
      "        [ 0.6599, -0.0381,  0.5390,  0.4417,  0.0602],\n",
      "        [ 0.5499, -1.0332,  0.1610, -0.1852,  0.1414]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6480,  0.8176,  0.8903, -1.9012,  0.6529],\n",
      "        [-0.5942,  1.8755, -2.1038, -0.0950,  2.1474],\n",
      "        [-1.1193, -1.6739, -0.4777, -0.2620,  0.5835],\n",
      "        [-0.5008, -0.2477,  1.8927,  1.1595,  0.2145]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2100],\n",
      "        [ 1.6224],\n",
      "        [-1.0130],\n",
      "        [ 0.1008]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3209,  0.7421,  0.8027,  0.6912, -0.5463],\n",
      "        [-0.8957, -0.4939, -0.1203, -2.9666, -1.3355],\n",
      "        [ 0.1486,  0.5288,  0.1896,  0.5083, -0.2529],\n",
      "        [ 1.5369,  0.4209, -0.1722,  1.4630,  0.2184]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2989,  0.5357,  0.7937,  0.8187,  1.0438],\n",
      "        [-0.4238, -0.9925, -0.8226, -0.5768, -1.4193],\n",
      "        [ 0.2270,  0.7627,  1.0625,  0.2003,  0.9764],\n",
      "        [ 0.2529, -0.1082, -0.2095, -0.1872, -0.2768]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3209,  0.7421,  0.8027,  0.6912, -0.5463],\n",
      "        [-0.8957, -0.4939, -0.1203, -2.9666, -1.3355],\n",
      "        [ 0.1486,  0.5288,  0.1896,  0.5083, -0.2529],\n",
      "        [ 1.5369,  0.4209, -0.1722,  1.4630,  0.2184]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6134],\n",
      "        [ 4.5754],\n",
      "        [ 0.4934],\n",
      "        [ 0.0447]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6981, -1.2459,  0.4726,  0.2367, -1.5264],\n",
      "        [-0.5103,  0.2007,  0.3961,  0.4986,  0.9904],\n",
      "        [ 1.1225, -0.1197, -0.1455, -0.4288, -0.0654],\n",
      "        [-0.1130, -1.2190,  1.6532,  1.1578,  0.5140]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6865,  0.8569,  0.7650,  0.9333,  0.9301],\n",
      "        [-1.9811, -1.9685, -1.8255, -2.4114, -3.8251],\n",
      "        [ 0.1773, -0.4388, -0.1278, -0.0667, -0.0235],\n",
      "        [ 0.2519, -0.2826,  0.4566,  0.0081, -0.2310]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6981, -1.2459,  0.4726,  0.2367, -1.5264],\n",
      "        [-0.5103,  0.2007,  0.3961,  0.4986,  0.9904],\n",
      "        [ 1.1225, -0.1197, -0.1455, -0.4288, -0.0654],\n",
      "        [-0.1130, -1.2190,  1.6532,  1.1578,  0.5140]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4256],\n",
      "        [-5.0976],\n",
      "        [ 0.3003],\n",
      "        [ 0.9615]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3482, -0.6623, -1.0785,  0.2560, -0.9615],\n",
      "        [ 0.9552,  2.5917,  2.0022,  1.9631, -1.1819],\n",
      "        [-0.4561,  0.4393,  1.2572, -0.1446, -0.6370],\n",
      "        [ 0.3336, -0.4204, -1.4381, -0.2680, -0.0537]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3590,  1.4635,  1.2101,  0.6146,  1.5762],\n",
      "        [-0.6370, -1.3318, -0.2964, -1.6508, -1.4464],\n",
      "        [ 0.1199,  0.0748, -0.1590, -0.0140, -0.3499],\n",
      "        [ 0.1272, -0.8000, -0.4934, -0.8269, -0.4391]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3482, -0.6623, -1.0785,  0.2560, -0.9615],\n",
      "        [ 0.9552,  2.5917,  2.0022,  1.9631, -1.1819],\n",
      "        [-0.4561,  0.4393,  1.2572, -0.1446, -0.6370],\n",
      "        [ 0.3336, -0.4204, -1.4381, -0.2680, -0.0537]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.1593],\n",
      "        [-6.1850],\n",
      "        [ 0.0032],\n",
      "        [ 1.3335]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8426,  0.0531,  0.9767, -0.9257,  0.9112],\n",
      "        [-1.3590,  0.4671,  0.2604,  0.2546, -0.3555],\n",
      "        [-0.1149, -1.4939, -2.9987,  0.8374,  0.6744],\n",
      "        [-1.3582,  1.9249,  1.5626,  0.8968, -0.9131]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1641,  0.0253,  0.0001, -0.5174,  0.1655],\n",
      "        [ 1.9205,  1.5690,  1.6710,  1.2839,  2.3104],\n",
      "        [ 0.3109,  0.2788, -0.5552,  0.1897,  0.5595],\n",
      "        [-0.3525, -1.3915, -0.9497, -0.6312, -0.4590]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8426,  0.0531,  0.9767, -0.9257,  0.9112],\n",
      "        [-1.3590,  0.4671,  0.2604,  0.2546, -0.3555],\n",
      "        [-0.1149, -1.4939, -2.9987,  0.8374,  0.6744],\n",
      "        [-1.3582,  1.9249,  1.5626,  0.8968, -0.9131]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3289],\n",
      "        [-1.9365],\n",
      "        [ 1.7489],\n",
      "        [-3.8309]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0758, -0.1348,  1.0649, -2.4265,  0.9190],\n",
      "        [-0.6463,  0.0414, -0.3636,  0.5694,  1.5828],\n",
      "        [ 1.5590, -1.6614,  0.3466,  0.6349,  0.2716],\n",
      "        [ 0.1843,  0.4441, -0.4407,  0.4072,  0.2390]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2054, -0.2144,  0.2823,  0.3321,  0.0730],\n",
      "        [ 1.8312,  2.1422,  2.8373,  1.6030,  3.4480],\n",
      "        [ 0.0034, -0.4828, -0.9995, -0.6116, -1.5335],\n",
      "        [ 0.8093,  0.7276,  1.8929,  1.6927,  1.1714]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0758, -0.1348,  1.0649, -2.4265,  0.9190],\n",
      "        [-0.6463,  0.0414, -0.3636,  0.5694,  1.5828],\n",
      "        [ 1.5590, -1.6614,  0.3466,  0.6349,  0.2716],\n",
      "        [ 0.1843,  0.4441, -0.4407,  0.4072,  0.2390]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4247],\n",
      "        [ 4.2435],\n",
      "        [-0.3438],\n",
      "        [ 0.6073]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9708,  0.6442,  2.0023,  1.2415, -0.2548],\n",
      "        [ 0.7309, -0.4235,  0.3939,  1.1333, -1.9379],\n",
      "        [-0.1637,  0.9520,  1.8059,  0.3230,  0.9347],\n",
      "        [-0.8693, -0.5503, -1.4244,  0.4052,  1.3795]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1121,  0.2371,  0.2247, -0.7117,  0.1457],\n",
      "        [ 1.0445,  1.4177,  1.1936,  1.4008,  1.3162],\n",
      "        [-0.4906, -0.0669,  0.1020, -0.0120, -0.2973],\n",
      "        [ 0.7631,  0.7457,  0.4448,  1.0176,  1.6390]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9708,  0.6442,  2.0023,  1.2415, -0.2548],\n",
      "        [ 0.7309, -0.4235,  0.3939,  1.1333, -1.9379],\n",
      "        [-0.1637,  0.9520,  1.8059,  0.3230,  0.9347],\n",
      "        [-0.8693, -0.5503, -1.4244,  0.4052,  1.3795]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2093],\n",
      "        [-0.3300],\n",
      "        [-0.0810],\n",
      "        [ 0.9661]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6217,  0.6805, -0.2371, -0.7326,  1.0854],\n",
      "        [ 0.2193,  0.2414,  0.3006,  0.0397, -0.7185],\n",
      "        [-0.9613,  0.8540,  0.0279,  2.5711, -0.5822],\n",
      "        [ 0.5866, -0.5126, -1.1463,  0.2304,  0.6859]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3132, -0.1121, -0.4903,  0.0892,  0.3139],\n",
      "        [-0.1323,  0.4337,  0.2715, -0.2457, -0.1027],\n",
      "        [ 0.1814,  0.0122,  0.6285,  0.0044,  0.0144],\n",
      "        [ 0.1447,  0.5754,  0.8922,  0.5438,  0.9961]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6217,  0.6805, -0.2371, -0.7326,  1.0854],\n",
      "        [ 0.2193,  0.2414,  0.3006,  0.0397, -0.7185],\n",
      "        [-0.9613,  0.8540,  0.0279,  2.5711, -0.5822],\n",
      "        [ 0.5866, -0.5126, -1.1463,  0.2304,  0.6859]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1206],\n",
      "        [ 0.2213],\n",
      "        [-0.1435],\n",
      "        [-0.4243]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2405, -1.3662, -1.6593,  0.6558,  0.3026],\n",
      "        [ 0.0944, -0.1681, -1.2860, -0.5366,  0.1808],\n",
      "        [-0.3426,  0.7738,  0.3680,  0.9547,  0.7967],\n",
      "        [-0.4684,  0.3289, -1.8069,  0.1365, -0.1324]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1981,  0.4802,  0.4720, -0.0679, -0.2019],\n",
      "        [-0.0569, -0.2399, -0.3022, -0.6647, -0.0503],\n",
      "        [ 0.1674, -0.2321,  0.4324, -0.0431,  0.1712],\n",
      "        [ 0.7323,  0.4417,  0.6497,  0.5768,  0.6516]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2405, -1.3662, -1.6593,  0.6558,  0.3026],\n",
      "        [ 0.0944, -0.1681, -1.2860, -0.5366,  0.1808],\n",
      "        [-0.3426,  0.7738,  0.3680,  0.9547,  0.7967],\n",
      "        [-0.4684,  0.3289, -1.8069,  0.1365, -0.1324]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5926],\n",
      "        [ 0.7712],\n",
      "        [ 0.0174],\n",
      "        [-1.3793]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7826,  1.0937, -0.2852, -0.8839,  2.0805],\n",
      "        [ 0.6813, -0.5501, -0.2048, -1.1871, -0.6019],\n",
      "        [ 0.6442, -0.5606,  0.1493,  0.5642, -0.1229],\n",
      "        [-1.3896,  0.3059,  0.7619,  0.2726, -0.9315]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6995,  0.9004,  1.1500,  0.8556,  1.4320],\n",
      "        [ 0.1628, -0.1239, -0.3693,  0.3243, -0.3134],\n",
      "        [ 0.4216, -0.1413, -0.6478, -0.3761, -0.3432],\n",
      "        [ 0.4468,  0.8198,  1.2728,  0.7128,  1.7201]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7826,  1.0937, -0.2852, -0.8839,  2.0805],\n",
      "        [ 0.6813, -0.5501, -0.2048, -1.1871, -0.6019],\n",
      "        [ 0.6442, -0.5606,  0.1493,  0.5642, -0.1229],\n",
      "        [-1.3896,  0.3059,  0.7619,  0.2726, -0.9315]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3324],\n",
      "        [ 0.0583],\n",
      "        [ 0.0841],\n",
      "        [-0.8083]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.6502, -0.1550, -0.1624,  0.1795,  0.3284],\n",
      "        [ 1.1674, -0.8741,  1.2131, -0.4913, -1.4201],\n",
      "        [-0.7773,  0.3893, -0.5801,  0.0005,  1.1463],\n",
      "        [ 0.2360,  0.2005,  0.0925, -0.1132,  1.2410]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4269, -1.1067, -0.9122, -0.8276, -0.9243],\n",
      "        [-0.2570,  0.0695, -0.2029,  0.6296, -0.2449],\n",
      "        [ 0.0995, -0.9041,  0.5658, -0.6062, -0.3448],\n",
      "        [ 0.5258,  0.7742,  1.6999,  1.4944,  1.4013]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.6502, -0.1550, -0.1624,  0.1795,  0.3284],\n",
      "        [ 1.1674, -0.8741,  1.2131, -0.4913, -1.4201],\n",
      "        [-0.7773,  0.3893, -0.5801,  0.0005,  1.1463],\n",
      "        [ 0.2360,  0.2005,  0.0925, -0.1132,  1.2410]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9989],\n",
      "        [-0.5685],\n",
      "        [-1.1530],\n",
      "        [ 2.0064]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0896, -0.0433,  2.1282,  0.1282, -0.6060],\n",
      "        [ 0.2209, -0.5604,  1.3993, -1.5717,  0.2965],\n",
      "        [-0.6170, -0.9109,  1.1105, -1.1948,  1.2787],\n",
      "        [-0.1382, -0.2031,  0.6818,  0.9875,  0.1751]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1319, -0.7560, -0.2860, -0.2161, -1.0763],\n",
      "        [ 0.4196, -0.1395,  0.0072,  0.8028, -0.5136],\n",
      "        [ 0.6832,  0.5230, -0.2042, -0.0610,  0.8265],\n",
      "        [ 0.7157,  0.6627,  0.9006,  0.5195,  0.8347]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0896, -0.0433,  2.1282,  0.1282, -0.6060],\n",
      "        [ 0.2209, -0.5604,  1.3993, -1.5717,  0.2965],\n",
      "        [-0.6170, -0.9109,  1.1105, -1.1948,  1.2787],\n",
      "        [-0.1382, -0.2031,  0.6818,  0.9875,  0.1751]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0368],\n",
      "        [-1.2331],\n",
      "        [ 0.0050],\n",
      "        [ 1.0397]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0001,  0.5680, -0.0573, -0.3733,  0.3360],\n",
      "        [-1.1604, -0.1956,  0.1159,  0.7003,  0.0362],\n",
      "        [-0.2586,  0.7953, -1.1467,  0.7401, -0.6227],\n",
      "        [-0.2756, -0.9352,  1.0656,  0.9270,  1.7809]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4466,  0.0441, -0.2678, -0.1719, -1.3254],\n",
      "        [ 0.3699,  0.8393,  0.7281,  0.4803,  1.1784],\n",
      "        [-0.0167, -0.4840, -1.1184, -0.0478, -0.1639],\n",
      "        [ 0.1690,  0.5209,  0.2832, -0.0937, -0.0059]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0001,  0.5680, -0.0573, -0.3733,  0.3360],\n",
      "        [-1.1604, -0.1956,  0.1159,  0.7003,  0.0362],\n",
      "        [-0.2586,  0.7953, -1.1467,  0.7401, -0.6227],\n",
      "        [-0.2756, -0.9352,  1.0656,  0.9270,  1.7809]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3408],\n",
      "        [-0.1300],\n",
      "        [ 0.9685],\n",
      "        [-0.3293]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3368,  2.7285, -0.2733,  0.5261, -0.5176],\n",
      "        [-0.5512, -0.7557, -1.5870,  0.4124,  0.7964],\n",
      "        [-0.3074,  2.0295,  1.3988,  0.5258,  0.1844],\n",
      "        [-0.6474,  0.2055,  1.1389,  1.1323,  1.6912]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0799, -0.0602,  0.4044, -0.6555, -0.1855],\n",
      "        [-0.0095,  0.3533,  0.0727,  0.3508,  0.0271],\n",
      "        [-0.0689, -0.3588, -0.5977, -0.3502, -0.9767],\n",
      "        [ 0.5508,  1.0055,  1.1108,  0.6081,  1.0938]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3368,  2.7285, -0.2733,  0.5261, -0.5176],\n",
      "        [-0.5512, -0.7557, -1.5870,  0.4124,  0.7964],\n",
      "        [-0.3074,  2.0295,  1.3988,  0.5258,  0.1844],\n",
      "        [-0.6474,  0.2055,  1.1389,  1.1323,  1.6912]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4168],\n",
      "        [-0.2108],\n",
      "        [-1.9073],\n",
      "        [ 3.6534]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1298,  0.8099,  1.4634, -1.3084, -0.8293],\n",
      "        [-1.1803,  0.1614,  0.2670, -0.1919, -0.2686],\n",
      "        [ 0.1445, -0.4592, -0.2425,  0.8070,  2.2061],\n",
      "        [ 1.3045, -1.2658, -0.0417, -1.9635, -0.4816]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2744, -0.6939, -0.5402,  0.4135, -0.6129],\n",
      "        [ 0.4838, -0.4414, -0.2288, -0.1440,  0.4279],\n",
      "        [ 0.0802,  1.5215,  0.5097,  0.2925,  0.7610],\n",
      "        [-1.1870, -1.1522, -0.7305, -0.8598, -1.1011]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1298,  0.8099,  1.4634, -1.3084, -0.8293],\n",
      "        [-1.1803,  0.1614,  0.2670, -0.1919, -0.2686],\n",
      "        [ 0.1445, -0.4592, -0.2425,  0.8070,  2.2061],\n",
      "        [ 1.3045, -1.2658, -0.0417, -1.9635, -0.4816]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3496],\n",
      "        [-0.7907],\n",
      "        [ 1.1041],\n",
      "        [ 2.1589]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9775, -2.4469, -0.6338, -1.9135,  0.5614],\n",
      "        [ 0.3275, -1.0309, -0.3981,  0.1606,  1.3620],\n",
      "        [-0.2557, -1.9343, -0.3207,  1.3563,  0.2721],\n",
      "        [ 0.5274, -1.0426,  0.2926, -0.0352,  1.4829]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0808,  0.1325,  0.7554, -0.0596,  0.4926],\n",
      "        [ 1.0302, -0.0643, -0.4978, -0.2964, -0.2174],\n",
      "        [ 0.1161,  0.1769, -0.1311,  0.2318,  0.2719],\n",
      "        [-1.7319, -1.2285, -2.4383, -2.2836, -3.4782]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9775, -2.4469, -0.6338, -1.9135,  0.5614],\n",
      "        [ 0.3275, -1.0309, -0.3981,  0.1606,  1.3620],\n",
      "        [-0.2557, -1.9343, -0.3207,  1.3563,  0.2721],\n",
      "        [ 0.5274, -1.0426,  0.2926, -0.0352,  1.4829]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5722],\n",
      "        [ 0.2582],\n",
      "        [ 0.0587],\n",
      "        [-5.4235]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0954,  0.5747,  0.3052,  1.3745,  0.0184],\n",
      "        [-0.4389, -0.5704, -0.3835,  0.4555,  0.1193],\n",
      "        [ 1.1223, -0.4901,  1.3564, -0.2301,  0.4220],\n",
      "        [-0.9782,  1.0768,  1.1228,  0.6087,  0.5652]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5212,  0.6392, -0.3767,  0.4887,  0.4516],\n",
      "        [ 0.5384, -0.4890, -0.3317, -0.0424, -0.3502],\n",
      "        [-0.0938,  0.0077,  0.4768, -0.3310, -0.0183],\n",
      "        [ 0.0572,  0.7108,  0.2100,  0.2947,  0.9398]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0954,  0.5747,  0.3052,  1.3745,  0.0184],\n",
      "        [-0.4389, -0.5704, -0.3835,  0.4555,  0.1193],\n",
      "        [ 1.1223, -0.4901,  1.3564, -0.2301,  0.4220],\n",
      "        [-0.9782,  1.0768,  1.1228,  0.6087,  0.5652]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9822],\n",
      "        [ 0.1087],\n",
      "        [ 0.6061],\n",
      "        [ 1.6558]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2686, -1.4827,  2.3586, -0.0724, -0.2060],\n",
      "        [-0.7404,  0.7238, -0.2483, -1.4235,  0.8602],\n",
      "        [ 0.3805,  1.2512,  0.4209, -0.4606,  1.3088],\n",
      "        [-2.0013, -0.8719,  1.7079,  0.9051,  0.8316]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4613, -0.1429, -0.4242,  0.0975,  0.1745],\n",
      "        [ 0.0731,  0.2543, -0.7134, -0.4273, -0.3121],\n",
      "        [-0.1441, -0.3309,  0.9193,  0.3414, -0.1798],\n",
      "        [-0.4211, -0.4846, -0.5171, -0.2830, -0.5207]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2686, -1.4827,  2.3586, -0.0724, -0.2060],\n",
      "        [-0.7404,  0.7238, -0.2483, -1.4235,  0.8602],\n",
      "        [ 0.3805,  1.2512,  0.4209, -0.4606,  1.3088],\n",
      "        [-2.0013, -0.8719,  1.7079,  0.9051,  0.8316]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7077],\n",
      "        [ 0.6468],\n",
      "        [-0.4745],\n",
      "        [-0.3071]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5545, -0.6786, -0.6958,  0.0836,  1.0090],\n",
      "        [ 0.4076, -1.6832,  1.7604,  0.0427,  0.5127],\n",
      "        [-0.9606,  0.4961, -0.4758,  1.1796,  0.0945],\n",
      "        [ 0.2162, -1.1808,  0.3703,  1.5278, -0.9119]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8399, -0.2461, -0.2239, -0.4624, -0.4858],\n",
      "        [-0.1962, -0.2773, -0.4489, -0.2805,  0.0474],\n",
      "        [-0.0387,  0.0687, -0.0083,  0.0657,  0.0737],\n",
      "        [-0.0760, -0.3461, -0.1155, -0.5603,  0.2500]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5545, -0.6786, -0.6958,  0.0836,  1.0090],\n",
      "        [ 0.4076, -1.6832,  1.7604,  0.0427,  0.5127],\n",
      "        [-0.9606,  0.4961, -0.4758,  1.1796,  0.0945],\n",
      "        [ 0.2162, -1.1808,  0.3703,  1.5278, -0.9119]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5116],\n",
      "        [-0.3910],\n",
      "        [ 0.1597],\n",
      "        [-0.7345]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1754, -1.3990, -1.0400,  0.9084, -0.5260],\n",
      "        [-1.0580, -1.0174,  1.9605,  1.7123,  0.7353],\n",
      "        [ 0.2779, -1.4359,  2.1841, -0.4182, -0.0282],\n",
      "        [ 2.1901,  0.7152,  0.6888,  1.0281, -1.4645]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0732,  0.3265,  0.8181,  0.5518,  0.8769],\n",
      "        [-0.2105,  0.0839, -0.0305,  0.2314,  0.3409],\n",
      "        [ 0.8036, -0.2683,  0.0181,  0.3903,  0.0127],\n",
      "        [ 0.0976, -0.3774, -0.1255, -0.0050,  0.7099]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1754, -1.3990, -1.0400,  0.9084, -0.5260],\n",
      "        [-1.0580, -1.0174,  1.9605,  1.7123,  0.7353],\n",
      "        [ 0.2779, -1.4359,  2.1841, -0.4182, -0.0282],\n",
      "        [ 2.1901,  0.7152,  0.6888,  1.0281, -1.4645]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2804],\n",
      "        [ 0.7244],\n",
      "        [ 0.4846],\n",
      "        [-1.1874]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7071, -0.2115, -0.4663,  1.5761,  0.5508],\n",
      "        [-0.0336, -0.1972, -0.3462,  1.4517,  0.6098],\n",
      "        [ 1.3767,  1.1550, -1.0433,  0.8718,  2.2340],\n",
      "        [ 2.2084, -0.3514,  1.8654,  1.2145, -0.8452]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4521,  0.9806,  2.5023,  0.6762,  1.0930],\n",
      "        [ 0.1390,  0.2172,  0.4118, -0.4081, -0.6195],\n",
      "        [-0.2346, -0.5415, -0.3192,  0.3671,  0.0867],\n",
      "        [ 0.5162,  0.8058, -0.5426, -0.6983,  1.1201]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7071, -0.2115, -0.4663,  1.5761,  0.5508],\n",
      "        [-0.0336, -0.1972, -0.3462,  1.4517,  0.6098],\n",
      "        [ 1.3767,  1.1550, -1.0433,  0.8718,  2.2340],\n",
      "        [ 2.2084, -0.3514,  1.8654,  1.2145, -0.8452]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0264],\n",
      "        [-1.1603],\n",
      "        [-0.1017],\n",
      "        [-1.9502]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3428,  0.7343,  0.5722,  2.0342, -1.9342],\n",
      "        [ 1.0515, -0.6334,  0.1398, -0.2250, -0.7692],\n",
      "        [-0.1264, -0.0231,  1.0818, -0.6711,  0.6689],\n",
      "        [-1.0572, -0.0462,  1.8641,  0.8789,  0.3099]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9641,  0.7526,  1.2290,  0.1467,  1.4213],\n",
      "        [ 0.9466,  0.1167,  0.5892,  0.4683,  0.8091],\n",
      "        [-0.2590, -0.4686,  0.0764,  0.3961, -0.4501],\n",
      "        [ 0.9548,  0.5062,  1.3824,  1.0478,  1.1527]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3428,  0.7343,  0.5722,  2.0342, -1.9342],\n",
      "        [ 1.0515, -0.6334,  0.1398, -0.2250, -0.7692],\n",
      "        [-0.1264, -0.0231,  1.0818, -0.6711,  0.6689],\n",
      "        [-1.0572, -0.0462,  1.8641,  0.8789,  0.3099]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8642],\n",
      "        [ 0.2761],\n",
      "        [-0.4408],\n",
      "        [ 2.8223]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6246, -0.8793, -1.5193,  0.6724, -0.7859],\n",
      "        [-0.3493,  0.0551,  0.3659, -0.8581, -1.4321],\n",
      "        [-0.1930,  1.4618,  0.7316, -1.7059, -1.3001],\n",
      "        [-0.1138,  0.2166, -0.2082,  0.4645, -0.3029]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4538,  0.6878,  1.1880,  0.6453,  1.8075],\n",
      "        [-0.0671,  0.0840, -0.0062, -0.0896,  0.1588],\n",
      "        [ 0.1451, -0.7969,  0.8038, -0.0417, -0.0236],\n",
      "        [ 0.1583, -0.0818, -0.6306,  0.1931, -0.5650]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6246, -0.8793, -1.5193,  0.6724, -0.7859],\n",
      "        [-0.3493,  0.0551,  0.3659, -0.8581, -1.4321],\n",
      "        [-0.1930,  1.4618,  0.7316, -1.7059, -1.3001],\n",
      "        [-0.1138,  0.2166, -0.2082,  0.4645, -0.3029]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.1128],\n",
      "        [-0.1248],\n",
      "        [-0.5032],\n",
      "        [ 0.3564]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7402,  0.6620,  0.7974,  0.2356, -0.6559],\n",
      "        [ 1.2799,  1.0879,  0.9658,  1.9516,  0.4558],\n",
      "        [-0.3909,  1.3089,  1.9342,  1.2269, -0.3743],\n",
      "        [-0.6975,  0.4830, -0.9668,  1.2763,  0.6697]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2567,  0.1863, -0.2989,  0.1113,  0.4040],\n",
      "        [ 0.5599,  0.4579, -0.0395, -0.2978, -0.0461],\n",
      "        [ 0.2324,  0.0165, -0.0618, -0.2066, -0.3175],\n",
      "        [ 0.4703,  0.4289, -0.0444,  0.3652, -0.4171]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7402,  0.6620,  0.7974,  0.2356, -0.6559],\n",
      "        [ 1.2799,  1.0879,  0.9658,  1.9516,  0.4558],\n",
      "        [-0.3909,  1.3089,  1.9342,  1.2269, -0.3743],\n",
      "        [-0.6975,  0.4830, -0.9668,  1.2763,  0.6697]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8004],\n",
      "        [ 0.5744],\n",
      "        [-0.3233],\n",
      "        [ 0.1088]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7235,  0.2554,  0.6382, -1.4473, -2.1302],\n",
      "        [ 2.3755,  0.2813,  0.1517, -0.4501,  0.4520],\n",
      "        [ 0.0564, -0.9696,  0.7495,  0.4688,  0.9319],\n",
      "        [-1.1948, -2.1580,  0.0160,  1.4539,  0.7381]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9172,  0.2653,  0.1646,  0.0824,  1.4241],\n",
      "        [ 0.2502,  0.2800, -0.1465,  0.0527, -0.1759],\n",
      "        [ 0.5424, -0.2977, -0.6642,  0.3579, -0.8722],\n",
      "        [ 0.1912,  0.1764,  0.4423,  0.2645, -0.0766]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7235,  0.2554,  0.6382, -1.4473, -2.1302],\n",
      "        [ 2.3755,  0.2813,  0.1517, -0.4501,  0.4520],\n",
      "        [ 0.0564, -0.9696,  0.7495,  0.4688,  0.9319],\n",
      "        [-1.1948, -2.1580,  0.0160,  1.4539,  0.7381]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.3165],\n",
      "        [ 0.5477],\n",
      "        [-0.8236],\n",
      "        [-0.2739]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0113, -0.8539, -0.5071,  0.9806,  1.1100],\n",
      "        [-1.3295,  0.2240, -1.7340,  0.8799,  1.4360],\n",
      "        [ 0.5528, -0.4208, -0.7969, -0.5549,  0.9771],\n",
      "        [-0.9315,  1.4773,  1.6439, -0.4603, -1.1662]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4424,  1.3015,  0.6430,  1.4546,  1.9448],\n",
      "        [-0.0173,  0.7001,  0.0524,  0.0879,  0.4556],\n",
      "        [ 0.3250, -0.4215,  0.3076, -0.2689, -0.1181],\n",
      "        [ 0.0974,  0.0654,  0.3650,  0.0198,  0.6535]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0113, -0.8539, -0.5071,  0.9806,  1.1100],\n",
      "        [-1.3295,  0.2240, -1.7340,  0.8799,  1.4360],\n",
      "        [ 0.5528, -0.4208, -0.7969, -0.5549,  0.9771],\n",
      "        [-0.9315,  1.4773,  1.6439, -0.4603, -1.1662]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1428],\n",
      "        [ 0.8206],\n",
      "        [ 0.1458],\n",
      "        [-0.1652]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7025, -0.4685, -0.8796,  0.2554,  0.9294],\n",
      "        [-1.0112, -0.8609,  0.5468,  0.6112, -0.1159],\n",
      "        [-0.3326,  1.5892,  0.3431, -2.0492, -0.1204],\n",
      "        [-0.6707, -1.7419, -0.7053,  2.3788,  1.6176]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2906,  0.3318, -0.5874,  0.2373,  0.2464],\n",
      "        [ 0.0703,  0.6438,  0.5523, -0.1305, -0.5312],\n",
      "        [-0.2968,  0.2193, -0.7143, -0.5220, -0.2748],\n",
      "        [ 0.6349, -0.2323, -0.0147, -0.7121,  0.2724]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7025, -0.4685, -0.8796,  0.2554,  0.9294],\n",
      "        [-1.0112, -0.8609,  0.5468,  0.6112, -0.1159],\n",
      "        [-0.3326,  1.5892,  0.3431, -2.0492, -0.1204],\n",
      "        [-0.6707, -1.7419, -0.7053,  2.3788,  1.6176]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1456],\n",
      "        [-0.3414],\n",
      "        [ 1.3049],\n",
      "        [-1.2640]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7787, -1.3515, -0.1408,  0.5038, -0.4325],\n",
      "        [-0.3195, -0.7032,  1.3887, -0.5011, -1.4365],\n",
      "        [-0.6508, -1.4672,  0.1926,  2.3125,  1.5796],\n",
      "        [ 1.1427,  1.1830, -1.2113, -0.0328, -0.5824]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0789,  0.0708, -0.2969, -0.1383, -0.2512],\n",
      "        [-0.2354, -0.0048,  0.1356, -0.0597,  0.4690],\n",
      "        [-0.1756, -1.0220,  0.1745, -0.4962, -0.7466],\n",
      "        [ 0.3044,  0.7447,  0.9688,  0.6551,  0.5339]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7787, -1.3515, -0.1408,  0.5038, -0.4325],\n",
      "        [-0.3195, -0.7032,  1.3887, -0.5011, -1.4365],\n",
      "        [-0.6508, -1.4672,  0.1926,  2.3125,  1.5796],\n",
      "        [ 1.1427,  1.1830, -1.2113, -0.0328, -0.5824]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0466],\n",
      "        [-0.3767],\n",
      "        [-0.6794],\n",
      "        [-0.2770]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8827, -0.1436, -0.6684,  0.4116, -1.7642],\n",
      "        [-1.6433, -0.1412, -0.4427, -1.0701,  0.0271],\n",
      "        [ 1.1473,  0.8796,  0.4591,  0.1285,  0.9893],\n",
      "        [ 1.1770,  1.3432, -0.0892,  0.4260,  2.1011]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1135,  0.4945,  0.1337,  0.7167, -0.1804],\n",
      "        [ 0.3638, -0.6513,  0.5674,  0.7003,  0.9007],\n",
      "        [ 0.4373, -0.2777,  0.0831, -0.1044, -0.8907],\n",
      "        [ 0.0986,  0.4887,  0.5377,  1.2384,  0.6964]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8827, -0.1436, -0.6684,  0.4116, -1.7642],\n",
      "        [-1.6433, -0.1412, -0.4427, -1.0701,  0.0271],\n",
      "        [ 1.1473,  0.8796,  0.4591,  0.1285,  0.9893],\n",
      "        [ 1.1770,  1.3432, -0.0892,  0.4260,  2.1011]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5530],\n",
      "        [-1.4820],\n",
      "        [-0.5989],\n",
      "        [ 2.7151]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2827,  1.2129,  0.1015,  0.8291, -0.0279],\n",
      "        [ 1.9888, -0.7223, -0.7354, -0.7805,  1.1052],\n",
      "        [-0.6968,  0.0103,  1.8653,  0.4858,  0.3732],\n",
      "        [-0.8049,  1.8389,  0.9097, -0.2407,  0.7161]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0493,  0.2005,  1.1441, -0.3571,  0.6534],\n",
      "        [ 0.6789,  1.3445,  0.9136,  1.0235,  1.1110],\n",
      "        [ 0.3315, -0.6204, -0.1751, -0.4135, -0.1872],\n",
      "        [-0.3590, -0.8831, -1.1851, -0.7702, -2.4948]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2827,  1.2129,  0.1015,  0.8291, -0.0279],\n",
      "        [ 1.9888, -0.7223, -0.7354, -0.7805,  1.1052],\n",
      "        [-0.6968,  0.0103,  1.8653,  0.4858,  0.3732],\n",
      "        [-0.8049,  1.8389,  0.9097, -0.2407,  0.7161]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1082],\n",
      "        [ 0.1362],\n",
      "        [-0.8348],\n",
      "        [-4.0143]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7573, -0.0509,  0.6485,  0.8179,  0.3665],\n",
      "        [-0.1974, -0.2500, -0.0464, -0.7859,  1.1696],\n",
      "        [-0.0340,  0.7518, -1.5248, -0.7542,  0.6091],\n",
      "        [-1.3546, -0.5345, -0.0564, -1.1285, -0.6052]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1867,  0.4226,  0.4011,  0.4544, -0.1881],\n",
      "        [ 0.4099,  0.6298,  0.6188, -0.0277,  1.1766],\n",
      "        [ 0.8884, -0.0228, -0.7734,  0.4608, -0.2693],\n",
      "        [ 0.5963,  1.1867,  1.5721,  0.9223,  1.4079]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7573, -0.0509,  0.6485,  0.8179,  0.3665],\n",
      "        [-0.1974, -0.2500, -0.0464, -0.7859,  1.1696],\n",
      "        [-0.0340,  0.7518, -1.5248, -0.7542,  0.6091],\n",
      "        [-1.3546, -0.5345, -0.0564, -1.1285, -0.6052]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4000],\n",
      "        [ 1.1309],\n",
      "        [ 0.6204],\n",
      "        [-3.4235]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2716,  1.1084,  0.2563, -1.2541, -0.5419],\n",
      "        [ 1.6733,  0.3092,  0.5661, -0.0054,  0.6223],\n",
      "        [-1.2144, -0.5851,  2.1158,  1.0135, -0.8574],\n",
      "        [-1.0632,  1.4523,  0.7270,  1.4602, -0.7542]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2979,  0.3389,  1.0833,  0.6869, -0.1949],\n",
      "        [-0.3679,  0.8072,  0.0791,  0.6503,  0.0165],\n",
      "        [ 0.4105, -0.0703, -0.6808, -0.9875, -0.2689],\n",
      "        [ 1.7951,  1.8297,  2.2710,  1.4298,  2.8073]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2716,  1.1084,  0.2563, -1.2541, -0.5419],\n",
      "        [ 1.6733,  0.3092,  0.5661, -0.0054,  0.6223],\n",
      "        [-1.2144, -0.5851,  2.1158,  1.0135, -0.8574],\n",
      "        [-1.0632,  1.4523,  0.7270,  1.4602, -0.7542]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1835],\n",
      "        [-0.3145],\n",
      "        [-2.6681],\n",
      "        [ 2.3702]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3419,  0.6013,  2.0952,  2.3893,  0.9677],\n",
      "        [ 0.2191,  1.0800,  0.6757,  0.8892,  2.7552],\n",
      "        [-0.5092, -0.3223,  1.0688,  0.6874, -0.1482],\n",
      "        [-2.1558,  0.8024,  1.7700,  0.6201, -1.2050]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5105,  0.7083,  1.2615,  0.6472,  0.8264],\n",
      "        [ 0.0530, -0.6192,  0.9313,  0.6249,  0.7276],\n",
      "        [ 1.1316,  1.3916,  1.2394,  0.8969,  1.6107],\n",
      "        [ 0.2657,  0.1565, -0.2703, -0.6249, -0.5349]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3419,  0.6013,  2.0952,  2.3893,  0.9677],\n",
      "        [ 0.2191,  1.0800,  0.6757,  0.8892,  2.7552],\n",
      "        [-0.5092, -0.3223,  1.0688,  0.6874, -0.1482],\n",
      "        [-2.1558,  0.8024,  1.7700,  0.6201, -1.2050]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.7302],\n",
      "        [ 2.5325],\n",
      "        [ 0.6778],\n",
      "        [-0.6686]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2863,  1.7079,  1.0439, -0.9056, -0.0628],\n",
      "        [-2.0873, -0.6645, -0.6916,  0.1099,  0.0612],\n",
      "        [-1.5008,  0.9628,  0.3127,  0.1455,  0.6671],\n",
      "        [-1.0489,  0.2781,  2.4835,  1.7131,  1.8393]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2817, -1.4591, -1.7365, -2.1505, -3.4302],\n",
      "        [-1.0529, -0.7508, -0.8880, -0.9485, -1.9748],\n",
      "        [ 1.1102,  0.1655,  0.2705,  1.3185,  0.8667],\n",
      "        [ 0.3817, -0.0592, -0.3053, -0.6222, -0.1325]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2863,  1.7079,  1.0439, -0.9056, -0.0628],\n",
      "        [-2.0873, -0.6645, -0.6916,  0.1099,  0.0612],\n",
      "        [-1.5008,  0.9628,  0.3127,  0.1455,  0.6671],\n",
      "        [-1.0489,  0.2781,  2.4835,  1.7131,  1.8393]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.7905],\n",
      "        [ 3.0856],\n",
      "        [-0.6523],\n",
      "        [-2.4845]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2978, -0.7420,  0.0787, -1.9002, -0.9126],\n",
      "        [-1.7292,  0.1297, -0.0414, -0.8376,  1.0523],\n",
      "        [-0.9652,  1.4061,  1.7422, -0.0388,  1.7217],\n",
      "        [-0.2608, -1.7046,  2.3465,  2.0355, -0.7816]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5761, -0.4846,  0.2893, -0.3915, -0.6838],\n",
      "        [-1.2177, -1.3760, -1.9128, -1.9976, -3.3143],\n",
      "        [ 0.8052,  0.4206,  0.2277,  0.0483,  0.9695],\n",
      "        [ 1.3794,  1.0207,  0.8210,  0.8176,  1.2991]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2978, -0.7420,  0.0787, -1.9002, -0.9126],\n",
      "        [-1.7292,  0.1297, -0.0414, -0.8376,  1.0523],\n",
      "        [-0.9652,  1.4061,  1.7422, -0.0388,  1.7217],\n",
      "        [-0.2608, -1.7046,  2.3465,  2.0355, -0.7816]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4980],\n",
      "        [ 0.1921],\n",
      "        [ 1.8782],\n",
      "        [ 0.4757]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8906,  1.3828,  0.0391, -1.8904,  0.2938],\n",
      "        [-0.2371, -0.5038, -0.5905, -0.1799, -1.5622],\n",
      "        [-0.4522, -1.0423,  1.8015, -2.2164,  0.1593],\n",
      "        [-1.2105,  0.1313, -1.2301,  0.4107,  2.2239]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7495, -1.2009, -1.4414, -1.3532, -2.8176],\n",
      "        [ 0.0914,  0.2455, -0.2382,  0.5680, -0.0922],\n",
      "        [ 0.2140, -0.8928, -0.7190, -0.1331, -0.3222],\n",
      "        [ 0.4217,  1.0120,  0.9789,  1.0635,  0.8071]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8906,  1.3828,  0.0391, -1.8904,  0.2938],\n",
      "        [-0.2371, -0.5038, -0.5905, -0.1799, -1.5622],\n",
      "        [-0.4522, -1.0423,  1.8015, -2.2164,  0.1593],\n",
      "        [-1.2105,  0.1313, -1.2301,  0.4107,  2.2239]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6542],\n",
      "        [ 0.0371],\n",
      "        [-0.2179],\n",
      "        [ 0.6497]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0297, -0.9536,  0.4331, -1.6182,  0.3833],\n",
      "        [ 0.5935, -0.7312,  0.0432,  2.4021,  0.3505],\n",
      "        [ 0.4747,  0.0916, -0.3948, -0.6286,  0.7194],\n",
      "        [-0.1034,  1.7795, -0.2026,  1.1735, -0.4560]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5318, -1.5079, -1.5602, -0.7708, -2.5397],\n",
      "        [ 0.1152, -0.4898,  0.2467,  0.2931, -0.0025],\n",
      "        [ 0.3939, -0.0720,  0.0056,  0.2279, -0.6411],\n",
      "        [ 0.2787,  0.0914,  0.4701,  0.0580,  0.8082]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0297, -0.9536,  0.4331, -1.6182,  0.3833],\n",
      "        [ 0.5935, -0.7312,  0.0432,  2.4021,  0.3505],\n",
      "        [ 0.4747,  0.0916, -0.3948, -0.6286,  0.7194],\n",
      "        [-0.1034,  1.7795, -0.2026,  1.1735, -0.4560]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0519],\n",
      "        [ 1.1403],\n",
      "        [-0.4263],\n",
      "        [-0.2618]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5670,  1.0923, -0.2619,  0.6220,  0.4527],\n",
      "        [ 1.6217,  0.4471,  2.2380, -0.5426,  0.8574],\n",
      "        [ 0.0606,  0.5901,  0.6656,  2.3540, -0.1522],\n",
      "        [-0.5750, -0.3997, -2.1623,  1.3875, -0.3490]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3022, -0.5373, -0.2040, -0.2775, -0.1405],\n",
      "        [ 0.4625, -0.0346, -0.3592,  0.0099, -1.1561],\n",
      "        [ 0.5655,  0.2240,  1.0359, -0.0507, -0.3424],\n",
      "        [ 0.4211,  0.0679,  0.3766, -0.0843,  0.6792]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5670,  1.0923, -0.2619,  0.6220,  0.4527],\n",
      "        [ 1.6217,  0.4471,  2.2380, -0.5426,  0.8574],\n",
      "        [ 0.0606,  0.5901,  0.6656,  2.3540, -0.1522],\n",
      "        [-0.5750, -0.3997, -2.1623,  1.3875, -0.3490]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2962],\n",
      "        [-1.0659],\n",
      "        [ 0.7887],\n",
      "        [-1.4377]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2170,  0.8217, -0.9195,  0.5059, -0.4265],\n",
      "        [ 2.0166,  1.2399,  1.3985, -0.3627, -0.0158],\n",
      "        [-0.3856,  0.1536, -0.4708,  1.0270,  0.2146],\n",
      "        [-0.5127,  0.4718, -1.2823, -0.3031, -0.1861]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3426, -0.4792, -0.6920, -0.4484, -0.4913],\n",
      "        [ 0.3305, -0.2732,  0.2550,  0.6654, -0.1446],\n",
      "        [ 0.1233,  0.3556,  0.5681,  0.2314,  0.1364],\n",
      "        [ 1.2337,  0.9862,  1.7240,  0.6199,  0.9918]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2170,  0.8217, -0.9195,  0.5059, -0.4265],\n",
      "        [ 2.0166,  1.2399,  1.3985, -0.3627, -0.0158],\n",
      "        [-0.3856,  0.1536, -0.4708,  1.0270,  0.2146],\n",
      "        [-0.5127,  0.4718, -1.2823, -0.3031, -0.1861]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1918],\n",
      "        [ 0.4452],\n",
      "        [ 0.0065],\n",
      "        [-2.7503]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7886,  1.0753, -0.9994, -1.3437,  1.2718],\n",
      "        [-0.0687, -0.6953,  1.1291,  1.3778, -0.0170],\n",
      "        [ 0.9652, -0.9454, -0.3670,  2.8769,  1.1563],\n",
      "        [-0.6883, -0.7522,  0.5996,  1.1510,  1.0821]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0777,  0.3858, -0.0028, -0.0304, -0.0255],\n",
      "        [ 0.1119,  0.0606,  0.3606, -0.2974, -0.2378],\n",
      "        [ 0.2367,  0.5476,  0.3829,  0.7735,  0.1857],\n",
      "        [ 1.2560,  1.9077,  2.0417,  1.2589,  2.7520]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7886,  1.0753, -0.9994, -1.3437,  1.2718],\n",
      "        [-0.0687, -0.6953,  1.1291,  1.3778, -0.0170],\n",
      "        [ 0.9652, -0.9454, -0.3670,  2.8769,  1.1563],\n",
      "        [-0.6883, -0.7522,  0.5996,  1.1510,  1.0821]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2870],\n",
      "        [-0.0484],\n",
      "        [ 2.0102],\n",
      "        [ 3.3517]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8075,  2.2930,  0.2258,  1.4456,  0.0753],\n",
      "        [-1.0333, -0.6891,  0.4381,  0.6922, -1.1724],\n",
      "        [ 0.6850,  0.6234,  1.5059, -0.4622,  1.4543],\n",
      "        [-1.1128, -0.7128,  0.2805, -0.0910,  1.3511]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4791, -0.1048, -0.0765,  0.1872, -0.3435],\n",
      "        [ 0.1054, -0.0112, -0.5918, -0.4730, -0.1633],\n",
      "        [-0.2874, -0.5928, -0.3854, -0.3735, -1.1186],\n",
      "        [-0.3206, -0.0735, -0.8111, -0.5779, -0.1696]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8075,  2.2930,  0.2258,  1.4456,  0.0753],\n",
      "        [-1.0333, -0.6891,  0.4381,  0.6922, -1.1724],\n",
      "        [ 0.6850,  0.6234,  1.5059, -0.4622,  1.4543],\n",
      "        [-1.1128, -0.7128,  0.2805, -0.0910,  1.3511]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3741],\n",
      "        [-0.4964],\n",
      "        [-2.6010],\n",
      "        [ 0.0049]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6130,  2.0439,  0.2005,  1.0387,  0.6425],\n",
      "        [ 1.5124, -0.1786,  0.7232, -0.5564, -1.1059],\n",
      "        [-1.0663, -0.9281, -0.1813,  1.0145,  0.8905],\n",
      "        [-0.7752, -1.9855, -1.0611,  1.3333, -0.7020]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0753,  0.0053, -0.3006,  0.5785, -0.3145],\n",
      "        [-0.0503,  0.1126, -0.1834, -0.1499, -0.4243],\n",
      "        [ 1.0214,  0.1234,  0.6627,  0.7045,  1.1240],\n",
      "        [-0.1681,  0.4113,  0.2354, -0.0086, -0.1544]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6130,  2.0439,  0.2005,  1.0387,  0.6425],\n",
      "        [ 1.5124, -0.1786,  0.7232, -0.5564, -1.1059],\n",
      "        [-1.0663, -0.9281, -0.1813,  1.0145,  0.8905],\n",
      "        [-0.7752, -1.9855, -1.0611,  1.3333, -0.7020]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3956],\n",
      "        [ 0.3237],\n",
      "        [ 0.3918],\n",
      "        [-0.8390]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7355, -0.2617,  0.9784, -0.2672,  2.4407],\n",
      "        [-0.9154,  0.0040,  1.3001, -0.2919,  1.6943],\n",
      "        [-0.3211, -1.2328, -0.7088, -0.2260,  0.1904],\n",
      "        [-1.5944, -0.3655,  1.2041, -1.7531,  1.2833]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1152, -0.2421,  0.8513,  0.2658,  0.0563],\n",
      "        [ 0.1117, -0.3229, -0.5307, -0.1544, -0.7111],\n",
      "        [ 0.1583,  0.8351,  0.1801,  0.3046,  0.5961],\n",
      "        [ 0.5245,  0.2122,  0.0189, -0.0146, -0.0175]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7355, -0.2617,  0.9784, -0.2672,  2.4407],\n",
      "        [-0.9154,  0.0040,  1.3001, -0.2919,  1.6943],\n",
      "        [-0.3211, -1.2328, -0.7088, -0.2260,  0.1904],\n",
      "        [-1.5944, -0.3655,  1.2041, -1.7531,  1.2833]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8780],\n",
      "        [-1.9532],\n",
      "        [-1.1633],\n",
      "        [-0.8878]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7803,  0.1545, -1.3318, -0.5706, -0.9714],\n",
      "        [-0.6238,  0.3458, -0.3429, -2.2364,  0.4761],\n",
      "        [ 0.6455,  1.3042, -0.4779, -0.6247, -0.6981],\n",
      "        [-1.0932,  0.8101,  1.0103,  0.3365,  0.0255]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0401, -0.2382,  1.0261,  0.5659,  0.0040],\n",
      "        [ 0.4299,  0.3732,  0.8160,  1.1272,  1.3690],\n",
      "        [ 0.3684,  0.8359,  0.9546,  0.9165,  1.1631],\n",
      "        [ 0.7479,  0.8147,  0.2942,  0.2732,  0.6593]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7803,  0.1545, -1.3318, -0.5706, -0.9714],\n",
      "        [-0.6238,  0.3458, -0.3429, -2.2364,  0.4761],\n",
      "        [ 0.6455,  1.3042, -0.4779, -0.6247, -0.6981],\n",
      "        [-1.0932,  0.8101,  1.0103,  0.3365,  0.0255]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.7613],\n",
      "        [-2.2881],\n",
      "        [-0.5127],\n",
      "        [ 0.2485]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0868,  0.0872, -0.9102, -0.0617,  1.3197],\n",
      "        [ 0.4202, -1.0081,  0.5866, -1.5789,  0.0206],\n",
      "        [-0.6127, -0.4447, -1.6632, -0.3503, -0.5025],\n",
      "        [-0.2291,  0.3948,  0.8865,  0.2875,  0.2014]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9407,  1.1433, -0.0548,  0.3687,  0.6821],\n",
      "        [ 1.3678,  2.2822,  1.4757,  1.3321,  1.6707],\n",
      "        [ 1.2450,  0.8008,  0.0007,  0.8517,  1.4320],\n",
      "        [ 0.5602,  0.0073,  0.0944, -0.5030,  0.3650]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0868,  0.0872, -0.9102, -0.0617,  1.3197],\n",
      "        [ 0.4202, -1.0081,  0.5866, -1.5789,  0.0206],\n",
      "        [-0.6127, -0.4447, -1.6632, -0.3503, -0.5025],\n",
      "        [-0.2291,  0.3948,  0.8865,  0.2875,  0.2014]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9454],\n",
      "        [-2.9292],\n",
      "        [-2.1381],\n",
      "        [-0.1129]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0267,  0.4380, -1.5161,  1.6518,  1.8353],\n",
      "        [-0.1677,  0.2800, -0.6195, -1.0844, -0.6694],\n",
      "        [ 0.1147, -1.5490, -0.0841, -0.3914,  0.0270],\n",
      "        [ 1.0291,  0.5070,  1.2024,  1.0861,  0.8020]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0690,  0.0723,  0.8764,  0.5247,  0.7955],\n",
      "        [-0.0598, -0.0062,  0.8398,  0.2644, -0.4327],\n",
      "        [ 1.2021,  1.7607,  1.2712,  1.2787,  2.0352],\n",
      "        [-0.1665, -0.3062,  0.0411, -0.1754,  0.0524]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0267,  0.4380, -1.5161,  1.6518,  1.8353],\n",
      "        [-0.1677,  0.2800, -0.6195, -1.0844, -0.6694],\n",
      "        [ 0.1147, -1.5490, -0.0841, -0.3914,  0.0270],\n",
      "        [ 1.0291,  0.5070,  1.2024,  1.0861,  0.8020]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8898],\n",
      "        [-0.5089],\n",
      "        [-3.1419],\n",
      "        [-0.4256]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0658, -0.7742,  0.7559, -0.6278,  1.4205],\n",
      "        [-0.9064, -0.5981, -0.5198, -0.9057, -0.0654],\n",
      "        [ 0.3434,  0.2029,  0.1878, -0.2026,  0.5788],\n",
      "        [-0.3746,  1.4650,  0.7924, -1.1266, -1.2036]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0166,  0.3295, -0.2412,  0.4511,  0.3018],\n",
      "        [ 0.1948,  0.2341,  0.1218,  0.1206,  0.3205],\n",
      "        [-0.0329,  0.2911, -0.1886,  0.0466,  0.1100],\n",
      "        [ 0.3778, -0.0472,  0.1245, -0.6168, -0.6874]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0658, -0.7742,  0.7559, -0.6278,  1.4205],\n",
      "        [-0.9064, -0.5981, -0.5198, -0.9057, -0.0654],\n",
      "        [ 0.3434,  0.2029,  0.1878, -0.2026,  0.5788],\n",
      "        [-0.3746,  1.4650,  0.7924, -1.1266, -1.2036]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2908],\n",
      "        [-0.5101],\n",
      "        [ 0.0665],\n",
      "        [ 1.4102]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.5314,  0.2085, -0.7804,  0.4060,  1.8370],\n",
      "        [-1.2040, -0.9711, -0.7607, -0.4339,  0.7462],\n",
      "        [ 0.1908,  2.1733, -0.4108,  0.0883, -1.0589],\n",
      "        [ 0.4249, -0.0798, -1.6980,  0.5874, -0.8065]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0140,  0.5570,  0.4696, -0.0738,  0.5222],\n",
      "        [ 0.1463,  0.0749, -0.4715,  0.0511, -0.4331],\n",
      "        [ 0.6175, -0.4987, -0.0036,  0.3405,  0.4092],\n",
      "        [-0.7380, -1.0120, -0.9163, -1.0250, -1.4175]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5314,  0.2085, -0.7804,  0.4060,  1.8370],\n",
      "        [-1.2040, -0.9711, -0.7607, -0.4339,  0.7462],\n",
      "        [ 0.1908,  2.1733, -0.4108,  0.0883, -1.0589],\n",
      "        [ 0.4249, -0.0798, -1.6980,  0.5874, -0.8065]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6716],\n",
      "        [-0.2355],\n",
      "        [-1.3678],\n",
      "        [ 1.8642]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0800, -0.3110, -0.6658, -0.6983,  1.3946],\n",
      "        [-1.0836, -0.1114,  0.9001, -0.1386, -0.8462],\n",
      "        [-1.2886, -0.6203, -0.2808, -0.7523, -1.0010],\n",
      "        [ 1.3834,  0.4972,  0.4567,  2.8496, -1.1010]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0761,  0.5132, -0.4814, -0.1813,  0.0701],\n",
      "        [ 0.2566, -0.8126,  0.2937,  0.1573, -0.8649],\n",
      "        [ 0.5327,  0.4223,  1.0855,  0.6334,  1.3912],\n",
      "        [-1.0793, -1.1441, -0.9467, -1.2836, -2.3516]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0800, -0.3110, -0.6658, -0.6983,  1.3946],\n",
      "        [-1.0836, -0.1114,  0.9001, -0.1386, -0.8462],\n",
      "        [-1.2886, -0.6203, -0.2808, -0.7523, -1.0010],\n",
      "        [ 1.3834,  0.4972,  0.4567,  2.8496, -1.1010]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3913],\n",
      "        [ 0.7869],\n",
      "        [-3.1223],\n",
      "        [-3.5628]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0156, -0.9702,  0.3331, -0.1610,  1.0490],\n",
      "        [ 2.8285,  0.5225,  0.2235, -0.6211,  0.9447],\n",
      "        [-0.2074,  0.7426,  1.4320,  0.4411,  0.9739],\n",
      "        [-0.4013, -0.4541, -0.5546,  0.0996,  0.4245]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0029,  0.2836,  1.3898,  0.2905,  1.0738],\n",
      "        [-0.4132, -0.5049, -0.6513, -0.3820, -1.0320],\n",
      "        [ 1.4980,  2.3970,  0.9332,  1.4846,  2.0659],\n",
      "        [ 0.7423,  0.6817,  0.3099,  0.7291,  1.2149]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0156, -0.9702,  0.3331, -0.1610,  1.0490],\n",
      "        [ 2.8285,  0.5225,  0.2235, -0.6211,  0.9447],\n",
      "        [-0.2074,  0.7426,  1.4320,  0.4411,  0.9739],\n",
      "        [-0.4013, -0.4541, -0.5546,  0.0996,  0.4245]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2704],\n",
      "        [-2.3158],\n",
      "        [ 5.4722],\n",
      "        [-0.1908]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1834,  0.2630, -2.4493,  0.7425, -0.0767],\n",
      "        [ 1.4489, -0.8004, -1.8356,  0.4081,  0.0317],\n",
      "        [-0.3823,  0.9273,  0.2708,  1.1856, -0.3688],\n",
      "        [-1.2358, -1.4545,  1.5891, -0.3459,  0.3689]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1286,  0.2387,  0.0156, -0.3682, -0.3518],\n",
      "        [ 0.9558,  0.6838,  1.4590,  0.0455,  1.4836],\n",
      "        [-0.0829, -0.6187, -0.7756,  0.1578, -1.4224],\n",
      "        [ 0.2835, -0.0665,  0.3037, -0.1442,  0.9653]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1834,  0.2630, -2.4493,  0.7425, -0.0767],\n",
      "        [ 1.4489, -0.8004, -1.8356,  0.4081,  0.0317],\n",
      "        [-0.3823,  0.9273,  0.2708,  1.1856, -0.3688],\n",
      "        [-1.2358, -1.4545,  1.5891, -0.3459,  0.3689]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0697],\n",
      "        [-1.7751],\n",
      "        [-0.0404],\n",
      "        [ 0.6350]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7887,  1.4904, -0.7289, -1.2932,  0.9550],\n",
      "        [-0.3682, -1.2534, -1.6055, -1.9380,  0.6382],\n",
      "        [-0.0781, -0.3526, -0.3356,  0.1267,  1.7123],\n",
      "        [-1.9825,  0.2626,  0.5487, -0.0539, -1.0568]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1444,  0.5774,  0.8873, -0.2560,  0.1454],\n",
      "        [ 1.2590,  1.2394,  1.6246,  1.0830,  1.7939],\n",
      "        [-0.3674,  0.2476, -0.2810,  0.0347, -0.5404],\n",
      "        [ 0.6458, -0.7224, -0.3639, -0.8285, -0.8903]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7887,  1.4904, -0.7289, -1.2932,  0.9550],\n",
      "        [-0.3682, -1.2534, -1.6055, -1.9380,  0.6382],\n",
      "        [-0.0781, -0.3526, -0.3356,  0.1267,  1.7123],\n",
      "        [-1.9825,  0.2626,  0.5487, -0.0539, -1.0568]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4253],\n",
      "        [-5.5793],\n",
      "        [-0.8853],\n",
      "        [-0.6841]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8158, -0.6873,  0.3727,  0.6174, -0.0002],\n",
      "        [-0.0754,  1.0222,  1.3461,  0.1268,  0.2420],\n",
      "        [ 1.1436,  0.7395, -1.0413,  1.8650,  0.0203],\n",
      "        [-0.0464, -1.3770, -0.1650, -0.8994,  1.6242]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2220,  0.6752,  0.2411, -0.0166,  0.6308],\n",
      "        [-0.3527,  0.0045,  0.2968,  0.4193, -0.2372],\n",
      "        [ 0.3339,  0.4548,  0.5618, -0.1028,  0.8637],\n",
      "        [ 0.3211, -0.8226, -0.5977, -0.6778, -0.5248]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8158, -0.6873,  0.3727,  0.6174, -0.0002],\n",
      "        [-0.0754,  1.0222,  1.3461,  0.1268,  0.2420],\n",
      "        [ 1.1436,  0.7395, -1.0413,  1.8650,  0.0203],\n",
      "        [-0.0464, -1.3770, -0.1650, -0.8994,  1.6242]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5657],\n",
      "        [ 0.4264],\n",
      "        [-0.0412],\n",
      "        [ 0.9737]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3047,  0.7347,  1.0119,  0.8384,  1.0617],\n",
      "        [ 0.9405,  0.7001, -0.8562,  0.5350,  1.1292],\n",
      "        [ 0.6047,  0.7720,  1.0872, -0.2474,  0.4849],\n",
      "        [-0.1309,  0.0481,  0.9271, -0.2805,  0.1034]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0358,  0.8268,  0.2894,  0.4519,  0.9601],\n",
      "        [ 0.5249, -0.6846, -0.4652,  0.3283, -0.1704],\n",
      "        [ 0.4996,  0.2044,  0.4980,  0.6320,  1.2174],\n",
      "        [-0.2074, -0.2797, -0.0483, -1.0830, -1.1850]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3047,  0.7347,  1.0119,  0.8384,  1.0617],\n",
      "        [ 0.9405,  0.7001, -0.8562,  0.5350,  1.1292],\n",
      "        [ 0.6047,  0.7720,  1.0872, -0.2474,  0.4849],\n",
      "        [-0.1309,  0.0481,  0.9271, -0.2805,  0.1034]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2876],\n",
      "        [ 0.3959],\n",
      "        [ 1.4353],\n",
      "        [ 0.1501]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0186, -1.1873,  2.0370, -0.1051, -0.2290],\n",
      "        [ 0.1230, -0.2330, -1.6129, -2.3319, -0.7137],\n",
      "        [ 0.9080, -0.2025,  0.7363,  1.4078,  0.0287],\n",
      "        [ 0.8945, -1.7668,  1.1687,  1.1964, -0.1616]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2030, -0.9729, -0.7930, -0.6407, -1.0330],\n",
      "        [ 0.6739, -0.3462,  0.3023,  0.1882,  0.2956],\n",
      "        [-0.5381, -0.0115,  0.9165, -0.0572, -0.1062],\n",
      "        [-0.3816, -0.8595, -0.5506, -0.5838, -1.7706]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0186, -1.1873,  2.0370, -0.1051, -0.2290],\n",
      "        [ 0.1230, -0.2330, -1.6129, -2.3319, -0.7137],\n",
      "        [ 0.9080, -0.2025,  0.7363,  1.4078,  0.0287],\n",
      "        [ 0.8945, -1.7668,  1.1687,  1.1964, -0.1616]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2536],\n",
      "        [-0.9739],\n",
      "        [ 0.1049],\n",
      "        [ 0.1212]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3824,  0.9990,  0.2737, -0.3982, -0.2278],\n",
      "        [ 0.7763,  0.6156,  0.2562,  0.5646, -1.1387],\n",
      "        [-0.4500,  1.3828, -0.2848,  0.5766,  0.7415],\n",
      "        [ 0.4166, -1.5450,  0.2277,  0.5635, -1.0999]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6601, -0.5940, -0.6466, -0.7698, -1.3221],\n",
      "        [ 0.5726,  0.3706,  1.0782,  0.5044,  1.4104],\n",
      "        [-0.4580,  0.5287,  0.7580,  0.1245,  1.2447],\n",
      "        [ 0.3857, -0.3697, -0.0359, -0.2394, -0.8230]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3824,  0.9990,  0.2737, -0.3982, -0.2278],\n",
      "        [ 0.7763,  0.6156,  0.2562,  0.5646, -1.1387],\n",
      "        [-0.4500,  1.3828, -0.2848,  0.5766,  0.7415],\n",
      "        [ 0.4166, -1.5450,  0.2277,  0.5635, -1.0999]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0752],\n",
      "        [-0.3725],\n",
      "        [ 1.7160],\n",
      "        [ 1.4940]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0611,  1.1492, -0.7323, -0.0122, -1.2170],\n",
      "        [-1.0368, -0.1961, -1.0586,  0.5264,  0.5111],\n",
      "        [ 0.2570,  0.4919, -0.7041, -0.0375,  1.0853],\n",
      "        [-1.5807,  0.9735, -0.8613, -1.5325, -0.9745]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2132, -0.7250, -0.3446,  0.2682, -0.2363],\n",
      "        [ 1.0652, -0.1505, -0.1136, -0.5858,  0.0104],\n",
      "        [-0.7653,  0.6084, -0.3387, -0.7438, -1.0265],\n",
      "        [-0.8255, -1.3503, -0.7268, -0.7223, -1.7357]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0611,  1.1492, -0.7323, -0.0122, -1.2170],\n",
      "        [-1.0368, -0.1961, -1.0586,  0.5264,  0.5111],\n",
      "        [ 0.2570,  0.4919, -0.7041, -0.0375,  1.0853],\n",
      "        [-1.5807,  0.9735, -0.8613, -1.5325, -0.9745]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1428],\n",
      "        [-1.2576],\n",
      "        [-0.7451],\n",
      "        [ 3.4145]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7246,  0.8675,  1.1372,  0.6751, -0.6167],\n",
      "        [-1.4906,  1.0517,  0.0969, -0.1483,  0.1125],\n",
      "        [-0.8203, -0.3689,  1.9934,  0.6454, -0.3216],\n",
      "        [-1.3158, -0.7045,  0.5652,  0.1360, -0.6028]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4062,  0.7298, -0.3250,  0.2515,  0.4673],\n",
      "        [ 1.3778,  0.8099,  0.4244,  0.8493,  0.8715],\n",
      "        [-0.4272,  0.6311,  0.7239,  0.2641,  0.1706],\n",
      "        [-1.2573, -1.9708, -2.4771, -1.6727, -4.0891]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7246,  0.8675,  1.1372,  0.6751, -0.6167],\n",
      "        [-1.4906,  1.0517,  0.0969, -0.1483,  0.1125],\n",
      "        [-0.8203, -0.3689,  1.9934,  0.6454, -0.3216],\n",
      "        [-1.3158, -0.7045,  0.5652,  0.1360, -0.6028]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4395],\n",
      "        [-1.1887],\n",
      "        [ 1.6762],\n",
      "        [ 3.8802]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6248, -0.0079,  0.0462,  1.0144, -0.8566],\n",
      "        [-1.7806,  1.1830,  1.9774,  1.4149,  0.6782],\n",
      "        [-0.1240,  1.1508,  1.0095,  1.5534, -0.5825],\n",
      "        [ 1.2043, -0.6511,  1.0874,  0.5962,  1.6487]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3440, -0.1984, -0.4232, -0.1185, -0.1946],\n",
      "        [ 1.0926,  0.4695,  1.0268,  0.8029,  1.7283],\n",
      "        [-0.4898, -0.3831, -0.4053, -0.5875,  0.0618],\n",
      "        [ 0.7967, -0.6599,  0.0138, -0.1167,  0.2001]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6248, -0.0079,  0.0462,  1.0144, -0.8566],\n",
      "        [-1.7806,  1.1830,  1.9774,  1.4149,  0.6782],\n",
      "        [-0.1240,  1.1508,  1.0095,  1.5534, -0.5825],\n",
      "        [ 1.2043, -0.6511,  1.0874,  0.5962,  1.6487]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5304],\n",
      "        [ 2.9484],\n",
      "        [-1.7378],\n",
      "        [ 1.6643]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0958, -0.0080,  0.0611, -0.5787, -0.1531],\n",
      "        [ 0.4001,  1.4267,  1.2809,  0.2015,  0.7912],\n",
      "        [-0.1932,  0.1926,  0.1166, -1.0994, -1.3367],\n",
      "        [ 0.1964,  0.1129, -1.2020,  1.3602,  1.6770]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4568, -0.2160, -0.1066, -0.6497, -0.4158],\n",
      "        [-0.2139, -1.0674, -0.2630, -0.7782, -1.3049],\n",
      "        [-0.1901,  0.5293,  1.4964,  0.6753,  1.3097],\n",
      "        [-0.7561, -0.8397, -1.0977, -0.9952, -0.8495]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0958, -0.0080,  0.0611, -0.5787, -0.1531],\n",
      "        [ 0.4001,  1.4267,  1.2809,  0.2015,  0.7912],\n",
      "        [-0.1932,  0.1926,  0.1166, -1.0994, -1.3367],\n",
      "        [ 0.1964,  0.1129, -1.2020,  1.3602,  1.6770]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0657],\n",
      "        [-3.1345],\n",
      "        [-2.1799],\n",
      "        [-1.7022]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3139,  0.4694, -1.6629,  0.3753,  0.1379],\n",
      "        [-1.0897,  0.1700, -0.0417,  1.2895,  1.5628],\n",
      "        [-0.4946,  1.7424, -0.1751, -0.6280,  0.8579],\n",
      "        [-0.2008,  0.2856,  0.1359,  0.5541, -1.0731]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1742, -0.1179,  0.1021,  0.5082,  0.3239],\n",
      "        [ 1.0671,  1.2505,  1.3718,  0.6997,  2.0649],\n",
      "        [ 1.2158,  0.8812,  0.7208,  1.0460,  1.6670],\n",
      "        [-0.0448, -0.0183,  0.1296,  0.2856,  0.2131]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3139,  0.4694, -1.6629,  0.3753,  0.1379],\n",
      "        [-1.0897,  0.1700, -0.0417,  1.2895,  1.5628],\n",
      "        [-0.4946,  1.7424, -0.1751, -0.6280,  0.8579],\n",
      "        [-0.2008,  0.2856,  0.1359,  0.5541, -1.0731]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0649],\n",
      "        [ 3.1218],\n",
      "        [ 1.5809],\n",
      "        [-0.0490]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0183,  1.0776,  1.0087,  1.0988, -0.2244],\n",
      "        [-0.7428,  0.2185, -0.7835, -1.7873, -0.9106],\n",
      "        [-1.2304,  0.8451,  0.0113,  1.7181,  0.9058],\n",
      "        [ 0.5840,  1.6837, -1.7584,  0.7794,  1.2442]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3363, -0.5490,  0.7503,  0.1383,  0.0874],\n",
      "        [-0.5627, -0.4422, -0.6267,  0.0674, -1.1223],\n",
      "        [ 0.7957,  0.9857,  0.5723,  0.7244,  0.7449],\n",
      "        [ 0.1081, -0.2490,  0.2175, -0.0724, -0.7070]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0183,  1.0776,  1.0087,  1.0988, -0.2244],\n",
      "        [-0.7428,  0.2185, -0.7835, -1.7873, -0.9106],\n",
      "        [-1.2304,  0.8451,  0.0113,  1.7181,  0.9058],\n",
      "        [ 0.5840,  1.6837, -1.7584,  0.7794,  1.2442]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3038],\n",
      "        [ 1.7139],\n",
      "        [ 1.7798],\n",
      "        [-1.6746]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0233, -1.3957,  0.1997,  0.5502, -0.7076],\n",
      "        [ 0.7177,  0.0850, -2.2550, -0.5955,  0.2394],\n",
      "        [-0.5322, -0.2884,  0.3765,  0.6530,  1.2925],\n",
      "        [-0.0160, -0.4062,  0.6802,  0.4025, -1.0760]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4527, -0.6845, -0.2049,  0.2734, -0.7062],\n",
      "        [-0.3107, -1.6541, -1.2066, -1.1933, -1.8022],\n",
      "        [-0.2434,  0.0395, -0.3577,  0.4736, -0.5006],\n",
      "        [ 0.5850,  1.1433,  0.2671,  1.1235,  1.4165]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0233, -1.3957,  0.1997,  0.5502, -0.7076],\n",
      "        [ 0.7177,  0.0850, -2.2550, -0.5955,  0.2394],\n",
      "        [-0.5322, -0.2884,  0.3765,  0.6530,  1.2925],\n",
      "        [-0.0160, -0.4062,  0.6802,  0.4025, -1.0760]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5541],\n",
      "        [ 2.6364],\n",
      "        [-0.3542],\n",
      "        [-1.3642]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3117,  0.7744,  0.7672, -0.3416,  0.3008],\n",
      "        [-0.3318,  0.0517,  0.5192,  1.0665,  0.0944],\n",
      "        [ 1.3369, -0.9540,  0.7128, -0.0443,  0.5994],\n",
      "        [-0.5327,  1.1899, -0.6433,  0.2270, -0.3281]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9473, -0.5985, -0.4298, -0.9302, -1.5527],\n",
      "        [-1.7613, -1.1565, -1.9342, -1.7119, -3.4829],\n",
      "        [ 0.1686,  0.6539, -0.0285,  0.6173,  0.2543],\n",
      "        [ 0.9079,  1.2646,  1.4748,  0.5976,  2.2234]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3117,  0.7744,  0.7672, -0.3416,  0.3008],\n",
      "        [-0.3318,  0.0517,  0.5192,  1.0665,  0.0944],\n",
      "        [ 1.3369, -0.9540,  0.7128, -0.0443,  0.5994],\n",
      "        [-0.5327,  1.1899, -0.6433,  0.2270, -0.3281]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6473],\n",
      "        [-2.6340],\n",
      "        [-0.2937],\n",
      "        [-0.5215]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5832, -0.7462,  0.4327,  1.2521,  1.1511],\n",
      "        [ 0.6217, -1.1507,  1.6841, -0.0548,  0.0928],\n",
      "        [ 0.0874,  1.0329,  0.0878, -0.6586, -0.4594],\n",
      "        [-0.5072, -0.6701, -1.8471,  0.4751,  3.3359]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5880, -0.0670, -0.2417, -0.5065, -1.0997],\n",
      "        [-0.7379, -1.3464, -0.9837, -0.8622, -1.3292],\n",
      "        [-0.2708,  0.5470,  0.4070,  0.3324,  1.2358],\n",
      "        [ 0.5844,  1.7327,  1.4231,  0.5948,  1.5596]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5832, -0.7462,  0.4327,  1.2521,  1.1511],\n",
      "        [ 0.6217, -1.1507,  1.6841, -0.0548,  0.0928],\n",
      "        [ 0.0874,  1.0329,  0.0878, -0.6586, -0.4594],\n",
      "        [-0.5072, -0.6701, -1.8471,  0.4751,  3.3359]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6118],\n",
      "        [-0.6423],\n",
      "        [-0.2096],\n",
      "        [ 1.3992]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1868,  0.6684, -1.0877, -3.0891, -1.1489],\n",
      "        [ 1.0163,  1.1023, -0.2609, -0.2611, -0.0641],\n",
      "        [ 0.6954, -1.7527, -0.8538, -0.7800, -1.1291],\n",
      "        [-0.7256, -0.9676,  0.7080, -1.0286,  0.7812]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0956,  0.8715,  0.4959, -0.0129,  0.0335],\n",
      "        [-0.2185, -1.1472, -1.1269, -1.1834, -0.7601],\n",
      "        [ 0.0586,  0.9587,  0.0931,  0.0852,  0.5663],\n",
      "        [ 0.6347,  0.8757,  0.1633,  0.6178,  0.8470]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1868,  0.6684, -1.0877, -3.0891, -1.1489],\n",
      "        [ 1.0163,  1.1023, -0.2609, -0.2611, -0.0641],\n",
      "        [ 0.6954, -1.7527, -0.8538, -0.7800, -1.1291],\n",
      "        [-0.7256, -0.9676,  0.7080, -1.0286,  0.7812]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0266],\n",
      "        [-0.8350],\n",
      "        [-2.4248],\n",
      "        [-1.1660]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0442,  0.3652, -0.2631, -0.1921,  1.2525],\n",
      "        [ 0.8026,  1.2549,  1.3693,  2.2453,  0.1981],\n",
      "        [-1.7928,  1.2659,  1.4508,  2.6561,  0.7656],\n",
      "        [-0.4447, -0.9987,  1.0636,  0.6780,  0.7108]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1652, -0.1174, -0.1439, -0.2031,  0.5630],\n",
      "        [-0.1046, -0.5806, -0.3691, -0.8569, -1.6555],\n",
      "        [ 0.5873,  1.1583,  1.3383,  1.1953,  1.8237],\n",
      "        [ 1.1990,  1.3507,  0.9974,  1.3745,  2.0344]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0442,  0.3652, -0.2631, -0.1921,  1.2525],\n",
      "        [ 0.8026,  1.2549,  1.3693,  2.2453,  0.1981],\n",
      "        [-1.7928,  1.2659,  1.4508,  2.6561,  0.7656],\n",
      "        [-0.4447, -0.9987,  1.0636,  0.6780,  0.7108]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5667],\n",
      "        [-3.5698],\n",
      "        [ 6.9262],\n",
      "        [ 1.5567]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0570,  0.8248, -0.3678,  2.1987,  0.9922],\n",
      "        [-0.4749,  1.8948,  0.0985, -1.2055, -1.1600],\n",
      "        [-1.7913,  0.4284, -0.2103,  0.7069, -0.0340],\n",
      "        [-2.4411,  0.2970, -1.1502,  0.9590,  1.0252]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0701,  0.1186, -0.0362, -0.8663, -0.0764],\n",
      "        [ 1.5544,  0.9204,  1.1422,  0.6342,  1.6108],\n",
      "        [-0.7691, -1.5085, -1.7286, -1.8773, -3.4832],\n",
      "        [ 0.7754,  0.5064,  1.2145,  1.0992,  0.4393]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0570,  0.8248, -0.3678,  2.1987,  0.9922],\n",
      "        [-0.4749,  1.8948,  0.0985, -1.2055, -1.1600],\n",
      "        [-1.7913,  0.4284, -0.2103,  0.7069, -0.0340],\n",
      "        [-2.4411,  0.2970, -1.1502,  0.9590,  1.0252]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9433],\n",
      "        [-1.5147],\n",
      "        [-0.1136],\n",
      "        [-1.6349]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8574,  0.0385, -0.1158, -1.2432,  1.7218],\n",
      "        [-1.1493,  0.5515,  0.1345, -1.5576,  2.1004],\n",
      "        [ 0.1320,  0.0132,  0.1483, -0.3663,  1.1934],\n",
      "        [-0.9235, -1.5358,  0.4009,  1.5157, -0.9559]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4647,  0.6419,  0.8716,  0.7861,  1.5097],\n",
      "        [ 1.8649,  1.4427,  1.7726,  1.3179,  2.2475],\n",
      "        [-1.7663, -1.9175, -1.0881, -2.2826, -3.2518],\n",
      "        [ 0.7588,  1.3659,  1.3818,  1.2060,  1.6967]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8574,  0.0385, -0.1158, -1.2432,  1.7218],\n",
      "        [-1.1493,  0.5515,  0.1345, -1.5576,  2.1004],\n",
      "        [ 0.1320,  0.0132,  0.1483, -0.3663,  1.1934],\n",
      "        [-0.9235, -1.5358,  0.4009,  1.5157, -0.9559]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1474],\n",
      "        [ 1.5587],\n",
      "        [-3.4644],\n",
      "        [-2.0385]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1972, -0.2573, -0.1298,  0.5029,  0.1934],\n",
      "        [ 1.0382,  1.2886,  0.6403, -1.5098, -1.0303],\n",
      "        [-0.5170, -0.6988,  0.9587, -0.9556,  2.0298],\n",
      "        [-0.3268,  1.3912, -0.4933,  2.2313, -0.0688]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1770,  0.1503,  0.0847, -0.0946, -1.4895],\n",
      "        [ 1.1864,  1.3272,  1.0127,  1.5429,  0.9071],\n",
      "        [-0.4430, -0.1819,  0.0959, -0.8367, -1.2122],\n",
      "        [ 0.3282, -0.1621,  0.1403,  0.1354, -0.3465]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1972, -0.2573, -0.1298,  0.5029,  0.1934],\n",
      "        [ 1.0382,  1.2886,  0.6403, -1.5098, -1.0303],\n",
      "        [-0.5170, -0.6988,  0.9587, -0.9556,  2.0298],\n",
      "        [-0.3268,  1.3912, -0.4933,  2.2313, -0.0688]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5973],\n",
      "        [ 0.3266],\n",
      "        [-1.2129],\n",
      "        [-0.0760]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0612,  1.1102, -0.3090, -0.9630, -0.2915],\n",
      "        [-0.9733,  0.2221, -1.0933, -1.3478, -0.7110],\n",
      "        [-1.6444, -0.8626, -0.1678,  0.7434, -0.0213],\n",
      "        [ 0.0617,  0.4277, -0.4258, -0.0767,  0.1734]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3249, -0.4303, -0.0205, -0.0912, -1.0306],\n",
      "        [ 0.8552,  0.1427,  0.2689,  0.1020,  1.3617],\n",
      "        [-0.1156, -0.5634, -0.4561, -0.6389, -0.9776],\n",
      "        [ 0.0934,  0.7984, -0.3299,  0.0482, -0.3561]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0612,  1.1102, -0.3090, -0.9630, -0.2915],\n",
      "        [-0.9733,  0.2221, -1.0933, -1.3478, -0.7110],\n",
      "        [-1.6444, -0.8626, -0.1678,  0.7434, -0.0213],\n",
      "        [ 0.0617,  0.4277, -0.4258, -0.0767,  0.1734]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2616],\n",
      "        [-2.2003],\n",
      "        [ 0.2985],\n",
      "        [ 0.4223]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3479, -1.6338, -0.9497,  0.1984,  0.4175],\n",
      "        [ 1.2047,  1.8751,  0.1092,  2.1913, -0.5259],\n",
      "        [-0.1151,  0.7564,  0.2693, -0.1846,  1.1492],\n",
      "        [-0.6290,  1.7389,  0.9829,  0.7711,  0.3199]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0142, -0.8019, -0.7153, -0.0349, -0.2358],\n",
      "        [ 1.2381,  0.8841,  1.6543,  1.5168,  2.1859],\n",
      "        [ 0.1116, -0.9065, -0.8161, -0.3526, -1.3827],\n",
      "        [ 0.1184,  0.1136,  0.5665,  0.1124, -0.4588]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3479, -1.6338, -0.9497,  0.1984,  0.4175],\n",
      "        [ 1.2047,  1.8751,  0.1092,  2.1913, -0.5259],\n",
      "        [-0.1151,  0.7564,  0.2693, -0.1846,  1.1492],\n",
      "        [-0.6290,  1.7389,  0.9829,  0.7711,  0.3199]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8792],\n",
      "        [ 5.5042],\n",
      "        [-2.4422],\n",
      "        [ 0.6197]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1723,  1.2749,  1.7999,  1.3429,  1.3443],\n",
      "        [ 1.7863,  1.1015,  2.1054, -1.3996, -2.9683],\n",
      "        [-2.7064,  0.1705,  2.8704,  1.1113,  0.3679],\n",
      "        [-0.1311,  0.4818,  0.6041, -1.4385,  1.1358]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7508, -0.7174, -1.2089, -1.5443, -1.9377],\n",
      "        [-0.4594, -0.6927, -1.0043, -1.3896, -2.0357],\n",
      "        [ 0.8373,  0.6048,  0.3677,  0.6860,  1.1624],\n",
      "        [ 0.0510,  0.5931,  0.2350, -0.2229,  0.0232]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1723,  1.2749,  1.7999,  1.3429,  1.3443],\n",
      "        [ 1.7863,  1.1015,  2.1054, -1.3996, -2.9683],\n",
      "        [-2.7064,  0.1705,  2.8704,  1.1113,  0.3679],\n",
      "        [-0.1311,  0.4818,  0.6041, -1.4385,  1.1358]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-7.6400],\n",
      "        [ 4.2895],\n",
      "        [ 0.0825],\n",
      "        [ 0.7681]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9959, -0.1254,  0.6322,  0.7521, -0.0096],\n",
      "        [ 1.5626, -1.2386,  2.2149, -0.1693, -0.0247],\n",
      "        [-0.8331,  1.0218,  0.7435, -2.3407,  0.7700],\n",
      "        [ 0.4198, -0.8141,  0.9936,  0.4934,  2.0370]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6989,  1.4451,  2.0935,  1.1857,  2.9188],\n",
      "        [-1.5141, -1.6872, -2.4395, -2.3154, -4.2758],\n",
      "        [ 0.5199, -0.3193, -1.1264, -0.4286,  0.9859],\n",
      "        [-0.4531,  0.0885,  0.4093,  0.7830, -0.0333]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9959, -0.1254,  0.6322,  0.7521, -0.0096],\n",
      "        [ 1.5626, -1.2386,  2.2149, -0.1693, -0.0247],\n",
      "        [-0.8331,  1.0218,  0.7435, -2.3407,  0.7700],\n",
      "        [ 0.4198, -0.8141,  0.9936,  0.4934,  2.0370]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.6980],\n",
      "        [-5.1820],\n",
      "        [ 0.1655],\n",
      "        [ 0.4631]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0357,  0.9407,  0.6120,  1.8246,  1.4345],\n",
      "        [ 0.0885,  0.2983,  0.4763, -0.4539, -0.9981],\n",
      "        [-0.7276,  1.6017,  0.0182,  1.6905,  1.0605],\n",
      "        [-0.0229,  0.2359, -0.8989,  3.4999, -0.8567]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8553,  1.4444,  0.9200,  1.0283,  1.8523],\n",
      "        [-0.4057, -1.3003, -1.4362, -1.0572, -0.9027],\n",
      "        [ 0.2263, -1.3176, -0.0819, -0.6645, -1.0169],\n",
      "        [ 0.1063, -0.2897,  0.2916,  0.8834,  0.1507]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0357,  0.9407,  0.6120,  1.8246,  1.4345],\n",
      "        [ 0.0885,  0.2983,  0.4763, -0.4539, -0.9981],\n",
      "        [-0.7276,  1.6017,  0.0182,  1.6905,  1.0605],\n",
      "        [-0.0229,  0.2359, -0.8989,  3.4999, -0.8567]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 6.4244],\n",
      "        [ 0.2731],\n",
      "        [-4.4782],\n",
      "        [ 2.6297]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2146,  0.0332,  1.3709,  1.8807,  0.1517],\n",
      "        [ 0.0295,  0.9926,  1.7759,  0.6770,  1.2129],\n",
      "        [-1.7775, -0.4963,  1.1430, -0.3548, -0.1799],\n",
      "        [-0.8876,  0.8129,  0.4680, -0.2378, -1.0198]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.5226, -1.0311, -1.7972, -2.1756, -3.2097],\n",
      "        [ 0.2347, -1.1383, -0.7226, -0.6020, -0.8048],\n",
      "        [ 0.5693,  1.8868,  1.6698,  1.6755,  2.2749],\n",
      "        [-0.4782, -0.6803, -1.2372, -1.3127, -2.0798]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2146,  0.0332,  1.3709,  1.8807,  0.1517],\n",
      "        [ 0.0295,  0.9926,  1.7759,  0.6770,  1.2129],\n",
      "        [-1.7775, -0.4963,  1.1430, -0.3548, -0.1799],\n",
      "        [-0.8876,  0.8129,  0.4680, -0.2378, -1.0198]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-7.4035],\n",
      "        [-3.7900],\n",
      "        [-1.0436],\n",
      "        [ 1.7254]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7370, -0.2232,  1.9173,  0.5909,  0.4220],\n",
      "        [ 0.5158, -0.0274,  0.2070,  0.6097, -0.3033],\n",
      "        [ 0.2622,  1.2907,  0.0111, -0.8299, -0.0687],\n",
      "        [-0.8006, -0.3185,  0.6855, -1.3772,  0.9565]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2175,  1.3683,  1.4842,  0.7882,  1.9034],\n",
      "        [ 0.9883,  1.0411,  1.2664,  1.0902,  1.9431],\n",
      "        [ 1.8176,  1.9185,  1.2050,  1.8252,  2.5582],\n",
      "        [-1.1138, -0.9225, -1.4210, -1.7471, -2.7157]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7370, -0.2232,  1.9173,  0.5909,  0.4220],\n",
      "        [ 0.5158, -0.0274,  0.2070,  0.6097, -0.3033],\n",
      "        [ 0.2622,  1.2907,  0.0111, -0.8299, -0.0687],\n",
      "        [-0.8006, -0.3185,  0.6855, -1.3772,  0.9565]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.9119],\n",
      "        [ 0.8187],\n",
      "        [ 1.2757],\n",
      "        [ 0.0199]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1435, -0.7690,  0.7997, -0.5917,  2.0637],\n",
      "        [-0.4265, -0.8221, -0.3540,  0.4976, -0.8144],\n",
      "        [ 0.6271, -1.0320,  0.2883, -1.0643, -0.3975],\n",
      "        [ 0.1263, -0.3464,  1.2469,  0.4847,  0.1966]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1033,  0.1384, -0.2433,  0.0588, -0.1516],\n",
      "        [ 0.9066,  0.6218,  1.6889,  0.3451,  1.4043],\n",
      "        [ 0.1842,  0.8091, -1.0609,  0.2053,  0.5916],\n",
      "        [ 0.5690,  0.2936,  0.0058, -0.5231,  0.3080]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1435, -0.7690,  0.7997, -0.5917,  2.0637],\n",
      "        [-0.4265, -0.8221, -0.3540,  0.4976, -0.8144],\n",
      "        [ 0.6271, -1.0320,  0.2883, -1.0643, -0.3975],\n",
      "        [ 0.1263, -0.3464,  1.2469,  0.4847,  0.1966]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6339],\n",
      "        [-2.4678],\n",
      "        [-1.4790],\n",
      "        [-0.2156]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6108,  0.8527,  0.4723,  0.9101,  0.9525],\n",
      "        [-0.8742,  0.0731,  0.5249, -1.3929, -0.1933],\n",
      "        [-1.3366,  0.1464, -0.6922,  1.4790,  1.4743],\n",
      "        [ 0.4733, -0.6930,  1.5853,  0.2437,  0.4053]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0283,  0.0441,  0.0397,  0.5506,  0.1959],\n",
      "        [ 1.3709,  2.4037,  2.1450,  1.3246,  2.3379],\n",
      "        [ 0.1703,  0.2310,  0.1531,  0.1092,  1.1078],\n",
      "        [ 0.3953, -0.3671, -0.2098, -0.2332, -0.3718]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6108,  0.8527,  0.4723,  0.9101,  0.9525],\n",
      "        [-0.8742,  0.0731,  0.5249, -1.3929, -0.1933],\n",
      "        [-1.3366,  0.1464, -0.6922,  1.4790,  1.4743],\n",
      "        [ 0.4733, -0.6930,  1.5853,  0.2437,  0.4053]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7267],\n",
      "        [-2.1939],\n",
      "        [ 1.4950],\n",
      "        [-0.0985]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1955,  0.3908,  2.7953, -1.3738,  1.1479],\n",
      "        [-1.1832, -0.7827,  0.8178,  1.5270, -0.3670],\n",
      "        [ 0.5669,  0.5727, -0.3192, -0.7659,  0.4967],\n",
      "        [ 0.6024, -0.4479,  1.4700, -1.5961, -1.0290]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0779,  0.3079,  0.3623,  0.8660,  0.4747],\n",
      "        [ 0.5313, -0.3098, -0.5674,  0.3978,  0.6840],\n",
      "        [ 0.1478,  0.0854,  0.2938,  0.1133, -0.4734],\n",
      "        [ 0.1554,  0.0500,  0.8929, -0.0859,  0.3808]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1955,  0.3908,  2.7953, -1.3738,  1.1479],\n",
      "        [-1.1832, -0.7827,  0.8178,  1.5270, -0.3670],\n",
      "        [ 0.5669,  0.5727, -0.3192, -0.7659,  0.4967],\n",
      "        [ 0.6024, -0.4479,  1.4700, -1.5961, -1.0290]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6592],\n",
      "        [-0.4937],\n",
      "        [-0.2830],\n",
      "        [ 1.1290]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0458,  1.3972, -0.5759, -0.9208, -0.0014],\n",
      "        [-0.7774, -0.5739, -0.9314,  1.0418,  0.0726],\n",
      "        [ 0.4041, -1.4995,  0.0989,  0.5609, -0.8458],\n",
      "        [ 1.2405,  0.7530,  0.2532, -1.7384, -0.2199]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0717,  0.2859, -0.1358,  0.2928, -0.0937],\n",
      "        [ 0.5456, -0.0539,  0.2680, -0.0491, -0.4859],\n",
      "        [-0.2728, -0.0034, -0.1250,  0.0831, -0.4602],\n",
      "        [-0.3281,  0.0868, -0.4594, -0.1989, -1.4364]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0458,  1.3972, -0.5759, -0.9208, -0.0014],\n",
      "        [-0.7774, -0.5739, -0.9314,  1.0418,  0.0726],\n",
      "        [ 0.4041, -1.4995,  0.0989,  0.5609, -0.8458],\n",
      "        [ 1.2405,  0.7530,  0.2532, -1.7384, -0.2199]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2833],\n",
      "        [-0.7292],\n",
      "        [ 0.3183],\n",
      "        [ 0.2036]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3716,  0.8664, -0.9211, -0.9909,  0.4841],\n",
      "        [-0.0724, -0.1358, -0.2724,  0.9044, -0.1543],\n",
      "        [-0.7328,  0.6363, -1.5983,  1.8810, -0.3813],\n",
      "        [ 1.3436, -0.8926, -0.6416, -1.0183, -1.1697]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1013,  0.8515,  0.3161,  0.5177,  0.7266],\n",
      "        [ 0.5705, -0.3231, -0.3838, -0.9375, -0.0533],\n",
      "        [ 0.6782,  0.2144,  0.2674,  0.8396, -0.6454],\n",
      "        [-0.1545, -0.2615, -0.0631, -0.1241, -0.5611]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3716,  0.8664, -0.9211, -0.9909,  0.4841],\n",
      "        [-0.0724, -0.1358, -0.2724,  0.9044, -0.1543],\n",
      "        [-0.7328,  0.6363, -1.5983,  1.8810, -0.3813],\n",
      "        [ 1.3436, -0.8926, -0.6416, -1.0183, -1.1697]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4244],\n",
      "        [-0.7325],\n",
      "        [ 1.0375],\n",
      "        [ 0.8489]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[ 1.1099, -0.8500,  0.4814,  0.8358,  3.0486],\n",
      "        [-0.3656,  0.2178, -0.0034, -0.0320,  0.1770],\n",
      "        [ 0.2628, -1.4841,  0.9469, -0.3436, -0.1551],\n",
      "        [ 1.4220,  0.8190, -0.4735, -0.5839,  1.8420]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3498,  0.4087,  0.8625,  0.3828,  0.0699],\n",
      "        [ 0.6522,  0.0617, -0.6658,  0.0679,  0.2850],\n",
      "        [-0.3036, -0.3473, -0.2161, -0.1921, -0.7068],\n",
      "        [-0.7868, -0.5081, -0.4218, -0.0219, -0.4019]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1099, -0.8500,  0.4814,  0.8358,  3.0486],\n",
      "        [-0.3656,  0.2178, -0.0034, -0.0320,  0.1770],\n",
      "        [ 0.2628, -1.4841,  0.9469, -0.3436, -0.1551],\n",
      "        [ 1.4220,  0.8190, -0.4735, -0.5839,  1.8420]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2126],\n",
      "        [-0.1744],\n",
      "        [ 0.4068],\n",
      "        [-2.0628]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9713,  0.1342,  0.7309,  0.1851, -0.3056],\n",
      "        [ 0.9674, -0.9773, -1.8524, -0.8935,  1.1415],\n",
      "        [-0.7607, -0.8937,  0.1337, -0.4458,  1.4487],\n",
      "        [ 1.0851,  0.1569,  1.8011,  2.0920, -0.3999]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4519,  0.9987,  0.5776,  0.2591,  0.5639],\n",
      "        [ 0.3764, -1.0537, -0.4685, -0.0822, -0.1734],\n",
      "        [-0.6679,  0.1243,  0.2061,  0.5450,  0.3293],\n",
      "        [ 0.6671,  1.4118,  1.0195,  0.9104,  1.0715]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9713,  0.1342,  0.7309,  0.1851, -0.3056],\n",
      "        [ 0.9674, -0.9773, -1.8524, -0.8935,  1.1415],\n",
      "        [-0.7607, -0.8937,  0.1337, -0.4458,  1.4487],\n",
      "        [ 1.0851,  0.1569,  1.8011,  2.0920, -0.3999]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8709],\n",
      "        [ 2.1373],\n",
      "        [ 0.6586],\n",
      "        [ 4.2577]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8958, -2.8815,  0.0066,  1.2077,  1.2781],\n",
      "        [ 1.1806, -0.2561,  1.8147,  0.7836,  0.3194],\n",
      "        [-0.0578, -0.9038, -1.0948,  1.2937,  0.4299],\n",
      "        [-0.0391,  0.4691,  1.8178,  1.7656,  0.5783]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3998,  0.6088,  0.1056,  0.2103, -0.3050],\n",
      "        [-0.8832, -0.4915, -0.9906, -1.3496, -1.5346],\n",
      "        [ 0.1863,  0.2922,  0.4925,  0.1372,  0.4018],\n",
      "        [-1.0258, -1.2852, -0.9928, -0.7922, -2.5266]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8958, -2.8815,  0.0066,  1.2077,  1.2781],\n",
      "        [ 1.1806, -0.2561,  1.8147,  0.7836,  0.3194],\n",
      "        [-0.0578, -0.9038, -1.0948,  1.2937,  0.4299],\n",
      "        [-0.0391,  0.4691,  1.8178,  1.7656,  0.5783]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5312],\n",
      "        [-4.2622],\n",
      "        [-0.4639],\n",
      "        [-5.2273]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2473,  0.6501,  1.4025,  0.9498, -0.1953],\n",
      "        [-1.4115, -0.1303, -0.0018,  0.4941, -1.6502],\n",
      "        [ 0.1714, -1.2117,  1.4519, -0.0041, -1.2805],\n",
      "        [-0.9950,  1.2447, -0.8583, -0.0687,  2.1775]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2190,  0.9748,  1.6305,  0.6886,  1.4443],\n",
      "        [ 0.8407,  0.7695,  0.9973,  0.2935,  1.8772],\n",
      "        [-0.1404,  0.8874,  0.7811,  0.1818,  0.1318],\n",
      "        [ 0.7748,  0.5828,  0.9194,  1.0915,  0.9688]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2473,  0.6501,  1.4025,  0.9498, -0.1953],\n",
      "        [-1.4115, -0.1303, -0.0018,  0.4941, -1.6502],\n",
      "        [ 0.1714, -1.2117,  1.4519, -0.0041, -1.2805],\n",
      "        [-0.9950,  1.2447, -0.8583, -0.0687,  2.1775]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.3465],\n",
      "        [-4.2416],\n",
      "        [-0.1349],\n",
      "        [ 1.1998]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2626, -0.5214, -0.4886,  0.3087, -1.3163],\n",
      "        [-1.9208,  0.5441,  1.2280,  1.1159,  0.9285],\n",
      "        [-1.3367, -0.4871, -0.7333,  1.2737,  1.1117],\n",
      "        [ 0.3393,  0.5757, -0.0293,  0.2633,  1.0202]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9081, -0.3072, -0.0603, -1.0340, -1.1033],\n",
      "        [ 2.6123,  1.7762,  2.2481,  1.4933,  3.1483],\n",
      "        [-0.5819,  0.1504,  0.4032,  0.7106,  0.6274],\n",
      "        [ 0.6296,  0.1777, -0.3867, -0.1391,  0.4072]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2626, -0.5214, -0.4886,  0.3087, -1.3163],\n",
      "        [-1.9208,  0.5441,  1.2280,  1.1159,  0.9285],\n",
      "        [-1.3367, -0.4871, -0.7333,  1.2737,  1.1117],\n",
      "        [ 0.3393,  0.5757, -0.0293,  0.2633,  1.0202]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4691],\n",
      "        [ 3.2991],\n",
      "        [ 2.0115],\n",
      "        [ 0.7061]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7985, -0.8359,  0.8678, -0.9754,  0.2592],\n",
      "        [ 0.9957, -0.1650, -0.4175,  2.1001, -1.7922],\n",
      "        [-1.2931,  1.2895, -1.2141,  0.7759, -0.2682],\n",
      "        [ 0.5019,  1.6822,  0.2094,  1.4516, -1.3169]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6534, -1.9183, -1.2432, -1.3337, -2.9140],\n",
      "        [-0.1559, -0.5146,  0.1870,  0.5185, -0.3456],\n",
      "        [-0.8585, -0.2592, -0.0233, -0.6799, -1.9577],\n",
      "        [ 0.0275,  0.0095, -0.3342,  0.6198, -0.0920]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7985, -0.8359,  0.8678, -0.9754,  0.2592],\n",
      "        [ 0.9957, -0.1650, -0.4175,  2.1001, -1.7922],\n",
      "        [-1.2931,  1.2895, -1.2141,  0.7759, -0.2682],\n",
      "        [ 0.5019,  1.6822,  0.2094,  1.4516, -1.3169]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5487],\n",
      "        [ 1.5600],\n",
      "        [ 0.8017],\n",
      "        [ 0.9806]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7171, -1.3325, -1.4296, -0.8519,  0.1887],\n",
      "        [-0.5645,  0.3630,  2.8509,  0.7529,  0.1558],\n",
      "        [-1.8713, -0.1214,  0.8183,  0.7981,  0.5097],\n",
      "        [-0.8790, -1.3949, -1.5244,  1.8979, -1.3243]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9043, -0.9852, -1.4634, -0.8337, -3.0551],\n",
      "        [-0.1375, -1.5042, -0.5503, -0.7942, -0.9412],\n",
      "        [-0.7017, -1.0273, -0.6971, -0.3329, -2.0497],\n",
      "        [-0.1987, -0.3508,  0.1386, -0.1551, -0.4421]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7171, -1.3325, -1.4296, -0.8519,  0.1887],\n",
      "        [-0.5645,  0.3630,  2.8509,  0.7529,  0.1558],\n",
      "        [-1.8713, -0.1214,  0.8183,  0.7981,  0.5097],\n",
      "        [-0.8790, -1.3949, -1.5244,  1.8979, -1.3243]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.1871],\n",
      "        [-2.7819],\n",
      "        [-0.4430],\n",
      "        [ 0.7438]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0103, -0.1643,  0.9553,  1.5575,  0.9220],\n",
      "        [-0.2732, -1.2683, -0.3633,  0.3594, -1.5313],\n",
      "        [ 0.7240, -1.5218, -0.9397,  0.2496,  0.7808],\n",
      "        [-0.1513, -1.4007, -0.1365,  0.5609,  0.4750]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7576,  0.4769,  0.3652, -0.5004,  0.0966],\n",
      "        [ 1.1231,  0.4695,  1.4881,  0.4696,  0.7667],\n",
      "        [-0.6630, -0.0092, -0.6107, -0.9242, -1.3174],\n",
      "        [-0.4872,  0.2893, -0.9327, -0.9972,  0.0764]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0103, -0.1643,  0.9553,  1.5575,  0.9220],\n",
      "        [-0.2732, -1.2683, -0.3633,  0.3594, -1.5313],\n",
      "        [ 0.7240, -1.5218, -0.9397,  0.2496,  0.7808],\n",
      "        [-0.1513, -1.4007, -0.1365,  0.5609,  0.4750]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4120],\n",
      "        [-2.4482],\n",
      "        [-1.1514],\n",
      "        [-0.7272]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3786,  0.9914,  1.8622, -1.9770,  2.0554],\n",
      "        [-1.1384,  1.1478, -0.9305,  1.1060,  1.6191],\n",
      "        [-0.5830, -1.1671,  1.3436, -1.4145, -0.3508],\n",
      "        [-0.2038, -0.3938,  0.6398,  0.0232,  0.4746]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6737,  0.0348, -0.5913, -0.5593, -0.0223],\n",
      "        [ 1.3114,  2.1118,  1.6086,  1.2257,  2.2214],\n",
      "        [-0.4102, -0.3386,  0.0663,  0.2446,  0.4241],\n",
      "        [ 0.0095, -0.0695, -0.1560, -0.0907,  0.8252]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3786,  0.9914,  1.8622, -1.9770,  2.0554],\n",
      "        [-1.1384,  1.1478, -0.9305,  1.1060,  1.6191],\n",
      "        [-0.5830, -1.1671,  1.3436, -1.4145, -0.3508],\n",
      "        [-0.2038, -0.3938,  0.6398,  0.0232,  0.4746]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2617],\n",
      "        [ 4.3866],\n",
      "        [ 0.2287],\n",
      "        [ 0.3152]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3642, -0.4145,  0.9190,  1.0120, -0.1947],\n",
      "        [-0.8314, -0.7455, -0.9606,  0.8033,  0.5337],\n",
      "        [ 0.5710, -0.7480, -0.5484,  1.1421, -1.1643],\n",
      "        [-0.3593, -0.1178,  0.2501, -1.2422,  0.7108]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1151, -0.6529,  0.1994, -0.3397,  0.0588],\n",
      "        [ 0.0004, -0.7131, -0.6637, -0.6835, -0.7266],\n",
      "        [-0.3218, -1.0069, -1.1876, -0.6867, -0.1223],\n",
      "        [-0.2155,  0.3084,  0.6576,  0.0290,  0.4516]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3642, -0.4145,  0.9190,  1.0120, -0.1947],\n",
      "        [-0.8314, -0.7455, -0.9606,  0.8033,  0.5337],\n",
      "        [ 0.5710, -0.7480, -0.5484,  1.1421, -1.1643],\n",
      "        [-0.3593, -0.1178,  0.2501, -1.2422,  0.7108]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0567],\n",
      "        [ 0.2320],\n",
      "        [ 0.5789],\n",
      "        [ 0.4906]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1394,  1.6975,  2.0896,  0.4553,  0.6974],\n",
      "        [-0.6034,  0.3029, -0.0118,  1.7539,  0.8700],\n",
      "        [-0.2514, -0.6382,  1.6370,  0.6739, -0.0261],\n",
      "        [ 0.4051,  0.3325,  0.0117,  0.9448, -0.6801]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2510,  0.1017,  0.1155, -0.1879, -0.1230],\n",
      "        [-0.0935, -0.1061,  0.6098,  0.8805,  0.2283],\n",
      "        [-0.2463, -0.4304, -1.0060, -0.2950, -0.2071],\n",
      "        [ 0.0398, -0.1684,  0.2435,  0.1243,  0.0779]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1394,  1.6975,  2.0896,  0.4553,  0.6974],\n",
      "        [-0.6034,  0.3029, -0.0118,  1.7539,  0.8700],\n",
      "        [-0.2514, -0.6382,  1.6370,  0.6739, -0.0261],\n",
      "        [ 0.4051,  0.3325,  0.0117,  0.9448, -0.6801]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2076],\n",
      "        [ 1.7599],\n",
      "        [-1.5036],\n",
      "        [ 0.0274]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1872, -1.8897,  1.2325,  1.2359, -1.4602],\n",
      "        [-0.9884,  0.7991,  1.4151, -1.2624, -1.1008],\n",
      "        [-0.9565,  0.6468, -0.4276, -0.0708, -1.2717],\n",
      "        [ 0.3381, -1.4447, -0.0276,  0.3709, -1.4360]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3523,  0.1622,  0.4053,  0.3590,  0.1202],\n",
      "        [-0.7231, -1.0969, -0.8620, -0.3386, -1.5308],\n",
      "        [ 0.9959,  0.5935,  0.5039,  0.1358,  0.2475],\n",
      "        [ 0.2476,  0.2085,  0.3936,  0.3368, -0.4603]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1872, -1.8897,  1.2325,  1.2359, -1.4602],\n",
      "        [-0.9884,  0.7991,  1.4151, -1.2624, -1.1008],\n",
      "        [-0.9565,  0.6468, -0.4276, -0.0708, -1.2717],\n",
      "        [ 0.3381, -1.4447, -0.0276,  0.3709, -1.4360]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3951],\n",
      "        [ 0.7309],\n",
      "        [-1.1085],\n",
      "        [ 0.5576]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4966,  0.7803,  0.3046, -1.1487,  0.3361],\n",
      "        [-0.1833, -0.5077, -0.9705,  0.2624, -0.0043],\n",
      "        [ 0.8456,  1.7503,  0.9261,  1.5837,  1.0624],\n",
      "        [-0.6678,  0.2211, -0.3845, -1.0715, -1.3358]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2595,  0.2505, -0.1394, -0.0730,  0.1539],\n",
      "        [-0.5240, -0.8734, -0.1969, -0.6130, -1.4124],\n",
      "        [ 0.1858,  0.6260, -0.1386,  0.4711,  0.9437],\n",
      "        [-0.4303,  0.0279, -0.0680, -0.2579, -0.4471]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4966,  0.7803,  0.3046, -1.1487,  0.3361],\n",
      "        [-0.1833, -0.5077, -0.9705,  0.2624, -0.0043],\n",
      "        [ 0.8456,  1.7503,  0.9261,  1.5837,  1.0624],\n",
      "        [-0.6678,  0.2211, -0.3845, -1.0715, -1.3358]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4174],\n",
      "        [ 0.5758],\n",
      "        [ 2.8732],\n",
      "        [ 1.1932]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2335,  1.9139,  0.3920, -1.3807,  1.4989],\n",
      "        [-1.2622,  1.2234, -0.1212, -0.6310,  0.3086],\n",
      "        [ 0.4388,  1.4997, -0.0299, -0.1283, -0.5004],\n",
      "        [-0.4232, -0.4778,  1.5238, -0.7026, -0.3255]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1349,  0.2635,  0.2602,  0.3940,  0.5076],\n",
      "        [-0.9507, -0.8597, -0.7812, -0.4375, -1.4426],\n",
      "        [-0.4887, -0.8342, -0.4528, -0.5485, -1.3604],\n",
      "        [-0.8847, -0.7558, -0.4249, -0.4753, -0.9950]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2335,  1.9139,  0.3920, -1.3807,  1.4989],\n",
      "        [-1.2622,  1.2234, -0.1212, -0.6310,  0.3086],\n",
      "        [ 0.4388,  1.4997, -0.0299, -0.1283, -0.5004],\n",
      "        [-0.4232, -0.4778,  1.5238, -0.7026, -0.3255]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7917],\n",
      "        [ 0.0738],\n",
      "        [-0.7008],\n",
      "        [ 0.7458]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6767,  0.8013, -0.2932, -1.3351,  1.1811],\n",
      "        [-0.5475,  0.7387, -0.1819,  1.0031,  0.9059],\n",
      "        [-0.2803,  1.2043, -0.5148, -0.3371,  2.1834],\n",
      "        [-0.8312, -1.2259, -1.3786, -0.7938, -0.8418]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0404,  0.1760, -0.4686, -0.6171, -0.7762],\n",
      "        [-0.4446,  0.1623, -0.6438, -0.1031, -1.4525],\n",
      "        [-0.2074, -1.1605,  0.2502, -0.6003, -1.3473],\n",
      "        [-0.3411, -0.5790, -0.6434, -0.1925, -1.1993]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6767,  0.8013, -0.2932, -1.3351,  1.1811],\n",
      "        [-0.5475,  0.7387, -0.1819,  1.0031,  0.9059],\n",
      "        [-0.2803,  1.2043, -0.5148, -0.3371,  2.1834],\n",
      "        [-0.8312, -1.2259, -1.3786, -0.7938, -0.8418]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1178],\n",
      "        [-0.9387],\n",
      "        [-4.2076],\n",
      "        [ 3.0427]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8883, -1.2066,  1.8235, -0.0834, -0.0382],\n",
      "        [-0.8767, -0.3707,  0.5773,  1.3358, -0.2884],\n",
      "        [ 0.1890,  0.4955,  0.9232, -0.4429,  1.6079],\n",
      "        [-1.7413, -0.1244, -0.1420,  0.2710, -0.6453]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1225,  0.2616,  0.1573,  0.1307,  0.5425],\n",
      "        [-0.3450, -0.5913, -0.1114, -0.1682, -0.3037],\n",
      "        [ 1.4313,  1.3025,  1.8333,  1.3274,  1.6421],\n",
      "        [-1.2136, -0.6259, -1.1204, -1.9355, -3.5184]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8883, -1.2066,  1.8235, -0.0834, -0.0382],\n",
      "        [-0.8767, -0.3707,  0.5773,  1.3358, -0.2884],\n",
      "        [ 0.1890,  0.4955,  0.9232, -0.4429,  1.6079],\n",
      "        [-1.7413, -0.1244, -0.1420,  0.2710, -0.6453]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2918],\n",
      "        [ 0.3201],\n",
      "        [ 4.6609],\n",
      "        [ 4.0961]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3627, -0.5509, -0.1169, -0.3440,  0.8212],\n",
      "        [-1.1357, -0.3085, -0.3608,  1.6290,  1.0192],\n",
      "        [ 0.8320,  1.5096,  0.3154,  0.6096,  0.5775],\n",
      "        [-0.1908,  0.9854,  1.1908,  0.0932, -0.0027]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4493, -0.1723, -0.0351,  0.2952, -0.2657],\n",
      "        [-0.2375, -0.1026, -0.8663,  0.2630,  0.3949],\n",
      "        [-0.6867, -0.6990, -1.3100, -0.4595, -1.8932],\n",
      "        [ 0.4812, -0.2992,  0.3092, -0.2717,  0.7862]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3627, -0.5509, -0.1169, -0.3440,  0.8212],\n",
      "        [-1.1357, -0.3085, -0.3608,  1.6290,  1.0192],\n",
      "        [ 0.8320,  1.5096,  0.3154,  0.6096,  0.5775],\n",
      "        [-0.1908,  0.9854,  1.1908,  0.0932, -0.0027]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3916],\n",
      "        [ 1.4447],\n",
      "        [-3.4132],\n",
      "        [-0.0458]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5080, -1.6429,  0.4930, -0.6006, -0.1855],\n",
      "        [-1.0550,  1.9012,  0.4505,  0.5950,  0.6812],\n",
      "        [-1.1004, -0.7947, -0.1165, -0.8305, -1.3040],\n",
      "        [ 1.0099,  2.2774, -0.5721, -0.0655,  0.9547]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1391,  0.4667, -0.3761,  0.1082,  0.3392],\n",
      "        [-0.5085, -0.4128, -1.6288, -0.7129, -1.8390],\n",
      "        [ 1.5284,  1.0587,  1.4511,  0.3539,  1.1013],\n",
      "        [ 0.1204, -0.0132,  0.6135, -0.0274,  0.0344]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5080, -1.6429,  0.4930, -0.6006, -0.1855],\n",
      "        [-1.0550,  1.9012,  0.4505,  0.5950,  0.6812],\n",
      "        [-1.1004, -0.7947, -0.1165, -0.8305, -1.3040],\n",
      "        [ 1.0099,  2.2774, -0.5721, -0.0655,  0.9547]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0094],\n",
      "        [-2.6592],\n",
      "        [-4.4222],\n",
      "        [-0.2248]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2775,  1.0790,  2.1744, -1.4778, -0.7130],\n",
      "        [ 0.2712,  0.2396, -0.3318, -1.3141, -1.2084],\n",
      "        [-1.4397,  2.2914, -0.5977,  0.7424,  0.5123],\n",
      "        [-1.2952,  0.8284, -0.7298,  2.8603, -0.6532]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1874,  0.4628,  0.6091,  0.3810,  1.0971],\n",
      "        [ 0.1992,  0.3866, -0.4238, -0.4609,  0.4732],\n",
      "        [ 1.9422,  2.3904,  2.2496,  1.8860,  3.1604],\n",
      "        [-0.0353,  0.6061, -0.6981,  0.1939, -0.0582]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2775,  1.0790,  2.1744, -1.4778, -0.7130],\n",
      "        [ 0.2712,  0.2396, -0.3318, -1.3141, -1.2084],\n",
      "        [-1.4397,  2.2914, -0.5977,  0.7424,  0.5123],\n",
      "        [-1.2952,  0.8284, -0.7298,  2.8603, -0.6532]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1492],\n",
      "        [ 0.3211],\n",
      "        [ 4.3560],\n",
      "        [ 1.6499]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3785,  0.7687, -1.4305, -1.1567, -0.2157],\n",
      "        [-1.5308,  0.7883,  1.1934, -0.1506,  0.3957],\n",
      "        [ 0.3000,  0.2576,  0.8607, -0.6717,  1.2747],\n",
      "        [-0.2762,  0.2261, -0.7716,  1.0090, -0.7638]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0333,  0.7386, -0.2584,  0.2842,  0.7354],\n",
      "        [ 0.0752, -0.5251,  0.8855, -0.2713, -0.7306],\n",
      "        [ 0.3425,  1.0718,  0.8366,  0.7492,  1.3715],\n",
      "        [-0.5310, -0.6230, -0.1886, -0.5699, -1.0172]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3785,  0.7687, -1.4305, -1.1567, -0.2157],\n",
      "        [-1.5308,  0.7883,  1.1934, -0.1506,  0.3957],\n",
      "        [ 0.3000,  0.2576,  0.8607, -0.6717,  1.2747],\n",
      "        [-0.2762,  0.2261, -0.7716,  1.0090, -0.7638]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4374],\n",
      "        [ 0.2794],\n",
      "        [ 2.3439],\n",
      "        [ 0.3531]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3723,  0.0356, -2.6283,  0.1040,  0.7190],\n",
      "        [-0.5469, -0.4301,  1.9950, -0.4437,  0.8193],\n",
      "        [ 2.3067,  0.4912, -0.8661,  0.6290, -2.0510],\n",
      "        [ 0.1537,  0.7511, -0.3192, -0.8075,  1.0835]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0078,  0.2138, -0.1985, -0.1851, -0.8695],\n",
      "        [-0.1239, -0.1987, -0.9063, -0.5192, -0.6225],\n",
      "        [-0.5019,  0.8484,  0.1975,  0.9359,  0.0752],\n",
      "        [-0.4993, -0.4883, -0.2733, -0.8649, -0.8653]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3723,  0.0356, -2.6283,  0.1040,  0.7190],\n",
      "        [-0.5469, -0.4301,  1.9950, -0.4437,  0.8193],\n",
      "        [ 2.3067,  0.4912, -0.8661,  0.6290, -2.0510],\n",
      "        [ 0.1537,  0.7511, -0.3192, -0.8075,  1.0835]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1180],\n",
      "        [-1.9345],\n",
      "        [-0.4777],\n",
      "        [-0.5955]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8244, -1.1150, -1.7535, -1.4371, -0.3298],\n",
      "        [ 1.6136,  0.8426, -1.6648, -0.7576, -0.2427],\n",
      "        [-0.3729,  0.5727, -0.8304, -1.2721, -1.3244],\n",
      "        [-1.4803,  0.0266, -1.0699, -0.9569,  0.4597]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5989, -0.3038, -0.0030, -0.1188,  0.1520],\n",
      "        [ 0.5463,  0.3533,  0.6482, -0.3317,  1.0110],\n",
      "        [-0.0186,  0.7754,  0.9422,  0.7403,  1.2124],\n",
      "        [-0.5849, -0.7536,  0.0495, -0.1006, -0.7980]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8244, -1.1150, -1.7535, -1.4371, -0.3298],\n",
      "        [ 1.6136,  0.8426, -1.6648, -0.7576, -0.2427],\n",
      "        [-0.3729,  0.5727, -0.8304, -1.2721, -1.3244],\n",
      "        [-1.4803,  0.0266, -1.0699, -0.9569,  0.4597]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9583],\n",
      "        [ 0.1061],\n",
      "        [-2.8788],\n",
      "        [ 0.5221]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4177, -0.3543, -0.2276,  1.6052,  0.0739],\n",
      "        [-0.5973, -0.1173, -0.2453, -0.7589, -0.7987],\n",
      "        [ 0.3632, -0.2204,  0.9621, -0.1728,  0.0078],\n",
      "        [ 1.8597,  0.4348,  0.4113,  2.6575,  1.1613]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5252,  0.4123, -0.2684, -0.0155, -1.1578],\n",
      "        [ 0.2786, -0.6711, -0.4301, -0.5619, -0.7170],\n",
      "        [-0.0107, -0.0994,  0.3289, -0.2467, -0.3599],\n",
      "        [-0.4394, -0.1374,  0.2320, -0.7947, -0.9857]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4177, -0.3543, -0.2276,  1.6052,  0.0739],\n",
      "        [-0.5973, -0.1173, -0.2453, -0.7589, -0.7987],\n",
      "        [ 0.3632, -0.2204,  0.9621, -0.1728,  0.0078],\n",
      "        [ 1.8597,  0.4348,  0.4113,  2.6575,  1.1613]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9401],\n",
      "        [ 1.0169],\n",
      "        [ 0.3743],\n",
      "        [-4.0382]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6664, -0.3826, -1.5395,  0.5993,  1.7070],\n",
      "        [-0.3524,  1.5492,  0.2459, -0.9916, -1.6073],\n",
      "        [ 0.3943,  1.2234, -0.8616,  1.1643, -0.2274],\n",
      "        [ 0.1959, -1.0014, -0.7552, -0.9189, -2.1613]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0896,  0.6161,  0.1666,  0.4824,  0.4217],\n",
      "        [-0.1109, -0.7165, -0.5921, -0.5733, -1.8757],\n",
      "        [ 0.3028, -0.4576, -0.1975, -0.1932, -0.0049],\n",
      "        [ 1.3724,  1.2809,  0.8059,  1.3449,  2.5559]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6664, -0.3826, -1.5395,  0.5993,  1.7070],\n",
      "        [-0.3524,  1.5492,  0.2459, -0.9916, -1.6073],\n",
      "        [ 0.3943,  1.2234, -0.8616,  1.1643, -0.2274],\n",
      "        [ 0.1959, -1.0014, -0.7552, -0.9189, -2.1613]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3675],\n",
      "        [ 2.3667],\n",
      "        [-0.4941],\n",
      "        [-8.3822]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.6625,  0.4584, -0.8455, -0.0320, -0.7180],\n",
      "        [-0.2950,  1.8016,  1.2627,  0.1289, -1.0315],\n",
      "        [ 0.0551, -0.7031,  0.4314, -0.5629,  1.1351],\n",
      "        [ 0.5351,  1.0895,  0.9199, -0.0509,  1.1269]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1338,  0.6358,  0.5090, -0.0481, -0.1415],\n",
      "        [-0.9867, -1.8129, -1.1993, -2.1394, -2.6605],\n",
      "        [ 0.8492,  0.5070, -0.0924,  0.1994, -0.5231],\n",
      "        [ 0.1959,  0.4227, -0.3234, -0.3788, -0.2828]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.6625,  0.4584, -0.8455, -0.0320, -0.7180],\n",
      "        [-0.2950,  1.8016,  1.2627,  0.1289, -1.0315],\n",
      "        [ 0.0551, -0.7031,  0.4314, -0.5629,  1.1351],\n",
      "        [ 0.5351,  1.0895,  0.9199, -0.0509,  1.1269]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3921],\n",
      "        [-2.0211],\n",
      "        [-1.0556],\n",
      "        [-0.0316]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1191, -1.7871, -0.1848, -0.2759,  0.4052],\n",
      "        [ 0.3395, -1.6607,  1.3247, -1.0909,  0.2992],\n",
      "        [-0.7620,  0.6770,  0.9145,  0.0548, -0.3685],\n",
      "        [-0.0035,  0.8721,  0.5456, -1.4746,  0.4444]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3914,  0.2072,  0.0335,  0.3376,  0.2644],\n",
      "        [-0.9632, -1.3393, -0.7214, -1.0565, -1.7561],\n",
      "        [ 0.0712,  0.5824,  0.2658,  0.2193,  1.3636],\n",
      "        [ 0.1904,  0.5424, -0.0419,  0.6494, -0.2287]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1191, -1.7871, -0.1848, -0.2759,  0.4052],\n",
      "        [ 0.3395, -1.6607,  1.3247, -1.0909,  0.2992],\n",
      "        [-0.7620,  0.6770,  0.9145,  0.0548, -0.3685],\n",
      "        [-0.0035,  0.8721,  0.5456, -1.4746,  0.4444]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4091],\n",
      "        [ 1.5687],\n",
      "        [ 0.0926],\n",
      "        [-0.6097]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2039,  1.5383, -0.3913, -0.1997,  0.4589],\n",
      "        [ 0.9562,  0.3417,  0.6742, -0.4377, -0.4675],\n",
      "        [-0.9074, -0.1173,  0.1104,  0.4088,  1.3912],\n",
      "        [-0.3834,  0.7526, -2.4028, -1.5635,  0.8681]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2275, -0.4019,  0.4770,  0.0058,  0.6062],\n",
      "        [ 0.0252, -0.2727, -0.1876, -0.2701,  0.0329],\n",
      "        [ 0.3420,  0.6946, -0.4315,  0.4209,  0.2144],\n",
      "        [ 0.1574,  0.2893, -0.0017, -0.3424, -0.3401]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2039,  1.5383, -0.3913, -0.1997,  0.4589],\n",
      "        [ 0.9562,  0.3417,  0.6742, -0.4377, -0.4675],\n",
      "        [-0.9074, -0.1173,  0.1104,  0.4088,  1.3912],\n",
      "        [-0.3834,  0.7526, -2.4028, -1.5635,  0.8681]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5743],\n",
      "        [-0.0927],\n",
      "        [ 0.0309],\n",
      "        [ 0.4014]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8322,  0.9094,  0.0289,  1.1332, -0.2519],\n",
      "        [ 0.9270,  0.4782,  0.3333, -0.6628,  0.5293],\n",
      "        [-0.3929, -0.4020,  0.2239,  0.9148, -0.0256],\n",
      "        [ 0.3018,  1.8613,  0.2011,  1.3896,  1.1336]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3368, -0.2784, -0.4254, -0.5355,  0.3187],\n",
      "        [ 0.7006,  0.5207,  0.1954, -0.5486, -0.0838],\n",
      "        [ 0.2011, -0.0185, -0.5827, -0.1146, -0.9295],\n",
      "        [-0.0073, -0.0080,  0.0773, -0.1277, -0.4513]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8322,  0.9094,  0.0289,  1.1332, -0.2519],\n",
      "        [ 0.9270,  0.4782,  0.3333, -0.6628,  0.5293],\n",
      "        [-0.3929, -0.4020,  0.2239,  0.9148, -0.0256],\n",
      "        [ 0.3018,  1.8613,  0.2011,  1.3896,  1.1336]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6723],\n",
      "        [ 1.2827],\n",
      "        [-0.2831],\n",
      "        [-0.6905]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3914, -0.2469, -1.1039, -0.2004,  0.7566],\n",
      "        [ 0.8866, -0.3079,  1.5913, -1.6869,  0.1973],\n",
      "        [-1.3739,  0.3642,  0.4204, -3.1384,  0.8990],\n",
      "        [-1.6275,  0.1099,  1.1815, -0.1671, -0.1141]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3351,  0.0240,  0.2336, -0.1374,  0.4823],\n",
      "        [-0.1730, -0.6404, -0.1445, -0.3088, -1.0766],\n",
      "        [ 0.1616, -0.0466,  0.4075, -0.2178,  0.0332],\n",
      "        [ 0.1728, -0.0938, -0.4974,  0.1699, -0.6347]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3914, -0.2469, -1.1039, -0.2004,  0.7566],\n",
      "        [ 0.8866, -0.3079,  1.5913, -1.6869,  0.1973],\n",
      "        [-1.3739,  0.3642,  0.4204, -3.1384,  0.8990],\n",
      "        [-1.6275,  0.1099,  1.1815, -0.1671, -0.1141]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0026],\n",
      "        [ 0.1223],\n",
      "        [ 0.6457],\n",
      "        [-0.8352]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2274,  0.6045, -0.3426,  1.2164,  1.4735],\n",
      "        [-1.8075,  0.5677,  0.3173,  1.0713, -0.9850],\n",
      "        [-1.5328,  0.1001, -0.3146,  0.5667,  1.1217],\n",
      "        [-1.4630, -1.1296,  0.9493, -0.8786, -0.1542]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3385,  0.1468,  0.0840, -0.3906, -0.2093],\n",
      "        [-0.7405,  0.2262, -0.8603,  0.3038, -0.2025],\n",
      "        [-0.2912,  0.0951, -0.4688,  0.4145, -0.6746],\n",
      "        [ 0.5336,  0.1130,  0.2263, -0.0184,  0.7363]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2274,  0.6045, -0.3426,  1.2164,  1.4735],\n",
      "        [-1.8075,  0.5677,  0.3173,  1.0713, -0.9850],\n",
      "        [-1.5328,  0.1001, -0.3146,  0.5667,  1.1217],\n",
      "        [-1.4630, -1.1296,  0.9493, -0.8786, -0.1542]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1391],\n",
      "        [ 1.7187],\n",
      "        [ 0.0815],\n",
      "        [-0.7910]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1796,  0.0494, -1.0863, -0.1315,  1.2193],\n",
      "        [-1.9711,  0.0420,  0.6314,  0.4296, -1.4726],\n",
      "        [ 0.8182,  0.8124, -0.4643, -1.7614, -0.4160],\n",
      "        [-0.5186,  1.6990,  0.9182, -1.2515,  0.1675]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8598,  0.6897, -0.3712,  0.1964,  0.5629],\n",
      "        [-0.4019, -0.1594, -0.3104, -0.4279, -1.7767],\n",
      "        [ 0.3612, -0.0352,  0.5065, -0.2899, -0.1499],\n",
      "        [ 0.1079,  0.0256,  0.3302,  0.7719,  0.6321]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1796,  0.0494, -1.0863, -0.1315,  1.2193],\n",
      "        [-1.9711,  0.0420,  0.6314,  0.4296, -1.4726],\n",
      "        [ 0.8182,  0.8124, -0.4643, -1.7614, -0.4160],\n",
      "        [-0.5186,  1.6990,  0.9182, -1.2515,  0.1675]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0837],\n",
      "        [ 3.0221],\n",
      "        [ 0.6048],\n",
      "        [-0.5695]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1237,  0.8915,  0.7715, -1.2447, -0.4232],\n",
      "        [ 0.0141, -2.3329,  1.7817,  0.4020, -0.6841],\n",
      "        [-0.0467,  0.0256,  0.3605, -0.7273,  0.3114],\n",
      "        [-1.8630, -0.7572,  0.1183,  1.5864,  0.1944]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4969, -0.6182, -0.0928, -0.3299,  0.6770],\n",
      "        [-1.8478, -1.8145, -1.3031, -1.5662, -3.7277],\n",
      "        [ 0.3986, -0.0638,  0.3422,  0.0044, -0.1010],\n",
      "        [ 0.9592,  0.0324,  0.6139, -0.2520,  0.8717]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1237,  0.8915,  0.7715, -1.2447, -0.4232],\n",
      "        [ 0.0141, -2.3329,  1.7817,  0.4020, -0.6841],\n",
      "        [-0.0467,  0.0256,  0.3605, -0.7273,  0.3114],\n",
      "        [-1.8630, -0.7572,  0.1183,  1.5864,  0.1944]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0570],\n",
      "        [ 3.8057],\n",
      "        [ 0.0685],\n",
      "        [-1.9693]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0646,  0.9021, -0.4293, -1.5352, -0.9049],\n",
      "        [ 0.1762, -0.6487,  0.8343, -0.0513, -0.1776],\n",
      "        [-2.3883, -1.1834, -0.3956, -0.4099,  2.2869],\n",
      "        [ 0.5777,  0.7921, -0.5827,  0.8359, -0.7364]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7395,  0.1996,  0.0794,  0.1381,  0.9858],\n",
      "        [ 0.1145,  0.0312, -0.3854,  0.0877,  0.3265],\n",
      "        [-0.4900,  0.1422,  0.0785,  0.5148,  0.1818],\n",
      "        [ 1.1724,  0.9633,  1.3615,  0.9063,  1.7248]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0646,  0.9021, -0.4293, -1.5352, -0.9049],\n",
      "        [ 0.1762, -0.6487,  0.8343, -0.0513, -0.1776],\n",
      "        [-2.3883, -1.1834, -0.3956, -0.4099,  2.2869],\n",
      "        [ 0.5777,  0.7921, -0.5827,  0.8359, -0.7364]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0059],\n",
      "        [-0.3840],\n",
      "        [ 1.1757],\n",
      "        [ 0.1345]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8584,  0.0363, -0.7753,  0.8234,  1.1887],\n",
      "        [-2.2703, -0.2203, -0.4782, -0.4762,  1.1626],\n",
      "        [-0.1065,  0.0444, -1.3188,  0.8934, -0.2902],\n",
      "        [-1.1316,  0.3125,  0.8835, -0.1173, -0.2726]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5702,  0.4677,  1.1325,  0.8035,  1.0534],\n",
      "        [ 0.1745, -0.2180, -0.3298,  0.3972,  0.1761],\n",
      "        [-0.2188, -0.4125, -1.1089, -0.3870, -0.9380],\n",
      "        [ 0.4768,  0.5719,  1.0681,  0.5804,  1.0006]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8584,  0.0363, -0.7753,  0.8234,  1.1887],\n",
      "        [-2.2703, -0.2203, -0.4782, -0.4762,  1.1626],\n",
      "        [-0.1065,  0.0444, -1.3188,  0.8934, -0.2902],\n",
      "        [-1.1316,  0.3125,  0.8835, -0.1173, -0.2726]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5633],\n",
      "        [-0.1748],\n",
      "        [ 1.3938],\n",
      "        [ 0.2419]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.5129, -0.1760, -0.8603,  0.1415, -0.1224],\n",
      "        [-1.1184,  1.0156,  1.8208, -1.2132,  0.0805],\n",
      "        [ 0.0075,  0.2791,  1.6680,  1.3226, -0.4750],\n",
      "        [ 0.3033,  0.0448,  0.5740, -1.4904,  2.7576]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5638, -0.5283, -0.2598, -0.4594, -0.1613],\n",
      "        [ 0.4685, -0.2454, -0.7962,  0.7599, -0.0347],\n",
      "        [-1.0471, -0.3939, -1.3452, -0.5808, -1.6651],\n",
      "        [ 0.4515,  0.1694,  1.2952,  1.0433,  0.9372]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5129, -0.1760, -0.8603,  0.1415, -0.1224],\n",
      "        [-1.1184,  1.0156,  1.8208, -1.2132,  0.0805],\n",
      "        [ 0.0075,  0.2791,  1.6680,  1.3226, -0.4750],\n",
      "        [ 0.3033,  0.0448,  0.5740, -1.4904,  2.7576]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0179],\n",
      "        [-3.1475],\n",
      "        [-2.3387],\n",
      "        [ 1.9173]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7205, -0.6161,  1.1607, -0.9384, -0.2944],\n",
      "        [ 0.8069,  0.5522,  0.8616,  0.8744,  0.8332],\n",
      "        [-0.9316,  1.1249,  3.6602,  0.0681,  1.8137],\n",
      "        [-1.2522, -0.4973, -0.6712,  1.3963, -0.7767]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3341, -0.1616,  0.1370, -0.1923,  0.0479],\n",
      "        [ 1.1819,  0.5872,  0.7698,  1.8556,  1.5827],\n",
      "        [-0.0480,  1.0528,  0.4347, -0.0082,  1.4218],\n",
      "        [ 0.4834, -0.1469,  0.2807,  0.5341, -0.8308]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7205, -0.6161,  1.1607, -0.9384, -0.2944],\n",
      "        [ 0.8069,  0.5522,  0.8616,  0.8744,  0.8332],\n",
      "        [-0.9316,  1.1249,  3.6602,  0.0681,  1.8137],\n",
      "        [-1.2522, -0.4973, -0.6712,  1.3963, -0.7767]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6656],\n",
      "        [ 4.8823],\n",
      "        [ 5.3982],\n",
      "        [ 0.6704]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7782, -1.1088,  0.9984,  1.2876, -0.7103],\n",
      "        [-0.6596,  0.0680,  0.6676,  0.7587, -3.0802],\n",
      "        [-2.0227,  0.8559,  0.4418, -0.6880,  1.1975],\n",
      "        [-0.5811,  0.2648, -1.1768, -0.4363, -0.4448]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0208,  0.3687,  0.2793, -0.2300, -0.1579],\n",
      "        [-0.1036, -1.1849, -0.6445, -0.9338, -1.1386],\n",
      "        [-1.6194, -1.4183, -2.1480, -1.9833, -3.3459],\n",
      "        [ 0.3155,  0.4942, -0.3897,  0.3567, -0.5415]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7782, -1.1088,  0.9984,  1.2876, -0.7103],\n",
      "        [-0.6596,  0.0680,  0.6676,  0.7587, -3.0802],\n",
      "        [-2.0227,  0.8559,  0.4418, -0.6880,  1.1975],\n",
      "        [-0.5811,  0.2648, -1.1768, -0.4363, -0.4448]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2770],\n",
      "        [ 2.3562],\n",
      "        [-1.5295],\n",
      "        [ 0.4914]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0366, -1.6247,  0.4860,  1.6202,  0.6421],\n",
      "        [ 0.2813,  0.8264,  0.5978,  0.1676, -0.9473],\n",
      "        [ 0.5078, -0.6698, -2.0017,  0.0825, -0.0406],\n",
      "        [ 1.2247, -1.5659, -0.5021, -1.0968, -0.1713]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0342, -0.5605, -0.2012,  0.5726,  0.0943],\n",
      "        [-1.3256, -0.8085, -1.7037, -1.7564, -3.0586],\n",
      "        [ 0.2854,  0.0433,  0.3548,  0.1823, -0.1762],\n",
      "        [ 0.5972,  0.5242,  0.1986,  0.2549,  1.2982]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0366, -1.6247,  0.4860,  1.6202,  0.6421],\n",
      "        [ 0.2813,  0.8264,  0.5978,  0.1676, -0.9473],\n",
      "        [ 0.5078, -0.6698, -2.0017,  0.0825, -0.0406],\n",
      "        [ 1.2247, -1.5659, -0.5021, -1.0968, -0.1713]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8025],\n",
      "        [ 0.5438],\n",
      "        [-0.5720],\n",
      "        [-0.6911]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6534,  1.5918, -1.3658, -1.3572,  0.7899],\n",
      "        [-0.7694,  1.3975,  0.4144,  0.6244,  0.0444],\n",
      "        [-0.3481,  0.8193,  0.0325,  0.0785, -0.0273],\n",
      "        [-1.6444, -0.7389, -0.3372,  0.3795,  1.2410]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1775, -1.1545, -0.9536, -0.9417, -0.6001],\n",
      "        [-0.7758, -1.1804, -2.0545, -1.8238, -3.0982],\n",
      "        [ 0.2864, -0.4699, -0.0026,  0.0938, -0.4031],\n",
      "        [ 0.2017,  0.5328,  1.4215,  0.9306,  1.5163]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6534,  1.5918, -1.3658, -1.3572,  0.7899],\n",
      "        [-0.7694,  1.3975,  0.4144,  0.6244,  0.0444],\n",
      "        [-0.3481,  0.8193,  0.0325,  0.0785, -0.0273],\n",
      "        [-1.6444, -0.7389, -0.3372,  0.3795,  1.2410]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0246],\n",
      "        [-3.1805],\n",
      "        [-0.4664],\n",
      "        [ 1.0303]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6647,  0.5269, -0.4240, -0.7127, -0.1286],\n",
      "        [ 0.7921, -0.7587,  1.7329, -0.7838,  1.5888],\n",
      "        [ 1.2056, -2.4700,  1.0931, -0.5615, -0.9732],\n",
      "        [-0.1209,  0.8638,  0.8152, -1.0877,  1.3139]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2163, -0.3206,  0.2755, -0.6527, -0.5273],\n",
      "        [-0.4592, -1.1683, -0.8543, -1.1839, -1.8918],\n",
      "        [ 0.3002, -0.1845, -1.0197, -0.2710, -0.0334],\n",
      "        [ 0.3626,  0.4725,  0.4590,  0.8670,  0.1715]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6647,  0.5269, -0.4240, -0.7127, -0.1286],\n",
      "        [ 0.7921, -0.7587,  1.7329, -0.7838,  1.5888],\n",
      "        [ 1.2056, -2.4700,  1.0931, -0.5615, -0.9732],\n",
      "        [-0.1209,  0.8638,  0.8152, -1.0877,  1.3139]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1034],\n",
      "        [-3.0355],\n",
      "        [-0.1123],\n",
      "        [ 0.0208]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1028,  0.8949,  0.0070, -0.3294,  0.0362],\n",
      "        [ 0.1681, -0.6142, -0.1555,  0.3151,  2.2218],\n",
      "        [-1.6170,  1.1810, -3.4814,  0.9180,  0.5137],\n",
      "        [-2.6217, -0.2295,  1.2731, -1.1657,  0.1418]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6552, -0.5019, -0.0137, -0.0437,  0.0788],\n",
      "        [ 0.9248,  0.7652,  0.0094,  0.5205,  1.1864],\n",
      "        [ 0.3006, -0.1083,  0.6235,  0.5852, -0.0739],\n",
      "        [ 0.2912,  0.6548,  0.8585,  0.7826,  1.1818]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1028,  0.8949,  0.0070, -0.3294,  0.0362],\n",
      "        [ 0.1681, -0.6142, -0.1555,  0.3151,  2.2218],\n",
      "        [-1.6170,  1.1810, -3.4814,  0.9180,  0.5137],\n",
      "        [-2.6217, -0.2295,  1.2731, -1.1657,  0.1418]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4993],\n",
      "        [ 2.4841],\n",
      "        [-2.2853],\n",
      "        [-0.5654]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9024,  0.1013,  1.3195,  0.3960,  0.9456],\n",
      "        [ 0.5395, -0.0301, -0.8049, -0.2961,  0.4918],\n",
      "        [-1.4159, -0.9193,  0.4069, -0.2770,  0.5497],\n",
      "        [ 0.3613, -0.7723,  2.0555, -0.1116,  1.7982]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2032,  0.4055,  0.3644,  0.1269,  0.6729],\n",
      "        [-0.2516, -1.5742, -0.7243, -1.2293, -1.9889],\n",
      "        [ 0.6338,  1.1264,  0.8008,  0.7348,  1.4304],\n",
      "        [ 0.7969,  1.0723,  1.1606,  1.5828,  0.4056]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9024,  0.1013,  1.3195,  0.3960,  0.9456],\n",
      "        [ 0.5395, -0.0301, -0.8049, -0.2961,  0.4918],\n",
      "        [-1.4159, -0.9193,  0.4069, -0.2770,  0.5497],\n",
      "        [ 0.3613, -0.7723,  2.0555, -0.1116,  1.7982]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3918],\n",
      "        [-0.1196],\n",
      "        [-1.0241],\n",
      "        [ 2.3980]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9045, -0.5993, -0.4745, -0.0448, -0.0327],\n",
      "        [ 0.3148,  0.9022,  0.0672, -0.7120,  1.0049],\n",
      "        [-2.3017, -1.5417,  1.6162,  0.5371, -0.1991],\n",
      "        [ 0.5060, -0.0202,  0.5978, -0.6000, -0.0020]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0294, -0.8594, -0.8415, -0.5600, -1.7372],\n",
      "        [-0.5397, -1.3794, -0.8555, -0.4842, -1.7559],\n",
      "        [ 1.2223,  2.0876,  1.9198,  1.1937,  1.1858],\n",
      "        [-0.4297, -1.3479, -0.9436, -0.6141, -1.5242]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9045, -0.5993, -0.4745, -0.0448, -0.0327],\n",
      "        [ 0.3148,  0.9022,  0.0672, -0.7120,  1.0049],\n",
      "        [-2.3017, -1.5417,  1.6162,  0.5371, -0.1991],\n",
      "        [ 0.5060, -0.0202,  0.5978, -0.6000, -0.0020]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0650],\n",
      "        [-2.8915],\n",
      "        [-2.5240],\n",
      "        [-0.3827]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0861,  0.9128, -0.6158,  1.8788,  0.0071],\n",
      "        [ 1.1501, -1.5372,  0.0562, -0.4291, -1.6406],\n",
      "        [-1.0542, -1.4254,  2.2645,  0.2909,  1.1299],\n",
      "        [ 1.3715, -0.0635, -0.0598,  0.0938,  0.6778]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0045,  0.1572,  0.1244,  0.1989, -0.0628],\n",
      "        [ 0.6589,  0.0989,  0.0341,  0.1845,  1.1167],\n",
      "        [ 1.7743,  2.0936,  1.6057,  1.2445,  2.7289],\n",
      "        [-0.4253,  0.1882,  0.8496,  0.7663, -0.1501]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0861,  0.9128, -0.6158,  1.8788,  0.0071],\n",
      "        [ 1.1501, -1.5372,  0.0562, -0.4291, -1.6406],\n",
      "        [-1.0542, -1.4254,  2.2645,  0.2909,  1.1299],\n",
      "        [ 1.3715, -0.0635, -0.0598,  0.0938,  0.6778]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4398],\n",
      "        [-1.3036],\n",
      "        [ 2.2268],\n",
      "        [-0.6760]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8143,  1.0848,  0.7814,  0.9519,  1.2643],\n",
      "        [ 1.0877, -0.9378, -1.6510,  0.8585,  1.7204],\n",
      "        [-0.7688,  3.0868, -1.0545,  1.3508,  0.9333],\n",
      "        [-1.1603,  0.7158,  0.0903,  1.0942, -0.3762]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2693,  0.5723, -0.7633,  0.0233,  0.8477],\n",
      "        [ 1.0112,  0.3800,  0.6394,  0.4966,  2.3307],\n",
      "        [ 0.2139, -0.3033, -0.0303, -0.1760,  0.2646],\n",
      "        [ 0.0796,  1.3571,  1.2304,  1.0772,  0.5833]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8143,  1.0848,  0.7814,  0.9519,  1.2643],\n",
      "        [ 1.0877, -0.9378, -1.6510,  0.8585,  1.7204],\n",
      "        [-0.7688,  3.0868, -1.0545,  1.3508,  0.9333],\n",
      "        [-1.1603,  0.7158,  0.0903,  1.0942, -0.3762]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8992],\n",
      "        [ 4.1239],\n",
      "        [-1.0596],\n",
      "        [ 1.9495]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1641,  0.2147,  1.0145,  1.6899,  0.4853],\n",
      "        [-0.3583, -0.2259,  1.1965,  0.8614, -1.5712],\n",
      "        [-0.2476, -0.0171,  2.6992,  1.0845,  1.4395],\n",
      "        [ 0.0728, -0.0491, -0.7276,  0.6509, -0.0707]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2950, -0.1297,  0.2107,  0.1771, -0.0113],\n",
      "        [-0.7014, -1.1632, -1.5682, -1.5168, -1.9369],\n",
      "        [ 0.3511,  0.3587,  0.2572,  1.3490,  0.7985],\n",
      "        [-0.0328, -0.3585,  0.0213, -0.9660, -0.5804]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1641,  0.2147,  1.0145,  1.6899,  0.4853],\n",
      "        [-0.3583, -0.2259,  1.1965,  0.8614, -1.5712],\n",
      "        [-0.2476, -0.0171,  2.6992,  1.0845,  1.4395],\n",
      "        [ 0.0728, -0.0491, -0.7276,  0.6509, -0.0707]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4312],\n",
      "        [ 0.3744],\n",
      "        [ 3.2137],\n",
      "        [-0.5880]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0833, -0.1680,  1.4387, -1.0087,  1.8372],\n",
      "        [-0.3063,  1.2483,  0.8876,  1.0667,  0.4104],\n",
      "        [-0.6500, -0.0410,  0.2532,  0.8093,  0.4813],\n",
      "        [ 0.3606,  0.6605, -0.3246, -0.0906,  0.8932]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1296,  0.8985,  0.1555,  0.2903,  0.2897],\n",
      "        [ 0.0912, -1.5633, -1.4269, -1.7527, -1.9855],\n",
      "        [-0.7265, -1.2853, -1.0193, -1.5224, -2.1703],\n",
      "        [ 0.0378,  0.7162,  0.0742,  0.1247,  0.5303]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0833, -0.1680,  1.4387, -1.0087,  1.8372],\n",
      "        [-0.3063,  1.2483,  0.8876,  1.0667,  0.4104],\n",
      "        [-0.6500, -0.0410,  0.2532,  0.8093,  0.4813],\n",
      "        [ 0.3606,  0.6605, -0.3246, -0.0906,  0.8932]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3014],\n",
      "        [-5.9303],\n",
      "        [-2.0098],\n",
      "        [ 0.9250]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1070,  0.8975, -0.1332,  1.2599,  0.7699],\n",
      "        [ 1.0058, -0.5138, -0.3080, -0.0753, -0.6084],\n",
      "        [-1.5384, -1.0339, -1.8084, -1.1129,  0.6905],\n",
      "        [ 0.0285, -0.2941, -1.9505,  0.7496,  1.3791]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2625,  0.3193,  0.0297,  0.8307,  0.5641],\n",
      "        [ 1.2566,  1.5364,  1.6440,  1.2144,  2.3661],\n",
      "        [ 0.1514, -0.9995, -0.4991, -0.2348, -0.7827],\n",
      "        [-0.0914,  0.5373,  0.1640,  1.0558,  0.0895]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1070,  0.8975, -0.1332,  1.2599,  0.7699],\n",
      "        [ 1.0058, -0.5138, -0.3080, -0.0753, -0.6084],\n",
      "        [-1.5384, -1.0339, -1.8084, -1.1129,  0.6905],\n",
      "        [ 0.0285, -0.2941, -1.9505,  0.7496,  1.3791]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7916],\n",
      "        [-1.5628],\n",
      "        [ 1.4238],\n",
      "        [ 0.4343]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2101,  1.3082, -1.3489,  0.1477, -2.0577],\n",
      "        [-1.4623, -0.4225,  1.5130,  2.2212,  1.5286],\n",
      "        [ 0.1393, -1.1948, -0.5899, -0.8828,  1.1308],\n",
      "        [ 0.4751,  2.5075, -0.8997,  1.3626,  0.7858]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5758, -0.3106,  0.2022, -0.9914, -1.2043],\n",
      "        [ 1.8406,  1.5407,  1.8467,  1.5063,  3.0695],\n",
      "        [-1.1725, -0.8986, -0.9158, -1.6004, -1.8362],\n",
      "        [-0.4529,  0.0267, -0.0348,  0.1745,  0.6992]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2101,  1.3082, -1.3489,  0.1477, -2.0577],\n",
      "        [-1.4623, -0.4225,  1.5130,  2.2212,  1.5286],\n",
      "        [ 0.1393, -1.1948, -0.5899, -0.8828,  1.1308],\n",
      "        [ 0.4751,  2.5075, -0.8997,  1.3626,  0.7858]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3494],\n",
      "        [ 7.4892],\n",
      "        [ 0.7870],\n",
      "        [ 0.6703]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9662,  0.3624,  0.5340, -0.2761,  3.2513],\n",
      "        [ 1.1322, -0.1506,  1.0621, -0.8330,  0.1079],\n",
      "        [-0.0739,  0.1161,  0.1036, -0.1413, -0.1386],\n",
      "        [-0.5745, -0.9465, -0.7318, -0.5451,  0.0196]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.4425, -1.0045, -1.5583, -2.3038, -2.4511],\n",
      "        [-0.2246, -0.5670, -0.8161, -1.3472, -2.8934],\n",
      "        [-0.2243, -1.5005, -0.7879, -1.0934, -2.1896],\n",
      "        [-0.0746,  0.8515,  0.4395,  0.4124,  0.7058]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9662,  0.3624,  0.5340, -0.2761,  3.2513],\n",
      "        [ 1.1322, -0.1506,  1.0621, -0.8330,  0.1079],\n",
      "        [-0.0739,  0.1161,  0.1036, -0.1413, -0.1386],\n",
      "        [-0.5745, -0.9465, -0.7318, -0.5451,  0.0196]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-5.6928],\n",
      "        [-0.2256],\n",
      "        [ 0.2186],\n",
      "        [-1.2957]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3404,  1.8471, -0.6622, -0.3656,  0.5219],\n",
      "        [ 0.5350,  0.0111, -0.6448, -0.1757, -1.3423],\n",
      "        [ 1.1029, -0.7618,  0.1602,  1.0261,  0.2983],\n",
      "        [ 1.0278, -0.1766,  0.2127,  1.2035, -0.4725]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7057,  1.1394,  1.4499,  1.1179,  1.1216],\n",
      "        [-0.5747, -1.1137, -1.0392, -0.4766, -2.1426],\n",
      "        [-0.0330, -1.1880, -0.6151, -0.9491, -1.2801],\n",
      "        [ 0.2358,  1.3287,  0.7607,  1.2316,  1.3826]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3404,  1.8471, -0.6622, -0.3656,  0.5219],\n",
      "        [ 0.5350,  0.0111, -0.6448, -0.1757, -1.3423],\n",
      "        [ 1.1029, -0.7618,  0.1602,  1.0261,  0.2983],\n",
      "        [ 1.0278, -0.1766,  0.2127,  1.2035, -0.4725]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5614],\n",
      "        [ 3.3101],\n",
      "        [-0.5857],\n",
      "        [ 0.9985]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6298, -0.4859, -0.5301, -0.8164,  1.4142],\n",
      "        [-0.5379,  1.8289,  1.3699,  1.3830, -1.7197],\n",
      "        [-0.3839, -0.4295, -0.6739,  0.6937,  0.0681],\n",
      "        [ 1.3314, -0.0858, -2.3225, -0.6625,  1.9366]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0083,  0.8935,  0.5518,  0.5468,  1.2511],\n",
      "        [-1.5859, -2.1317, -1.4722, -2.0132, -3.2563],\n",
      "        [-0.2609, -1.1094, -0.8456, -0.7074, -1.6137],\n",
      "        [-0.1211,  1.0179,  1.0146,  1.5695,  0.9482]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6298, -0.4859, -0.5301, -0.8164,  1.4142],\n",
      "        [-0.5379,  1.8289,  1.3699,  1.3830, -1.7197],\n",
      "        [-0.3839, -0.4295, -0.6739,  0.6937,  0.0681],\n",
      "        [ 1.3314, -0.0858, -2.3225, -0.6625,  1.9366]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6016],\n",
      "        [-2.2468],\n",
      "        [ 0.5459],\n",
      "        [-1.8085]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0642,  1.8394,  0.3843,  0.9897, -0.0676],\n",
      "        [-0.1681,  0.3273,  0.2641,  0.7644,  1.3939],\n",
      "        [-1.2309, -0.3206, -0.5044, -0.9597,  2.0815],\n",
      "        [-0.8833,  0.4732,  0.5635,  1.2292,  0.5436]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1371,  0.6875, -0.1619, -0.2041,  0.7501],\n",
      "        [-0.7175, -2.2522, -0.6296, -1.0942, -2.6792],\n",
      "        [-0.7758, -0.2506, -0.3583, -0.7669, -1.6228],\n",
      "        [ 0.6387,  1.7707,  1.5495,  1.8625,  1.4194]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0642,  1.8394,  0.3843,  0.9897, -0.0676],\n",
      "        [-0.1681,  0.3273,  0.2641,  0.7644,  1.3939],\n",
      "        [-1.2309, -0.3206, -0.5044, -0.9597,  2.0815],\n",
      "        [-0.8833,  0.4732,  0.5635,  1.2292,  0.5436]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8036],\n",
      "        [-5.3536],\n",
      "        [-1.4259],\n",
      "        [ 4.2079]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1461, -1.0165, -0.0220, -0.2493,  1.3528],\n",
      "        [ 1.6024,  0.5949, -1.0646, -0.1887, -0.3278],\n",
      "        [ 0.5735, -1.1737,  0.7083,  0.9105, -0.7963],\n",
      "        [ 0.3129,  0.2824,  0.9201, -0.3524,  1.0842]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0878,  0.2986, -0.0961,  0.1662,  0.0318],\n",
      "        [ 1.5867,  0.6407,  0.9607,  0.4844,  2.1611],\n",
      "        [-0.0610, -1.3951, -0.1941, -0.7022, -1.3855],\n",
      "        [-0.7481, -0.2261, -0.6549, -0.6309, -1.4399]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1461, -1.0165, -0.0220, -0.2493,  1.3528],\n",
      "        [ 1.6024,  0.5949, -1.0646, -0.1887, -0.3278],\n",
      "        [ 0.5735, -1.1737,  0.7083,  0.9105, -0.7963],\n",
      "        [ 0.3129,  0.2824,  0.9201, -0.3524,  1.0842]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2870],\n",
      "        [ 1.1009],\n",
      "        [ 1.9288],\n",
      "        [-2.2392]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8153, -0.1296,  0.1338,  0.9933,  0.6346],\n",
      "        [-0.1845,  0.4099,  0.1221, -1.5171, -0.0674],\n",
      "        [ 0.8669,  0.2795, -1.3582, -0.1665, -0.0518],\n",
      "        [ 0.0309,  1.8485, -0.2876,  0.7958,  1.0671]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0895,  0.0840, -0.0935,  0.2762,  0.5102],\n",
      "        [ 0.7062, -0.3098, -0.2959, -0.1623,  0.7319],\n",
      "        [ 0.0155, -1.9198, -1.1455, -1.1775, -2.4108],\n",
      "        [-0.5289,  0.3025,  1.0875,  0.5943,  0.9160]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8153, -0.1296,  0.1338,  0.9933,  0.6346],\n",
      "        [-0.1845,  0.4099,  0.1221, -1.5171, -0.0674],\n",
      "        [ 0.8669,  0.2795, -1.3582, -0.1665, -0.0518],\n",
      "        [ 0.0309,  1.8485, -0.2876,  0.7958,  1.0671]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5017],\n",
      "        [-0.0965],\n",
      "        [ 1.3537],\n",
      "        [ 1.6805]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0529, -0.5079,  0.3474,  0.1127,  0.3331],\n",
      "        [-1.5347,  0.5845,  0.2041,  0.4888,  0.6989],\n",
      "        [-0.0413,  0.3012,  0.2181,  1.7127,  0.3812],\n",
      "        [-0.5148,  0.6442,  0.6294, -1.0403, -0.5440]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2348,  0.2921,  0.3619,  0.1475,  0.4285],\n",
      "        [ 0.7714, -0.9345, -0.1114, -0.3142,  0.0936],\n",
      "        [ 0.3656, -0.4490,  0.0442,  0.1916,  0.3527],\n",
      "        [-0.5487,  0.0703,  0.6565,  0.0563,  0.5630]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0529, -0.5079,  0.3474,  0.1127,  0.3331],\n",
      "        [-1.5347,  0.5845,  0.2041,  0.4888,  0.6989],\n",
      "        [-0.0413,  0.3012,  0.2181,  1.7127,  0.3812],\n",
      "        [-0.5148,  0.6442,  0.6294, -1.0403, -0.5440]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1491],\n",
      "        [-1.8410],\n",
      "        [ 0.3219],\n",
      "        [ 0.3762]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1208,  1.1588,  1.0532, -0.0514, -2.0540],\n",
      "        [-0.2563,  1.1869, -1.0507,  1.0485,  1.1300],\n",
      "        [-0.0003, -0.1665,  1.3475, -0.9822,  0.8153],\n",
      "        [-2.0360, -1.8050, -1.5639, -1.2920,  0.6693]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2225,  1.0487,  0.7145,  0.3400,  0.2766],\n",
      "        [ 0.4591,  0.5858,  0.2424,  0.1365,  1.4360],\n",
      "        [-0.3291, -0.4811, -0.0847, -0.3917,  0.6042],\n",
      "        [-0.6122,  0.5163,  0.5613,  0.2257,  1.1220]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1208,  1.1588,  1.0532, -0.0514, -2.0540],\n",
      "        [-0.2563,  1.1869, -1.0507,  1.0485,  1.1300],\n",
      "        [-0.0003, -0.1665,  1.3475, -0.9822,  0.8153],\n",
      "        [-2.0360, -1.8050, -1.5639, -1.2920,  0.6693]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6315],\n",
      "        [ 2.0888],\n",
      "        [ 0.8434],\n",
      "        [-0.1040]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8742, -1.4007,  0.9113, -0.7136,  0.6753],\n",
      "        [-0.0452,  0.5503, -1.0196,  1.2144, -1.2693],\n",
      "        [-0.3598, -0.4648,  2.1162, -0.1513, -0.8644],\n",
      "        [ 0.9846,  1.3753,  1.1017, -0.7606, -0.1960]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8303, -0.3523, -1.2229, -0.1221, -1.0094],\n",
      "        [-0.5560, -1.2538, -1.2948, -0.7992, -0.3246],\n",
      "        [ 0.2785, -0.0635, -0.7597,  0.1615, -0.5038],\n",
      "        [-0.3846,  0.5405,  0.2143,  0.6148,  1.1655]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8742, -1.4007,  0.9113, -0.7136,  0.6753],\n",
      "        [-0.0452,  0.5503, -1.0196,  1.2144, -1.2693],\n",
      "        [-0.3598, -0.4648,  2.1162, -0.1513, -0.8644],\n",
      "        [ 0.9846,  1.3753,  1.1017, -0.7606, -0.1960]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4897],\n",
      "        [ 0.0970],\n",
      "        [-1.2673],\n",
      "        [-0.0952]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0893, -0.0235,  0.0090, -0.6794, -0.1652],\n",
      "        [ 1.7634, -1.2452,  0.3889, -0.8229,  0.3654],\n",
      "        [-0.4154, -1.4156, -0.7165,  0.6417,  0.0088],\n",
      "        [ 0.7374,  0.0364,  0.4067, -0.1036,  0.5331]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0867, -0.4463,  0.5232,  0.1288, -0.5112],\n",
      "        [ 0.2598,  0.3197, -1.3872, -0.9761, -0.5102],\n",
      "        [ 0.3841,  0.7178,  1.0757,  0.2030,  0.8963],\n",
      "        [ 0.0873,  1.0693,  0.3503,  0.2775,  0.5467]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0893, -0.0235,  0.0090, -0.6794, -0.1652],\n",
      "        [ 1.7634, -1.2452,  0.3889, -0.8229,  0.3654],\n",
      "        [-0.4154, -1.4156, -0.7165,  0.6417,  0.0088],\n",
      "        [ 0.7374,  0.0364,  0.4067, -0.1036,  0.5331]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0198],\n",
      "        [ 0.1373],\n",
      "        [-1.8082],\n",
      "        [ 0.5084]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3847, -0.1148,  0.8637,  0.7218,  0.1516],\n",
      "        [ 0.0894, -0.9067, -0.8864, -1.0433, -1.4831],\n",
      "        [-1.1330,  1.3789, -0.8702,  0.3709, -0.0839],\n",
      "        [ 0.4442,  0.6486, -1.6063, -1.2440,  0.0811]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2834,  0.5541, -0.0677, -0.2062,  0.0093],\n",
      "        [-0.0586, -0.8676,  0.1789, -0.7793, -1.1621],\n",
      "        [ 1.0497,  1.3738,  0.7751,  1.1044,  1.0377],\n",
      "        [-0.2989,  0.4298,  0.5797,  0.8554,  0.3514]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3847, -0.1148,  0.8637,  0.7218,  0.1516],\n",
      "        [ 0.0894, -0.9067, -0.8864, -1.0433, -1.4831],\n",
      "        [-1.1330,  1.3789, -0.8702,  0.3709, -0.0839],\n",
      "        [ 0.4442,  0.6486, -1.6063, -1.2440,  0.0811]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1605],\n",
      "        [ 3.1594],\n",
      "        [ 0.3532],\n",
      "        [-1.8208]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8189,  2.2273, -2.3796,  0.6972,  0.8341],\n",
      "        [-0.7323, -1.8559,  2.9521,  0.0437, -0.6739],\n",
      "        [ 0.1519,  0.5177,  0.4939, -1.4795, -0.5878],\n",
      "        [-1.2813, -0.2647, -0.4437,  1.7617,  1.2563]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0418, -0.1226, -0.1364,  0.1969,  0.2659],\n",
      "        [-1.0795, -0.7605, -1.8219, -1.7544, -2.4725],\n",
      "        [ 0.7160,  0.5238,  0.7295,  0.4925,  1.6640],\n",
      "        [ 0.3540,  1.3018,  1.2159,  1.1747,  1.8583]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8189,  2.2273, -2.3796,  0.6972,  0.8341],\n",
      "        [-0.7323, -1.8559,  2.9521,  0.0437, -0.6739],\n",
      "        [ 0.1519,  0.5177,  0.4939, -1.4795, -0.5878],\n",
      "        [-1.2813, -0.2647, -0.4437,  1.7617,  1.2563]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4447],\n",
      "        [-1.5868],\n",
      "        [-0.9666],\n",
      "        [ 3.0664]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6729, -1.6758, -0.1853, -2.0199,  0.0027],\n",
      "        [ 0.4594,  0.6189,  0.4391,  0.6383,  1.1126],\n",
      "        [-0.0502,  1.2004,  0.7165,  3.2400,  0.0587],\n",
      "        [-0.7842,  0.3086,  0.4956, -0.5134,  0.7041]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0197,  0.2379,  0.3522,  0.2839,  0.4565],\n",
      "        [-0.5813, -1.0534, -0.3879, -1.0179, -1.1676],\n",
      "        [ 0.9222,  0.4825,  0.7844,  1.0141,  1.1984],\n",
      "        [-0.1544,  0.7773,  0.2700, -0.5110, -0.1480]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6729, -1.6758, -0.1853, -2.0199,  0.0027],\n",
      "        [ 0.4594,  0.6189,  0.4391,  0.6383,  1.1126],\n",
      "        [-0.0502,  1.2004,  0.7165,  3.2400,  0.0587],\n",
      "        [-0.7842,  0.3086,  0.4956, -0.5134,  0.7041]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0493],\n",
      "        [-3.0381],\n",
      "        [ 4.4509],\n",
      "        [ 0.6529]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6503, -0.7829, -0.7346,  0.9590, -0.0406],\n",
      "        [-1.1423, -1.6838,  1.0465, -1.5012, -1.2984],\n",
      "        [ 0.5442, -0.7869,  0.0054,  1.3207, -0.8038],\n",
      "        [ 0.0970,  0.2150, -0.9047,  0.1998, -0.3338]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1068, -0.1059,  0.5433, -0.1933,  0.6481],\n",
      "        [ 0.5178,  0.3830,  0.6620, -0.2441,  0.7991],\n",
      "        [-0.3436, -1.2190, -1.1785, -0.8068, -1.3912],\n",
      "        [-0.3433,  0.2643,  0.3147, -0.7923,  0.5666]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6503, -0.7829, -0.7346,  0.9590, -0.0406],\n",
      "        [-1.1423, -1.6838,  1.0465, -1.5012, -1.2984],\n",
      "        [ 0.5442, -0.7869,  0.0054,  1.3207, -0.8038],\n",
      "        [ 0.0970,  0.2150, -0.9047,  0.1998, -0.3338]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5973],\n",
      "        [-1.2148],\n",
      "        [ 0.8186],\n",
      "        [-0.6086]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1452,  0.3099,  0.9070, -1.1511,  0.2729],\n",
      "        [-2.0526, -0.1978,  0.2405,  0.5012, -1.1969],\n",
      "        [-0.4699,  0.5889,  0.6318,  1.5351, -0.3021],\n",
      "        [ 0.4552, -0.0758,  2.1466,  0.1385,  1.8709]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2409,  0.1912,  0.6356,  0.2110,  0.8050],\n",
      "        [ 0.8051,  0.6713,  0.7396, -0.4158,  1.3366],\n",
      "        [-0.5668, -0.9297, -1.8690, -1.1746, -2.0358],\n",
      "        [-0.2369,  0.8608,  1.1353,  1.2531,  0.5101]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1452,  0.3099,  0.9070, -1.1511,  0.2729],\n",
      "        [-2.0526, -0.1978,  0.2405,  0.5012, -1.1969],\n",
      "        [-0.4699,  0.5889,  0.6318,  1.5351, -0.3021],\n",
      "        [ 0.4552, -0.0758,  2.1466,  0.1385,  1.8709]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5776],\n",
      "        [-3.4157],\n",
      "        [-2.6499],\n",
      "        [ 3.3920]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3362, -0.2602, -0.5115, -0.4872,  0.9110],\n",
      "        [-0.5358, -1.5365,  0.0109, -0.2400,  0.9925],\n",
      "        [-1.3814, -0.8963,  0.4319,  1.0037, -1.0929],\n",
      "        [-0.0863,  1.1608, -0.7740, -0.6014, -0.1184]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3471,  0.3845, -0.1378, -0.2786,  0.4417],\n",
      "        [ 2.0166,  1.9368,  2.0255,  1.6044,  2.5195],\n",
      "        [-0.0208, -0.3920, -0.4864, -0.2303, -0.3369],\n",
      "        [-0.7486, -0.8874, -1.3192, -0.9970, -2.2573]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3362, -0.2602, -0.5115, -0.4872,  0.9110],\n",
      "        [-0.5358, -1.5365,  0.0109, -0.2400,  0.9925],\n",
      "        [-1.3814, -0.8963,  0.4319,  1.0037, -1.0929],\n",
      "        [-0.0863,  1.1608, -0.7740, -0.6014, -0.1184]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3918],\n",
      "        [-1.9186],\n",
      "        [ 0.3071],\n",
      "        [ 0.9225]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.0870, -0.0029, -1.0620,  0.3501, -0.6351],\n",
      "        [-0.5907, -1.5505,  0.1266,  0.2287,  1.1604],\n",
      "        [-1.8043, -0.3946, -0.8874, -0.4651,  1.8659],\n",
      "        [ 0.3074,  0.0301,  1.0100,  0.4519,  0.5565]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1313,  0.3525,  0.1987,  0.4436, -0.1447],\n",
      "        [ 0.3040,  0.6048, -0.2675, -0.3482, -0.0604],\n",
      "        [ 0.3376, -0.4793, -0.5475, -0.5815, -0.9885],\n",
      "        [-1.3388, -0.7406, -1.2830, -1.4361, -2.6448]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.0870, -0.0029, -1.0620,  0.3501, -0.6351],\n",
      "        [-0.5907, -1.5505,  0.1266,  0.2287,  1.1604],\n",
      "        [-1.8043, -0.3946, -0.8874, -0.4651,  1.8659],\n",
      "        [ 0.3074,  0.0301,  1.0100,  0.4519,  0.5565]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2388],\n",
      "        [-1.3010],\n",
      "        [-1.5080],\n",
      "        [-3.8504]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6425, -1.6130, -1.6418, -0.2362, -1.3502],\n",
      "        [-1.5182,  1.4053,  1.6116, -0.0384, -1.7750],\n",
      "        [-1.9294, -0.2886, -0.2750, -0.7145, -0.0343],\n",
      "        [ 0.0386,  0.8673, -0.6944,  0.5754,  2.1190]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4868, -0.9560,  0.5358,  0.2416, -0.2224],\n",
      "        [ 0.3278,  0.6307,  0.5693, -0.0629,  1.2125],\n",
      "        [ 0.3513,  0.3723,  0.5118,  0.6256,  0.6252],\n",
      "        [ 0.1354,  0.5103, -0.8293,  0.0701, -0.5357]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6425, -1.6130, -1.6418, -0.2362, -1.3502],\n",
      "        [-1.5182,  1.4053,  1.6116, -0.0384, -1.7750],\n",
      "        [-1.9294, -0.2886, -0.2750, -0.7145, -0.0343],\n",
      "        [ 0.0386,  0.8673, -0.6944,  0.5754,  2.1190]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1059],\n",
      "        [-0.8437],\n",
      "        [-1.3945],\n",
      "        [-0.0713]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4959, -0.4986,  2.1047, -0.2481, -0.0517],\n",
      "        [-0.0540, -0.5136, -0.2725, -1.8503, -2.0000],\n",
      "        [-1.0360, -0.7425, -0.2536,  0.9222, -1.6207],\n",
      "        [-1.1217,  0.3537,  2.5395,  0.3382,  0.1752]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0204,  0.0612,  0.0562, -0.6174, -0.2832],\n",
      "        [ 0.7311,  0.5742,  0.7478,  0.7943,  1.4772],\n",
      "        [ 0.5786,  1.3154,  1.2861,  1.6554,  1.4465],\n",
      "        [ 0.7849, -0.2146, -0.0411, -0.0798,  0.3789]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4959, -0.4986,  2.1047, -0.2481, -0.0517],\n",
      "        [-0.0540, -0.5136, -0.2725, -1.8503, -2.0000],\n",
      "        [-1.0360, -0.7425, -0.2536,  0.9222, -1.6207],\n",
      "        [-1.1217,  0.3537,  2.5395,  0.3382,  0.1752]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2251],\n",
      "        [-4.9624],\n",
      "        [-2.7200],\n",
      "        [-1.0211]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1598,  0.7636, -0.7957,  1.3285, -0.9326],\n",
      "        [-0.1857, -0.6953, -0.2890,  0.9984,  1.3496],\n",
      "        [-0.5974,  0.5447,  2.1724,  1.3074,  1.1582],\n",
      "        [-0.3746, -0.8440,  0.7976,  0.5430,  0.1018]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0343, -0.4938,  0.0449,  0.2140, -0.0015],\n",
      "        [ 2.1480,  2.7158,  2.4909,  2.0663,  2.3412],\n",
      "        [ 1.7008,  1.5232,  1.3970,  1.1748,  2.6533],\n",
      "        [ 0.3432,  0.3041,  0.1919,  0.5856,  1.1063]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1598,  0.7636, -0.7957,  1.3285, -0.9326],\n",
      "        [-0.1857, -0.6953, -0.2890,  0.9984,  1.3496],\n",
      "        [-0.5974,  0.5447,  2.1724,  1.3074,  1.1582],\n",
      "        [-0.3746, -0.8440,  0.7976,  0.5430,  0.1018]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1325],\n",
      "        [ 2.2155],\n",
      "        [ 7.4574],\n",
      "        [ 0.1984]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7588,  2.1036, -0.3910,  1.8220,  1.9826],\n",
      "        [ 0.3480,  0.0029,  1.0212, -1.0017,  1.1428],\n",
      "        [-1.1898,  1.5460, -0.3719, -1.2703,  0.8163],\n",
      "        [ 0.1605, -1.4324,  1.0903,  1.1279,  0.6650]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3875,  0.9346, -0.1373,  0.1412, -0.8174],\n",
      "        [-0.1170,  0.0279,  0.0178, -0.2597,  0.2851],\n",
      "        [-1.1178, -0.7266, -1.1098, -1.9142, -2.4310],\n",
      "        [ 0.0119,  0.5366,  0.4144, -0.4772, -0.1824]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7588,  2.1036, -0.3910,  1.8220,  1.9826],\n",
      "        [ 0.3480,  0.0029,  1.0212, -1.0017,  1.1428],\n",
      "        [-1.1898,  1.5460, -0.3719, -1.2703,  0.8163],\n",
      "        [ 0.1605, -1.4324,  1.0903,  1.1279,  0.6650]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3625],\n",
      "        [ 0.5635],\n",
      "        [ 1.0662],\n",
      "        [-0.9744]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2083,  1.2438, -0.7075, -1.4949, -2.0490],\n",
      "        [ 1.5322,  1.1838, -0.6692, -1.6022,  1.2188],\n",
      "        [-0.7053,  0.7670, -0.2069,  0.0328,  0.2500],\n",
      "        [-0.4271, -0.7790,  0.4356, -1.4319, -0.2244]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0284, -0.4853,  0.0186,  0.0406, -0.0400],\n",
      "        [ 0.1366,  0.0937,  0.0120,  0.1276,  0.3256],\n",
      "        [-1.1583, -1.0687, -1.6106, -1.4926, -3.2719],\n",
      "        [ 0.3647,  0.4359,  0.5173,  0.9730,  0.7002]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2083,  1.2438, -0.7075, -1.4949, -2.0490],\n",
      "        [ 1.5322,  1.1838, -0.6692, -1.6022,  1.2188],\n",
      "        [-0.7053,  0.7670, -0.2069,  0.0328,  0.2500],\n",
      "        [-0.4271, -0.7790,  0.4356, -1.4319, -0.2244]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5612],\n",
      "        [ 0.5046],\n",
      "        [-0.5365],\n",
      "        [-1.8205]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1360, -0.0532,  0.8670,  1.1975, -1.0591],\n",
      "        [ 1.2234,  0.0054, -0.6499, -0.5376,  0.4700],\n",
      "        [ 0.4694, -1.4278,  0.4093, -0.0308, -0.2130],\n",
      "        [ 1.0536,  1.2462,  0.2393, -1.1849,  1.0734]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1959,  0.2302, -0.5962,  0.4212, -0.7193],\n",
      "        [-0.1637,  0.3679,  0.5340,  0.2094,  0.6604],\n",
      "        [-0.7122, -1.0469, -1.1993, -1.3820, -2.7742],\n",
      "        [ 1.7908,  1.3296,  0.9384,  1.3155,  1.5577]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1360, -0.0532,  0.8670,  1.1975, -1.0591],\n",
      "        [ 1.2234,  0.0054, -0.6499, -0.5376,  0.4700],\n",
      "        [ 0.4694, -1.4278,  0.4093, -0.0308, -0.2130],\n",
      "        [ 1.0536,  1.2462,  0.2393, -1.1849,  1.0734]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9595],\n",
      "        [-0.3475],\n",
      "        [ 1.3030],\n",
      "        [ 3.8817]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8978,  0.5568,  0.4351,  0.1691,  1.3895],\n",
      "        [-0.8187,  0.0769,  1.0855, -1.2046, -0.6790],\n",
      "        [ 0.4580,  0.0544,  0.0647, -0.8953,  0.8958],\n",
      "        [ 0.5233,  0.0882,  0.5847, -0.0189, -0.7329]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0658, -0.1787, -0.6540, -0.6104, -0.7987],\n",
      "        [-0.0879,  0.1314,  0.5882,  0.0088,  0.1436],\n",
      "        [-1.0577, -1.8454, -1.4100, -1.7514, -3.0361],\n",
      "        [-0.6713, -1.5622, -1.1490, -1.3060, -1.4722]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8978,  0.5568,  0.4351,  0.1691,  1.3895],\n",
      "        [-0.8187,  0.0769,  1.0855, -1.2046, -0.6790],\n",
      "        [ 0.4580,  0.0544,  0.0647, -0.8953,  0.8958],\n",
      "        [ 0.5233,  0.0882,  0.5847, -0.0189, -0.7329]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5381],\n",
      "        [ 0.6125],\n",
      "        [-1.8276],\n",
      "        [-0.0573]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3163,  0.3874, -0.7798, -0.4627, -1.6093],\n",
      "        [-0.3933, -1.9895, -0.4373,  2.4174,  0.4260],\n",
      "        [ 0.2013,  0.6375, -0.5004, -0.2390,  1.6780],\n",
      "        [ 0.5662, -1.2938,  0.9801,  0.3766,  1.1834]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2980,  0.1009, -0.3457, -0.4711,  0.1543],\n",
      "        [ 0.3877,  0.8454, -0.7382,  0.4792, -0.0651],\n",
      "        [ 0.5213, -0.8470, -0.1431, -0.3789, -0.1342],\n",
      "        [-0.5845, -0.9635, -0.5329, -0.3152, -1.3620]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3163,  0.3874, -0.7798, -0.4627, -1.6093],\n",
      "        [-0.3933, -1.9895, -0.4373,  2.4174,  0.4260],\n",
      "        [ 0.2013,  0.6375, -0.5004, -0.2390,  1.6780],\n",
      "        [ 0.5662, -1.2938,  0.9801,  0.3766,  1.1834]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3726],\n",
      "        [-0.3808],\n",
      "        [-0.4981],\n",
      "        [-1.3372]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1057,  1.1797, -0.1054,  0.4380,  0.9713],\n",
      "        [ 0.2702,  1.0324,  1.2721, -1.0384, -0.5683],\n",
      "        [ 1.2609, -0.1467,  0.1358,  1.8444,  0.7762],\n",
      "        [-0.9245, -0.9735,  0.0592,  0.4394,  1.2515]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0300, -0.6501,  0.0909,  0.1551, -0.4515],\n",
      "        [ 0.3866, -0.1988, -0.5217,  0.3230,  0.7335],\n",
      "        [-0.0589, -0.1225, -0.0011, -0.2878, -0.0101],\n",
      "        [ 0.5194,  0.1743,  0.2327,  0.1315,  0.8782]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1057,  1.1797, -0.1054,  0.4380,  0.9713],\n",
      "        [ 0.2702,  1.0324,  1.2721, -1.0384, -0.5683],\n",
      "        [ 1.2609, -0.1467,  0.1358,  1.8444,  0.7762],\n",
      "        [-0.9245, -0.9735,  0.0592,  0.4394,  1.2515]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1439],\n",
      "        [-1.5167],\n",
      "        [-0.5950],\n",
      "        [ 0.5207]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0609, -0.3028, -0.4875,  1.4231, -0.0756],\n",
      "        [ 0.2144,  0.4624, -1.2901,  0.8904, -0.1723],\n",
      "        [ 0.8321,  1.3191,  0.3987, -1.8044,  1.4640],\n",
      "        [-0.5357,  2.1631, -0.4321, -0.0231,  0.6476]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0083,  0.2017,  0.1972, -0.1583, -0.0406],\n",
      "        [ 0.7583,  1.0588,  0.5417,  1.1937,  1.5980],\n",
      "        [ 0.4143, -0.1317, -0.1772, -0.7863, -0.1947],\n",
      "        [ 0.3505,  0.0271,  0.0062, -0.3450, -0.1957]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0609, -0.3028, -0.4875,  1.4231, -0.0756],\n",
      "        [ 0.2144,  0.4624, -1.2901,  0.8904, -0.1723],\n",
      "        [ 0.8321,  1.3191,  0.3987, -1.8044,  1.4640],\n",
      "        [-0.5357,  2.1631, -0.4321, -0.0231,  0.6476]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3789],\n",
      "        [ 0.7410],\n",
      "        [ 1.2342],\n",
      "        [-0.2506]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5051,  0.1875, -0.0270, -0.4311, -1.0468],\n",
      "        [-0.4709, -1.0711,  0.0789,  0.2347,  0.9459],\n",
      "        [-0.9891, -0.7585, -0.5332,  0.5033,  0.5921],\n",
      "        [ 0.1085,  1.9546,  0.2282,  1.1156,  1.2285]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5222, -0.0354, -0.4007, -0.3566, -0.3550],\n",
      "        [-0.0523,  0.5491,  0.3327, -0.0262, -0.3533],\n",
      "        [-0.3515, -0.4774, -0.4521, -0.9079, -0.8058],\n",
      "        [-0.0563, -0.4612, -0.3588,  0.0195, -0.1882]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5051,  0.1875, -0.0270, -0.4311, -1.0468],\n",
      "        [-0.4709, -1.0711,  0.0789,  0.2347,  0.9459],\n",
      "        [-0.9891, -0.7585, -0.5332,  0.5033,  0.5921],\n",
      "        [ 0.1085,  1.9546,  0.2282,  1.1156,  1.2285]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7933],\n",
      "        [-0.8776],\n",
      "        [ 0.0168],\n",
      "        [-1.1990]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.9839, -0.5576, -0.5262, -0.9086,  1.7095],\n",
      "        [-0.1305,  0.8048, -0.5148,  0.7228,  1.3536],\n",
      "        [ 2.3876, -3.2701,  1.7844,  0.4854,  0.5949],\n",
      "        [ 0.2562,  1.7660, -2.4694, -0.9999, -0.1264]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9024, -0.7096, -1.1173, -0.4248, -0.5734],\n",
      "        [ 0.3757,  0.1986,  1.0920,  1.0436,  0.3474],\n",
      "        [ 0.3611, -0.4355, -0.6892, -0.5724, -0.8060],\n",
      "        [-0.0651,  0.8538,  0.2294,  0.2603,  1.2642]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.9839, -0.5576, -0.5262, -0.9086,  1.7095],\n",
      "        [-0.1305,  0.8048, -0.5148,  0.7228,  1.3536],\n",
      "        [ 2.3876, -3.2701,  1.7844,  0.4854,  0.5949],\n",
      "        [ 0.2562,  1.7660, -2.4694, -0.9999, -0.1264]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4008],\n",
      "        [ 0.7732],\n",
      "        [ 0.2992],\n",
      "        [ 0.5047]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1928,  0.5502, -0.6003,  1.8827, -0.7166],\n",
      "        [-0.4349,  1.4046, -1.7117,  0.9503,  1.4593],\n",
      "        [-1.1066,  0.7120, -0.4326,  0.1698,  0.0166],\n",
      "        [ 1.7126, -1.3394,  1.4340,  1.0154,  1.5385]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5136, -0.4151,  0.0476, -0.0219,  0.6531],\n",
      "        [ 0.3155,  0.6444,  0.0881,  0.6521,  1.2228],\n",
      "        [ 0.2657, -0.5782,  0.3868,  0.4118, -0.3377],\n",
      "        [ 0.6093, -0.2407, -0.2358, -0.0473, -0.5348]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1928,  0.5502, -0.6003,  1.8827, -0.7166],\n",
      "        [-0.4349,  1.4046, -1.7117,  0.9503,  1.4593],\n",
      "        [-1.1066,  0.7120, -0.4326,  0.1698,  0.0166],\n",
      "        [ 1.7126, -1.3394,  1.4340,  1.0154,  1.5385]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1535],\n",
      "        [ 3.0212],\n",
      "        [-0.8087],\n",
      "        [ 0.1569]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5689, -1.0849,  0.4295, -0.2983, -1.2124],\n",
      "        [ 0.9385, -1.1181,  1.0113, -1.2794,  0.9163],\n",
      "        [-1.3272,  1.2799,  1.3765,  0.5772,  0.0279],\n",
      "        [ 0.4626, -1.1797, -0.3868,  0.3578,  0.8852]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1550, -1.0331,  0.1155, -0.0024, -1.0637],\n",
      "        [-0.9478, -0.7163, -1.0732, -1.1484, -2.2746],\n",
      "        [ 0.4154, -0.3592, -0.0932, -0.6298, -0.0454],\n",
      "        [ 0.7250,  0.1540,  0.3513,  0.7609,  0.1587]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5689, -1.0849,  0.4295, -0.2983, -1.2124],\n",
      "        [ 0.9385, -1.1181,  1.0113, -1.2794,  0.9163],\n",
      "        [-1.3272,  1.2799,  1.3765,  0.5772,  0.0279],\n",
      "        [ 0.4626, -1.1797, -0.3868,  0.3578,  0.8852]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.5491],\n",
      "        [-1.7888],\n",
      "        [-1.5042],\n",
      "        [ 0.4306]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0994, -1.5581, -0.4263, -1.3548,  0.4200],\n",
      "        [-0.2991,  1.4297,  0.1826, -0.0918,  0.1285],\n",
      "        [ 0.5502, -1.5177, -0.0236,  2.3078,  1.7221],\n",
      "        [ 0.1364,  1.5700,  3.6267,  0.3818,  0.6764]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4190, -1.5341, -0.8927, -1.2773, -2.3603],\n",
      "        [ 0.0660, -0.2243, -0.2270,  0.5398, -0.6051],\n",
      "        [ 0.2426,  0.1083,  0.8979,  0.4264,  0.9573],\n",
      "        [ 0.0476,  0.4096, -0.0960,  0.2334,  0.1918]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0994, -1.5581, -0.4263, -1.3548,  0.4200],\n",
      "        [-0.2991,  1.4297,  0.1826, -0.0918,  0.1285],\n",
      "        [ 0.5502, -1.5177, -0.0236,  2.3078,  1.7221],\n",
      "        [ 0.1364,  1.5700,  3.6267,  0.3818,  0.6764]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.4685],\n",
      "        [-0.5092],\n",
      "        [ 2.5806],\n",
      "        [ 0.5202]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1497, -1.2768,  0.9376, -1.8335, -0.6239],\n",
      "        [-1.8240, -0.7339,  0.2851,  0.4317,  0.4788],\n",
      "        [ 1.7236,  0.6024,  1.6178,  1.7835,  1.6441],\n",
      "        [-0.1400, -1.3354,  0.9086, -0.2734,  0.9347]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.3762, -2.1199, -1.3819, -2.9900, -4.2072],\n",
      "        [ 0.3637, -0.0693, -0.2934, -1.0784, -0.0630],\n",
      "        [-0.3713, -0.4916, -0.6232, -0.5058, -1.4011],\n",
      "        [ 0.0731,  0.3020,  0.7797, -0.2323,  0.2363]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1497, -1.2768,  0.9376, -1.8335, -0.6239],\n",
      "        [-1.8240, -0.7339,  0.2851,  0.4317,  0.4788],\n",
      "        [ 1.7236,  0.6024,  1.6178,  1.7835,  1.6441],\n",
      "        [-0.1400, -1.3354,  0.9086, -0.2734,  0.9347]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 9.7242],\n",
      "        [-1.1920],\n",
      "        [-5.1499],\n",
      "        [ 0.5794]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3774,  0.7340,  0.1415,  0.4017,  0.9367],\n",
      "        [-1.1367, -0.4845,  1.9777, -1.6429,  0.5085],\n",
      "        [-1.1932, -0.4442,  0.7989,  1.0637, -1.0536],\n",
      "        [ 0.1321, -0.6326,  0.9031,  1.2418,  0.4756]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1373, -0.2867,  0.3596, -0.0593,  0.2498],\n",
      "        [ 0.8963,  0.6363,  0.7188,  0.3580,  0.7520],\n",
      "        [ 0.7734,  1.6557,  1.6668,  1.4091,  1.8427],\n",
      "        [ 0.1080,  0.9043,  0.5337,  0.4009,  0.0447]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3774,  0.7340,  0.1415,  0.4017,  0.9367],\n",
      "        [-1.1367, -0.4845,  1.9777, -1.6429,  0.5085],\n",
      "        [-1.1932, -0.4442,  0.7989,  1.0637, -1.0536],\n",
      "        [ 0.1321, -0.6326,  0.9031,  1.2418,  0.4756]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1024],\n",
      "        [-0.1112],\n",
      "        [-0.7692],\n",
      "        [ 0.4433]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2912, -0.1650,  2.5102,  0.1816, -1.7764],\n",
      "        [-0.5914, -1.0518, -0.1742, -1.0884, -0.7242],\n",
      "        [ 2.4173, -1.2693, -1.4917, -1.1398,  0.6324],\n",
      "        [ 0.8874,  0.1682,  2.4054,  1.5076,  1.0086]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0532, -0.2314,  0.8720, -0.2021,  0.2089],\n",
      "        [ 0.3655, -0.5187,  0.1029, -0.4124,  0.0652],\n",
      "        [ 1.2427,  2.0433,  1.7084,  1.9013,  1.8155],\n",
      "        [-0.2409, -0.1380, -0.1633,  0.8925,  0.2901]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2912, -0.1650,  2.5102,  0.1816, -1.7764],\n",
      "        [-0.5914, -1.0518, -0.1742, -1.0884, -0.7242],\n",
      "        [ 2.4173, -1.2693, -1.4917, -1.1398,  0.6324],\n",
      "        [ 0.8874,  0.1682,  2.4054,  1.5076,  1.0086]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8038],\n",
      "        [ 0.7132],\n",
      "        [-3.1567],\n",
      "        [ 1.0083]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6047, -0.7671,  2.2934,  0.8673,  1.3188],\n",
      "        [ 1.3789, -0.1746,  0.7230, -0.0670,  0.4003],\n",
      "        [-1.3908,  0.5862, -0.9690, -2.3036,  0.4449],\n",
      "        [ 1.8553,  2.1415,  0.9997,  0.1296,  0.2201]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7744, -1.2671, -0.6796, -0.8958, -2.0987],\n",
      "        [ 0.2905,  0.1780,  0.5436,  0.1378, -0.5714],\n",
      "        [ 0.0450,  0.2947, -0.1045, -0.0983,  0.7191],\n",
      "        [-0.5837,  0.2188,  0.5725,  0.1665, -0.6488]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6047, -0.7671,  2.2934,  0.8673,  1.3188],\n",
      "        [ 1.3789, -0.1746,  0.7230, -0.0670,  0.4003],\n",
      "        [-1.3908,  0.5862, -0.9690, -2.3036,  0.4449],\n",
      "        [ 1.8553,  2.1415,  0.9997,  0.1296,  0.2201]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.5996],\n",
      "        [ 0.5246],\n",
      "        [ 0.7579],\n",
      "        [-0.1632]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0746,  1.9680,  0.0720,  1.2813, -1.0525],\n",
      "        [ 0.4910, -1.4225,  1.3602, -0.6810, -0.5990],\n",
      "        [-0.1985,  0.4982,  1.9377,  0.6011, -0.6430],\n",
      "        [-0.5954,  0.1124,  2.5296, -1.9466,  0.3113]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3890,  2.1442,  1.9424,  0.1845,  1.8741],\n",
      "        [-0.1740, -0.4492,  0.4376, -0.3066, -0.3632],\n",
      "        [ 0.2269, -0.1995, -0.1511,  0.0265,  0.1981],\n",
      "        [-0.3805,  0.8899,  0.2892,  0.6655,  0.5473]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0746,  1.9680,  0.0720,  1.2813, -1.0525],\n",
      "        [ 0.4910, -1.4225,  1.3602, -0.6810, -0.5990],\n",
      "        [-0.1985,  0.4982,  1.9377,  0.6011, -0.6430],\n",
      "        [-0.5954,  0.1124,  2.5296, -1.9466,  0.3113]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1307],\n",
      "        [ 1.5752],\n",
      "        [-0.5486],\n",
      "        [-0.0670]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9152, -0.0107,  0.8392,  0.8510,  0.7120],\n",
      "        [ 0.3292, -1.9887,  0.9397, -0.2624,  1.8579],\n",
      "        [-0.2088,  0.1740,  0.4702, -0.0814, -0.9635],\n",
      "        [-1.4531, -0.2710,  0.8417,  0.2866, -0.8534]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1950,  1.4171,  1.5308,  0.8248,  1.5481],\n",
      "        [-0.3963, -0.1265, -0.7213, -0.4039, -1.3083],\n",
      "        [ 0.2128, -0.0503, -0.1495, -0.1189, -0.3526],\n",
      "        [-0.3298,  0.1347,  0.4770, -0.0876,  0.8974]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9152, -0.0107,  0.8392,  0.8510,  0.7120],\n",
      "        [ 0.3292, -1.9887,  0.9397, -0.2624,  1.8579],\n",
      "        [-0.2088,  0.1740,  0.4702, -0.0814, -0.9635],\n",
      "        [-1.4531, -0.2710,  0.8417,  0.2866, -0.8534]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.1674],\n",
      "        [-2.8814],\n",
      "        [ 0.2260],\n",
      "        [ 0.0532]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4022,  1.9500,  0.5016, -1.7771, -0.1847],\n",
      "        [ 0.0430, -1.4997, -0.1203, -1.2326,  0.5031],\n",
      "        [ 0.4032, -0.5105,  0.2012, -3.1828, -0.7720],\n",
      "        [ 0.1354,  1.0285,  0.3536,  2.6218,  1.2915]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6121, -1.8708, -0.3092, -0.9199, -1.5705],\n",
      "        [ 0.7009,  1.0126,  0.5858,  0.8655,  0.5717],\n",
      "        [ 0.0532,  0.5280, -0.3423,  0.0215,  0.0274],\n",
      "        [-0.1844,  0.2851,  1.3216,  0.2695,  0.4516]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4022,  1.9500,  0.5016, -1.7771, -0.1847],\n",
      "        [ 0.0430, -1.4997, -0.1203, -1.2326,  0.5031],\n",
      "        [ 0.4032, -0.5105,  0.2012, -3.1828, -0.7720],\n",
      "        [ 0.1354,  1.0285,  0.3536,  2.6218,  1.2915]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1244],\n",
      "        [-2.3383],\n",
      "        [-0.4065],\n",
      "        [ 2.0252]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4643,  2.3794,  0.6144, -0.2839, -0.5497],\n",
      "        [-0.6002, -0.3979, -0.5723,  1.1731, -1.5578],\n",
      "        [ 0.7685,  1.2170, -0.0414,  1.2784,  0.0352],\n",
      "        [-0.2578, -0.9684,  0.4610,  0.2656, -0.1536]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2847, -0.1927,  0.3215, -0.1388,  0.0826],\n",
      "        [ 1.6035,  1.7121,  1.3800,  1.0995,  2.2986],\n",
      "        [ 0.4696, -0.1474,  0.3623, -0.3570, -0.6239],\n",
      "        [-0.7141,  0.1757,  0.0987, -0.5085, -1.7170]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4643,  2.3794,  0.6144, -0.2839, -0.5497],\n",
      "        [-0.6002, -0.3979, -0.5723,  1.1731, -1.5578],\n",
      "        [ 0.7685,  1.2170, -0.0414,  1.2784,  0.0352],\n",
      "        [-0.2578, -0.9684,  0.4610,  0.2656, -0.1536]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3991],\n",
      "        [-4.7243],\n",
      "        [-0.3118],\n",
      "        [ 0.1880]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4886,  1.3901,  0.4559,  0.7426, -0.5094],\n",
      "        [-0.7864, -0.6287,  0.6653, -0.1555,  1.0268],\n",
      "        [-2.0049,  0.9483, -0.5671,  1.0852,  0.7335],\n",
      "        [ 0.1314,  0.2929,  0.3757,  0.7822, -0.3111]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1444, -0.0240,  0.1236, -0.3076, -0.1135],\n",
      "        [-0.0101, -0.3644, -0.0527,  0.0942,  0.5379],\n",
      "        [ 0.5627, -0.3002, -0.3393,  0.3804,  0.4370],\n",
      "        [-0.7023, -0.2380, -0.5472, -0.4756, -0.8903]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4886,  1.3901,  0.4559,  0.7426, -0.5094],\n",
      "        [-0.7864, -0.6287,  0.6653, -0.1555,  1.0268],\n",
      "        [-2.0049,  0.9483, -0.5671,  1.0852,  0.7335],\n",
      "        [ 0.1314,  0.2929,  0.3757,  0.7822, -0.3111]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2181],\n",
      "        [ 0.7396],\n",
      "        [-0.4871],\n",
      "        [-0.4626]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1636,  0.3802, -0.1976, -0.7932,  1.3798],\n",
      "        [-1.5034,  1.1494,  0.2889,  1.3551, -0.2127],\n",
      "        [ 0.1642,  1.7289, -0.4536,  0.2218, -0.8413],\n",
      "        [ 1.2036, -0.8113, -0.9529, -0.8723, -1.3547]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1663, -0.1088,  1.1143, -0.0920,  0.5431],\n",
      "        [ 0.2012, -0.1665, -0.0207,  0.1255,  0.3662],\n",
      "        [ 0.6746,  0.2861, -0.1310, -0.1235,  0.1523],\n",
      "        [-0.1167,  0.9915,  0.4109,  0.7470, -0.3634]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1636,  0.3802, -0.1976, -0.7932,  1.3798],\n",
      "        [-1.5034,  1.1494,  0.2889,  1.3551, -0.2127],\n",
      "        [ 0.1642,  1.7289, -0.4536,  0.2218, -0.8413],\n",
      "        [ 1.2036, -0.8113, -0.9529, -0.8723, -1.3547]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5336],\n",
      "        [-0.4077],\n",
      "        [ 0.5092],\n",
      "        [-1.4957]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8430,  0.4864, -0.0942, -0.3219, -0.0623],\n",
      "        [ 0.6802,  0.2101, -0.5517,  0.9755, -0.4928],\n",
      "        [-0.2067,  1.0179,  0.7093,  1.4940,  1.4000],\n",
      "        [-0.7615, -1.2091,  0.0303,  0.2548, -0.4803]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1904,  0.5787, -0.2674,  0.1204, -0.2537],\n",
      "        [-0.0146,  0.4275, -1.2468, -0.2540,  0.1052],\n",
      "        [-0.0348, -0.8602,  0.0924,  1.0134, -0.1208],\n",
      "        [ 0.9427,  0.8351,  0.5318,  0.5974,  0.9570]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8430,  0.4864, -0.0942, -0.3219, -0.0623],\n",
      "        [ 0.6802,  0.2101, -0.5517,  0.9755, -0.4928],\n",
      "        [-0.2067,  1.0179,  0.7093,  1.4940,  1.4000],\n",
      "        [-0.7615, -1.2091,  0.0303,  0.2548, -0.4803]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4442],\n",
      "        [ 0.4682],\n",
      "        [ 0.5420],\n",
      "        [-2.0189]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6732, -1.0148, -0.4325,  0.4323,  0.3089],\n",
      "        [-0.0948, -1.5181,  0.9342,  0.7754, -0.1261],\n",
      "        [-0.6088,  1.2451,  1.1082,  0.3053,  0.7170],\n",
      "        [ 1.2653,  0.8030,  0.5666, -1.6931,  0.0871]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2461,  0.5005,  0.4941,  0.1160,  0.3583],\n",
      "        [-0.0055,  0.1954,  0.0880,  0.4894,  0.0530],\n",
      "        [ 0.0508,  0.0799, -0.6007,  0.3283,  0.1426],\n",
      "        [ 1.0621,  0.9051,  1.4561,  1.0281,  2.3976]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6732, -1.0148, -0.4325,  0.4323,  0.3089],\n",
      "        [-0.0948, -1.5181,  0.9342,  0.7754, -0.1261],\n",
      "        [-0.6088,  1.2451,  1.1082,  0.3053,  0.7170],\n",
      "        [ 1.2653,  0.8030,  0.5666, -1.6931,  0.0871]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9725],\n",
      "        [ 0.1589],\n",
      "        [-0.3947],\n",
      "        [ 1.3638]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6104,  1.3182,  0.7582,  0.5483, -0.5257],\n",
      "        [ 0.7966,  0.2626, -0.0344, -0.4887,  0.2844],\n",
      "        [ 0.1676, -1.3782,  0.2428, -1.6021,  0.8918],\n",
      "        [-1.1840, -0.3991, -0.0077, -0.2986, -0.0546]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6958,  1.1807,  1.0933,  0.2101,  0.7776],\n",
      "        [-0.0121,  0.3972, -0.3491,  0.1771, -0.2841],\n",
      "        [ 0.1163, -0.1187, -0.0988, -0.5812,  0.0945],\n",
      "        [ 0.4106,  0.8389,  1.4432,  0.9675,  1.3777]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6104,  1.3182,  0.7582,  0.5483, -0.5257],\n",
      "        [ 0.7966,  0.2626, -0.0344, -0.4887,  0.2844],\n",
      "        [ 0.1676, -1.3782,  0.2428, -1.6021,  0.8918],\n",
      "        [-1.1840, -0.3991, -0.0077, -0.2986, -0.0546]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6671],\n",
      "        [-0.0607],\n",
      "        [ 1.1746],\n",
      "        [-1.1962]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5049, -1.2776,  0.8383, -0.2747, -0.6254],\n",
      "        [-0.9193,  0.5373, -0.0495,  1.2448, -0.1027],\n",
      "        [-0.8112, -1.7263,  0.0055,  0.2265, -0.2018],\n",
      "        [-0.4040,  0.3317,  0.0172, -0.6368, -0.0295]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5279, -0.5627, -0.1560,  0.6138, -0.6836],\n",
      "        [-0.0629,  0.2264, -0.2585,  0.0962,  0.2515],\n",
      "        [-0.0658, -0.1693,  0.3026, -0.5758, -0.9500],\n",
      "        [ 0.5704,  0.4461,  0.9609,  0.6616,  1.5297]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5049, -1.2776,  0.8383, -0.2747, -0.6254],\n",
      "        [-0.9193,  0.5373, -0.0495,  1.2448, -0.1027],\n",
      "        [-0.8112, -1.7263,  0.0055,  0.2265, -0.2018],\n",
      "        [-0.4040,  0.3317,  0.0172, -0.6368, -0.0295]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5805],\n",
      "        [ 0.2861],\n",
      "        [ 0.4085],\n",
      "        [-0.5325]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8052,  1.0544, -0.2185, -0.0727, -1.0875],\n",
      "        [-1.2247,  0.7219,  0.2215,  0.6691,  0.0500],\n",
      "        [ 0.7783,  3.6746, -0.5270, -0.4218,  0.4599],\n",
      "        [-0.9974,  1.2653,  0.8190,  1.0778, -0.0467]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1404, -0.4430, -0.3554,  0.0680, -0.8042],\n",
      "        [ 0.4779,  0.3568, -0.1950, -0.2163, -0.3218],\n",
      "        [-0.3451, -0.3510, -0.0905, -0.2375, -1.0330],\n",
      "        [ 0.8673,  0.7852,  1.0252,  1.0563,  1.5011]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8052,  1.0544, -0.2185, -0.0727, -1.0875],\n",
      "        [-1.2247,  0.7219,  0.2215,  0.6691,  0.0500],\n",
      "        [ 0.7783,  3.6746, -0.5270, -0.4218,  0.4599],\n",
      "        [-0.9974,  1.2653,  0.8190,  1.0778, -0.0467]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2267],\n",
      "        [-0.5317],\n",
      "        [-1.8856],\n",
      "        [ 2.0366]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3116,  2.5188,  1.6441,  0.4686, -0.3856],\n",
      "        [-1.3491, -0.1878,  0.7300, -0.6849,  0.3000],\n",
      "        [-0.9721,  0.8429,  0.7871, -0.4995,  1.5775],\n",
      "        [ 0.2835,  0.5497,  1.5485,  0.3958, -0.3591]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1927,  0.1537, -0.3266,  0.6429,  0.3495],\n",
      "        [ 0.0582,  0.0195, -0.2120,  0.6041,  0.1154],\n",
      "        [ 0.5665,  1.5893,  0.8115,  0.5508,  1.3155],\n",
      "        [ 0.2948,  0.2931,  0.4256,  1.3605, -0.2567]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3116,  2.5188,  1.6441,  0.4686, -0.3856],\n",
      "        [-1.3491, -0.1878,  0.7300, -0.6849,  0.3000],\n",
      "        [-0.9721,  0.8429,  0.7871, -0.4995,  1.5775],\n",
      "        [ 0.2835,  0.5497,  1.5485,  0.3958, -0.3591]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2694],\n",
      "        [-0.6160],\n",
      "        [ 3.2277],\n",
      "        [ 1.5343]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3916, -0.1255,  0.6305,  0.5409, -1.8528],\n",
      "        [-0.5831,  1.6030,  0.2835,  0.1321, -0.8357],\n",
      "        [-0.8991,  1.5051,  0.6212, -0.4072, -0.5666],\n",
      "        [ 1.5038, -0.1682, -0.7574,  0.9002,  0.0846]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2661,  0.1397, -0.6302,  0.3509,  0.3918],\n",
      "        [-0.3526,  0.1023,  0.3687,  0.1909, -0.1194],\n",
      "        [-0.4112, -0.5163, -0.9932, -0.5780, -1.8674],\n",
      "        [-0.4426,  0.6362, -0.5890,  0.2103, -0.9405]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3916, -0.1255,  0.6305,  0.5409, -1.8528],\n",
      "        [-0.5831,  1.6030,  0.2835,  0.1321, -0.8357],\n",
      "        [-0.8991,  1.5051,  0.6212, -0.4072, -0.5666],\n",
      "        [ 1.5038, -0.1682, -0.7574,  0.9002,  0.0846]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8469],\n",
      "        [ 0.5991],\n",
      "        [ 0.2690],\n",
      "        [-0.2167]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8736,  1.8587, -0.7363,  0.1464,  2.3912],\n",
      "        [ 0.9805, -0.3080,  0.7722,  1.4844, -1.0939],\n",
      "        [ 0.3580,  0.3805,  0.5060,  0.4108, -0.2777],\n",
      "        [ 0.1234,  0.3565, -0.2971,  0.1194,  0.1441]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2719,  0.9172,  0.2242,  0.8851,  1.2277],\n",
      "        [ 0.1413, -0.1702, -0.3973, -0.1585, -0.0180],\n",
      "        [-0.4478, -1.9279, -0.8478, -0.6739, -1.6621],\n",
      "        [-0.3797,  0.3239,  0.7871,  0.2734,  0.2129]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8736,  1.8587, -0.7363,  0.1464,  2.3912],\n",
      "        [ 0.9805, -0.3080,  0.7722,  1.4844, -1.0939],\n",
      "        [ 0.3580,  0.3805,  0.5060,  0.4108, -0.2777],\n",
      "        [ 0.1234,  0.3565, -0.2971,  0.1194,  0.1441]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.8423],\n",
      "        [-0.3314],\n",
      "        [-1.1382],\n",
      "        [-0.1019]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4592, -1.0112,  0.8913, -0.5908,  1.0151],\n",
      "        [ 1.6054,  0.3612,  0.7420, -0.6162, -0.1028],\n",
      "        [-0.4193,  0.2801,  0.8302, -1.0834, -1.3856],\n",
      "        [-0.9632,  0.2021,  0.9059, -1.2058, -0.7476]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.6169, -1.5937, -1.3913, -2.1541, -3.2836],\n",
      "        [-0.1309,  0.3734,  0.5773, -0.2271, -0.5162],\n",
      "        [-0.3159, -0.9227,  0.3857, -0.8674, -0.1702],\n",
      "        [-0.5070,  0.1917,  0.5105,  1.2785,  1.4425]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4592, -1.0112,  0.8913, -0.5908,  1.0151],\n",
      "        [ 1.6054,  0.3612,  0.7420, -0.6162, -0.1028],\n",
      "        [-0.4193,  0.2801,  0.8302, -1.0834, -1.3856],\n",
      "        [-0.9632,  0.2021,  0.9059, -1.2058, -0.7476]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4315],\n",
      "        [ 0.5461],\n",
      "        [ 1.3698],\n",
      "        [-1.6305]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8266,  2.0980,  0.2663,  0.0637,  1.6120],\n",
      "        [ 2.5077, -0.0618, -0.0910, -1.0917,  0.8956],\n",
      "        [-0.1358,  0.2648,  0.1888,  0.5670,  0.2871],\n",
      "        [-0.1470,  2.3087,  0.9714,  0.5154,  0.8519]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0039, -1.0668, -0.7633, -0.4742, -1.5609],\n",
      "        [ 0.1700, -0.3830,  0.0321,  0.0080, -0.2924],\n",
      "        [-0.9095, -1.3326, -1.3503, -1.5652, -2.2027],\n",
      "        [ 0.0381,  1.5933,  0.2720,  1.0830,  1.2900]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8266,  2.0980,  0.2663,  0.0637,  1.6120],\n",
      "        [ 2.5077, -0.0618, -0.0910, -1.0917,  0.8956],\n",
      "        [-0.1358,  0.2648,  0.1888,  0.5670,  0.2871],\n",
      "        [-0.1470,  2.3087,  0.9714,  0.5154,  0.8519]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.9910],\n",
      "        [ 0.1764],\n",
      "        [-2.0041],\n",
      "        [ 5.5942]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9587,  0.2252, -1.6261, -2.0432,  1.2938],\n",
      "        [-0.1630, -0.3287, -0.1501, -0.4389, -0.2639],\n",
      "        [-1.2223, -1.8249, -0.1662,  0.7709,  0.0734],\n",
      "        [-0.7716,  2.1044,  0.6604, -0.2317,  3.0385]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.2429,  2.0593,  1.1436,  1.9466,  1.9839],\n",
      "        [ 0.2603,  0.1431,  0.0595,  0.3273,  0.2646],\n",
      "        [ 0.1088,  0.0974, -0.7621, -0.4031, -0.5932],\n",
      "        [-0.7769, -0.3237, -1.6117, -1.8814, -2.7081]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9587,  0.2252, -1.6261, -2.0432,  1.2938],\n",
      "        [-0.1630, -0.3287, -0.1501, -0.4389, -0.2639],\n",
      "        [-1.2223, -1.8249, -0.1662,  0.7709,  0.0734],\n",
      "        [-0.7716,  2.1044,  0.6604, -0.2317,  3.0385]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.9981],\n",
      "        [-0.3119],\n",
      "        [-0.5384],\n",
      "        [-8.9387]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9125, -0.7451, -0.9945,  0.7611,  0.5780],\n",
      "        [-1.7463,  1.4308, -0.0288, -0.6061,  0.1671],\n",
      "        [-0.2217,  0.8794,  1.4993, -0.1991, -0.7703],\n",
      "        [-0.2034,  0.6916, -0.8667,  0.2096,  1.1336]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.1334,  2.5883,  2.9269,  1.9091,  3.4922],\n",
      "        [ 0.0374,  0.5281, -0.0435,  0.0060,  0.2611],\n",
      "        [-0.1051, -0.6261, -1.0050, -0.6621, -1.0267],\n",
      "        [ 1.2970,  2.0671,  2.7047,  1.8339,  2.3081]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9125, -0.7451, -0.9945,  0.7611,  0.5780],\n",
      "        [-1.7463,  1.4308, -0.0288, -0.6061,  0.1671],\n",
      "        [-0.2217,  0.8794,  1.4993, -0.1991, -0.7703],\n",
      "        [-0.2034,  0.6916, -0.8667,  0.2096,  1.1336]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5788],\n",
      "        [ 0.7317],\n",
      "        [-1.1113],\n",
      "        [ 1.8225]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7507, -1.2426,  1.7463,  0.5505, -0.5780],\n",
      "        [ 0.5663,  0.5101,  0.0633,  1.6459, -0.2728],\n",
      "        [ 1.1732,  0.9872, -0.4054, -0.0708, -0.1997],\n",
      "        [ 0.1553, -0.7082,  1.2191, -0.7601, -1.1479]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2271, -0.3844,  0.3533, -0.2850, -0.4710],\n",
      "        [-0.1886,  0.1765, -0.0706,  0.8230, -0.2381],\n",
      "        [ 0.2438, -0.6151, -0.5884, -0.4838, -0.7116],\n",
      "        [ 0.6316,  1.7308,  1.4969,  1.7828,  2.0350]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7507, -1.2426,  1.7463,  0.5505, -0.5780],\n",
      "        [ 0.5663,  0.5101,  0.0633,  1.6459, -0.2728],\n",
      "        [ 1.1732,  0.9872, -0.4054, -0.0708, -0.1997],\n",
      "        [ 0.1553, -0.7082,  1.2191, -0.7601, -1.1479]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8125],\n",
      "        [ 1.3983],\n",
      "        [ 0.0937],\n",
      "        [-2.9939]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6169, -1.3246, -0.9389,  1.0506, -1.1478],\n",
      "        [-1.4147, -0.0396,  0.4465, -0.7118, -0.0893],\n",
      "        [-1.0124,  0.0817,  0.9135, -0.5924, -0.7259],\n",
      "        [-0.1920,  0.1311, -0.0426,  0.3778,  1.3557]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7006, -0.2913, -0.2966,  0.1665, -0.5052],\n",
      "        [-0.4025, -0.4771, -0.9245, -0.7246, -1.0555],\n",
      "        [-0.1132, -1.0174, -0.2441, -1.0957, -1.2954],\n",
      "        [ 0.1002,  0.2912, -0.1728, -0.2047, -0.1830]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6169, -1.3246, -0.9389,  1.0506, -1.1478],\n",
      "        [-1.4147, -0.0396,  0.4465, -0.7118, -0.0893],\n",
      "        [-1.0124,  0.0817,  0.9135, -0.5924, -0.7259],\n",
      "        [-0.1920,  0.1311, -0.0426,  0.3778,  1.3557]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.8513],\n",
      "        [ 0.7854],\n",
      "        [ 1.3979],\n",
      "        [-0.2990]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6737,  0.5275,  0.1111,  0.3277,  0.9463],\n",
      "        [-2.4350,  0.4774, -1.2147,  0.8242,  0.5875],\n",
      "        [-0.5460, -1.8974,  0.0373,  1.2676, -0.1606],\n",
      "        [ 0.3300,  2.5748, -0.4969, -1.0759, -2.2509]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5240, -0.7380, -0.6258, -0.9328, -1.3500],\n",
      "        [-0.3780, -0.3222, -0.5274, -0.5695, -1.0555],\n",
      "        [-0.6709, -0.8225, -0.9914, -1.5298, -1.8458],\n",
      "        [ 0.3627, -0.4289, -0.5318,  0.3804,  0.4718]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6737,  0.5275,  0.1111,  0.3277,  0.9463],\n",
      "        [-2.4350,  0.4774, -1.2147,  0.8242,  0.5875],\n",
      "        [-0.5460, -1.8974,  0.0373,  1.2676, -0.1606],\n",
      "        [ 0.3300,  2.5748, -0.4969, -1.0759, -2.2509]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6889],\n",
      "        [ 0.3178],\n",
      "        [ 0.2470],\n",
      "        [-2.1916]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4020,  1.7069, -1.0387, -0.3997, -0.3971],\n",
      "        [-0.1750,  0.4755, -0.5819, -0.1460,  1.3197],\n",
      "        [ 0.0183,  1.0521,  1.0963,  1.2932,  1.4228],\n",
      "        [ 0.4425, -0.5879,  2.5861, -0.1362, -0.5096]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1542,  0.1173,  0.7053,  0.2756, -0.0833],\n",
      "        [-0.6527, -0.7655, -0.9034, -0.7220, -1.4431],\n",
      "        [-0.4177, -1.3850, -1.0382, -0.9444, -1.4570],\n",
      "        [ 1.0170,  0.4435,  1.6044,  0.6724,  1.0971]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4020,  1.7069, -1.0387, -0.3997, -0.3971],\n",
      "        [-0.1750,  0.4755, -0.5819, -0.1460,  1.3197],\n",
      "        [ 0.0183,  1.0521,  1.0963,  1.2932,  1.4228],\n",
      "        [ 0.4425, -0.5879,  2.5861, -0.1362, -0.5096]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6715],\n",
      "        [-1.5231],\n",
      "        [-5.8974],\n",
      "        [ 3.6878]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6834, -0.3128,  1.0850,  0.2308,  0.0640],\n",
      "        [ 0.0285,  0.6436,  1.1499, -0.9736,  2.0097],\n",
      "        [-0.7359,  0.7063, -0.8354, -0.2176,  0.3749],\n",
      "        [ 0.0943, -0.3292, -1.3701,  1.1047,  0.8245]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0087, -0.5790, -0.2437, -0.4321, -0.1458],\n",
      "        [-0.1023,  0.0559, -0.9364, -0.1049, -0.2237],\n",
      "        [ 1.5844,  1.4509,  1.5757,  0.4656,  1.8487],\n",
      "        [-0.9098, -0.7309, -0.4343, -1.0731, -1.9036]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6834, -0.3128,  1.0850,  0.2308,  0.0640],\n",
      "        [ 0.0285,  0.6436,  1.1499, -0.9736,  2.0097],\n",
      "        [-0.7359,  0.7063, -0.8354, -0.2176,  0.3749],\n",
      "        [ 0.0943, -0.3292, -1.3701,  1.1047,  0.8245]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1983],\n",
      "        [-1.3910],\n",
      "        [-0.8657],\n",
      "        [-2.0052]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5361,  1.7090, -0.9793, -0.9473,  1.2643],\n",
      "        [ 0.3708, -0.0694, -0.2100,  0.1778,  0.1319],\n",
      "        [ 0.1349,  0.9743, -0.6516,  1.0055, -0.9082],\n",
      "        [ 0.3620, -0.3367,  0.1859, -0.4193,  0.2712]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4808, -0.3466, -0.4897, -0.5969, -0.7659],\n",
      "        [ 0.0102,  1.2077,  0.1666,  0.8708,  1.2057],\n",
      "        [ 1.8220,  1.1355,  1.3142,  1.3857,  2.1929],\n",
      "        [ 0.8471,  0.4031, -0.4459,  0.7082,  0.1428]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5361,  1.7090, -0.9793, -0.9473,  1.2643],\n",
      "        [ 0.3708, -0.0694, -0.2100,  0.1778,  0.1319],\n",
      "        [ 0.1349,  0.9743, -0.6516,  1.0055, -0.9082],\n",
      "        [ 0.3620, -0.3367,  0.1859, -0.4193,  0.2712]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7733],\n",
      "        [ 0.1989],\n",
      "        [-0.1026],\n",
      "        [-0.1702]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1580,  1.9805,  0.9387,  0.1338,  0.6570],\n",
      "        [-1.4427, -0.7254, -0.3599, -0.3826,  0.5114],\n",
      "        [-0.3444,  1.3155,  0.6861,  1.0080, -1.5286],\n",
      "        [-0.1911,  0.1127,  3.0409,  0.2887, -0.0276]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6250, -0.4795, -0.2529, -0.6592, -1.8796],\n",
      "        [ 0.1213, -0.2104, -0.0121, -0.1401,  0.5353],\n",
      "        [ 1.2709,  1.9495,  1.6068,  1.3068,  1.8602],\n",
      "        [ 0.1733, -0.0444, -0.6084,  0.1425, -0.1824]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1580,  1.9805,  0.9387,  0.1338,  0.6570],\n",
      "        [-1.4427, -0.7254, -0.3599, -0.3826,  0.5114],\n",
      "        [-0.3444,  1.3155,  0.6861,  1.0080, -1.5286],\n",
      "        [-0.1911,  0.1127,  3.0409,  0.2887, -0.0276]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4112],\n",
      "        [ 0.3093],\n",
      "        [ 1.7028],\n",
      "        [-1.8422]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3413,  1.1697, -0.9985, -0.3729,  0.0961],\n",
      "        [ 0.1844,  0.0923, -0.0882, -0.1463,  0.1489],\n",
      "        [-2.6872,  0.6019,  1.2660,  1.2665,  1.5449],\n",
      "        [-0.0894,  0.8358,  0.3905,  0.7623,  1.1400]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1746,  0.7392,  0.9876,  0.6194,  1.8309],\n",
      "        [ 0.7366, -0.1952, -0.6892,  0.3441, -0.0469],\n",
      "        [-0.2020,  0.0918,  0.1525, -0.2619, -0.0489],\n",
      "        [ 0.7852,  0.8095,  0.7920,  0.8790,  2.0945]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3413,  1.1697, -0.9985, -0.3729,  0.0961],\n",
      "        [ 0.1844,  0.0923, -0.0882, -0.1463,  0.1489],\n",
      "        [-2.6872,  0.6019,  1.2660,  1.2665,  1.5449],\n",
      "        [-0.0894,  0.8358,  0.3905,  0.7623,  1.1400]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3990],\n",
      "        [ 0.1213],\n",
      "        [ 0.3839],\n",
      "        [ 3.9734]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2665,  0.3096,  0.5252, -0.6260,  0.8313],\n",
      "        [-0.1950, -0.7361,  1.8520,  0.4216, -0.9334],\n",
      "        [-0.6551, -0.7960,  0.9687, -0.2207, -0.6870],\n",
      "        [ 1.5421, -0.1722, -0.5754,  1.1624,  0.7366]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7709, -0.2497, -0.1253, -0.2197, -0.1827],\n",
      "        [ 0.5069,  0.0855, -0.4511,  0.0535, -1.0602],\n",
      "        [ 0.3098, -0.2092,  0.5037, -0.0492, -0.1901],\n",
      "        [-0.6363, -1.8843, -0.8026, -0.9662, -1.6897]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2665,  0.3096,  0.5252, -0.6260,  0.8313],\n",
      "        [-0.1950, -0.7361,  1.8520,  0.4216, -0.9334],\n",
      "        [-0.6551, -0.7960,  0.9687, -0.2207, -0.6870],\n",
      "        [ 1.5421, -0.1722, -0.5754,  1.1624,  0.7366]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3630],\n",
      "        [ 0.0150],\n",
      "        [ 0.5929],\n",
      "        [-2.5626]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6979,  1.0858,  0.2617,  2.3237,  1.5868],\n",
      "        [ 2.3192, -0.9061,  0.1966,  0.0928,  0.0277],\n",
      "        [-1.0942,  0.5728,  0.4974,  0.0370, -0.8134],\n",
      "        [-1.1596, -0.8035,  0.1748, -0.7853,  0.0372]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0353, -1.0497,  0.1218, -0.5731,  0.0232],\n",
      "        [-0.1981, -0.4038, -0.6907, -0.7232, -0.2076],\n",
      "        [ 0.4137, -0.4700,  0.1722,  0.0224,  0.3459],\n",
      "        [ 0.9537, -0.2106,  0.3105, -0.0256,  0.3562]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6979,  1.0858,  0.2617,  2.3237,  1.5868],\n",
      "        [ 2.3192, -0.9061,  0.1966,  0.0928,  0.0277],\n",
      "        [-1.0942,  0.5728,  0.4974,  0.0370, -0.8134],\n",
      "        [-1.1596, -0.8035,  0.1748, -0.7853,  0.0372]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4276],\n",
      "        [-0.3023],\n",
      "        [-0.9168],\n",
      "        [-0.8490]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1356, -0.3972, -1.3158, -1.8035,  1.5821],\n",
      "        [-1.3443, -0.5750,  0.0813,  0.3739,  1.1917],\n",
      "        [-0.1997,  1.1484,  1.0259, -0.6426, -0.7563],\n",
      "        [-0.7549,  0.1068, -1.9590, -0.3229,  0.4277]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2783,  0.4642,  1.4791,  0.0982,  0.8840],\n",
      "        [ 0.7747, -0.3932, -1.2696,  0.6289, -0.1167],\n",
      "        [ 0.0027,  0.5978,  0.3198, -0.0816,  0.8852],\n",
      "        [ 0.2496,  0.3075, -0.2300,  0.6370,  0.4986]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1356, -0.3972, -1.3158, -1.8035,  1.5821],\n",
      "        [-1.3443, -0.5750,  0.0813,  0.3739,  1.1917],\n",
      "        [-0.1997,  1.1484,  1.0259, -0.6426, -0.7563],\n",
      "        [-0.7549,  0.1068, -1.9590, -0.3229,  0.4277]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.6392],\n",
      "        [-0.8226],\n",
      "        [ 0.3971],\n",
      "        [ 0.3026]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4050, -0.2080, -0.7628, -0.7750,  0.7031],\n",
      "        [ 0.1848,  0.1123,  1.4402, -0.9203, -0.7771],\n",
      "        [-1.0461,  1.2298, -0.5119,  0.2994,  0.7622],\n",
      "        [ 0.1547,  0.7332, -0.9884,  0.5550, -1.1019]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.0122,  1.9940,  2.4187,  1.8374,  2.7379],\n",
      "        [ 0.5384, -0.6562, -0.7739, -0.7450,  0.0608],\n",
      "        [ 0.0181,  0.0213,  0.0181,  0.1697,  0.1747],\n",
      "        [-0.2175,  0.3743, -0.3454, -0.1251,  0.0437]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4050, -0.2080, -0.7628, -0.7750,  0.7031],\n",
      "        [ 0.1848,  0.1123,  1.4402, -0.9203, -0.7771],\n",
      "        [-1.0461,  1.2298, -0.5119,  0.2994,  0.7622],\n",
      "        [ 0.1547,  0.7332, -0.9884,  0.5550, -1.1019]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9440],\n",
      "        [-0.4504],\n",
      "        [ 0.1820],\n",
      "        [ 0.4646]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4765, -0.5264,  1.7291,  1.9217, -1.0990],\n",
      "        [-0.5839, -0.0307, -0.9192,  0.0454,  0.7722],\n",
      "        [-1.9578,  0.5927,  0.1050, -0.4878,  0.4347],\n",
      "        [-2.4000, -1.7483,  0.7806, -0.8358,  0.1085]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0138,  0.3078,  0.0932,  0.2238, -0.4586],\n",
      "        [ 0.6307, -1.6924, -1.2702, -0.5925, -0.6967],\n",
      "        [ 0.3131,  0.3311,  0.6149,  0.3765,  0.7595],\n",
      "        [ 0.0922,  0.2107,  0.3201, -0.2912, -0.1075]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4765, -0.5264,  1.7291,  1.9217, -1.0990],\n",
      "        [-0.5839, -0.0307, -0.9192,  0.0454,  0.7722],\n",
      "        [-1.9578,  0.5927,  0.1050, -0.4878,  0.4347],\n",
      "        [-2.4000, -1.7483,  0.7806, -0.8358,  0.1085]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9267],\n",
      "        [ 0.2864],\n",
      "        [-0.2055],\n",
      "        [-0.1081]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7980,  0.8049, -0.5357, -0.5202, -1.0694],\n",
      "        [-0.9381, -0.4897, -0.8128,  0.4492, -0.1880],\n",
      "        [ 1.9180,  0.5871,  1.3582,  0.2944, -0.5648],\n",
      "        [ 0.3235, -0.1988,  1.5522, -0.4465, -0.6114]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0699, -0.5341, -0.3003,  0.2005, -1.1913],\n",
      "        [ 0.1320, -0.4630, -0.4191, -0.9555, -0.5490],\n",
      "        [ 0.3600,  0.2461, -0.3495, -0.1020, -0.8123],\n",
      "        [ 0.2533, -0.1272, -0.0527, -0.1010,  0.9088]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7980,  0.8049, -0.5357, -0.5202, -1.0694],\n",
      "        [-0.9381, -0.4897, -0.8128,  0.4492, -0.1880],\n",
      "        [ 1.9180,  0.5871,  1.3582,  0.2944, -0.5648],\n",
      "        [ 0.3235, -0.1988,  1.5522, -0.4465, -0.6114]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8448],\n",
      "        [ 0.1175],\n",
      "        [ 0.7891],\n",
      "        [-0.4851]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7323, -0.7949,  0.8126, -0.9343, -0.4673],\n",
      "        [-0.0126, -0.6040, -0.6997,  0.5013,  0.3571],\n",
      "        [-0.6541, -0.9537,  0.8926,  0.2448, -0.8492],\n",
      "        [-0.2157, -1.1955,  2.8287,  0.8185,  0.7993]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3339, -0.4368,  0.3468,  0.3872, -0.3433],\n",
      "        [-0.2055, -0.6101, -0.2653, -0.8246, -0.3772],\n",
      "        [ 0.4417, -0.0287,  0.2402,  0.3826, -0.0520],\n",
      "        [ 0.4042,  0.6888, -0.4439, -0.7203,  0.3157]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7323, -0.7949,  0.8126, -0.9343, -0.4673],\n",
      "        [-0.0126, -0.6040, -0.6997,  0.5013,  0.3571],\n",
      "        [-0.6541, -0.9537,  0.8926,  0.2448, -0.8492],\n",
      "        [-0.2157, -1.1955,  2.8287,  0.8185,  0.7993]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6722],\n",
      "        [ 0.0086],\n",
      "        [ 0.0906],\n",
      "        [-2.5036]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6217,  0.9967,  2.6200, -0.7959,  1.6713],\n",
      "        [-0.6694, -1.3703,  2.0712,  1.0547,  0.1457],\n",
      "        [-0.3167, -0.3077,  0.6352, -1.2478,  0.7918],\n",
      "        [-0.6166,  0.5255,  1.8271,  0.2539, -1.3688]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5487, -0.9740, -0.1337, -0.2477, -1.0911],\n",
      "        [ 0.2929, -0.4983, -0.3434, -0.4097, -0.0152],\n",
      "        [-0.2919,  0.3396,  0.2626,  0.3649,  0.0228],\n",
      "        [ 0.4329,  1.1066,  1.6372,  0.6121,  1.7636]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6217,  0.9967,  2.6200, -0.7959,  1.6713],\n",
      "        [-0.6694, -1.3703,  2.0712,  1.0547,  0.1457],\n",
      "        [-0.3167, -0.3077,  0.6352, -1.2478,  0.7918],\n",
      "        [-0.6166,  0.5255,  1.8271,  0.2539, -1.3688]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.8373],\n",
      "        [-0.6589],\n",
      "        [-0.2826],\n",
      "        [ 1.0471]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7224,  0.1420, -1.7111,  1.1076,  0.9129],\n",
      "        [-1.0531, -0.4025, -0.0723,  0.0035, -1.4557],\n",
      "        [-0.0485, -1.0554, -1.7342, -0.0248,  0.0242],\n",
      "        [-2.2274,  0.0389, -0.5217,  0.8511,  0.3947]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2217,  1.8722,  1.9422,  0.9006,  1.7195],\n",
      "        [ 0.0913, -0.9569, -0.3077, -0.8950, -1.0167],\n",
      "        [ 0.3092,  0.1889, -0.0673,  0.3862,  0.5477],\n",
      "        [ 0.5765,  0.0599,  0.3661,  0.7045,  0.9311]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7224,  0.1420, -1.7111,  1.1076,  0.9129],\n",
      "        [-1.0531, -0.4025, -0.0723,  0.0035, -1.4557],\n",
      "        [-0.0485, -1.0554, -1.7342, -0.0248,  0.0242],\n",
      "        [-2.2274,  0.0389, -0.5217,  0.8511,  0.3947]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3922],\n",
      "        [ 1.7882],\n",
      "        [-0.0941],\n",
      "        [-0.5055]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0066,  0.2177, -0.4159, -1.0064, -2.5482],\n",
      "        [ 1.2989,  1.4113, -0.8636,  0.4517,  0.0832],\n",
      "        [-0.3667,  1.1332,  0.7353,  0.2238, -0.4971],\n",
      "        [-0.2298, -0.7504,  1.5634,  0.1252,  0.6315]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0066,  1.3234,  0.9781,  0.7240,  1.6187],\n",
      "        [-0.6670, -0.6205, -0.7097, -1.3361, -1.9981],\n",
      "        [ 0.1621, -0.7981,  0.3951,  0.2987, -0.0385],\n",
      "        [ 0.1673,  0.6359,  0.5609,  0.6933,  0.4606]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0066,  0.2177, -0.4159, -1.0064, -2.5482],\n",
      "        [ 1.2989,  1.4113, -0.8636,  0.4517,  0.0832],\n",
      "        [-0.3667,  1.1332,  0.7353,  0.2238, -0.4971],\n",
      "        [-0.2298, -0.7504,  1.5634,  0.1252,  0.6315]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-6.9918],\n",
      "        [-1.8988],\n",
      "        [-0.5874],\n",
      "        [ 0.7390]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6484,  1.0958,  0.3364, -0.4592,  1.5872],\n",
      "        [-1.3081,  0.4979,  2.2999, -0.0010, -0.2784],\n",
      "        [-1.4728, -0.5022,  0.5970, -0.4969, -0.0804],\n",
      "        [-0.7888, -0.3448,  0.2943, -1.5560, -0.3217]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.9012,  3.1999,  2.8432,  2.4165,  4.5624],\n",
      "        [ 0.2650, -0.9077, -0.1705, -0.8031, -1.0412],\n",
      "        [ 0.3279,  0.4311, -0.2738, -0.5768, -0.0052],\n",
      "        [ 0.6465,  0.2979,  0.2038,  1.0684, -0.3981]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6484,  1.0958,  0.3364, -0.4592,  1.5872],\n",
      "        [-1.3081,  0.4979,  2.2999, -0.0010, -0.2784],\n",
      "        [-1.4728, -0.5022,  0.5970, -0.4969, -0.0804],\n",
      "        [-0.7888, -0.3448,  0.2943, -1.5560, -0.3217]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 8.7132],\n",
      "        [-0.9002],\n",
      "        [-0.5759],\n",
      "        [-2.0872]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5081,  0.2209,  1.7560, -0.7018, -0.7904],\n",
      "        [ 0.2559,  1.1837, -0.9332, -0.2844,  0.6185],\n",
      "        [ 1.2375,  0.6503,  1.3807, -0.8432,  0.3518],\n",
      "        [-0.2207,  1.4885,  0.1520,  0.4468,  1.3553]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3845,  0.2519,  0.0503,  0.2733,  0.2279],\n",
      "        [ 0.7861, -0.7244, -0.0121, -0.5639, -0.0913],\n",
      "        [ 0.2641,  0.2165, -0.3609, -0.5653, -0.3812],\n",
      "        [ 1.0888,  1.6322,  1.2997,  0.6825,  1.3821]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5081,  0.2209,  1.7560, -0.7018, -0.7904],\n",
      "        [ 0.2559,  1.1837, -0.9332, -0.2844,  0.6185],\n",
      "        [ 1.2375,  0.6503,  1.3807, -0.8432,  0.3518],\n",
      "        [-0.2207,  1.4885,  0.1520,  0.4468,  1.3553]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4233],\n",
      "        [-0.5411],\n",
      "        [ 0.3117],\n",
      "        [ 4.5650]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7249,  0.1318,  0.4441,  0.3154,  0.8295],\n",
      "        [-0.3692,  0.1078, -1.1129,  0.4910,  0.0889],\n",
      "        [-0.9780, -0.6860,  1.0308,  0.3294,  0.1315],\n",
      "        [-0.3236, -0.7705, -0.4350, -0.9686,  0.7031]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1654,  0.3645, -0.0922,  0.6546,  0.1521],\n",
      "        [-0.0600, -0.8688, -0.5853, -0.8252, -0.3771],\n",
      "        [ 0.1088, -0.5368, -0.4733, -0.4472,  0.0893],\n",
      "        [-0.5412, -1.5555, -0.4821, -1.1571, -1.7344]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7249,  0.1318,  0.4441,  0.3154,  0.8295],\n",
      "        [-0.3692,  0.1078, -1.1129,  0.4910,  0.0889],\n",
      "        [-0.9780, -0.6860,  1.0308,  0.3294,  0.1315],\n",
      "        [-0.3236, -0.7705, -0.4350, -0.9686,  0.7031]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4597],\n",
      "        [ 0.1412],\n",
      "        [-0.3616],\n",
      "        [ 1.4847]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6664, -0.6803,  1.6700,  0.6729, -0.2110],\n",
      "        [ 0.7048,  0.6380, -0.0726,  0.8935,  1.9588],\n",
      "        [-0.2127,  0.6728,  0.7553,  0.0587,  0.2901],\n",
      "        [ 0.4269, -0.2798, -0.1659, -0.8961,  2.0787]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0941,  0.3840, -0.1744, -0.2780,  0.1904],\n",
      "        [-0.0136, -0.3544,  0.3337, -0.9970, -0.7297],\n",
      "        [ 0.4210,  0.4687,  0.3304, -0.4139, -0.1630],\n",
      "        [-0.8525, -0.7257, -1.3194, -1.8672, -2.6630]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6664, -0.6803,  1.6700,  0.6729, -0.2110],\n",
      "        [ 0.7048,  0.6380, -0.0726,  0.8935,  1.9588],\n",
      "        [-0.2127,  0.6728,  0.7553,  0.0587,  0.2901],\n",
      "        [ 0.4269, -0.2798, -0.1659, -0.8961,  2.0787]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6229],\n",
      "        [-2.5803],\n",
      "        [ 0.4037],\n",
      "        [-3.8043]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8690, -0.0820,  0.1400,  0.0594, -0.9726],\n",
      "        [-0.6810, -0.1272, -0.3363, -0.4392,  1.3490],\n",
      "        [ 0.9416,  1.7625,  0.8785,  1.7341,  1.1107],\n",
      "        [ 0.2008, -1.1563,  1.9950, -0.2096, -0.3919]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0653, -0.3803, -0.3932, -0.3581, -0.0597],\n",
      "        [ 0.7161,  0.0405,  0.6984,  0.8076,  0.9038],\n",
      "        [ 0.0577, -0.3991, -0.2213, -0.1118,  0.2003],\n",
      "        [ 0.5113,  0.6540,  0.5102,  0.4456,  1.1749]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8690, -0.0820,  0.1400,  0.0594, -0.9726],\n",
      "        [-0.6810, -0.1272, -0.3363, -0.4392,  1.3490],\n",
      "        [ 0.9416,  1.7625,  0.8785,  1.7341,  1.1107],\n",
      "        [ 0.2008, -1.1563,  1.9950, -0.2096, -0.3919]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0697],\n",
      "        [ 0.1369],\n",
      "        [-0.8151],\n",
      "        [-0.1895]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8742, -1.0389,  0.4593,  1.1579,  0.5214],\n",
      "        [ 2.2377,  0.5889, -2.5642,  0.2160, -1.4552],\n",
      "        [ 0.6798,  0.5535, -1.0714,  0.2388, -0.9994],\n",
      "        [-0.1537, -0.7638,  1.1108, -0.2557,  1.5620]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7196, -0.1591, -0.4600, -0.0979,  0.6082],\n",
      "        [ 0.6734, -0.0073,  0.7263,  0.3817,  1.0120],\n",
      "        [ 0.3160,  0.4766, -0.1581, -0.4254,  0.5064],\n",
      "        [ 0.2385,  0.0896, -0.4656,  0.1357,  0.2568]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8742, -1.0389,  0.4593,  1.1579,  0.5214],\n",
      "        [ 2.2377,  0.5889, -2.5642,  0.2160, -1.4552],\n",
      "        [ 0.6798,  0.5535, -1.0714,  0.2388, -0.9994],\n",
      "        [-0.1537, -0.7638,  1.1108, -0.2557,  1.5620]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1910],\n",
      "        [-1.7500],\n",
      "        [ 0.0404],\n",
      "        [-0.2558]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2871,  1.0115, -1.0907, -1.6188,  0.4278],\n",
      "        [-1.0141,  0.5844,  0.6822,  2.4312, -0.1432],\n",
      "        [-2.0835,  0.3913,  1.1065,  0.1114,  0.5904],\n",
      "        [ 0.8729,  0.6892, -0.1766, -1.6197, -0.0536]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2191,  0.2325,  0.1803,  0.4895,  0.5829],\n",
      "        [ 1.1614,  0.9579,  1.8234,  0.8556,  1.5829],\n",
      "        [ 0.2550, -0.0625, -0.5929, -0.2772, -0.4039],\n",
      "        [ 0.0768, -0.0194, -0.0158, -0.1037,  0.1702]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2871,  1.0115, -1.0907, -1.6188,  0.4278],\n",
      "        [-1.0141,  0.5844,  0.6822,  2.4312, -0.1432],\n",
      "        [-2.0835,  0.3913,  1.1065,  0.1114,  0.5904],\n",
      "        [ 0.8729,  0.6892, -0.1766, -1.6197, -0.0536]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2225],\n",
      "        [ 2.4795],\n",
      "        [-1.4812],\n",
      "        [ 0.2153]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5259, -0.9423,  1.2969,  0.1835, -0.3132],\n",
      "        [-1.5060, -0.3156, -0.0952, -0.2349,  0.5551],\n",
      "        [-0.0250, -0.8290,  0.9280, -1.2328,  0.2363],\n",
      "        [-0.0094, -0.5695,  1.0449,  0.5439, -0.6256]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5511, -0.1206, -0.0504,  0.0649,  0.5828],\n",
      "        [ 0.6249,  0.2810, -0.6223, -0.5984, -0.6537],\n",
      "        [ 0.3376,  0.7604,  0.4212,  0.5884,  0.6713],\n",
      "        [ 0.3596,  0.0499, -0.8473,  0.0094, -0.6139]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5259, -0.9423,  1.2969,  0.1835, -0.3132],\n",
      "        [-1.5060, -0.3156, -0.0952, -0.2349,  0.5551],\n",
      "        [-0.0250, -0.8290,  0.9280, -1.2328,  0.2363],\n",
      "        [-0.0094, -0.5695,  1.0449,  0.5439, -0.6256]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1674],\n",
      "        [-1.1928],\n",
      "        [-0.8147],\n",
      "        [-0.5280]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0403, -0.2560,  0.8711, -0.2892,  0.2272],\n",
      "        [-0.2058, -0.1864,  1.7778,  1.1646,  1.8679],\n",
      "        [-0.6969,  0.5132,  0.7872,  1.4426, -0.4694],\n",
      "        [ 0.4415, -0.8220,  0.0017, -0.1539,  0.5231]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2151, -0.2737,  0.0842, -0.2746,  0.2851],\n",
      "        [ 0.7460, -0.2412,  0.8159,  0.4962,  0.9446],\n",
      "        [ 0.7968,  1.0620,  1.5195,  0.6026,  0.9524],\n",
      "        [ 0.8773,  0.5841,  0.4034, -0.1608, -0.4425]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0403, -0.2560,  0.8711, -0.2892,  0.2272],\n",
      "        [-0.2058, -0.1864,  1.7778,  1.1646,  1.8679],\n",
      "        [-0.6969,  0.5132,  0.7872,  1.4426, -0.4694],\n",
      "        [ 0.4415, -0.8220,  0.0017, -0.1539,  0.5231]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5114],\n",
      "        [ 3.6842],\n",
      "        [ 1.6083],\n",
      "        [-0.2988]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3672,  0.8588,  0.3401,  0.2725,  1.4023],\n",
      "        [-0.3214,  0.1537, -0.2388, -1.2828, -0.2383],\n",
      "        [-0.3699,  1.3387,  0.5790, -0.8984,  1.6940],\n",
      "        [ 0.8473, -0.5899,  1.6715,  0.5449, -0.3714]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6764, -0.6834, -0.1859, -0.6837,  0.3853],\n",
      "        [-0.3178, -1.5910, -0.3890, -1.7872, -2.3421],\n",
      "        [ 0.3615, -0.0165, -0.1247, -0.0333, -0.6446],\n",
      "        [ 0.1298, -0.0676, -0.0359,  0.0112, -0.5240]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3672,  0.8588,  0.3401,  0.2725,  1.4023],\n",
      "        [-0.3214,  0.1537, -0.2388, -1.2828, -0.2383],\n",
      "        [-0.3699,  1.3387,  0.5790, -0.8984,  1.6940],\n",
      "        [ 0.8473, -0.5899,  1.6715,  0.5449, -0.3714]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5444],\n",
      "        [ 2.8013],\n",
      "        [-1.2900],\n",
      "        [ 0.2906]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2985, -0.6967, -0.1474,  0.7385, -0.3346],\n",
      "        [-0.6859, -1.0626,  1.1514, -0.4721,  0.2090],\n",
      "        [-1.6349,  1.6533,  1.7391, -0.5373,  0.5303],\n",
      "        [-0.4924,  1.6280,  0.7421, -0.9434,  0.3257]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4078, -0.1839,  0.1298,  0.3597, -0.6813],\n",
      "        [-1.0483, -2.3344, -1.6814, -2.2041, -3.7585],\n",
      "        [ 0.3265, -0.2307,  0.5066,  0.7426,  1.4217],\n",
      "        [ 0.0328, -0.6106, -0.1779,  0.0055, -0.7850]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2985, -0.6967, -0.1474,  0.7385, -0.3346],\n",
      "        [-0.6859, -1.0626,  1.1514, -0.4721,  0.2090],\n",
      "        [-1.6349,  1.6533,  1.7391, -0.5373,  0.5303],\n",
      "        [-0.4924,  1.6280,  0.7421, -0.9434,  0.3257]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7243],\n",
      "        [ 1.5187],\n",
      "        [ 0.3208],\n",
      "        [-1.4030]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8149, -1.3795, -0.9636,  1.5129, -0.8788],\n",
      "        [-0.6853,  0.7917, -0.4850, -0.8171, -0.9474],\n",
      "        [-1.7248, -1.6754,  0.2610,  0.0166,  0.0857],\n",
      "        [-2.5545,  0.3598,  0.9015,  0.1535,  0.3187]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1689,  0.4293, -0.8794,  0.1338, -0.0838],\n",
      "        [-0.9786, -2.6670, -2.6808, -2.4333, -4.8574],\n",
      "        [ 0.5664,  0.2335, -0.0559,  0.1572,  0.5627],\n",
      "        [ 0.3503,  0.1913,  0.4415,  0.6092,  1.3648]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8149, -1.3795, -0.9636,  1.5129, -0.8788],\n",
      "        [-0.6853,  0.7917, -0.4850, -0.8171, -0.9474],\n",
      "        [-1.7248, -1.6754,  0.2610,  0.0166,  0.0857],\n",
      "        [-2.5545,  0.3598,  0.9015,  0.1535,  0.3187]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6690],\n",
      "        [ 6.4492],\n",
      "        [-1.3318],\n",
      "        [ 0.1005]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8157, -0.4213,  0.1331, -0.3368,  0.5955],\n",
      "        [ 0.0328,  0.1942,  0.8712,  0.3463,  1.1930],\n",
      "        [ 0.5130, -0.3956, -0.0153,  0.5766,  0.7496],\n",
      "        [ 0.3062, -0.0952,  2.1411,  1.7436,  1.0860]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0683,  0.2998,  0.2666,  0.1348,  0.3248],\n",
      "        [ 0.2315, -0.2954, -0.1712, -0.1471, -0.3778],\n",
      "        [ 1.0846,  0.3027,  0.7840,  0.8124,  0.7877],\n",
      "        [ 0.6290, -0.4650, -0.4215, -0.1127,  0.7220]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8157, -0.4213,  0.1331, -0.3368,  0.5955],\n",
      "        [ 0.0328,  0.1942,  0.8712,  0.3463,  1.1930],\n",
      "        [ 0.5130, -0.3956, -0.0153,  0.5766,  0.7496],\n",
      "        [ 0.3062, -0.0952,  2.1411,  1.7436,  1.0860]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0015],\n",
      "        [-0.7006],\n",
      "        [ 1.4836],\n",
      "        [-0.0780]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2651, -2.2483,  0.7187, -0.8018,  1.3828],\n",
      "        [-0.3042, -0.5510, -1.0321,  0.1372,  0.4162],\n",
      "        [ 1.4381, -0.5560,  0.9810, -0.4832, -0.9714],\n",
      "        [-0.0990, -0.4225,  0.2885,  0.9512,  0.0025]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5636,  0.5492,  0.6485,  0.6030, -0.8014],\n",
      "        [ 0.1113, -0.2029,  0.0548,  0.0214,  0.8570],\n",
      "        [ 0.2985,  0.0397, -0.5630, -0.3019,  0.5558],\n",
      "        [ 0.3111, -0.2187,  0.0628,  0.1550,  0.1734]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2651, -2.2483,  0.7187, -0.8018,  1.3828],\n",
      "        [-0.3042, -0.5510, -1.0321,  0.1372,  0.4162],\n",
      "        [ 1.4381, -0.5560,  0.9810, -0.4832, -0.9714],\n",
      "        [-0.0990, -0.4225,  0.2885,  0.9512,  0.0025]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.2108],\n",
      "        [ 0.3811],\n",
      "        [-0.5392],\n",
      "        [ 0.2276]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1434,  0.7282,  1.6020, -0.2559, -0.4747],\n",
      "        [ 0.3028, -0.4398,  1.5424,  0.1488, -0.1253],\n",
      "        [-0.4344,  0.1466,  0.1139,  0.3706,  0.6818],\n",
      "        [ 0.7102, -1.3141,  0.9750, -0.3960,  0.7794]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1487,  0.9632,  0.5846,  1.2026,  1.1559],\n",
      "        [ 0.0287, -0.1669, -0.1798,  0.1354,  0.2927],\n",
      "        [ 0.2318, -0.3062, -0.7253,  0.4101,  0.2619],\n",
      "        [ 0.5242,  0.1713,  0.1868,  0.4326, -0.3882]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1434,  0.7282,  1.6020, -0.2559, -0.4747],\n",
      "        [ 0.3028, -0.4398,  1.5424,  0.1488, -0.1253],\n",
      "        [-0.4344,  0.1466,  0.1139,  0.3706,  0.6818],\n",
      "        [ 0.7102, -1.3141,  0.9750, -0.3960,  0.7794]], device='cuda:0') torch.Size([4, 5])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "action:\n",
      "tensor([[ 0.6167],\n",
      "        [-0.2117],\n",
      "        [ 0.1023],\n",
      "        [-0.1446]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6007,  0.8981,  0.8069,  1.4975,  0.5473],\n",
      "        [-0.5965,  1.1369, -1.2110, -0.7910,  1.2755],\n",
      "        [ 1.1869,  1.2937,  1.2145, -0.5421,  0.1840],\n",
      "        [-1.6232,  0.1865,  0.1767, -0.5958,  0.7473]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5589,  1.2684,  0.4946,  1.5706,  1.2379],\n",
      "        [ 0.0064, -0.4651,  0.1352,  0.1159,  0.1403],\n",
      "        [ 0.1837, -0.2820, -0.0229, -0.3180,  0.0410],\n",
      "        [ 0.9459, -0.0108,  0.3372, -0.2593, -0.1844]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6007,  0.8981,  0.8069,  1.4975,  0.5473],\n",
      "        [-0.5965,  1.1369, -1.2110, -0.7910,  1.2755],\n",
      "        [ 1.1869,  1.2937,  1.2145, -0.5421,  0.1840],\n",
      "        [-1.6232,  0.1865,  0.1767, -0.5958,  0.7473]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.4622],\n",
      "        [-0.6090],\n",
      "        [ 0.0053],\n",
      "        [-1.4612]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3369,  0.1907,  1.9995, -0.9125,  0.4099],\n",
      "        [-1.4500, -1.2527, -0.6799, -1.9441, -0.9580],\n",
      "        [ 0.0556, -0.1949,  1.2242,  0.1594, -1.2090],\n",
      "        [ 0.1282, -0.7675, -1.2022,  0.7095, -0.3834]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0240, -1.4026, -1.5099, -2.2325, -3.6410],\n",
      "        [ 0.1431, -0.2009,  0.1546, -0.0513, -0.2749],\n",
      "        [ 0.2957,  0.6545, -0.3417,  0.4971,  0.7324],\n",
      "        [ 0.7370,  0.4770,  0.5309, -0.0033,  1.4868]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3369,  0.1907,  1.9995, -0.9125,  0.4099],\n",
      "        [-1.4500, -1.2527, -0.6799, -1.9441, -0.9580],\n",
      "        [ 0.0556, -0.1949,  1.2242,  0.1594, -1.2090],\n",
      "        [ 0.1282, -0.7675, -1.2022,  0.7095, -0.3834]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.1108],\n",
      "        [ 0.3022],\n",
      "        [-1.3356],\n",
      "        [-1.4823]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6601,  0.0287,  0.3794, -0.3720,  0.2459],\n",
      "        [-0.2413,  1.5517, -1.0528,  1.7003,  0.7971],\n",
      "        [-0.0400,  1.8540,  1.1469,  1.6408,  0.1563],\n",
      "        [-1.4471, -0.1819, -0.0481,  1.2095, -0.1463]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0898, -0.2546,  0.4141, -0.4652, -0.6124],\n",
      "        [ 0.3503,  0.2782, -0.3241, -0.1784,  0.0076],\n",
      "        [ 0.5963,  1.1307,  1.0763,  0.5166,  1.0324],\n",
      "        [ 1.5506,  0.4776,  1.5854,  0.9554,  1.6781]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6601,  0.0287,  0.3794, -0.3720,  0.2459],\n",
      "        [-0.2413,  1.5517, -1.0528,  1.7003,  0.7971],\n",
      "        [-0.0400,  1.8540,  1.1469,  1.6408,  0.1563],\n",
      "        [-1.4471, -0.1819, -0.0481,  1.2095, -0.1463]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2315],\n",
      "        [ 0.3911],\n",
      "        [ 4.3159],\n",
      "        [-1.4971]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6745,  0.1508,  0.6148,  0.2407,  0.4149],\n",
      "        [ 0.5680,  0.8948, -0.1619,  0.9465, -0.6901],\n",
      "        [ 0.0349,  0.0517,  1.0675,  0.3753, -0.7977],\n",
      "        [ 0.4349, -2.2110,  1.2918, -0.0240, -0.1327]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0289, -0.4285, -0.2795,  0.2358, -0.6819],\n",
      "        [-0.0761,  0.1358, -0.5916,  0.0898, -0.0838],\n",
      "        [-1.1301, -1.6488, -0.0476, -1.6308, -2.0064],\n",
      "        [ 2.2380,  1.8404,  1.5735,  1.4782,  2.2419]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6745,  0.1508,  0.6148,  0.2407,  0.4149],\n",
      "        [ 0.5680,  0.8948, -0.1619,  0.9465, -0.6901],\n",
      "        [ 0.0349,  0.0517,  1.0675,  0.3753, -0.7977],\n",
      "        [ 0.4349, -2.2110,  1.2918, -0.0240, -0.1327]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5110],\n",
      "        [ 0.3168],\n",
      "        [ 0.8131],\n",
      "        [-1.3962]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4519,  0.5130,  1.1316, -0.4349, -0.0067],\n",
      "        [-1.7513,  0.5241,  0.2642,  0.9437,  0.9311],\n",
      "        [ 1.0837, -1.0209,  1.2537,  1.4391,  0.3786],\n",
      "        [-0.1783, -0.4389,  0.5317, -0.0175, -1.1382]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0905, -0.5436,  0.7818, -0.2000, -1.0502],\n",
      "        [-0.0570, -0.2474, -0.6430,  0.0139,  0.1165],\n",
      "        [-1.0163, -1.5436, -0.7228, -0.6640, -2.2990],\n",
      "        [ 0.5186, -0.4730,  0.3372, -0.2497, -0.5064]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4519,  0.5130,  1.1316, -0.4349, -0.0067],\n",
      "        [-1.7513,  0.5241,  0.2642,  0.9437,  0.9311],\n",
      "        [ 1.0837, -1.0209,  1.2537,  1.4391,  0.3786],\n",
      "        [-0.1783, -0.4389,  0.5317, -0.0175, -1.1382]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8312],\n",
      "        [-0.0781],\n",
      "        [-2.2576],\n",
      "        [ 0.8752]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8209,  1.0858,  0.9638, -0.5462,  0.1176],\n",
      "        [-1.0355, -0.1361, -0.5985,  0.3463, -0.9875],\n",
      "        [-1.2889,  1.2283,  1.1682, -0.6715,  0.7834],\n",
      "        [-1.4946, -1.0851, -1.6264, -0.2833, -0.1540]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5136, -0.4001, -1.2563, -0.6422, -1.2111],\n",
      "        [-0.1024, -0.1277,  0.1192, -0.0294,  0.2085],\n",
      "        [-0.2106, -0.9378, -0.7221,  0.0308, -0.7374],\n",
      "        [ 0.0209, -0.2316,  0.0589, -0.2356, -0.7769]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8209,  1.0858,  0.9638, -0.5462,  0.1176],\n",
      "        [-1.0355, -0.1361, -0.5985,  0.3463, -0.9875],\n",
      "        [-1.2889,  1.2283,  1.1682, -0.6715,  0.7834],\n",
      "        [-1.4946, -1.0851, -1.6264, -0.2833, -0.1540]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8585],\n",
      "        [-0.1640],\n",
      "        [-2.3223],\n",
      "        [ 0.3106]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5552,  1.7904,  0.4432,  0.4503, -0.3085],\n",
      "        [ 1.1584, -0.5199,  3.0968,  0.9639,  1.3009],\n",
      "        [-1.2893,  0.3459, -0.3043,  1.0961,  1.0470],\n",
      "        [ 1.2629, -1.6402, -1.1006,  0.1772,  0.7651]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6712,  0.6955,  0.9249,  0.0423,  1.5698],\n",
      "        [ 0.1663, -0.0066,  0.4553, -0.2918, -0.3000],\n",
      "        [ 0.9006,  0.1648,  0.3540,  0.3288,  1.1998],\n",
      "        [-0.0612,  1.1888,  0.2832,  0.0272,  0.3130]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5552,  1.7904,  0.4432,  0.4503, -0.3085],\n",
      "        [ 1.1584, -0.5199,  3.0968,  0.9639,  1.3009],\n",
      "        [-1.2893,  0.3459, -0.3043,  1.0961,  1.0470],\n",
      "        [ 1.2629, -1.6402, -1.1006,  0.1772,  0.7651]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8173],\n",
      "        [ 0.9346],\n",
      "        [ 0.4047],\n",
      "        [-2.0946]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7371, -0.9116,  0.4741,  1.3073,  0.5681],\n",
      "        [-2.3373,  0.7668, -0.7801, -0.5869, -0.3982],\n",
      "        [-0.1432, -0.4117, -0.1054,  0.0133,  0.3452],\n",
      "        [ 1.5696, -0.9206, -0.1853, -0.2368, -0.0555]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0038, -0.3477, -0.2374, -1.2860, -0.9994],\n",
      "        [-0.5145, -0.4396, -0.2217,  0.1419, -0.5868],\n",
      "        [ 0.4945, -0.4761,  0.0486, -0.5424,  0.4655],\n",
      "        [ 0.3300,  1.4260,  1.7274,  0.9057,  1.6452]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7371, -0.9116,  0.4741,  1.3073,  0.5681],\n",
      "        [-2.3373,  0.7668, -0.7801, -0.5869, -0.3982],\n",
      "        [-0.1432, -0.4117, -0.1054,  0.0133,  0.3452],\n",
      "        [ 1.5696, -0.9206, -0.1853, -0.2368, -0.0555]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0416],\n",
      "        [ 1.1888],\n",
      "        [ 0.2736],\n",
      "        [-1.4206]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2409,  0.5711, -0.9303, -0.8693, -0.3744],\n",
      "        [-0.3007, -0.3362,  0.1976,  1.9677, -0.6904],\n",
      "        [ 0.5240,  0.2234,  0.9290,  0.2221,  0.4450],\n",
      "        [-0.6455, -1.6994,  0.5436, -0.8753,  0.8568]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8036,  0.9221, -0.1352,  0.9355,  2.5902],\n",
      "        [-0.8568, -0.4803,  0.0031, -0.4232, -1.2328],\n",
      "        [ 0.5361, -0.4417, -0.4753, -0.3378, -0.0852],\n",
      "        [ 1.4558,  1.8313,  0.6181,  1.2622,  1.4372]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2409,  0.5711, -0.9303, -0.8693, -0.3744],\n",
      "        [-0.3007, -0.3362,  0.1976,  1.9677, -0.6904],\n",
      "        [ 0.5240,  0.2234,  0.9290,  0.2221,  0.4450],\n",
      "        [-0.6455, -1.6994,  0.5436, -0.8753,  0.8568]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3241],\n",
      "        [ 0.4381],\n",
      "        [-0.3722],\n",
      "        [-3.5894]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9318, -0.3324,  1.3825,  1.8056, -1.2573],\n",
      "        [-1.9914, -1.3646,  1.4193,  1.1151,  0.6582],\n",
      "        [-1.0359, -0.8847,  1.1130,  2.2807, -0.2698],\n",
      "        [-0.2523,  0.3494, -0.0501, -0.0289, -0.1939]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5506,  0.8229,  0.6997,  0.5839,  1.4324],\n",
      "        [-0.0780, -1.2727, -0.8383, -0.5654, -1.2133],\n",
      "        [ 0.8229, -0.2082,  0.3033,  0.2639,  0.1235],\n",
      "        [ 0.6393, -0.4674, -0.0719,  0.0236, -0.3924]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9318, -0.3324,  1.3825,  1.8056, -1.2573],\n",
      "        [-1.9914, -1.3646,  1.4193,  1.1151,  0.6582],\n",
      "        [-1.0359, -0.8847,  1.1130,  2.2807, -0.2698],\n",
      "        [-0.2523,  0.3494, -0.0501, -0.0289, -0.1939]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4977],\n",
      "        [-0.7267],\n",
      "        [ 0.2378],\n",
      "        [-0.2456]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8416, -0.0032, -1.1825, -1.1403, -0.5971],\n",
      "        [ 0.6281, -1.5034,  1.3265, -0.2488, -0.9262],\n",
      "        [-0.4248, -0.5088,  0.0517,  0.2221,  2.4317],\n",
      "        [-0.6958,  0.4542,  0.1333,  0.7774, -0.3199]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6441,  1.1486,  2.3692,  1.5782,  2.2032],\n",
      "        [-0.2927, -0.8174, -0.2518, -0.5924, -0.5764],\n",
      "        [ 0.1318,  0.3213, -0.1908, -0.2886, -0.4746],\n",
      "        [ 0.1254,  0.1681, -0.6618,  0.3396, -0.0957]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8416, -0.0032, -1.1825, -1.1403, -0.5971],\n",
      "        [ 0.6281, -1.5034,  1.3265, -0.2488, -0.9262],\n",
      "        [-0.4248, -0.5088,  0.0517,  0.2221,  2.4317],\n",
      "        [-0.6958,  0.4542,  0.1333,  0.7774, -0.3199]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-7.3039],\n",
      "        [ 1.3924],\n",
      "        [-1.4474],\n",
      "        [ 0.1955]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6001,  1.3848,  1.1339, -0.1812, -0.3623],\n",
      "        [-1.0312,  0.3865, -0.8317, -0.9322,  1.9314],\n",
      "        [-1.9746, -0.1339, -0.1100,  0.0811,  1.2284],\n",
      "        [ 0.1218, -0.4853,  1.2539,  1.6997,  1.0010]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2043, -0.3383,  0.2653, -0.6322, -0.0452],\n",
      "        [-0.7952, -0.6873, -0.9330, -1.1646, -2.1852],\n",
      "        [ 0.9293,  0.2995,  0.6511,  1.1135,  1.2706],\n",
      "        [ 0.4394, -0.3258,  0.1826,  0.4055,  0.1990]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6001,  1.3848,  1.1339, -0.1812, -0.3623],\n",
      "        [-1.0312,  0.3865, -0.8317, -0.9322,  1.9314],\n",
      "        [-1.9746, -0.1339, -0.1100,  0.0811,  1.2284],\n",
      "        [ 0.1218, -0.4853,  1.2539,  1.6997,  1.0010]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1593],\n",
      "        [-1.8043],\n",
      "        [-0.2956],\n",
      "        [ 1.3289]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2437, -1.0393,  0.8837, -1.3001, -0.6390],\n",
      "        [-1.1726, -0.0823, -0.4051, -0.6881,  0.8727],\n",
      "        [-0.0217,  0.8845,  0.5382,  1.4164,  2.4596],\n",
      "        [ 0.6856,  0.9359,  0.6944,  1.2298, -0.2645]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0508,  0.2235, -0.1153, -0.2657, -0.1333],\n",
      "        [ 0.5958,  0.2390,  0.1189, -0.1298,  0.5743],\n",
      "        [ 0.7493,  0.4317,  0.6504,  0.3942,  0.3483],\n",
      "        [-0.3137, -0.7615, -0.8518, -0.4951, -1.1610]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2437, -1.0393,  0.8837, -1.3001, -0.6390],\n",
      "        [-1.1726, -0.0823, -0.4051, -0.6881,  0.8727],\n",
      "        [-0.0217,  0.8845,  0.5382,  1.4164,  2.4596],\n",
      "        [ 0.6856,  0.9359,  0.6944,  1.2298, -0.2645]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1596],\n",
      "        [-0.1760],\n",
      "        [ 2.1306],\n",
      "        [-1.8210]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9732,  0.5186, -0.2406, -0.4680, -0.6509],\n",
      "        [ 1.6467,  1.4266, -1.3424, -0.6666,  1.5031],\n",
      "        [-0.1886,  0.1632, -0.3136,  0.9882,  0.8475],\n",
      "        [-0.7217,  0.3104, -0.6238,  0.7820, -1.2362]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1008, -0.2682, -0.5788,  0.8343,  0.1328],\n",
      "        [-0.1592, -0.8537,  0.1221, -0.5846, -0.4212],\n",
      "        [-0.1097, -0.9527, -0.4297, -0.7867, -1.9221],\n",
      "        [ 0.6639,  0.3756,  0.2004,  0.5298,  0.4025]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9732,  0.5186, -0.2406, -0.4680, -0.6509],\n",
      "        [ 1.6467,  1.4266, -1.3424, -0.6666,  1.5031],\n",
      "        [-0.1886,  0.1632, -0.3136,  0.9882,  0.8475],\n",
      "        [-0.7217,  0.3104, -0.6238,  0.7820, -1.2362]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5748],\n",
      "        [-1.8873],\n",
      "        [-2.4064],\n",
      "        [-0.5707]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9648,  1.5788, -0.5163,  1.3282,  0.5750],\n",
      "        [ 0.7471, -0.3930, -0.4337,  1.1234,  0.5336],\n",
      "        [ 1.8990, -0.4998,  0.0540,  0.4452,  1.2090],\n",
      "        [ 0.8743,  0.2492, -0.5279, -0.9732,  1.4922]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1047,  0.2292,  0.1467, -0.3915,  0.3113],\n",
      "        [ 0.3111,  0.8470,  0.3449,  0.2736,  1.2908],\n",
      "        [ 0.6373,  0.1814,  0.1839,  0.6928,  1.0883],\n",
      "        [ 0.1181,  0.4630,  1.1362,  0.7234,  0.7587]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9648,  1.5788, -0.5163,  1.3282,  0.5750],\n",
      "        [ 0.7471, -0.3930, -0.4337,  1.1234,  0.5336],\n",
      "        [ 1.8990, -0.4998,  0.0540,  0.4452,  1.2090],\n",
      "        [ 0.8743,  0.2492, -0.5279, -0.9732,  1.4922]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1560],\n",
      "        [ 0.7462],\n",
      "        [ 2.7538],\n",
      "        [ 0.0470]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5213, -0.4352,  2.0035, -0.9058,  1.3388],\n",
      "        [ 1.8573, -0.4740,  0.1774,  1.4533,  0.1186],\n",
      "        [-1.3248, -0.5305,  1.4800,  0.9915, -1.0232],\n",
      "        [-0.5897, -0.1223,  0.4072,  0.3987,  0.4964]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1911, -0.4793,  0.4536,  0.1053,  0.4888],\n",
      "        [ 0.1705,  0.7293, -1.1087, -0.0733, -0.5329],\n",
      "        [-0.9134, -0.9911, -1.5563, -0.9268, -1.4237],\n",
      "        [ 0.7921,  0.0216,  0.0425,  0.2770,  0.0430]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5213, -0.4352,  2.0035, -0.9058,  1.3388],\n",
      "        [ 1.8573, -0.4740,  0.1774,  1.4533,  0.1186],\n",
      "        [-1.3248, -0.5305,  1.4800,  0.9915, -1.0232],\n",
      "        [-0.5897, -0.1223,  0.4072,  0.3987,  0.4964]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5768],\n",
      "        [-0.3953],\n",
      "        [-0.0297],\n",
      "        [-0.3206]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8918,  0.7773, -0.6019,  1.4185,  0.9369],\n",
      "        [ 1.7944,  1.1737,  0.2509,  0.4669,  1.2014],\n",
      "        [ 0.1431,  0.0450, -0.2253, -0.9644,  2.2409],\n",
      "        [ 0.2268, -0.9189,  0.4085, -0.6240, -1.1560]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2847, -0.4850,  0.0122, -0.4430, -1.2542],\n",
      "        [ 0.4982, -0.5020, -0.5849, -0.5324, -0.8059],\n",
      "        [-0.0882, -1.1953, -1.3956, -0.3257, -1.1192],\n",
      "        [ 0.4348, -0.0083, -0.7202, -0.6476, -0.0508]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8918,  0.7773, -0.6019,  1.4185,  0.9369],\n",
      "        [ 1.7944,  1.1737,  0.2509,  0.4669,  1.2014],\n",
      "        [ 0.1431,  0.0450, -0.2253, -0.9644,  2.2409],\n",
      "        [ 0.2268, -0.9189,  0.4085, -0.6240, -1.1560]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6491],\n",
      "        [-1.0589],\n",
      "        [-1.9459],\n",
      "        [ 0.2748]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4919,  1.2267,  1.9540,  1.1596,  0.5117],\n",
      "        [ 0.3468, -0.2710,  1.6434,  0.5374, -0.0924],\n",
      "        [ 0.7433, -0.3983,  0.1935,  0.1645,  1.5900],\n",
      "        [-0.7261,  0.7116,  0.3833,  0.8959, -0.6460]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4840, -0.2809, -0.2564, -0.1725,  0.0888],\n",
      "        [ 0.7387, -0.3936,  0.0964, -0.0410,  0.1903],\n",
      "        [ 0.2529,  0.1924,  0.2137, -0.1650,  0.4407],\n",
      "        [-0.0488, -0.0671, -0.0164,  0.2792,  0.2438]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4919,  1.2267,  1.9540,  1.1596,  0.5117],\n",
      "        [ 0.3468, -0.2710,  1.6434,  0.5374, -0.0924],\n",
      "        [ 0.7433, -0.3983,  0.1935,  0.1645,  1.5900],\n",
      "        [-0.7261,  0.7116,  0.3833,  0.8959, -0.6460]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7621],\n",
      "        [ 0.4817],\n",
      "        [ 0.8263],\n",
      "        [ 0.0741]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5634, -0.5672, -0.0451, -0.5242,  0.0880],\n",
      "        [-0.3510, -0.7426,  0.7705,  1.0661, -0.5855],\n",
      "        [-1.4716, -0.8746, -1.0341, -0.3549, -0.7674],\n",
      "        [ 0.3602,  1.8265,  0.1186,  1.9720,  0.8111]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5350, -0.3311, -0.4214,  0.0728,  0.3674],\n",
      "        [ 0.0610, -0.4776, -0.0754, -1.0473, -0.3605],\n",
      "        [ 0.2902, -0.3314, -0.3040, -0.6569, -0.4189],\n",
      "        [ 0.1561, -0.1976,  0.1718, -0.0102, -0.0078]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5634, -0.5672, -0.0451, -0.5242,  0.0880],\n",
      "        [-0.3510, -0.7426,  0.7705,  1.0661, -0.5855],\n",
      "        [-1.4716, -0.8746, -1.0341, -0.3549, -0.7674],\n",
      "        [ 0.3602,  1.8265,  0.1186,  1.9720,  0.8111]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6354],\n",
      "        [-0.6303],\n",
      "        [ 0.7317],\n",
      "        [-0.3108]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9804, -0.5373, -0.5408,  1.2011, -1.3142],\n",
      "        [ 1.6163, -1.0546, -1.5237,  0.6549,  0.2688],\n",
      "        [-0.0326,  0.8327, -0.4325,  0.2420,  0.9888],\n",
      "        [ 2.0943,  1.4828, -0.4455,  1.5650,  0.0784]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9470, -0.3244, -0.1216, -0.3686,  0.1659],\n",
      "        [ 0.1086, -1.0665,  0.8823, -0.7883, -0.3629],\n",
      "        [-0.3984, -0.1241, -0.6187, -0.6707, -0.4099],\n",
      "        [ 0.0621, -0.7090, -0.5685,  0.5577,  0.1315]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9804, -0.5373, -0.5408,  1.2011, -1.3142],\n",
      "        [ 1.6163, -1.0546, -1.5237,  0.6549,  0.2688],\n",
      "        [-0.0326,  0.8327, -0.4325,  0.2420,  0.9888],\n",
      "        [ 2.0943,  1.4828, -0.4455,  1.5650,  0.0784]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5077],\n",
      "        [-0.6579],\n",
      "        [-0.3904],\n",
      "        [ 0.2152]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1353,  2.3747,  0.4551, -2.0633, -1.3961],\n",
      "        [ 0.6666, -0.3778,  1.1828,  1.0612, -0.0944],\n",
      "        [-1.3619, -0.3801, -0.0567, -0.1810, -0.6231],\n",
      "        [-0.0372,  1.0323,  0.6952,  0.3215,  0.8378]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1541, -0.5255, -0.4006,  0.1237, -0.3127],\n",
      "        [ 0.4740, -0.2285, -0.1287, -0.6312, -0.3600],\n",
      "        [ 0.5833, -0.7441, -0.0502, -0.5656,  0.0538],\n",
      "        [ 0.1535,  0.0659, -0.1945, -0.0063, -0.4341]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1353,  2.3747,  0.4551, -2.0633, -1.3961],\n",
      "        [ 0.6666, -0.3778,  1.1828,  1.0612, -0.0944],\n",
      "        [-1.3619, -0.3801, -0.0567, -0.1810, -0.6231],\n",
      "        [-0.0372,  1.0323,  0.6952,  0.3215,  0.8378]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2280],\n",
      "        [-0.3858],\n",
      "        [-0.4399],\n",
      "        [-0.4387]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7736, -1.5267, -0.3825,  1.2454, -1.3773],\n",
      "        [-1.4994, -0.1634, -0.4027, -0.9165, -0.4539],\n",
      "        [-1.2174,  1.1542,  0.2314, -0.5363,  0.3038],\n",
      "        [-0.1417,  0.9897,  0.3546,  0.2517,  0.8124]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2873,  0.3269, -0.1428,  0.0030,  0.2963],\n",
      "        [ 0.1533, -0.8889, -0.0597, -1.3825, -0.5834],\n",
      "        [ 0.2033,  0.2483, -0.5119, -0.1689,  0.0386],\n",
      "        [ 0.4003, -0.8541,  0.1338, -1.1699,  0.0625]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7736, -1.5267, -0.3825,  1.2454, -1.3773],\n",
      "        [-1.4994, -0.1634, -0.4027, -0.9165, -0.4539],\n",
      "        [-1.2174,  1.1542,  0.2314, -0.5363,  0.3038],\n",
      "        [-0.1417,  0.9897,  0.3546,  0.2517,  0.8124]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0711],\n",
      "        [ 1.4713],\n",
      "        [ 0.0230],\n",
      "        [-1.0983]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6014,  0.1721, -0.8153,  1.9279,  0.9084],\n",
      "        [-1.9130, -0.4879,  0.2884, -0.4341,  0.5552],\n",
      "        [ 0.2622,  1.6685,  1.4238,  1.2609, -0.1845],\n",
      "        [ 0.8940,  1.3099, -0.2356,  0.5455,  1.2100]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0660,  0.5118,  1.2227, -0.1373,  0.9352],\n",
      "        [-0.5316, -1.4055, -0.8782, -1.5795, -1.5625],\n",
      "        [ 0.0396, -0.0101, -0.4969, -0.6685,  0.4804],\n",
      "        [ 0.4677,  0.3612,  0.1403, -0.6580,  0.9874]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6014,  0.1721, -0.8153,  1.9279,  0.9084],\n",
      "        [-1.9130, -0.4879,  0.2884, -0.4341,  0.5552],\n",
      "        [ 0.2622,  1.6685,  1.4238,  1.2609, -0.1845],\n",
      "        [ 0.8940,  1.3099, -0.2356,  0.5455,  1.2100]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.0311],\n",
      "        [ 1.2676],\n",
      "        [-1.6454],\n",
      "        [ 1.6939]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1363,  0.6974,  0.7304,  1.9066,  1.5852],\n",
      "        [-1.0187,  0.1889,  0.5571,  0.0484, -1.2015],\n",
      "        [ 0.8280,  0.0157, -0.1891,  0.6898,  1.5441],\n",
      "        [-0.9425,  2.5234,  0.6194,  1.0154, -0.1987]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2662,  1.4278,  1.7031,  1.0397,  2.0615],\n",
      "        [-1.0737, -1.5683, -1.4562, -1.2830, -2.2963],\n",
      "        [ 0.3641,  0.3439,  0.9405,  0.3928,  1.4592],\n",
      "        [-0.4091, -0.4032, -0.6223, -0.5988, -1.3150]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1363,  0.6974,  0.7304,  1.9066,  1.5852],\n",
      "        [-1.0187,  0.1889,  0.5571,  0.0484, -1.2015],\n",
      "        [ 0.8280,  0.0157, -0.1891,  0.6898,  1.5441],\n",
      "        [-0.9425,  2.5234,  0.6194,  1.0154, -0.1987]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 8.9287],\n",
      "        [ 2.6832],\n",
      "        [ 2.6531],\n",
      "        [-1.3640]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9278,  0.6670, -1.3829,  0.1678,  1.6715],\n",
      "        [-1.2937,  1.8073, -1.3465,  0.6157,  1.9168],\n",
      "        [ 0.4515,  2.0413,  0.8758,  0.0453, -0.8239],\n",
      "        [ 0.4681, -0.5982,  0.3554,  0.7530,  2.3985]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.4101, -2.3070, -2.4449, -2.1218, -4.3296],\n",
      "        [-1.3237, -2.1106, -2.2872, -3.1035, -3.6360],\n",
      "        [-0.6203, -0.9246, -0.9074, -1.1045, -1.9928],\n",
      "        [ 0.6260,  0.0314,  0.3988, -0.1658,  0.5132]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9278,  0.6670, -1.3829,  0.1678,  1.6715],\n",
      "        [-1.2937,  1.8073, -1.3465,  0.6157,  1.9168],\n",
      "        [ 0.4515,  2.0413,  0.8758,  0.0453, -0.8239],\n",
      "        [ 0.4681, -0.5982,  0.3554,  0.7530,  2.3985]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.4426],\n",
      "        [-7.9027],\n",
      "        [-1.3705],\n",
      "        [ 1.5220]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0264,  0.1778, -1.1559, -0.0620,  1.0008],\n",
      "        [-1.0706, -0.9360, -0.2398,  0.3344,  1.0842],\n",
      "        [-0.0440, -0.6256, -0.5353,  1.2045, -0.5214],\n",
      "        [ 0.4728, -0.0588,  1.0228,  0.0821,  1.5187]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4804, -1.3197, -1.1317, -1.0864, -2.1961],\n",
      "        [ 0.0342,  0.1455,  0.2298,  0.3334, -0.0875],\n",
      "        [ 0.6785, -0.7817, -0.1837,  0.0981, -0.0504],\n",
      "        [-0.1898, -0.6096,  0.5638, -0.8082, -0.9943]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0264,  0.1778, -1.1559, -0.0620,  1.0008],\n",
      "        [-1.0706, -0.9360, -0.2398,  0.3344,  1.0842],\n",
      "        [-0.0440, -0.6256, -0.5353,  1.2045, -0.5214],\n",
      "        [ 0.4728, -0.0588,  1.0228,  0.0821,  1.5187]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5639],\n",
      "        [-0.2114],\n",
      "        [ 0.7019],\n",
      "        [-1.0536]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7771,  0.3064, -0.2010,  0.8570,  1.1949],\n",
      "        [-0.6040,  2.3509,  0.4829,  0.2098, -0.1117],\n",
      "        [-0.5683,  0.6714, -0.0919,  0.4368,  0.3507],\n",
      "        [-0.1186,  0.5626,  1.5363,  0.1618, -1.0371]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1740, -0.7720, -1.1797, -0.7476, -1.3947],\n",
      "        [ 0.1950, -0.0325, -0.7587,  0.7027, -0.0351],\n",
      "        [-0.1066,  0.2638, -0.5020,  0.4437, -0.7460],\n",
      "        [ 0.2908,  0.4980, -0.8702,  0.0721,  0.4433]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7771,  0.3064, -0.2010,  0.8570,  1.1949],\n",
      "        [-0.6040,  2.3509,  0.4829,  0.2098, -0.1117],\n",
      "        [-0.5683,  0.6714, -0.0919,  0.4368,  0.3507],\n",
      "        [-0.1186,  0.5626,  1.5363,  0.1618, -1.0371]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1714],\n",
      "        [-0.4091],\n",
      "        [ 0.2160],\n",
      "        [-1.5393]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4754,  0.7023, -0.4902,  1.3998,  1.1754],\n",
      "        [-0.6057, -0.9197, -1.5909, -0.2860, -0.1326],\n",
      "        [ 0.9666,  0.1833,  2.5151, -1.5283, -0.1600],\n",
      "        [ 0.6813,  0.6625,  1.2621,  0.3797, -0.6088]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0359,  0.1081,  0.5309,  0.3361,  1.2846],\n",
      "        [ 0.5400, -0.2449,  0.4011, -0.2563, -0.2463],\n",
      "        [-0.1639,  0.0031, -0.5500, -1.0413, -0.4215],\n",
      "        [ 0.8655,  0.2579,  1.3247,  0.7635,  1.1999]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4754,  0.7023, -0.4902,  1.3998,  1.1754],\n",
      "        [-0.6057, -0.9197, -1.5909, -0.2860, -0.1326],\n",
      "        [ 0.9666,  0.1833,  2.5151, -1.5283, -0.1600],\n",
      "        [ 0.6813,  0.6625,  1.2621,  0.3797, -0.6088]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3036],\n",
      "        [-0.6340],\n",
      "        [ 0.1177],\n",
      "        [ 1.9918]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3850,  0.2035, -1.6116,  2.3072,  1.1761],\n",
      "        [-2.2745,  1.1881, -1.4333, -2.2838,  1.1990],\n",
      "        [-1.1094, -0.0730,  1.3559,  2.5060, -0.9757],\n",
      "        [-1.4969, -0.9428,  1.6520,  0.4765, -0.5480]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3937, -0.3796, -0.6625, -1.0093, -1.3116],\n",
      "        [-0.1810, -0.6229, -0.0357, -0.3226, -0.6423],\n",
      "        [ 0.6053, -0.6516, -0.5852, -0.7415, -0.3891],\n",
      "        [ 0.1598, -0.4742, -0.3071, -0.6633, -1.3104]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3850,  0.2035, -1.6116,  2.3072,  1.1761],\n",
      "        [-2.2745,  1.1881, -1.4333, -2.2838,  1.1990],\n",
      "        [-1.1094, -0.0730,  1.3559,  2.5060, -0.9757],\n",
      "        [-1.4969, -0.9428,  1.6520,  0.4765, -0.5480]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.4261],\n",
      "        [-0.3106],\n",
      "        [-2.8960],\n",
      "        [ 0.1025]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0237, -0.1254,  0.9024, -0.7926,  0.5586],\n",
      "        [ 0.9104,  1.6392, -0.3206,  0.5812, -0.4149],\n",
      "        [-0.9378,  1.2799,  0.7206,  0.9466,  1.1321],\n",
      "        [-0.9806, -0.1683,  1.8504,  0.0917, -1.4706]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8285,  0.6032,  1.3816,  1.4276,  1.9450],\n",
      "        [ 0.2584,  0.4413, -0.1909, -0.3943,  0.0266],\n",
      "        [ 1.2312,  0.8013,  0.9634,  1.7552,  1.6303],\n",
      "        [-0.2526,  0.2625, -0.1181,  0.0183,  0.3208]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0237, -0.1254,  0.9024, -0.7926,  0.5586],\n",
      "        [ 0.9104,  1.6392, -0.3206,  0.5812, -0.4149],\n",
      "        [-0.9378,  1.2799,  0.7206,  0.9466,  1.1321],\n",
      "        [-0.9806, -0.1683,  1.8504,  0.0917, -1.4706]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1065],\n",
      "        [ 0.7797],\n",
      "        [ 4.0723],\n",
      "        [-0.4852]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0801, -0.8984,  2.5840,  2.9861,  0.5804],\n",
      "        [-0.0617,  0.8884,  0.3941,  0.3644, -1.2289],\n",
      "        [-0.6199,  1.0432,  2.1137,  0.3137, -1.0235],\n",
      "        [-2.9810, -1.1973,  0.9033,  0.2602,  2.5358]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0315,  0.0676,  0.5323,  0.2688,  0.7622],\n",
      "        [ 0.3753, -0.3136, -0.4924, -0.6112, -1.5562],\n",
      "        [-0.4938, -0.6581, -0.9730, -1.4938, -1.0001],\n",
      "        [ 0.4700,  0.4753,  0.0348, -0.3244,  0.5096]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0801, -0.8984,  2.5840,  2.9861,  0.5804],\n",
      "        [-0.0617,  0.8884,  0.3941,  0.3644, -1.2289],\n",
      "        [-0.6199,  1.0432,  2.1137,  0.3137, -1.0235],\n",
      "        [-2.9810, -1.1973,  0.9033,  0.2602,  2.5358]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.4773],\n",
      "        [ 1.1940],\n",
      "        [-1.8821],\n",
      "        [-0.7309]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3358,  0.9924, -0.6101,  0.0239,  0.4795],\n",
      "        [-0.8905,  1.5878, -0.2108,  1.2483, -0.5133],\n",
      "        [-0.3636,  1.9145, -1.0364, -0.6287, -0.5282],\n",
      "        [-0.3975,  0.4285, -0.8115,  0.4656, -1.2984]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2983, -1.5224, -0.6860, -0.9228, -1.2924],\n",
      "        [-0.6972, -0.7289, -0.6024, -1.4257, -1.8973],\n",
      "        [ 0.2835,  0.0617,  0.0600, -0.0105,  0.3977],\n",
      "        [ 0.8599, -0.1581,  0.0397, -0.0334,  0.5493]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3358,  0.9924, -0.6101,  0.0239,  0.4795],\n",
      "        [-0.8905,  1.5878, -0.2108,  1.2483, -0.5133],\n",
      "        [-0.3636,  1.9145, -1.0364, -0.6287, -0.5282],\n",
      "        [-0.3975,  0.4285, -0.8115,  0.4656, -1.2984]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6338],\n",
      "        [-1.2152],\n",
      "        [-0.2507],\n",
      "        [-1.1705]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3400,  1.1134,  0.0996, -1.1928, -0.1240],\n",
      "        [-1.1438, -0.0854, -0.2481, -1.1711,  0.3083],\n",
      "        [-0.6767,  0.8556, -0.8425,  0.9304, -1.1924],\n",
      "        [ 0.5502, -0.3159, -0.1092, -0.3990, -0.9342]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7513,  0.6513, -0.8723, -0.2966,  0.4978],\n",
      "        [ 0.3964, -0.4714,  0.1754,  0.0382, -0.0323],\n",
      "        [ 0.0769,  0.1508, -0.3157, -0.2089, -0.2961],\n",
      "        [ 0.6060,  0.7283,  0.0291,  0.6839,  0.9540]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3400,  1.1134,  0.0996, -1.1928, -0.1240],\n",
      "        [-1.1438, -0.0854, -0.2481, -1.1711,  0.3083],\n",
      "        [-0.6767,  0.8556, -0.8425,  0.9304, -1.1924],\n",
      "        [ 0.5502, -0.3159, -0.1092, -0.3990, -0.9342]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.9373],\n",
      "        [-0.5114],\n",
      "        [ 0.5016],\n",
      "        [-1.0640]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3103, -0.6471,  4.2653, -0.0065, -0.9167],\n",
      "        [ 1.3878,  0.7467, -1.0723, -0.9039,  1.2029],\n",
      "        [ 0.7784,  0.2117,  0.8451, -0.4133, -0.3656],\n",
      "        [ 0.0346, -0.2916, -0.7073,  1.6761,  1.6138]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9016, -1.3340, -1.0123, -1.2191, -1.6539],\n",
      "        [ 0.8896, -0.5858, -0.2133,  0.0956, -0.5070],\n",
      "        [ 0.0863, -0.5522, -0.3740, -0.0783,  0.6197],\n",
      "        [ 0.9896, -0.5275,  1.4047,  0.6895,  1.0663]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.3103, -0.6471,  4.2653, -0.0065, -0.9167],\n",
      "        [ 1.3878,  0.7467, -1.0723, -0.9039,  1.2029],\n",
      "        [ 0.7784,  0.2117,  0.8451, -0.4133, -0.3656],\n",
      "        [ 0.0346, -0.2916, -0.7073,  1.6761,  1.6138]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.1120],\n",
      "        [ 0.3297],\n",
      "        [-0.5600],\n",
      "        [ 2.0709]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.3027, -0.8493,  0.3196, -0.1554,  0.8301],\n",
      "        [-0.3576, -0.1334, -0.7572, -0.9284,  0.0002],\n",
      "        [ 0.9313, -1.2811,  0.3905,  0.8223,  0.0534],\n",
      "        [-0.9745,  0.2875, -0.8272,  0.5847, -0.2192]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5289,  0.0193,  0.5449,  0.4989,  0.9154],\n",
      "        [-0.4448, -0.8924, -0.0086, -0.0492,  0.0140],\n",
      "        [ 0.2743, -1.0879, -0.6102,  0.1302,  0.1325],\n",
      "        [ 0.0956, -0.1412, -0.1068, -0.5742, -0.5668]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.3027, -0.8493,  0.3196, -0.1554,  0.8301],\n",
      "        [-0.3576, -0.1334, -0.7572, -0.9284,  0.0002],\n",
      "        [ 0.9313, -1.2811,  0.3905,  0.8223,  0.0534],\n",
      "        [-0.9745,  0.2875, -0.8272,  0.5847, -0.2192]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3778],\n",
      "        [ 0.3303],\n",
      "        [ 1.5250],\n",
      "        [-0.2569]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2998,  1.7838, -0.4793,  0.4181, -1.7454],\n",
      "        [-1.1364, -1.4355,  0.2771,  1.3162, -1.0719],\n",
      "        [ 0.1738, -0.1472,  0.3109,  1.1488, -0.9038],\n",
      "        [ 1.6257,  1.1707, -0.1206,  1.2010, -0.2180]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9225, -0.6186,  0.4947,  0.3006,  1.8164],\n",
      "        [ 0.4625, -0.9343, -0.7487, -0.2159, -0.0213],\n",
      "        [-0.0540, -0.5444, -1.3498, -0.4243, -1.1863],\n",
      "        [ 0.3141,  0.7090, -0.2329,  0.5019,  0.1924]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2998,  1.7838, -0.4793,  0.4181, -1.7454],\n",
      "        [-1.1364, -1.4355,  0.2771,  1.3162, -1.0719],\n",
      "        [ 0.1738, -0.1472,  0.3109,  1.1488, -0.9038],\n",
      "        [ 1.6257,  1.1707, -0.1206,  1.2010, -0.2180]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.1086],\n",
      "        [ 0.3467],\n",
      "        [ 0.2359],\n",
      "        [ 1.9297]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8246,  0.0022,  0.1624,  0.5757,  0.5890],\n",
      "        [-0.0313, -0.7962,  0.9394, -1.0267, -1.8096],\n",
      "        [ 1.7310,  0.0329,  0.0442,  0.8294, -0.1417],\n",
      "        [ 0.1133, -0.0360,  0.2308,  1.1124,  1.0810]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.7752,  1.8956,  1.8652,  1.5148,  2.8489],\n",
      "        [-0.0192, -0.5032, -0.2641,  0.0516, -0.6707],\n",
      "        [-0.1365, -0.7335, -0.4429, -1.0236, -1.0653],\n",
      "        [-0.2663, -0.5888, -0.1850, -0.5993, -1.5794]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8246,  0.0022,  0.1624,  0.5757,  0.5890],\n",
      "        [-0.0313, -0.7962,  0.9394, -1.0267, -1.8096],\n",
      "        [ 1.7310,  0.0329,  0.0442,  0.8294, -0.1417],\n",
      "        [ 0.1133, -0.0360,  0.2308,  1.1124,  1.0810]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3935],\n",
      "        [ 1.3138],\n",
      "        [-0.9778],\n",
      "        [-2.4257]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2877,  0.8368, -0.3863, -1.1071,  1.4148],\n",
      "        [ 0.0431, -0.7671, -0.1628, -0.4470, -1.4918],\n",
      "        [ 1.1278,  0.0714, -0.5767,  0.4589,  0.5393],\n",
      "        [-1.6342, -0.7735,  0.6803, -1.0847, -0.5823]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3311, -0.0924, -0.1863,  0.4984, -0.9041],\n",
      "        [-1.0245, -1.3074, -0.6725, -0.6510, -1.5711],\n",
      "        [-0.0917, -0.1565, -0.0975, -0.2848, -0.5495],\n",
      "        [ 0.6736,  1.1413,  0.1315,  1.1193,  1.8476]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2877,  0.8368, -0.3863, -1.1071,  1.4148],\n",
      "        [ 0.0431, -0.7671, -0.1628, -0.4470, -1.4918],\n",
      "        [ 1.1278,  0.0714, -0.5767,  0.4589,  0.5393],\n",
      "        [-1.6342, -0.7735,  0.6803, -1.0847, -0.5823]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4098],\n",
      "        [ 3.7029],\n",
      "        [-0.4853],\n",
      "        [-4.1842]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8797,  1.7560, -0.8509,  0.4897,  1.7293],\n",
      "        [ 0.9611,  0.2557, -0.0041, -1.6451, -1.3384],\n",
      "        [-1.4110,  0.6607, -0.2041,  0.3302, -1.5984],\n",
      "        [-0.5593,  0.1069,  0.4590,  1.4303, -0.7569]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6250,  0.1633,  1.1855,  0.1965,  1.2546],\n",
      "        [-1.5786, -1.4410, -1.5825, -1.9039, -3.5155],\n",
      "        [ 0.2734, -0.2191, -0.6377, -0.4509, -0.5796],\n",
      "        [ 1.8261,  2.4742,  2.4207,  2.0690,  3.1140]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8797,  1.7560, -0.8509,  0.4897,  1.7293],\n",
      "        [ 0.9611,  0.2557, -0.0041, -1.6451, -1.3384],\n",
      "        [-1.4110,  0.6607, -0.2041,  0.3302, -1.5984],\n",
      "        [-0.5593,  0.1069,  0.4590,  1.4303, -0.7569]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9940],\n",
      "        [ 5.9581],\n",
      "        [ 0.3771],\n",
      "        [ 0.9565]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1342,  0.9799, -0.1193, -0.1819, -1.5799],\n",
      "        [-0.3967, -0.6663,  0.4887,  0.4570, -0.6607],\n",
      "        [ 1.1040, -0.2580,  1.0464, -0.7349,  1.5133],\n",
      "        [-0.5368,  2.1939,  0.1506, -0.0446,  0.6579]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2496,  0.0993,  0.3575,  0.5012,  0.6809],\n",
      "        [-0.5549, -0.3245, -0.0823,  0.1863,  0.7035],\n",
      "        [ 0.8221, -0.5273, -0.8428, -0.3198,  0.1635],\n",
      "        [ 0.1039,  0.2654,  0.2066,  0.0505, -0.1690]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1342,  0.9799, -0.1193, -0.1819, -1.5799],\n",
      "        [-0.3967, -0.6663,  0.4887,  0.4570, -0.6607],\n",
      "        [ 1.1040, -0.2580,  1.0464, -0.7349,  1.5133],\n",
      "        [-0.5368,  2.1939,  0.1506, -0.0446,  0.6579]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1458],\n",
      "        [ 0.0164],\n",
      "        [ 0.6442],\n",
      "        [ 0.4443]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5448,  0.8022,  0.3702,  0.1755, -1.2687],\n",
      "        [-1.1669,  0.0917, -0.1435, -1.3644, -0.7102],\n",
      "        [-1.4682, -0.3588, -0.1995, -0.0964, -0.5437],\n",
      "        [-0.8511,  0.2421, -1.0080,  0.2012,  0.3186]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6671,  0.9975,  0.3014,  0.2604,  1.0844],\n",
      "        [ 0.3017,  0.0412, -0.3891, -0.1447,  0.1388],\n",
      "        [-0.1528, -0.1760, -0.1463, -0.4628, -0.4059],\n",
      "        [ 0.5034, -0.1954,  0.2971,  0.0177,  0.0082]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5448,  0.8022,  0.3702,  0.1755, -1.2687],\n",
      "        [-1.1669,  0.0917, -0.1435, -1.3644, -0.7102],\n",
      "        [-1.4682, -0.3588, -0.1995, -0.0964, -0.5437],\n",
      "        [-0.8511,  0.2421, -1.0080,  0.2012,  0.3186]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0550],\n",
      "        [-0.1935],\n",
      "        [ 0.5820],\n",
      "        [-0.7692]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8558,  1.1226,  0.2039, -0.4358,  0.3523],\n",
      "        [-1.3759,  0.7670,  1.5323, -1.1775,  0.3568],\n",
      "        [ 1.9970,  0.0527,  0.3246, -1.1101,  1.1829],\n",
      "        [ 0.4153, -0.3820, -1.5911, -0.6106, -0.5174]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5020,  0.4248, -0.1897, -0.5517,  0.2087],\n",
      "        [ 0.3864, -0.5799,  0.0361, -0.1466, -0.4704],\n",
      "        [ 0.0926,  0.1399, -0.9095, -0.1835, -0.9966],\n",
      "        [ 0.4148,  0.0674,  0.3201,  0.1440,  0.4709]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8558,  1.1226,  0.2039, -0.4358,  0.3523],\n",
      "        [-1.3759,  0.7670,  1.5323, -1.1775,  0.3568],\n",
      "        [ 1.9970,  0.0527,  0.3246, -1.1101,  1.1829],\n",
      "        [ 0.4153, -0.3820, -1.5911, -0.6106, -0.5174]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1817],\n",
      "        [-0.9163],\n",
      "        [-1.0781],\n",
      "        [-0.6944]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0479,  0.6765,  0.5371,  0.5988, -0.5232],\n",
      "        [-0.0176,  0.2157, -0.4387,  0.0086,  1.9018],\n",
      "        [-1.4437, -2.0560,  1.0087,  1.8472,  0.4949],\n",
      "        [ 1.0721,  0.3091,  1.3084,  0.1358, -0.1961]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2578, -0.5167, -0.1034, -0.8874, -0.1997],\n",
      "        [ 0.7700,  0.3573, -0.5354,  0.2717,  0.3336],\n",
      "        [ 1.0063, -0.1251,  0.3916, -0.0922,  0.7705],\n",
      "        [ 0.1460,  0.1582,  0.0895, -0.2740,  0.3120]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0479,  0.6765,  0.5371,  0.5988, -0.5232],\n",
      "        [-0.0176,  0.2157, -0.4387,  0.0086,  1.9018],\n",
      "        [-1.4437, -2.0560,  1.0087,  1.8472,  0.4949],\n",
      "        [ 1.0721,  0.3091,  1.3084,  0.1358, -0.1961]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8197],\n",
      "        [ 0.9352],\n",
      "        [-0.5895],\n",
      "        [ 0.2241]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2192, -0.8930,  0.9691, -0.9189,  1.3144],\n",
      "        [ 0.6235,  0.3783,  0.4410, -0.2605,  0.5651],\n",
      "        [-0.8027, -0.5588,  0.7635,  1.7925, -0.1472],\n",
      "        [-0.0709, -0.3320, -1.4662,  1.6419,  1.2566]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4456, -0.2179, -0.3124, -0.3473, -1.0485],\n",
      "        [-0.0321, -0.9926, -0.1145,  0.0026, -0.6230],\n",
      "        [ 0.1955, -0.2236, -0.6195,  0.4075,  0.1649],\n",
      "        [ 0.1580, -0.0444,  0.1574,  0.0279,  0.1493]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2192, -0.8930,  0.9691, -0.9189,  1.3144],\n",
      "        [ 0.6235,  0.3783,  0.4410, -0.2605,  0.5651],\n",
      "        [-0.8027, -0.5588,  0.7635,  1.7925, -0.1472],\n",
      "        [-0.0709, -0.3320, -1.4662,  1.6419,  1.2566]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0695],\n",
      "        [-0.7987],\n",
      "        [ 0.2011],\n",
      "        [ 0.0062]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7785, -0.5255,  0.0310,  0.3025, -0.3759],\n",
      "        [ 1.5080, -1.2779,  1.6236, -0.0075,  0.7712],\n",
      "        [-2.0260, -0.7160, -1.1157, -0.4164,  0.2236],\n",
      "        [-0.1801, -0.3356,  0.8512,  1.0994, -1.3694]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7622,  0.1942,  1.3583,  0.6581,  0.8531],\n",
      "        [-0.0075, -0.3869,  0.3991, -0.0404,  0.3489],\n",
      "        [ 0.6395, -0.4502, -0.4023, -0.4324, -0.2620],\n",
      "        [-0.3484,  0.2212, -0.5628,  0.2591,  0.1948]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7785, -0.5255,  0.0310,  0.3025, -0.3759],\n",
      "        [ 1.5080, -1.2779,  1.6236, -0.0075,  0.7712],\n",
      "        [-2.0260, -0.7160, -1.1157, -0.4164,  0.2236],\n",
      "        [-0.1801, -0.3356,  0.8512,  1.0994, -1.3694]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4119],\n",
      "        [ 1.4004],\n",
      "        [-0.4029],\n",
      "        [-0.4725]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9861,  0.8062, -0.0380,  0.9354,  0.7260],\n",
      "        [ 0.3979,  0.6884,  0.6029,  0.7126,  0.2500],\n",
      "        [ 1.7994,  0.1599, -0.3407, -0.5145, -0.9989],\n",
      "        [-0.6031,  0.6607,  0.3300, -0.2404, -0.0607]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4603,  0.0932, -0.4362,  0.2311,  1.0041],\n",
      "        [-0.2787, -0.3252, -0.3599,  0.0154, -1.1554],\n",
      "        [ 0.2839, -0.8845,  0.4258, -0.4540, -0.5979],\n",
      "        [ 0.2576, -0.2558,  0.5790,  0.6218, -0.4992]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9861,  0.8062, -0.0380,  0.9354,  0.7260],\n",
      "        [ 0.3979,  0.6884,  0.6029,  0.7126,  0.2500],\n",
      "        [ 1.7994,  0.1599, -0.3407, -0.5145, -0.9989],\n",
      "        [-0.6031,  0.6607,  0.3300, -0.2404, -0.0607]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4907],\n",
      "        [-0.8296],\n",
      "        [ 1.0553],\n",
      "        [-0.2525]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4276,  0.2776, -1.2623, -1.1172,  1.1117],\n",
      "        [-0.8798, -0.9381,  1.0426,  0.4409, -0.2386],\n",
      "        [ 1.0474,  3.2286, -1.4885, -0.3904,  0.6240],\n",
      "        [ 0.5304,  0.2753,  0.3201, -0.2161,  1.3743]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6389, -0.5193, -0.9017, -0.3922, -1.4189],\n",
      "        [ 0.0389, -0.1910, -0.2048, -0.2361, -0.8587],\n",
      "        [ 0.1251, -0.4928, -0.4461, -0.8307, -1.6460],\n",
      "        [ 0.3934,  0.2432,  0.0288, -0.4024, -0.4895]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4276,  0.2776, -1.2623, -1.1172,  1.1117],\n",
      "        [-0.8798, -0.9381,  1.0426,  0.4409, -0.2386],\n",
      "        [ 1.0474,  3.2286, -1.4885, -0.3904,  0.6240],\n",
      "        [ 0.5304,  0.2753,  0.3201, -0.2161,  1.3743]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1280],\n",
      "        [ 0.0322],\n",
      "        [-1.4989],\n",
      "        [-0.3009]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8255,  0.2920, -1.2317,  0.3928, -0.4915],\n",
      "        [-0.3749, -1.4607,  1.1535,  2.2067,  0.6491],\n",
      "        [ 0.1023, -0.0700,  0.1629, -1.3775, -0.9211],\n",
      "        [-1.6984,  0.0332,  2.0532,  0.3077, -2.9007]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0520, -0.1342,  0.3010, -0.9624, -0.2863],\n",
      "        [-0.1428, -0.2228, -0.2985,  0.1674, -0.0640],\n",
      "        [ 0.4050, -0.1223,  0.1968, -0.2414,  0.3553],\n",
      "        [ 0.6415, -0.4323, -0.2078, -0.3112, -0.0449]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8255,  0.2920, -1.2317,  0.3928, -0.4915],\n",
      "        [-0.3749, -1.4607,  1.1535,  2.2067,  0.6491],\n",
      "        [ 0.1023, -0.0700,  0.1629, -1.3775, -0.9211],\n",
      "        [-1.6984,  0.0332,  2.0532,  0.3077, -2.9007]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6902],\n",
      "        [ 0.3625],\n",
      "        [ 0.0873],\n",
      "        [-1.4959]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2570,  1.1272, -0.8756, -1.3301,  0.9743],\n",
      "        [-0.5853, -0.4070,  0.9270,  0.4997,  0.8238],\n",
      "        [-0.3587, -0.4008, -0.5205, -1.6364,  0.9318],\n",
      "        [-0.1760,  1.1713, -0.9342,  0.1874, -0.0556]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4752, -0.7646, -0.0129, -0.9294,  0.4639],\n",
      "        [ 0.1377,  0.0509,  0.3297, -0.5024, -0.2234],\n",
      "        [ 0.7536, -1.3199, -1.0474, -0.0224, -0.2465],\n",
      "        [ 0.5102,  0.6649,  0.9905,  0.7147,  1.0672]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2570,  1.1272, -0.8756, -1.3301,  0.9743],\n",
      "        [-0.5853, -0.4070,  0.9270,  0.4997,  0.8238],\n",
      "        [-0.3587, -0.4008, -0.5205, -1.6364,  0.9318],\n",
      "        [-0.1760,  1.1713, -0.9342,  0.1874, -0.0556]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2402],\n",
      "        [-0.2308],\n",
      "        [ 0.6108],\n",
      "        [-0.1619]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8249, -2.2145,  1.1355, -0.1106,  0.5939],\n",
      "        [ 1.1666, -0.3316, -0.1788, -0.0954, -1.6288],\n",
      "        [ 0.6472, -0.1416,  0.8046,  1.3450, -0.7204],\n",
      "        [-0.5706, -1.8848, -1.3665,  0.2966,  0.1741]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2765,  0.0458,  0.2385,  0.1344,  0.1205],\n",
      "        [ 0.0199, -0.1542, -0.2341, -1.1845,  0.1308],\n",
      "        [ 0.1033, -0.3057,  0.0390, -0.6978, -0.7912],\n",
      "        [ 0.3221, -0.1386, -0.0035,  0.1846,  0.1673]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8249, -2.2145,  1.1355, -0.1106,  0.5939],\n",
      "        [ 1.1666, -0.3316, -0.1788, -0.0954, -1.6288],\n",
      "        [ 0.6472, -0.1416,  0.8046,  1.3450, -0.7204],\n",
      "        [-0.5706, -1.8848, -1.3665,  0.2966,  0.1741]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0021],\n",
      "        [ 0.0162],\n",
      "        [-0.2271],\n",
      "        [ 0.1661]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7706, -1.5155, -0.2571,  0.4993,  1.1557],\n",
      "        [-1.3797, -0.0741, -1.2729,  0.6535, -2.3535],\n",
      "        [ 0.7176,  0.1733, -1.8239,  0.5018,  0.2685],\n",
      "        [-0.7915, -1.1486, -1.8541, -0.6679,  0.7088]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2117,  0.1461, -0.1527,  0.1525,  0.2732],\n",
      "        [ 0.5072, -0.1347, -0.5137, -0.9307, -0.4501],\n",
      "        [ 0.2729, -0.6409, -0.2257, -0.1127, -0.2979],\n",
      "        [ 0.9606,  0.4736, -0.4579, -0.0511, -0.0607]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7706, -1.5155, -0.2571,  0.4993,  1.1557],\n",
      "        [-1.3797, -0.0741, -1.2729,  0.6535, -2.3535],\n",
      "        [ 0.7176,  0.1733, -1.8239,  0.5018,  0.2685],\n",
      "        [-0.7915, -1.1486, -1.8541, -0.6679,  0.7088]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5845],\n",
      "        [ 0.4150],\n",
      "        [ 0.3600],\n",
      "        [-0.4643]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1822, -0.0969,  0.0730, -0.7210,  0.1581],\n",
      "        [-2.3328, -0.3632,  0.2847,  0.1980, -1.5581],\n",
      "        [-0.4502,  1.0781,  0.8484, -0.0412,  0.8571],\n",
      "        [ 0.1205, -1.2260, -0.5967, -0.4012, -0.0112]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5727, -0.0837,  0.1611,  0.3316, -0.2360],\n",
      "        [-0.1626, -0.1510,  0.0598, -0.4450,  0.1555],\n",
      "        [-0.1159,  0.2588, -0.0743, -0.2762, -0.3620],\n",
      "        [ 0.4628, -0.1492, -0.1448, -0.2378, -0.2536]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1822, -0.0969,  0.0730, -0.7210,  0.1581],\n",
      "        [-2.3328, -0.3632,  0.2847,  0.1980, -1.5581],\n",
      "        [-0.4502,  1.0781,  0.8484, -0.0412,  0.8571],\n",
      "        [ 0.1205, -1.2260, -0.5967, -0.4012, -0.0112]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9335],\n",
      "        [ 0.1207],\n",
      "        [-0.0308],\n",
      "        [ 0.4234]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1767, -1.4257,  0.2466, -0.2639,  1.5903],\n",
      "        [-0.1615,  0.1771, -0.5576, -0.4223, -1.7129],\n",
      "        [-1.2967,  0.7449, -2.1962, -0.4607, -0.1990],\n",
      "        [-0.6765,  0.1948,  0.4706,  0.5945, -0.3824]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1782,  0.2340,  0.7121, -0.1707,  1.5391],\n",
      "        [-0.0327,  0.0119, -0.1645,  0.0890, -0.7854],\n",
      "        [ 0.0890,  0.2109, -0.3806, -0.8322, -0.4043],\n",
      "        [ 0.5211,  0.0681, -0.2874, -0.8310, -0.1283]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1767, -1.4257,  0.2466, -0.2639,  1.5903],\n",
      "        [-0.1615,  0.1771, -0.5576, -0.4223, -1.7129],\n",
      "        [-1.2967,  0.7449, -2.1962, -0.4607, -0.1990],\n",
      "        [-0.6765,  0.1948,  0.4706,  0.5945, -0.3824]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1250],\n",
      "        [ 1.4069],\n",
      "        [ 1.3414],\n",
      "        [-0.9195]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5361,  0.2767,  0.3637, -1.2259, -1.5436],\n",
      "        [-0.5102,  1.1905,  0.9893, -1.6210, -0.2381],\n",
      "        [-1.1389,  0.3877,  0.8828,  0.9393, -1.2267],\n",
      "        [ 1.1400, -0.0358, -2.1593,  1.4620,  0.4756]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0610, -1.2825, -0.6522, -0.6632, -1.3851],\n",
      "        [-0.2648, -0.7147, -1.1057, -0.8101, -1.5444],\n",
      "        [-0.3220, -0.2077, -0.6062, -1.0927, -1.5171],\n",
      "        [ 0.4827,  0.3702, -0.3429,  0.6251,  0.7007]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5361,  0.2767,  0.3637, -1.2259, -1.5436],\n",
      "        [-0.5102,  1.1905,  0.9893, -1.6210, -0.2381],\n",
      "        [-1.1389,  0.3877,  0.8828,  0.9393, -1.2267],\n",
      "        [ 1.1400, -0.0358, -2.1593,  1.4620,  0.4756]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3264],\n",
      "        [-0.1287],\n",
      "        [ 0.5856],\n",
      "        [ 2.5246]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9935,  1.6145, -0.7957, -0.4954, -1.1558],\n",
      "        [-0.5729,  0.3489, -0.5028,  1.6479, -0.1947],\n",
      "        [-1.9711,  1.3560, -0.1063,  0.7400,  1.6812],\n",
      "        [-2.2673,  0.0198,  0.9992, -1.2342, -0.1805]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2787, -0.9538, -1.1176, -1.4211, -2.3341],\n",
      "        [-0.1226, -0.3055, -0.4073,  0.2863, -0.4596],\n",
      "        [-0.4828, -0.9058, -0.7936, -0.8805, -1.3250],\n",
      "        [-0.8900, -1.4884, -1.5002, -0.2732, -1.3060]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9935,  1.6145, -0.7957, -0.4954, -1.1558],\n",
      "        [-0.5729,  0.3489, -0.5028,  1.6479, -0.1947],\n",
      "        [-1.9711,  1.3560, -0.1063,  0.7400,  1.6812],\n",
      "        [-2.2673,  0.0198,  0.9992, -1.2342, -0.1805]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4811],\n",
      "        [ 0.7297],\n",
      "        [-3.0713],\n",
      "        [ 1.0625]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1929,  0.8166,  1.1065,  0.9342,  2.4096],\n",
      "        [-0.5407,  0.0753, -0.0370,  2.0189,  1.7030],\n",
      "        [-0.0066, -0.7196,  1.4946,  0.6744,  1.4542],\n",
      "        [-0.0093, -0.4758, -1.0139,  0.7192,  0.4443]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7195, -1.2934, -2.1663, -2.0885, -3.9309],\n",
      "        [-0.1452,  0.1798, -0.3822, -0.1469, -0.8604],\n",
      "        [ 0.7697,  0.7363,  1.0378,  0.9814,  1.1959],\n",
      "        [-0.8327, -1.7363, -1.7767, -1.0614, -1.6530]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1929,  0.8166,  1.1065,  0.9342,  2.4096],\n",
      "        [-0.5407,  0.0753, -0.0370,  2.0189,  1.7030],\n",
      "        [-0.0066, -0.7196,  1.4946,  0.6744,  1.4542],\n",
      "        [-0.0093, -0.4758, -1.0139,  0.7192,  0.4443]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-15.0152],\n",
      "        [ -1.6557],\n",
      "        [  3.4171],\n",
      "        [  1.1374]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5466,  0.5949, -1.4552, -0.3682,  0.8080],\n",
      "        [ 0.1578, -2.1977, -0.6039, -1.8033,  0.8241],\n",
      "        [-0.3105, -1.4092,  0.7974, -1.1216, -1.0611],\n",
      "        [-1.2529,  1.5808,  0.1279,  0.0729,  1.4539]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.5804,  3.8326,  3.3259,  2.4174,  4.9587],\n",
      "        [ 0.2497,  0.0416,  0.5097,  0.5818,  1.2566],\n",
      "        [-0.4176, -0.7965, -1.2352, -1.0337, -1.9418],\n",
      "        [-0.9814, -1.5224, -2.1608, -1.6266, -2.8419]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5466,  0.5949, -1.4552, -0.3682,  0.8080],\n",
      "        [ 0.1578, -2.1977, -0.6039, -1.8033,  0.8241],\n",
      "        [-0.3105, -1.4092,  0.7974, -1.1216, -1.0611],\n",
      "        [-1.2529,  1.5808,  0.1279,  0.0729,  1.4539]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8541],\n",
      "        [-0.3732],\n",
      "        [ 3.4869],\n",
      "        [-5.7037]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1446, -1.6274, -1.4474,  0.6254,  0.1862],\n",
      "        [ 1.9901, -0.8544,  0.0896,  0.2595, -1.3172],\n",
      "        [-0.0231, -1.2854, -0.0346,  0.2956,  1.4217],\n",
      "        [ 0.4170,  0.3430, -0.3516,  0.5572,  1.2272]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0590,  0.3891,  0.6173,  0.4812, -0.7553],\n",
      "        [ 0.6827, -0.8462,  0.0292, -0.2105,  0.8605],\n",
      "        [-1.2237, -2.0748, -2.4840, -2.3049, -4.0195],\n",
      "        [ 1.3358,  0.4978,  0.5206,  0.8249,  1.7825]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1446, -1.6274, -1.4474,  0.6254,  0.1862],\n",
      "        [ 1.9901, -0.8544,  0.0896,  0.2595, -1.3172],\n",
      "        [-0.0231, -1.2854, -0.0346,  0.2956,  1.4217],\n",
      "        [ 0.4170,  0.3430, -0.3516,  0.5572,  1.2272]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3750],\n",
      "        [ 0.8962],\n",
      "        [-3.6147],\n",
      "        [ 3.1917]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7309,  1.0716,  0.4468,  0.9359, -0.6962],\n",
      "        [-0.4915,  1.1174, -0.8923, -0.9134,  0.7289],\n",
      "        [-0.6052,  0.2089,  1.1535,  1.9866,  1.1526],\n",
      "        [-0.5443,  1.9685,  0.6137, -0.4454,  0.2294]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3872,  0.2575,  0.8506,  0.9949,  0.9696],\n",
      "        [-0.0668, -0.5662, -0.4842,  0.0233, -0.4647],\n",
      "        [-0.0605, -0.1095,  0.2665, -0.4977,  0.3253],\n",
      "        [-0.0814, -0.8458, -1.1498, -1.3871, -1.3204]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7309,  1.0716,  0.4468,  0.9359, -0.6962],\n",
      "        [-0.4915,  1.1174, -0.8923, -0.9134,  0.7289],\n",
      "        [-0.6052,  0.2089,  1.1535,  1.9866,  1.1526],\n",
      "        [-0.5443,  1.9685,  0.6137, -0.4454,  0.2294]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1951],\n",
      "        [-0.5278],\n",
      "        [-0.2928],\n",
      "        [-2.0115]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2653,  0.5758,  1.6091,  1.4220, -1.1775],\n",
      "        [-0.2150,  0.3333,  0.1832, -0.0834, -0.3188],\n",
      "        [ 0.8318, -0.2203, -1.2904,  0.6998, -0.5925],\n",
      "        [ 1.6352, -1.1340,  1.4694,  0.4755,  0.6693]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0877,  0.5120, -0.0269,  0.0526, -0.2179],\n",
      "        [ 0.0783, -0.4102,  1.0251,  0.0874, -0.7935],\n",
      "        [-0.0555,  0.7963, -0.0083, -0.1180, -0.2361],\n",
      "        [ 0.4520, -0.4428, -0.6306, -0.7381,  0.8902]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2653,  0.5758,  1.6091,  1.4220, -1.1775],\n",
      "        [-0.2150,  0.3333,  0.1832, -0.0834, -0.3188],\n",
      "        [ 0.8318, -0.2203, -1.2904,  0.6998, -0.5925],\n",
      "        [ 1.6352, -1.1340,  1.4694,  0.4755,  0.6693]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4720],\n",
      "        [ 0.2799],\n",
      "        [-0.1536],\n",
      "        [ 0.5595]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3408,  0.2925,  0.2663, -0.2664, -1.0920],\n",
      "        [ 0.3363, -1.5241,  0.0885,  0.4793,  0.0620],\n",
      "        [-0.4667,  1.0030, -0.3934,  1.5968,  1.3490],\n",
      "        [ 0.2704, -0.1729, -1.3054, -0.8232,  0.5300]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4668,  0.4172,  0.1141,  0.2515, -0.0620],\n",
      "        [ 0.3348, -0.3329, -0.7555, -0.4481, -0.6555],\n",
      "        [-0.0443, -0.3708, -0.4211, -0.4541, -0.1482],\n",
      "        [ 0.4670, -0.4936, -0.8764, -1.3041, -0.0895]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3408,  0.2925,  0.2663, -0.2664, -1.0920],\n",
      "        [ 0.3363, -1.5241,  0.0885,  0.4793,  0.0620],\n",
      "        [-0.4667,  1.0030, -0.3934,  1.5968,  1.3490],\n",
      "        [ 0.2704, -0.1729, -1.3054, -0.8232,  0.5300]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3122],\n",
      "        [ 0.2977],\n",
      "        [-1.1105],\n",
      "        [ 2.3817]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6612,  0.4675, -0.4069,  0.3008,  1.6801],\n",
      "        [ 0.3056,  0.3703,  1.3280,  0.2485, -1.6973],\n",
      "        [-2.1919, -0.2051, -0.8515, -0.6472,  0.6216],\n",
      "        [-1.1514,  0.5238,  0.2355,  0.5523,  0.4864]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0208,  0.7739, -0.2137,  0.1767,  0.0262],\n",
      "        [ 0.3229, -0.3889, -0.6160,  0.1446, -0.6386],\n",
      "        [ 0.4424,  0.7406,  0.9119,  0.0548,  0.9213],\n",
      "        [-1.1550, -0.7376, -1.4758, -1.5386, -2.5309]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6612,  0.4675, -0.4069,  0.3008,  1.6801],\n",
      "        [ 0.3056,  0.3703,  1.3280,  0.2485, -1.6973],\n",
      "        [-2.1919, -0.2051, -0.8515, -0.6472,  0.6216],\n",
      "        [-1.1514,  0.5238,  0.2355,  0.5523,  0.4864]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5597],\n",
      "        [ 0.2564],\n",
      "        [-1.3610],\n",
      "        [-1.4847]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8231,  0.3505,  0.6940, -0.0487, -0.9287],\n",
      "        [ 0.8575,  0.8704, -0.1253,  1.5206,  1.3960],\n",
      "        [-0.4213,  0.4175, -0.8676,  0.8548, -1.3553],\n",
      "        [ 0.6087,  0.2360, -2.4012, -0.2100, -1.5557]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2252,  0.4922,  0.5641, -0.1760,  0.7575],\n",
      "        [ 0.0941, -0.1684, -0.3111,  0.5084, -0.1374],\n",
      "        [ 0.2149,  0.8149,  1.0476,  1.1921,  0.8149],\n",
      "        [-0.5805, -0.3480, -1.3060, -0.8225, -1.4683]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8231,  0.3505,  0.6940, -0.0487, -0.9287],\n",
      "        [ 0.8575,  0.8704, -0.1253,  1.5206,  1.3960],\n",
      "        [-0.4213,  0.4175, -0.8676,  0.8548, -1.3553],\n",
      "        [ 0.6087,  0.2360, -2.4012, -0.2100, -1.5557]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0545],\n",
      "        [ 0.5544],\n",
      "        [-0.7446],\n",
      "        [ 5.1577]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3088, -1.5851,  0.9249, -0.0445, -1.7784],\n",
      "        [ 1.4703,  0.1480,  0.0366,  1.3089, -0.1002],\n",
      "        [-1.7424,  0.8810, -1.4747, -0.1898,  1.8939],\n",
      "        [-0.6741, -0.8949, -1.0829,  0.0124, -0.0353]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1752, -0.3052,  0.4275, -0.0808,  0.8564],\n",
      "        [-0.3743,  0.4533, -0.1535,  0.9324, -0.3129],\n",
      "        [ 0.8563,  0.9421,  0.7397,  0.9465,  2.0679],\n",
      "        [ 0.4735,  0.5985, -0.1142, -0.3582, -0.6639]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3088, -1.5851,  0.9249, -0.0445, -1.7784],\n",
      "        [ 1.4703,  0.1480,  0.0366,  1.3089, -0.1002],\n",
      "        [-1.7424,  0.8810, -1.4747, -0.1898,  1.8939],\n",
      "        [-0.6741, -0.8949, -1.0829,  0.0124, -0.0353]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5861],\n",
      "        [ 0.7630],\n",
      "        [ 1.9837],\n",
      "        [-0.7121]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0553,  0.5283, -0.5155,  0.3312, -0.0713],\n",
      "        [-1.8445,  0.7218,  0.5089,  0.2075, -0.1405],\n",
      "        [-0.9153,  0.6971,  1.0121, -0.4930,  1.5492],\n",
      "        [-0.7733, -0.4816,  1.9448,  0.6789,  0.9888]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0670,  0.7702, -0.0255,  0.5258,  0.6463],\n",
      "        [-0.5005, -0.4316, -0.6079, -0.6341, -0.0379],\n",
      "        [ 0.0862,  0.3667,  0.0692,  0.0507,  0.2157],\n",
      "        [ 0.5259, -0.3600, -0.2951,  0.0777, -0.2510]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0553,  0.5283, -0.5155,  0.3312, -0.0713],\n",
      "        [-1.8445,  0.7218,  0.5089,  0.2075, -0.1405],\n",
      "        [-0.9153,  0.6971,  1.0121, -0.4930,  1.5492],\n",
      "        [-0.7733, -0.4816,  1.9448,  0.6789,  0.9888]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4891],\n",
      "        [ 0.1761],\n",
      "        [ 0.5559],\n",
      "        [-1.0026]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1213, -0.3163, -0.8847, -1.4368,  0.7955],\n",
      "        [-0.7747, -0.9221,  2.1110, -0.1085,  0.9602],\n",
      "        [-1.1805, -0.9811,  0.5278,  0.6945,  0.0727],\n",
      "        [ 0.1309,  0.5544, -0.4111,  1.7948,  0.4117]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0856,  0.4687,  0.4681,  0.3749,  0.6243],\n",
      "        [ 0.2820,  0.6316,  0.0754,  0.5896,  0.0433],\n",
      "        [-0.1938,  0.9011,  0.3167,  0.6158,  0.4433],\n",
      "        [ 0.4466,  0.3461, -0.2975,  0.0530,  0.7899]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1213, -0.3163, -0.8847, -1.4368,  0.7955],\n",
      "        [-0.7747, -0.9221,  2.1110, -0.1085,  0.9602],\n",
      "        [-1.1805, -0.9811,  0.5278,  0.6945,  0.0727],\n",
      "        [ 0.1309,  0.5544, -0.4111,  1.7948,  0.4117]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7003],\n",
      "        [-0.6642],\n",
      "        [-0.0282],\n",
      "        [ 0.7929]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4450,  0.6357, -0.4022, -0.3914, -1.5037],\n",
      "        [-1.4788, -0.4689, -0.5093,  1.2836, -0.4905],\n",
      "        [ 0.2577,  1.7530, -0.1813,  0.3142,  0.9421],\n",
      "        [ 0.5908, -0.8223, -0.6390,  0.2134,  0.7194]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5581,  0.6304, -0.1327, -0.0016,  0.2973],\n",
      "        [ 0.2042,  0.4893,  0.5054,  0.2659,  0.6148],\n",
      "        [ 0.3419,  0.9814,  1.2658,  0.5205,  0.7091],\n",
      "        [ 0.3872,  0.2934,  0.4873, -0.2492, -0.5259]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4450,  0.6357, -0.4022, -0.3914, -1.5037],\n",
      "        [-1.4788, -0.4689, -0.5093,  1.2836, -0.4905],\n",
      "        [ 0.2577,  1.7530, -0.1813,  0.3142,  0.9421],\n",
      "        [ 0.5908, -0.8223, -0.6390,  0.2134,  0.7194]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2561],\n",
      "        [-0.7490],\n",
      "        [ 2.4107],\n",
      "        [-0.7554]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6784, -0.2734,  0.3533,  0.5172,  0.0206],\n",
      "        [ 1.1820,  1.8701,  1.2365,  0.6491,  0.1113],\n",
      "        [-0.7095, -0.2607,  0.7220,  0.2930, -0.2910],\n",
      "        [-0.5028, -0.9748, -1.3810,  2.4338, -2.6267]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0365,  0.6408,  0.0872,  0.1160, -0.1480],\n",
      "        [-0.1561, -0.1099,  0.5776,  0.2376,  0.8629],\n",
      "        [-0.6754, -0.2718, -1.4046, -0.7290, -1.0922],\n",
      "        [ 0.2612,  0.4536, -0.8801,  0.4048, -0.8879]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6784, -0.2734,  0.3533,  0.5172,  0.0206],\n",
      "        [ 1.1820,  1.8701,  1.2365,  0.6491,  0.1113],\n",
      "        [-0.7095, -0.2607,  0.7220,  0.2930, -0.2910],\n",
      "        [-0.5028, -0.9748, -1.3810,  2.4338, -2.6267]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0263],\n",
      "        [ 0.5744],\n",
      "        [-0.3599],\n",
      "        [ 3.9593]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.5408, -0.6317,  0.4458,  0.4374, -1.3293],\n",
      "        [ 0.0579,  1.2658, -1.4630,  1.9706,  0.2091],\n",
      "        [ 0.5903, -1.3862, -0.4876,  0.4274,  1.7282],\n",
      "        [-0.7241,  1.2397,  0.8194, -1.0204,  0.1255]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1310,  0.4161,  0.3075,  0.4745,  1.0223],\n",
      "        [ 0.0527,  0.3803,  0.5139, -0.1290, -0.5003],\n",
      "        [-0.6163,  0.1957,  0.7835,  0.0852, -0.2768],\n",
      "        [-1.4188, -0.9861, -1.7345, -1.9988, -3.6981]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5408, -0.6317,  0.4458,  0.4374, -1.3293],\n",
      "        [ 0.0579,  1.2658, -1.4630,  1.9706,  0.2091],\n",
      "        [ 0.5903, -1.3862, -0.4876,  0.4274,  1.7282],\n",
      "        [-0.7241,  1.2397,  0.8194, -1.0204,  0.1255]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2063],\n",
      "        [-0.6263],\n",
      "        [-1.4591],\n",
      "        [-0.0410]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2560, -0.6787, -0.9359, -0.5795, -0.4403],\n",
      "        [-0.6337, -0.4801,  1.6907,  1.0826,  0.1637],\n",
      "        [-0.1846, -0.0051,  1.9379, -1.0746,  0.0058],\n",
      "        [ 1.5754,  0.4335,  0.6728,  1.4572,  0.1378]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8844,  1.1054,  0.1863,  0.5571,  1.4563],\n",
      "        [-0.0558,  0.0847, -0.1427, -0.4233,  0.2015],\n",
      "        [ 0.4043,  0.1959, -0.0590,  1.3077,  0.8641],\n",
      "        [-0.9121, -1.4455, -1.8362, -2.3793, -2.2913]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2560, -0.6787, -0.9359, -0.5795, -0.4403],\n",
      "        [-0.6337, -0.4801,  1.6907,  1.0826,  0.1637],\n",
      "        [-0.1846, -0.0051,  1.9379, -1.0746,  0.0058],\n",
      "        [ 1.5754,  0.4335,  0.6728,  1.4572,  0.1378]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9994],\n",
      "        [-0.6718],\n",
      "        [-1.5903],\n",
      "        [-7.0818]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2732, -0.0293, -0.3785,  0.7996,  1.1058],\n",
      "        [-0.8763, -0.1843, -0.6311,  0.3532,  0.5101],\n",
      "        [-0.9295, -0.2499, -0.7994,  1.7262,  0.9041],\n",
      "        [-0.1270, -0.8679,  1.5491,  1.3219,  0.7975]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2703,  2.5119,  1.8717,  0.5768,  2.6164],\n",
      "        [ 0.1745, -0.2294,  0.3073, -0.2367,  0.3955],\n",
      "        [ 1.2955,  1.3411,  1.2056,  1.0883,  1.1088],\n",
      "        [ 0.8373,  0.9112,  0.9924,  0.4376,  1.6144]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2732, -0.0293, -0.3785,  0.7996,  1.1058],\n",
      "        [-0.8763, -0.1843, -0.6311,  0.3532,  0.5101],\n",
      "        [-0.9295, -0.2499, -0.7994,  1.7262,  0.9041],\n",
      "        [-0.1270, -0.8679,  1.5491,  1.3219,  0.7975]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2252],\n",
      "        [-0.1865],\n",
      "        [ 0.3781],\n",
      "        [ 2.5061]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5082,  0.2226,  1.2125,  2.3278,  0.2512],\n",
      "        [-1.5228,  0.5052,  1.5358,  0.4048,  1.8478],\n",
      "        [-0.2838,  0.0144, -0.9672,  0.7724,  0.5910],\n",
      "        [-2.4565,  0.8850,  0.4067,  0.1967, -0.3441]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2994,  1.1148,  0.9279,  1.1291,  1.3285],\n",
      "        [ 0.6316, -0.2057, -0.8472, -0.4577, -0.0892],\n",
      "        [ 1.1396,  1.8043,  1.0490,  0.8074,  1.4589],\n",
      "        [ 0.7169, -0.5568, -0.4730, -0.0318,  0.3720]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5082,  0.2226,  1.2125,  2.3278,  0.2512],\n",
      "        [-1.5228,  0.5052,  1.5358,  0.4048,  1.8478],\n",
      "        [-0.2838,  0.0144, -0.9672,  0.7724,  0.5910],\n",
      "        [-2.4565,  0.8850,  0.4067,  0.1967, -0.3441]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.3755],\n",
      "        [-2.7169],\n",
      "        [ 0.1737],\n",
      "        [-2.5804]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0923, -0.0360, -1.3353, -1.6964,  1.0815],\n",
      "        [-0.3974, -1.5634,  0.8339,  0.2267,  1.6333],\n",
      "        [ 0.7749, -2.4501, -1.0104,  1.7487, -0.2730],\n",
      "        [-0.6522,  0.4998,  2.1968,  0.6092,  0.0546]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5812,  0.5727,  0.2206,  0.7087, -0.3751],\n",
      "        [ 0.8131,  1.1347,  0.7017,  0.4966,  1.6473],\n",
      "        [-0.1436, -0.0497,  0.6134,  1.1907,  1.4346],\n",
      "        [ 1.0727,  0.2066,  0.6460,  0.3946,  1.7942]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0923, -0.0360, -1.3353, -1.6964,  1.0815],\n",
      "        [-0.3974, -1.5634,  0.8339,  0.2267,  1.6333],\n",
      "        [ 0.7749, -2.4501, -1.0104,  1.7487, -0.2730],\n",
      "        [-0.6522,  0.4998,  2.1968,  0.6092,  0.0546]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8696],\n",
      "        [ 1.2912],\n",
      "        [ 1.0812],\n",
      "        [ 1.1611]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1413, -0.0301, -0.2343, -2.0487, -0.7292],\n",
      "        [ 0.3948,  1.3050, -0.8972,  2.1522,  0.6747],\n",
      "        [-0.0346,  0.5437,  1.0743,  0.5761,  0.9079],\n",
      "        [-0.5219, -0.2552, -1.2338,  0.8522,  1.5537]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0912,  1.2157,  1.5860,  1.2364,  1.1226],\n",
      "        [ 1.0141,  0.7718,  0.6063, -0.0399,  0.3887],\n",
      "        [-0.0459,  0.3722,  0.0921,  0.7415,  1.2570],\n",
      "        [ 0.8456, -0.1595, -0.1725,  0.7295,  0.6963]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1413, -0.0301, -0.2343, -2.0487, -0.7292],\n",
      "        [ 0.3948,  1.3050, -0.8972,  2.1522,  0.6747],\n",
      "        [-0.0346,  0.5437,  1.0743,  0.5761,  0.9079],\n",
      "        [-0.5219, -0.2552, -1.2338,  0.8522,  1.5537]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.7468],\n",
      "        [ 1.0400],\n",
      "        [ 1.8712],\n",
      "        [ 1.5156]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2732, -1.1862, -1.4794,  0.0251,  0.4176],\n",
      "        [ 0.3003,  0.1650,  2.4896, -0.9355,  0.6819],\n",
      "        [ 0.3786,  0.8754,  1.8413, -2.0815,  0.8277],\n",
      "        [ 0.5289, -0.5282,  1.0921,  0.8915,  1.5689]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3455, -0.6480, -0.3806, -0.4150, -0.1869],\n",
      "        [ 0.0396,  0.6905, -0.0041, -0.0198, -0.2622],\n",
      "        [-0.1212, -0.0959,  0.1169, -0.3011, -0.6883],\n",
      "        [-0.1144, -1.0969, -0.7780, -0.9207, -1.2335]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2732, -1.1862, -1.4794,  0.0251,  0.4176],\n",
      "        [ 0.3003,  0.1650,  2.4896, -0.9355,  0.6819],\n",
      "        [ 0.3786,  0.8754,  1.8413, -2.0815,  0.8277],\n",
      "        [ 0.5289, -0.5282,  1.0921,  0.8915,  1.5689]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3377],\n",
      "        [-0.0447],\n",
      "        [ 0.1425],\n",
      "        [-3.0867]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6675, -0.6446,  1.0692,  1.1452, -0.6311],\n",
      "        [-2.1268, -0.3159, -0.3045, -0.3512,  0.9846],\n",
      "        [ 1.6797,  0.0689, -1.6819, -1.4663, -0.6958],\n",
      "        [-2.8866,  0.8803,  0.6488,  0.1985,  1.2354]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9955, -0.6086, -0.0346, -0.4221, -1.7155],\n",
      "        [-0.0378,  0.1906,  0.2082,  0.0613,  0.0670],\n",
      "        [ 0.1904,  0.6620, -0.1037,  0.3577, -0.0772],\n",
      "        [ 0.8847,  1.5659,  1.3921,  1.0102,  1.1744]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6675, -0.6446,  1.0692,  1.1452, -0.6311],\n",
      "        [-2.1268, -0.3159, -0.3045, -0.3512,  0.9846],\n",
      "        [ 1.6797,  0.0689, -1.6819, -1.4663, -0.6958],\n",
      "        [-2.8866,  0.8803,  0.6488,  0.1985,  1.2354]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6191],\n",
      "        [ 0.0013],\n",
      "        [ 0.0691],\n",
      "        [ 1.3795]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4618,  0.3311,  1.1765, -0.6043, -1.7781],\n",
      "        [-0.6423,  1.6835, -1.3053, -0.5915, -0.3451],\n",
      "        [-1.1327,  0.7089,  0.0023,  0.9897, -1.9940],\n",
      "        [ 2.4810,  1.1105,  1.7429, -0.0174,  1.6906]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5846, -0.1591, -0.2387, -1.1581, -2.0080],\n",
      "        [ 0.8658,  0.8967,  0.2568,  1.1514,  0.7429],\n",
      "        [-0.3305,  0.6099,  1.0462,  0.2871,  0.6923],\n",
      "        [ 1.0246, -0.8096, -0.0587, -0.0765,  0.8095]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4618,  0.3311,  1.1765, -0.6043, -1.7781],\n",
      "        [-0.6423,  1.6835, -1.3053, -0.5915, -0.3451],\n",
      "        [-1.1327,  0.7089,  0.0023,  0.9897, -1.9940],\n",
      "        [ 2.4810,  1.1105,  1.7429, -0.0174,  1.6906]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.6669],\n",
      "        [-0.3192],\n",
      "        [-0.2870],\n",
      "        [ 2.9105]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3377, -1.4775, -0.1276, -1.2534,  1.2972],\n",
      "        [-0.3424, -1.8778,  0.2907,  0.8150, -0.4300],\n",
      "        [ 0.7979, -1.6343, -0.0922, -0.9257, -1.0032],\n",
      "        [ 0.3039, -0.4168,  0.0042,  2.5487,  1.2530]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.9367, -1.7698, -1.2380, -2.5745, -4.4470],\n",
      "        [ 0.6166, -0.1158,  0.1832,  0.4488,  0.5885],\n",
      "        [-0.3109,  0.4001,  1.4141,  1.4906,  0.4118],\n",
      "        [-0.1323, -1.1702, -0.7492, -1.1392, -2.2596]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3377, -1.4775, -0.1276, -1.2534,  1.2972],\n",
      "        [-0.3424, -1.8778,  0.2907,  0.8150, -0.4300],\n",
      "        [ 0.7979, -1.6343, -0.0922, -0.9257, -1.0032],\n",
      "        [ 0.3039, -0.4168,  0.0042,  2.5487,  1.2530]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8853],\n",
      "        [ 0.1723],\n",
      "        [-2.8254],\n",
      "        [-5.2904]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0269,  0.3401, -0.8500,  0.4241, -0.3953],\n",
      "        [-1.3726,  0.9473, -0.2067,  0.7131,  1.1585],\n",
      "        [-0.7577,  1.7883, -2.1116,  0.2506,  1.1907],\n",
      "        [ 0.3136,  1.6966, -0.2839,  0.0800, -0.1653]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2651, -0.0075, -0.1200, -0.2106, -0.1484],\n",
      "        [ 0.4045,  0.7150,  0.6035,  1.0619,  0.5990],\n",
      "        [ 0.9434,  1.6273,  2.2308,  2.0743,  2.0050],\n",
      "        [ 1.0008,  1.0344,  1.3802,  0.7806,  2.0698]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0269,  0.3401, -0.8500,  0.4241, -0.3953],\n",
      "        [-1.3726,  0.9473, -0.2067,  0.7131,  1.1585],\n",
      "        [-0.7577,  1.7883, -2.1116,  0.2506,  1.1907],\n",
      "        [ 0.3136,  1.6966, -0.2839,  0.0800, -0.1653]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0617],\n",
      "        [ 1.4485],\n",
      "        [ 0.3918],\n",
      "        [ 1.3972]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8153, -1.3804,  0.1068, -0.3595,  0.3269],\n",
      "        [-0.6857,  0.5875,  2.2105,  0.7515, -1.2460],\n",
      "        [ 0.3751,  0.7801, -0.8451, -0.5176,  0.4979],\n",
      "        [ 0.1603,  0.3583, -1.1228,  0.3069, -0.1117]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2939, -0.1437,  0.4480,  0.2950, -0.9140],\n",
      "        [-0.5188, -0.5573,  0.3727, -0.4174, -0.6711],\n",
      "        [ 0.2930,  2.0945,  1.2808,  0.8479,  1.6951],\n",
      "        [ 0.9975,  0.4966,  1.1594,  0.2884,  1.5579]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8153, -1.3804,  0.1068, -0.3595,  0.3269],\n",
      "        [-0.6857,  0.5875,  2.2105,  0.7515, -1.2460],\n",
      "        [ 0.3751,  0.7801, -0.8451, -0.5176,  0.4979],\n",
      "        [ 0.1603,  0.3583, -1.1228,  0.3069, -0.1117]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0809],\n",
      "        [ 1.3747],\n",
      "        [ 1.0667],\n",
      "        [-1.0494]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1859,  0.1112, -0.5203, -0.1916,  1.1436],\n",
      "        [-0.3031, -0.4871, -0.4903,  0.6056,  0.4236],\n",
      "        [-2.0447, -0.3659,  1.8179,  0.8706,  0.9503],\n",
      "        [ 0.8223,  1.6948,  0.5339,  0.2258, -1.3149]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1593, -0.2248,  0.3921,  0.3127, -0.2289],\n",
      "        [-0.6511, -0.7386, -0.5423, -0.8299, -1.2541],\n",
      "        [ 0.7071,  1.5829,  1.5041,  1.3655,  1.4543],\n",
      "        [ 1.2683,  1.0094,  2.2291,  1.3055,  1.3421]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1859,  0.1112, -0.5203, -0.1916,  1.1436],\n",
      "        [-0.3031, -0.4871, -0.4903,  0.6056,  0.4236],\n",
      "        [-2.0447, -0.3659,  1.8179,  0.8706,  0.9503],\n",
      "        [ 0.8223,  1.6948,  0.5339,  0.2258, -1.3149]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5211],\n",
      "        [-0.2109],\n",
      "        [ 3.2803],\n",
      "        [ 2.4738]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8832,  0.0975,  1.9759, -0.9894, -0.8224],\n",
      "        [-0.5076,  0.6545, -0.4644, -0.1465,  0.0806],\n",
      "        [ 0.1819, -0.5055, -0.2992, -1.1374,  0.0368],\n",
      "        [ 0.4523,  1.3783, -0.7303, -1.9396, -0.0270]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3169, -0.2361, -0.2720, -0.0768, -0.5738],\n",
      "        [-0.6208, -0.2795, -0.2497, -0.2873, -0.4150],\n",
      "        [-0.6353,  0.0626, -0.2455, -0.7012, -1.5013],\n",
      "        [ 0.2030,  0.1495,  0.2556,  0.6222, -0.4680]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8832,  0.0975,  1.9759, -0.9894, -0.8224],\n",
      "        [-0.5076,  0.6545, -0.4644, -0.1465,  0.0806],\n",
      "        [ 0.1819, -0.5055, -0.2992, -1.1374,  0.0368],\n",
      "        [ 0.4523,  1.3783, -0.7303, -1.9396, -0.0270]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6093],\n",
      "        [ 0.2568],\n",
      "        [ 0.6685],\n",
      "        [-1.0830]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4907,  0.6591,  1.2839,  0.0718,  1.2144],\n",
      "        [ 0.9661,  0.9828,  1.3298, -0.5631, -0.5114],\n",
      "        [-1.2943,  0.3002, -0.6683,  1.2544,  0.5981],\n",
      "        [ 1.3829, -1.4632,  0.6345,  1.6919, -2.0620]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0811, -0.5183, -0.1533, -0.3371, -0.7025],\n",
      "        [-0.1171, -0.0039,  0.1568,  0.2563, -0.1492],\n",
      "        [-0.7553, -0.4418,  0.0375, -0.4536, -1.2462],\n",
      "        [ 1.1528,  1.2187,  1.0959,  0.3652,  0.5630]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4907,  0.6591,  1.2839,  0.0718,  1.2144],\n",
      "        [ 0.9661,  0.9828,  1.3298, -0.5631, -0.5114],\n",
      "        [-1.2943,  0.3002, -0.6683,  1.2544,  0.5981],\n",
      "        [ 1.3829, -1.4632,  0.6345,  1.6919, -2.0620]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4556],\n",
      "        [ 0.0234],\n",
      "        [-0.4946],\n",
      "        [-0.0366]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4061,  1.3470,  0.4024, -0.0660, -0.2975],\n",
      "        [-0.2392,  0.0925, -0.1265,  0.4708,  1.8167],\n",
      "        [-0.3900, -0.0197,  1.6875,  0.1035,  1.9455],\n",
      "        [ 0.4742, -0.5330, -0.9850, -0.3573,  0.5482]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6762,  0.6275,  0.2828,  0.6352,  0.6075],\n",
      "        [-0.2706,  0.0938,  1.0695,  0.3045, -0.1262],\n",
      "        [-0.8141,  0.5054,  0.8482,  0.7989,  0.4447],\n",
      "        [ 0.3190,  0.3991,  0.3954,  1.0737,  0.8124]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4061,  1.3470,  0.4024, -0.0660, -0.2975],\n",
      "        [-0.2392,  0.0925, -0.1265,  0.4708,  1.8167],\n",
      "        [-0.3900, -0.0197,  1.6875,  0.1035,  1.9455],\n",
      "        [ 0.4742, -0.5330, -0.9850, -0.3573,  0.5482]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4618],\n",
      "        [-0.1477],\n",
      "        [ 2.6867],\n",
      "        [-0.3891]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0073,  0.4649, -1.8039, -0.2035,  0.4606],\n",
      "        [-2.6314, -0.4619,  1.0562,  1.2566,  0.7432],\n",
      "        [-2.4485,  0.4511, -1.1409,  0.7768,  0.8418],\n",
      "        [ 0.7451,  0.6913, -0.6204,  1.1626, -0.6754]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7832, -0.6101,  0.1946, -0.1645,  0.1405],\n",
      "        [-0.5796,  0.2483,  0.7780, -0.0310,  0.9819],\n",
      "        [-0.9685, -0.0567, -0.4456, -1.5246, -2.7926],\n",
      "        [ 0.7914, -0.2097,  0.2697,  0.7826,  0.8932]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0073,  0.4649, -1.8039, -0.2035,  0.4606],\n",
      "        [-2.6314, -0.4619,  1.0562,  1.2566,  0.7432],\n",
      "        [-2.4485,  0.4511, -1.1409,  0.7768,  0.8418],\n",
      "        [ 0.7451,  0.6913, -0.6204,  1.1626, -0.6754]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5422],\n",
      "        [ 2.9231],\n",
      "        [-0.6810],\n",
      "        [ 0.5840]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2313,  0.3823, -0.8619, -0.1901,  2.1813],\n",
      "        [ 1.2293, -0.4233,  1.2522,  1.8908, -0.3237],\n",
      "        [-0.4418,  0.0618,  0.1392, -2.0581, -0.1705],\n",
      "        [-1.0009, -0.6377,  1.1242,  1.3058,  0.4647]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3831, -0.5812, -0.3187, -0.0629, -0.4024],\n",
      "        [-0.7152, -0.4263, -1.5048, -1.6120, -2.2425],\n",
      "        [-0.4893, -0.1935, -0.2012,  0.2559, -0.7562],\n",
      "        [ 1.0735,  0.1791,  1.2095,  0.7225,  1.0060]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2313,  0.3823, -0.8619, -0.1901,  2.1813],\n",
      "        [ 1.2293, -0.4233,  1.2522,  1.8908, -0.3237],\n",
      "        [-0.4418,  0.0618,  0.1392, -2.0581, -0.1705],\n",
      "        [-1.0009, -0.6377,  1.1242,  1.3058,  0.4647]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9018],\n",
      "        [-4.9052],\n",
      "        [-0.2216],\n",
      "        [ 1.5821]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1955,  1.2658,  0.2797, -2.5049,  0.9360],\n",
      "        [-0.5313, -1.1444,  0.7372,  2.0108,  0.5572],\n",
      "        [ 0.6427,  0.7320,  0.3882, -0.1149,  1.7897],\n",
      "        [ 0.3624, -0.7587, -1.4745, -0.5228, -1.4450]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2975,  0.7196,  1.3208,  0.3525,  0.7518],\n",
      "        [ 0.7623,  0.7614,  1.3119,  0.9107,  1.4382],\n",
      "        [-0.8935,  0.7196, -0.3297,  0.8424,  0.4149],\n",
      "        [ 0.3261, -0.1430,  0.1995,  0.0406, -0.2343]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1955,  1.2658,  0.2797, -2.5049,  0.9360],\n",
      "        [-0.5313, -1.1444,  0.7372,  2.0108,  0.5572],\n",
      "        [ 0.6427,  0.7320,  0.3882, -0.1149,  1.7897],\n",
      "        [ 0.3624, -0.7587, -1.4745, -0.5228, -1.4450]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1592],\n",
      "        [ 2.3234],\n",
      "        [ 0.4703],\n",
      "        [ 0.2498]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0854,  1.3013,  0.4951,  0.5540, -1.0835],\n",
      "        [-1.7366,  0.2600,  1.0172,  1.0809,  0.1429],\n",
      "        [ 0.3633, -0.0815, -1.6029,  0.4369, -0.3534],\n",
      "        [ 0.7552, -1.2409, -0.9241, -0.6973, -1.6352]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4423, -0.8418, -1.1724, -0.3848, -1.1186],\n",
      "        [-0.4828,  0.1279,  0.8312,  0.9219, -0.1239],\n",
      "        [-0.6277,  0.3130, -0.8114, -0.2840, -0.6054],\n",
      "        [ 0.5339,  1.3018,  0.3592,  1.1152,  0.3144]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0854,  1.3013,  0.4951,  0.5540, -1.0835],\n",
      "        [-1.7366,  0.2600,  1.0172,  1.0809,  0.1429],\n",
      "        [ 0.3633, -0.0815, -1.6029,  0.4369, -0.3534],\n",
      "        [ 0.7552, -1.2409, -0.9241, -0.6973, -1.6352]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7147],\n",
      "        [ 2.6960],\n",
      "        [ 1.1369],\n",
      "        [-2.8358]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4336, -0.8240, -0.1473, -0.2196, -0.3110],\n",
      "        [-1.6859,  1.0231, -1.1825,  0.4144, -0.3540],\n",
      "        [ 0.2838,  0.0197, -0.5171,  0.2193,  0.9597],\n",
      "        [-2.3572,  0.6455, -0.7928,  2.9647,  0.5879]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2717,  0.1387, -0.8430, -0.7642,  0.0453],\n",
      "        [-0.7859, -0.7863, -0.3107, -1.3175, -2.4653],\n",
      "        [-1.0287, -0.1458, -1.1091, -0.7714, -0.9503],\n",
      "        [ 2.0135,  1.6208,  1.4292,  1.0732,  2.6019]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4336, -0.8240, -0.1473, -0.2196, -0.3110],\n",
      "        [-1.6859,  1.0231, -1.1825,  0.4144, -0.3540],\n",
      "        [ 0.2838,  0.0197, -0.5171,  0.2193,  0.9597],\n",
      "        [-2.3572,  0.6455, -0.7928,  2.9647,  0.5879]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2814],\n",
      "        [ 1.2150],\n",
      "        [-0.8025],\n",
      "        [-0.1218]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2513, -0.3256,  1.6389, -0.1736,  0.6190],\n",
      "        [ 0.0889,  0.1950, -0.2324,  0.4324,  0.2674],\n",
      "        [-0.3596, -0.4360,  0.7815,  0.1344,  0.2019],\n",
      "        [-1.7254, -0.0588, -0.4285,  0.4242,  1.0999]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4211,  0.1618,  0.1020, -0.2004, -0.2655],\n",
      "        [-1.1355, -0.8075, -0.9245, -1.8452, -3.1693],\n",
      "        [-0.2709, -0.3224,  0.0881, -0.4219, -0.4697],\n",
      "        [ 0.1703,  0.2452,  0.2263, -0.0901,  0.0061]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2513, -0.3256,  1.6389, -0.1736,  0.6190],\n",
      "        [ 0.0889,  0.1950, -0.2324,  0.4324,  0.2674],\n",
      "        [-0.3596, -0.4360,  0.7815,  0.1344,  0.2019],\n",
      "        [-1.7254, -0.0588, -0.4285,  0.4242,  1.0999]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5119],\n",
      "        [-1.6891],\n",
      "        [ 0.1553],\n",
      "        [-0.4367]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4506,  1.4277, -0.3814, -1.8790,  1.9915],\n",
      "        [ 0.5991, -0.2807,  0.7023, -0.1567, -0.5128],\n",
      "        [-0.5539,  0.0898,  0.3210,  1.7912, -0.2283],\n",
      "        [-1.9532,  1.1619, -0.0657,  0.0615, -0.7220]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1837, -0.1616, -0.4081, -0.3517, -0.3786],\n",
      "        [-0.0224,  0.4498,  0.4448, -0.3377,  0.5569],\n",
      "        [-0.5045,  0.1730, -0.4386,  0.6574, -0.0462],\n",
      "        [ 0.4913, -0.0122,  0.4439,  0.1121, -0.2474]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4506,  1.4277, -0.3814, -1.8790,  1.9915],\n",
      "        [ 0.5991, -0.2807,  0.7023, -0.1567, -0.5128],\n",
      "        [-0.5539,  0.0898,  0.3210,  1.7912, -0.2283],\n",
      "        [-1.9532,  1.1619, -0.0657,  0.0615, -0.7220]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2509],\n",
      "        [-0.0599],\n",
      "        [ 1.3423],\n",
      "        [-0.8176]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1998,  1.5083,  0.8271, -0.5213,  2.4516],\n",
      "        [-0.0880, -1.0632,  0.5850,  1.5317,  0.6863],\n",
      "        [ 0.0432, -0.1263,  0.4245, -0.2142, -0.0273],\n",
      "        [-0.1153,  0.2824, -0.8947,  0.1417,  0.5078]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1721, -0.7231, -0.3106, -0.8069, -0.2585],\n",
      "        [ 0.6491, -0.4820, -0.1556,  0.0288,  0.2887],\n",
      "        [-0.0126, -0.0584, -0.0021, -0.1651,  0.4282],\n",
      "        [ 0.6839,  0.0633, -0.1055,  0.1603, -0.1816]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1998,  1.5083,  0.8271, -0.5213,  2.4516],\n",
      "        [-0.0880, -1.0632,  0.5850,  1.5317,  0.6863],\n",
      "        [ 0.0432, -0.1263,  0.4245, -0.2142, -0.0273],\n",
      "        [-0.1153,  0.2824, -0.8947,  0.1417,  0.5078]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5950],\n",
      "        [ 0.6066],\n",
      "        [ 0.0296],\n",
      "        [-0.0361]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2761, -0.2230, -0.6820, -0.5825,  0.4016],\n",
      "        [ 0.3779, -0.6709, -1.4379, -0.7187, -0.1093],\n",
      "        [-0.5123,  1.2982,  0.1510, -0.7648, -0.8367],\n",
      "        [-0.8879,  0.3122,  0.7789, -0.5422, -2.0537]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5957,  0.6165,  0.8216,  0.8202,  0.7145],\n",
      "        [ 0.3021,  0.0007,  0.6823,  0.1466,  0.2360],\n",
      "        [-0.2364, -0.1339,  0.0011, -0.5726, -0.0106],\n",
      "        [ 0.7374, -0.4820,  0.6873,  0.3284,  0.0388]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2761, -0.2230, -0.6820, -0.5825,  0.4016],\n",
      "        [ 0.3779, -0.6709, -1.4379, -0.7187, -0.1093],\n",
      "        [-0.5123,  1.2982,  0.1510, -0.7648, -0.8367],\n",
      "        [-0.8879,  0.3122,  0.7789, -0.5422, -2.0537]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0531],\n",
      "        [-0.9986],\n",
      "        [ 0.3943],\n",
      "        [-0.5277]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2914,  0.7179, -0.8929, -0.4725,  0.0729],\n",
      "        [-1.1843,  0.4941, -0.9632, -0.8173, -1.6325],\n",
      "        [ 1.3326, -0.3593,  0.2833, -0.6776, -0.6809],\n",
      "        [ 1.5026,  0.7065,  1.1595, -0.3646,  0.3849]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3306,  0.8914,  0.4220,  0.8832,  1.8684],\n",
      "        [ 0.3927,  0.0882,  1.0766,  0.2130,  1.0251],\n",
      "        [ 0.2490, -0.8200,  0.4992, -0.4886, -0.3493],\n",
      "        [ 0.5700, -0.5842, -0.6375, -0.4458, -0.3777]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2914,  0.7179, -0.8929, -0.4725,  0.0729],\n",
      "        [-1.1843,  0.4941, -0.9632, -0.8173, -1.6325],\n",
      "        [ 1.3326, -0.3593,  0.2833, -0.6776, -0.6809],\n",
      "        [ 1.5026,  0.7065,  1.1595, -0.3646,  0.3849]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3698],\n",
      "        [-3.3060],\n",
      "        [ 1.3368],\n",
      "        [-0.2783]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1315, -2.1390, -0.2980,  0.3548, -0.6273],\n",
      "        [-1.6969, -0.1436, -0.8865,  2.5629, -0.5973],\n",
      "        [-1.4158, -0.9755, -0.3483, -0.8261,  0.7590],\n",
      "        [-0.4370, -0.0918, -1.3249,  0.5451,  0.5009]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4508,  0.8300,  0.7226,  0.6476,  1.3209],\n",
      "        [ 0.9367,  1.5127,  2.3655,  1.4085,  2.1446],\n",
      "        [-0.4396, -0.3390, -0.2734, -0.1217, -1.2492],\n",
      "        [ 0.2660, -0.2150, -0.3405, -0.9649, -0.5302]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1315, -2.1390, -0.2980,  0.3548, -0.6273],\n",
      "        [-1.6969, -0.1436, -0.8865,  2.5629, -0.5973],\n",
      "        [-1.4158, -0.9755, -0.3483, -0.8261,  0.7590],\n",
      "        [-0.4370, -0.0918, -1.3249,  0.5451,  0.5009]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5303],\n",
      "        [-1.5751],\n",
      "        [ 0.2008],\n",
      "        [-0.4369]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1658,  1.6425, -0.5190, -0.3014, -1.2042],\n",
      "        [ 0.5512,  0.2844, -0.6468, -0.3111,  0.4459],\n",
      "        [-0.7520,  1.3477,  0.9027,  1.4004, -0.4241],\n",
      "        [ 1.1773,  0.1507, -0.3366,  0.7855,  2.9236]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3079,  2.1993,  1.8305,  2.0641,  1.6590],\n",
      "        [ 1.6997,  2.3383,  1.7940,  1.3573,  2.2137],\n",
      "        [ 0.1767,  0.0458,  0.0230, -0.3473, -0.4581],\n",
      "        [ 0.6115,  0.2235, -0.2560, -0.7850, -0.3118]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1658,  1.6425, -0.5190, -0.3014, -1.2042],\n",
      "        [ 0.5512,  0.2844, -0.6468, -0.3111,  0.4459],\n",
      "        [-0.7520,  1.3477,  0.9027,  1.4004, -0.4241],\n",
      "        [ 1.1773,  0.1507, -0.3366,  0.7855,  2.9236]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2592],\n",
      "        [ 1.0062],\n",
      "        [-0.3424],\n",
      "        [-0.6884]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2836, -0.1073, -1.0941, -2.3732,  0.5528],\n",
      "        [ 0.4258,  0.4954,  0.3717,  0.4019, -2.3798],\n",
      "        [ 1.7214, -0.3733, -1.5364,  1.1045, -0.3682],\n",
      "        [-0.5819,  0.1972, -0.0643,  0.4863,  1.1038]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2958,  0.9029, -0.0812, -0.1337,  0.7068],\n",
      "        [ 0.3005, -0.4218,  0.1590, -0.2899,  0.1694],\n",
      "        [-0.3389, -0.0687, -0.0941,  0.3444, -0.4173],\n",
      "        [ 0.7833, -0.3614, -0.1781, -0.0376, -0.5076]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2836, -0.1073, -1.0941, -2.3732,  0.5528],\n",
      "        [ 0.4258,  0.4954,  0.3717,  0.4019, -2.3798],\n",
      "        [ 1.7214, -0.3733, -1.5364,  1.1045, -0.3682],\n",
      "        [-0.5819,  0.1972, -0.0643,  0.4863,  1.1038]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6159],\n",
      "        [-0.5415],\n",
      "        [ 0.1209],\n",
      "        [-1.0942]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0641, -0.2572,  1.8865,  1.6838,  0.6618],\n",
      "        [-0.6754,  0.6428,  1.0447,  0.5442,  0.8617],\n",
      "        [ 0.5046, -0.5109, -0.8139, -0.2774,  2.1098],\n",
      "        [-1.8297,  0.3447,  2.0916, -0.1315, -0.9502]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2596, -0.0374,  0.6213, -0.0957,  0.2900],\n",
      "        [ 0.3163, -0.2677,  0.0546, -0.4826, -0.3124],\n",
      "        [-0.0302,  0.2831,  0.3333, -0.1269,  0.2517],\n",
      "        [ 0.6553,  0.0939,  0.1545,  0.1676,  1.0962]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0641, -0.2572,  1.8865,  1.6838,  0.6618],\n",
      "        [-0.6754,  0.6428,  1.0447,  0.5442,  0.8617],\n",
      "        [ 0.5046, -0.5109, -0.8139, -0.2774,  2.1098],\n",
      "        [-1.8297,  0.3447,  2.0916, -0.1315, -0.9502]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9361],\n",
      "        [-0.8605],\n",
      "        [ 0.1351],\n",
      "        [-1.9071]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5205,  1.3297,  1.0623, -0.1830, -0.6216],\n",
      "        [-1.8244, -1.3486,  0.7390,  0.5719, -1.0940],\n",
      "        [-1.1739,  0.1535,  1.2593,  0.3946,  0.7040],\n",
      "        [-0.2582,  0.0061,  0.3329,  1.3331, -0.2756]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2040,  0.5105, -0.2907, -0.1579, -1.0424],\n",
      "        [ 0.6927,  0.0787, -0.0887, -0.4093,  0.7360],\n",
      "        [-0.4849, -0.5025, -0.4463,  0.0404, -0.2366],\n",
      "        [ 1.7301,  0.9437,  1.6998,  0.4235,  1.7547]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5205,  1.3297,  1.0623, -0.1830, -0.6216],\n",
      "        [-1.8244, -1.3486,  0.7390,  0.5719, -1.0940],\n",
      "        [-1.1739,  0.1535,  1.2593,  0.3946,  0.7040],\n",
      "        [-0.2582,  0.0061,  0.3329,  1.3331, -0.2756]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7367],\n",
      "        [-2.4748],\n",
      "        [-0.2206],\n",
      "        [ 0.2059]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1279,  2.0609,  0.9852, -0.4028, -1.0118],\n",
      "        [-0.0286,  2.5997,  0.7188,  2.6129,  1.0939],\n",
      "        [ 0.0756,  0.8783,  0.4047, -0.7574,  0.4907],\n",
      "        [-0.6410,  1.0940,  1.5748, -1.4618,  0.4176]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2543, -0.2745, -0.2249,  0.3190, -0.4421],\n",
      "        [ 1.5160,  1.2907,  0.9811,  1.0088,  1.0717],\n",
      "        [ 0.1716,  0.1091,  0.9037, -0.6066, -0.0279],\n",
      "        [ 1.0273,  0.2001,  0.8460,  0.2593,  1.1195]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1279,  2.0609,  0.9852, -0.4028, -1.0118],\n",
      "        [-0.0286,  2.5997,  0.7188,  2.6129,  1.0939],\n",
      "        [ 0.0756,  0.8783,  0.4047, -0.7574,  0.4907],\n",
      "        [-0.6410,  1.0940,  1.5748, -1.4618,  0.4176]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5010],\n",
      "        [ 7.8256],\n",
      "        [ 0.9202],\n",
      "        [ 0.9813]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4968,  0.0151, -0.8012, -0.7443,  0.8033],\n",
      "        [-0.0892,  1.4619,  0.4705, -0.7807, -0.2150],\n",
      "        [-0.7397,  1.5787,  0.0086,  0.2485,  0.0907],\n",
      "        [-0.8215,  0.6681,  1.9413, -0.4155,  0.3969]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4515, -0.3967,  0.1603, -0.3274,  0.0736],\n",
      "        [-1.6133, -2.2074, -2.0661, -2.6321, -4.3266],\n",
      "        [-0.4263, -0.5206, -0.2524, -0.0845, -0.8057],\n",
      "        [ 0.8756, -0.4973, -0.9271,  0.3812,  0.3117]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4968,  0.0151, -0.8012, -0.7443,  0.8033],\n",
      "        [-0.0892,  1.4619,  0.4705, -0.7807, -0.2150],\n",
      "        [-0.7397,  1.5787,  0.0086,  0.2485,  0.0907],\n",
      "        [-0.8215,  0.6681,  1.9413, -0.4155,  0.3969]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8442],\n",
      "        [-1.0702],\n",
      "        [-0.6027],\n",
      "        [-2.8860]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2912, -0.3035, -1.5972, -0.4016,  0.2694],\n",
      "        [ 0.1743, -0.4588, -0.7294, -0.0653,  1.7925],\n",
      "        [-0.4955,  0.0635,  0.7273,  0.4665, -0.6103],\n",
      "        [-0.0386,  1.9465,  0.5986,  2.4093,  0.7047]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0062, -0.3332,  0.0265,  0.0634, -0.2296],\n",
      "        [-1.0753, -0.5647, -2.1251, -1.9063, -3.4334],\n",
      "        [ 0.0673,  0.0132, -0.2040, -0.4529, -0.1750],\n",
      "        [ 1.0635,  1.7895,  1.5781,  1.2633,  2.2173]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2912, -0.3035, -1.5972, -0.4016,  0.2694],\n",
      "        [ 0.1743, -0.4588, -0.7294, -0.0653,  1.7925],\n",
      "        [-0.4955,  0.0635,  0.7273,  0.4665, -0.6103],\n",
      "        [-0.0386,  1.9465,  0.5986,  2.4093,  0.7047]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0303],\n",
      "        [-4.4085],\n",
      "        [-0.2854],\n",
      "        [ 8.9931]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7159,  0.4556,  1.2123,  0.9174, -0.6425],\n",
      "        [-0.0965,  0.8561, -0.6389,  0.7447, -0.2788],\n",
      "        [ 0.4117,  0.8739, -0.3299, -0.7933,  0.3065],\n",
      "        [-1.9289, -0.9931, -0.1234, -0.2660, -0.2058]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5971,  0.2908,  0.2246,  0.4311,  0.0365],\n",
      "        [ 0.3402, -1.2043, -0.5515, -0.9637, -0.5992],\n",
      "        [-0.5339,  0.0544, -0.5902,  0.0505,  0.4443],\n",
      "        [-0.6586, -2.2155, -2.8304, -1.8961, -4.2289]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7159,  0.4556,  1.2123,  0.9174, -0.6425],\n",
      "        [-0.0965,  0.8561, -0.6389,  0.7447, -0.2788],\n",
      "        [ 0.4117,  0.8739, -0.3299, -0.7933,  0.3065],\n",
      "        [-1.9289, -0.9931, -0.1234, -0.2660, -0.2058]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3493],\n",
      "        [-1.2621],\n",
      "        [ 0.1185],\n",
      "        [ 5.1945]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights:     tensor([[-0.2852, -0.8715,  0.7112,  1.5643, -0.6355],\n",
      "        [-0.4627,  1.2707, -1.0973, -0.2862,  0.5656],\n",
      "        [-0.0589,  1.4791, -0.5786,  0.5644,  1.4476],\n",
      "        [-1.5140, -1.6539,  1.1367, -1.7443,  0.6792]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1983,  0.8997,  0.2125,  0.7717,  0.1454],\n",
      "        [ 0.2295, -0.1233, -1.0348, -0.7908,  0.1483],\n",
      "        [ 0.0596, -0.4980, -0.4531, -0.3136, -0.7412],\n",
      "        [-2.5511, -3.6070, -3.8094, -3.6132, -7.1684]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2852, -0.8715,  0.7112,  1.5643, -0.6355],\n",
      "        [-0.4627,  1.2707, -1.0973, -0.2862,  0.5656],\n",
      "        [-0.0589,  1.4791, -0.5786,  0.5644,  1.4476],\n",
      "        [-1.5140, -1.6539,  1.1367, -1.7443,  0.6792]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5384],\n",
      "        [ 1.1828],\n",
      "        [-1.7279],\n",
      "        [ 6.9314]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6569, -0.3330, -1.8353,  0.6389,  0.3559],\n",
      "        [-0.3659,  1.0047, -2.1635,  0.0961, -0.2815],\n",
      "        [-0.2401,  0.4050, -0.7211, -1.2054,  0.0290],\n",
      "        [-0.4386,  0.1196, -0.7673, -0.6880,  0.7396]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6082, -0.5590, -0.4181, -0.3835,  0.1410],\n",
      "        [-0.1025, -0.7258, -0.8392, -1.2721, -1.7997],\n",
      "        [ 0.4966,  1.4079,  0.2993,  0.6030,  1.2396],\n",
      "        [-0.0723,  0.2779, -0.1146, -0.6276, -0.5347]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6569, -0.3330, -1.8353,  0.6389,  0.3559],\n",
      "        [-0.3659,  1.0047, -2.1635,  0.0961, -0.2815],\n",
      "        [-0.2401,  0.4050, -0.7211, -1.2054,  0.0290],\n",
      "        [-0.4386,  0.1196, -0.7673, -0.6880,  0.7396]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1583],\n",
      "        [ 1.5082],\n",
      "        [-0.4558],\n",
      "        [ 0.1892]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6304, -0.0829, -0.7703, -0.3007, -0.0923],\n",
      "        [ 0.3567, -1.2631,  0.4706, -0.8465,  0.0726],\n",
      "        [ 0.9445,  1.1629, -0.3476, -0.2373,  1.1359],\n",
      "        [ 0.3959,  0.1504,  0.9414, -2.0794, -0.2847]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3193, -0.1470, -0.1248, -0.3936, -0.3926],\n",
      "        [-0.8634, -2.0038, -1.7718, -1.7103, -2.5885],\n",
      "        [ 0.1989,  0.9535,  0.5259,  0.4707,  1.4118],\n",
      "        [ 0.1425,  0.0412,  0.0940, -0.0626,  0.0487]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6304, -0.0829, -0.7703, -0.3007, -0.0923],\n",
      "        [ 0.3567, -1.2631,  0.4706, -0.8465,  0.0726],\n",
      "        [ 0.9445,  1.1629, -0.3476, -0.2373,  1.1359],\n",
      "        [ 0.3959,  0.1504,  0.9414, -2.0794, -0.2847]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0616],\n",
      "        [ 2.6492],\n",
      "        [ 2.6057],\n",
      "        [ 0.2673]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0804, -1.7413, -1.3884,  1.0011,  1.0856],\n",
      "        [ 2.0972,  1.9142,  1.9851,  0.5259,  0.4210],\n",
      "        [ 0.1262,  3.0315,  0.2801, -0.9748,  2.7255],\n",
      "        [-0.8429,  0.7627, -1.4104,  1.6667,  1.3935]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4320,  0.3523,  0.0827,  0.1015, -0.3440],\n",
      "        [ 0.4568, -0.3411, -0.1668,  0.5890, -0.0510],\n",
      "        [-0.5689, -1.1158, -0.0727, -1.1777, -1.3296],\n",
      "        [ 0.3254, -0.1273, -0.3098,  0.2412, -0.3278]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0804, -1.7413, -1.3884,  1.0011,  1.0856],\n",
      "        [ 2.0972,  1.9142,  1.9851,  0.5259,  0.4210],\n",
      "        [ 0.1262,  3.0315,  0.2801, -0.9748,  2.7255],\n",
      "        [-0.8429,  0.7627, -1.4104,  1.6667,  1.3935]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0350],\n",
      "        [ 0.2622],\n",
      "        [-5.9505],\n",
      "        [ 0.0108]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8630,  0.3980, -0.4081, -0.9888,  1.9566],\n",
      "        [-0.3072, -0.2703,  0.6583, -0.3809, -1.5318],\n",
      "        [-1.0293,  2.8359,  2.1767, -0.1586,  2.0018],\n",
      "        [ 0.2846,  0.2157,  0.2278, -1.8892,  1.5131]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4714,  0.6813,  0.2582,  0.6523,  0.7292],\n",
      "        [ 0.1692,  0.4815,  0.3121,  0.2414, -0.0325],\n",
      "        [ 1.2587,  2.1545,  1.5472,  1.2984,  2.2602],\n",
      "        [-0.1037,  0.3366, -0.0642,  0.0539,  0.3796]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8630,  0.3980, -0.4081, -0.9888,  1.9566],\n",
      "        [-0.3072, -0.2703,  0.6583, -0.3809, -1.5318],\n",
      "        [-1.0293,  2.8359,  2.1767, -0.1586,  2.0018],\n",
      "        [ 0.2846,  0.2157,  0.2278, -1.8892,  1.5131]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[  0.5407],\n",
      "        [ -0.0189],\n",
      "        [ 12.5006],\n",
      "        [  0.5010]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0840,  1.5599,  0.2120, -0.9066, -0.0697],\n",
      "        [-1.0105, -0.2101,  0.6634, -0.0973,  2.8511],\n",
      "        [-0.2798,  0.7689,  0.0063, -1.5227,  0.3231],\n",
      "        [ 0.2716,  0.3201, -0.6392,  0.3591,  1.2075]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4009,  0.3485,  0.7476, -0.3269,  0.2829],\n",
      "        [ 0.5199,  0.0557,  0.0150, -0.2866, -0.1053],\n",
      "        [-1.5682, -2.7439, -2.6756, -3.5272, -5.4345],\n",
      "        [-0.0658, -0.1216,  0.5926, -0.3361,  0.2289]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0840,  1.5599,  0.2120, -0.9066, -0.0697],\n",
      "        [-1.0105, -0.2101,  0.6634, -0.0973,  2.8511],\n",
      "        [-0.2798,  0.7689,  0.0063, -1.5227,  0.3231],\n",
      "        [ 0.2716,  0.3201, -0.6392,  0.3591,  1.2075]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9451],\n",
      "        [-0.7995],\n",
      "        [ 1.9272],\n",
      "        [-0.2800]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3779, -0.2049,  0.5218, -0.4916, -0.1081],\n",
      "        [ 0.1804, -1.1237, -0.2277,  0.7818,  0.4023],\n",
      "        [ 0.1517, -0.6349,  0.5735, -0.3634, -0.2671],\n",
      "        [-1.2510, -0.2123,  1.1197, -0.3221, -0.2546]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5895, -0.0739,  0.4847,  0.2616, -0.0674],\n",
      "        [ 0.2550,  0.8576,  0.1451, -0.4325,  0.8686],\n",
      "        [ 0.4860, -0.0731, -0.5357,  0.4246,  0.1766],\n",
      "        [ 0.1383,  0.2233, -0.7601, -0.0363, -0.1363]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3779, -0.2049,  0.5218, -0.4916, -0.1081],\n",
      "        [ 0.1804, -1.1237, -0.2277,  0.7818,  0.4023],\n",
      "        [ 0.1517, -0.6349,  0.5735, -0.3634, -0.2671],\n",
      "        [-1.2510, -0.2123,  1.1197, -0.3221, -0.2546]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0760],\n",
      "        [-0.9394],\n",
      "        [-0.3886],\n",
      "        [-1.0251]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1949, -1.2770,  0.1076, -1.3533, -0.0935],\n",
      "        [-0.4925,  1.0527, -0.5412,  0.9373,  1.2300],\n",
      "        [-1.5807,  0.5554, -0.3960, -0.2590,  0.9163],\n",
      "        [ 1.1161, -2.0616,  1.2531, -0.2684,  1.2467]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4644,  0.6367,  0.5379,  0.4717,  0.2759],\n",
      "        [ 0.1616,  0.8953,  0.6328,  0.9879,  0.9183],\n",
      "        [ 0.1151,  0.4262, -0.3983, -0.0207, -0.2611],\n",
      "        [ 0.5645,  0.5878, -0.2552,  0.6828,  0.3341]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1949, -1.2770,  0.1076, -1.3533, -0.0935],\n",
      "        [-0.4925,  1.0527, -0.5412,  0.9373,  1.2300],\n",
      "        [-1.5807,  0.5554, -0.3960, -0.2590,  0.9163],\n",
      "        [ 1.1161, -2.0616,  1.2531, -0.2684,  1.2467]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3289],\n",
      "        [ 2.5760],\n",
      "        [-0.0214],\n",
      "        [-0.6682]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1292, -1.9925, -0.7736, -0.6719, -0.1439],\n",
      "        [-0.2629,  2.2122,  0.8873,  0.6608,  1.0552],\n",
      "        [ 0.8660, -0.9472,  0.0190,  0.1206, -1.2792],\n",
      "        [ 0.2854, -0.3280, -0.2059,  0.5746, -1.1072]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4088,  1.2500,  1.4484,  0.7407,  1.0993],\n",
      "        [-0.7469, -1.5703, -0.9266, -1.0385, -1.9734],\n",
      "        [ 0.3949,  0.4741,  0.1757,  0.1572, -0.2938],\n",
      "        [ 0.9974,  0.0552,  0.5013,  0.1011,  0.3282]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1292, -1.9925, -0.7736, -0.6719, -0.1439],\n",
      "        [-0.2629,  2.2122,  0.8873,  0.6608,  1.0552],\n",
      "        [ 0.8660, -0.9472,  0.0190,  0.1206, -1.2792],\n",
      "        [ 0.2854, -0.3280, -0.2059,  0.5746, -1.1072]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-4.7285],\n",
      "        [-6.8684],\n",
      "        [ 0.2909],\n",
      "        [-0.1419]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7776,  1.9947,  1.3338, -1.5543,  0.0273],\n",
      "        [-0.7999,  0.0442,  0.7720, -1.1154, -1.0317],\n",
      "        [-0.6697,  1.3770, -0.4818, -1.7303, -0.6746],\n",
      "        [-1.3213, -0.2272,  3.2021, -0.4772, -1.7772]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.8217,  2.3686,  1.9071,  1.8365,  2.7878],\n",
      "        [ 1.7037,  2.2658,  2.3612,  1.8778,  2.9387],\n",
      "        [ 0.4344,  0.3311, -0.0035, -0.3093,  0.4726],\n",
      "        [ 0.2395, -0.2782,  0.1948,  0.0932, -0.1519]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7776,  1.9947,  1.3338, -1.5543,  0.0273],\n",
      "        [-0.7999,  0.0442,  0.7720, -1.1154, -1.0317],\n",
      "        [-0.6697,  1.3770, -0.4818, -1.7303, -0.6746],\n",
      "        [-1.3213, -0.2272,  3.2021, -0.4772, -1.7772]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2515],\n",
      "        [-4.5661],\n",
      "        [ 0.3831],\n",
      "        [ 0.5961]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0753, -0.3689, -1.6187,  1.0838,  1.8291],\n",
      "        [-2.9075,  0.5987, -1.0364,  0.9568,  0.2971],\n",
      "        [ 0.7024,  1.2386,  1.0243, -0.9205,  0.2056],\n",
      "        [ 0.9062, -0.0823, -0.3699, -0.7407, -1.2775]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1616,  2.3655,  2.0727,  2.0143,  2.6213],\n",
      "        [-0.4782,  0.4649, -0.0130, -0.2165, -0.0874],\n",
      "        [ 0.1305,  0.0398, -0.3146,  0.0890, -0.1338],\n",
      "        [ 0.1682, -0.0602, -0.3135, -0.3050, -0.0361]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0753, -0.3689, -1.6187,  1.0838,  1.8291],\n",
      "        [-2.9075,  0.5987, -1.0364,  0.9568,  0.2971],\n",
      "        [ 0.7024,  1.2386,  1.0243, -0.9205,  0.2056],\n",
      "        [ 0.9062, -0.0823, -0.3699, -0.7407, -1.2775]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.9990],\n",
      "        [ 1.4492],\n",
      "        [-0.2906],\n",
      "        [ 0.5454]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5463,  0.8923, -0.3536, -0.5957, -0.1972],\n",
      "        [ 0.4905, -2.9352,  0.1820, -0.1878,  0.1533],\n",
      "        [-0.3111,  1.5797, -0.0499, -0.7777,  1.0495],\n",
      "        [-0.4511,  0.0707,  1.2262,  0.5219, -0.5380]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1111, -0.2192,  0.1237, -0.1521,  0.6977],\n",
      "        [-0.3772, -0.0683,  0.0027, -0.6833, -1.1829],\n",
      "        [ 0.0646,  0.0839, -0.6582, -0.2663,  0.0138],\n",
      "        [ 0.1656, -0.0143, -1.0446,  0.3806, -0.3101]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5463,  0.8923, -0.3536, -0.5957, -0.1972],\n",
      "        [ 0.4905, -2.9352,  0.1820, -0.1878,  0.1533],\n",
      "        [-0.3111,  1.5797, -0.0499, -0.7777,  1.0495],\n",
      "        [-0.4511,  0.0707,  1.2262,  0.5219, -0.5380]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2256],\n",
      "        [-0.0370],\n",
      "        [ 0.3668],\n",
      "        [-0.9911]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1444,  0.2981, -2.1453,  0.8491, -1.0252],\n",
      "        [ 1.1425, -0.9197, -1.1605,  0.4575,  1.0818],\n",
      "        [-0.9060,  1.0964, -0.5482,  0.0385,  2.7246],\n",
      "        [-0.9042,  1.0759,  0.2714, -1.5357, -0.7860]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4372,  0.2745,  0.3078, -0.3093,  0.0488],\n",
      "        [-0.4488, -0.3373,  0.6053, -0.1819,  0.0182],\n",
      "        [-0.4120, -0.4681, -0.1995,  0.9313,  0.0194],\n",
      "        [ 0.2434,  0.2715, -0.0193,  0.0892,  0.7670]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1444,  0.2981, -2.1453,  0.8491, -1.0252],\n",
      "        [ 1.1425, -0.9197, -1.1605,  0.4575,  1.0818],\n",
      "        [-0.9060,  1.0964, -0.5482,  0.0385,  2.7246],\n",
      "        [-0.9042,  1.0759,  0.2714, -1.5357, -0.7860]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8280],\n",
      "        [-0.9686],\n",
      "        [ 0.0581],\n",
      "        [-0.6731]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6480, -0.3460,  1.6259,  1.0979, -1.2202],\n",
      "        [ 1.5994, -0.6801, -1.0292,  0.3430, -0.0013],\n",
      "        [ 0.7830,  0.5238, -0.0841, -2.2604,  1.6555],\n",
      "        [-2.2540,  0.5733, -0.1107, -0.4661, -0.9059]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1502,  0.4566,  0.0834, -0.1385,  0.9423],\n",
      "        [ 0.3996, -0.1019,  0.8138, -0.1053,  0.1040],\n",
      "        [-0.4045, -0.5789,  0.2774, -0.2987,  0.2136],\n",
      "        [ 0.5678,  0.4431, -0.1344, -0.0294,  0.7876]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6480, -0.3460,  1.6259,  1.0979, -1.2202],\n",
      "        [ 1.5994, -0.6801, -1.0292,  0.3430, -0.0013],\n",
      "        [ 0.7830,  0.5238, -0.0841, -2.2604,  1.6555],\n",
      "        [-2.2540,  0.5733, -0.1107, -0.4661, -0.9059]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5717],\n",
      "        [-0.1655],\n",
      "        [ 0.3856],\n",
      "        [-1.7106]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2145,  0.2089, -0.3142,  0.5638,  0.1259],\n",
      "        [ 0.7323, -0.4872,  0.1822, -0.8182, -0.0482],\n",
      "        [-0.9755,  1.2072,  2.1016,  0.5380,  2.2390],\n",
      "        [-1.2466, -0.1903,  0.0208,  0.0903,  1.0592]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7037,  1.3417,  1.2342,  1.0657,  1.1948],\n",
      "        [-0.1512,  0.1254,  0.0339,  0.5830, -0.1591],\n",
      "        [ 0.2236,  0.0246, -0.5911, -0.7240,  0.0854],\n",
      "        [ 0.9543,  0.7525,  1.8368,  1.0492,  1.1344]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2145,  0.2089, -0.3142,  0.5638,  0.1259],\n",
      "        [ 0.7323, -0.4872,  0.1822, -0.8182, -0.0482],\n",
      "        [-0.9755,  1.2072,  2.1016,  0.5380,  2.2390],\n",
      "        [-1.2466, -0.1903,  0.0208,  0.0903,  1.0592]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4928],\n",
      "        [-0.6350],\n",
      "        [-1.6290],\n",
      "        [ 0.0016]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1490, -1.0204,  0.5585,  0.4422,  1.1581],\n",
      "        [ 0.0280,  0.0732,  0.2438,  1.3164,  0.9538],\n",
      "        [ 0.8111, -0.5202,  0.1687,  0.4570,  0.5615],\n",
      "        [ 0.3608, -0.1454,  0.2235,  0.5194,  0.8310]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3827, -0.3795, -0.1940, -0.5114,  0.3175],\n",
      "        [ 0.4280, -0.0933, -0.5429,  0.2594, -0.4514],\n",
      "        [ 1.0107,  0.8780,  1.0106,  1.0671,  1.3484],\n",
      "        [ 0.1645,  1.1305,  0.2950,  1.2844,  1.7242]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1490, -1.0204,  0.5585,  0.4422,  1.1581],\n",
      "        [ 0.0280,  0.0732,  0.2438,  1.3164,  0.9538],\n",
      "        [ 0.8111, -0.5202,  0.1687,  0.4570,  0.5615],\n",
      "        [ 0.3608, -0.1454,  0.2235,  0.5194,  0.8310]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3634],\n",
      "        [-0.2163],\n",
      "        [ 1.7782],\n",
      "        [ 2.0610]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1312, -1.1183,  0.2708,  0.4444, -0.6869],\n",
      "        [-2.0858,  3.7799,  1.7594, -0.1908,  0.4344],\n",
      "        [-0.9526,  0.7408,  1.1590,  0.6003, -0.4477],\n",
      "        [ 0.3057,  0.6239,  1.7602, -0.2592,  0.0123]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3096, -0.5872, -0.0086, -0.1304,  1.0521],\n",
      "        [ 0.1121,  0.0620, -0.1548, -0.6107, -0.2324],\n",
      "        [-0.0230,  0.6828,  0.4109,  0.2350, -0.2293],\n",
      "        [ 0.4123, -0.2448, -0.4657,  0.4261, -0.2223]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1312, -1.1183,  0.2708,  0.4444, -0.6869],\n",
      "        [-2.0858,  3.7799,  1.7594, -0.1908,  0.4344],\n",
      "        [-0.9526,  0.7408,  1.1590,  0.6003, -0.4477],\n",
      "        [ 0.3057,  0.6239,  1.7602, -0.2592,  0.0123]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2240],\n",
      "        [-0.2562],\n",
      "        [ 1.2477],\n",
      "        [-0.9595]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0754,  1.8776,  0.0420, -0.6854, -1.5567],\n",
      "        [-0.4832, -0.6624,  0.1101, -1.2799, -0.9245],\n",
      "        [-0.0873,  0.5086, -1.5063,  1.2597, -0.1310],\n",
      "        [ 0.4217,  0.1421, -1.9827,  0.7157,  0.7964]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2092,  0.1163,  0.1819,  0.1114,  0.4985],\n",
      "        [ 0.7010, -0.6298, -0.0684, -0.8745, -0.8235],\n",
      "        [-0.3835, -0.4253, -0.1171, -0.2627, -1.3159],\n",
      "        [ 0.4620,  0.2524,  0.6665,  0.4712,  0.3549]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0754,  1.8776,  0.0420, -0.6854, -1.5567],\n",
      "        [-0.4832, -0.6624,  0.1101, -1.2799, -0.9245],\n",
      "        [-0.0873,  0.5086, -1.5063,  1.2597, -0.1310],\n",
      "        [ 0.4217,  0.1421, -1.9827,  0.7157,  0.7964]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8515],\n",
      "        [ 1.9515],\n",
      "        [-0.1650],\n",
      "        [-0.4709]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6230, -0.7354,  0.8738, -1.6386, -1.7444],\n",
      "        [ 0.4418,  1.3211, -0.3447,  0.3781, -0.0732],\n",
      "        [ 0.1154,  0.9315, -0.0454, -0.8850, -0.0746],\n",
      "        [-0.9980,  0.5458,  0.1116, -0.2048, -0.1517]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9051,  0.7790,  0.8574,  0.3042,  0.6889],\n",
      "        [-0.7566, -1.0602, -1.0932, -0.4596, -2.1942],\n",
      "        [ 0.0027,  0.6414, -0.0786,  0.4170,  0.1666],\n",
      "        [ 0.3323,  0.4651,  0.1499,  0.1662,  0.5402]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6230, -0.7354,  0.8738, -1.6386, -1.7444],\n",
      "        [ 0.4418,  1.3211, -0.3447,  0.3781, -0.0732],\n",
      "        [ 0.1154,  0.9315, -0.0454, -0.8850, -0.0746],\n",
      "        [-0.9980,  0.5458,  0.1116, -0.2048, -0.1517]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9599],\n",
      "        [-1.3712],\n",
      "        [ 0.2199],\n",
      "        [-0.1770]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0980, -1.1419,  0.7391,  0.1315, -1.2286],\n",
      "        [ 0.0596,  0.4119,  0.6574,  0.7216,  1.1957],\n",
      "        [ 0.5306, -1.4416, -0.1503, -0.5757,  0.1781],\n",
      "        [-1.2951, -1.0100,  0.6490, -1.6851, -0.2415]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8994,  1.4257,  0.8234,  0.7678,  1.3092],\n",
      "        [ 0.1750, -0.3762, -1.2926, -0.8926, -0.6329],\n",
      "        [-0.2489,  0.2548,  0.6909,  0.5206,  0.2428],\n",
      "        [ 0.8269,  0.1695,  0.5404,  0.2765, -0.0043]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0980, -1.1419,  0.7391,  0.1315, -1.2286],\n",
      "        [ 0.0596,  0.4119,  0.6574,  0.7216,  1.1957],\n",
      "        [ 0.5306, -1.4416, -0.1503, -0.5757,  0.1781],\n",
      "        [-1.2951, -1.0100,  0.6490, -1.6851, -0.2415]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4386],\n",
      "        [-2.3951],\n",
      "        [-0.8597],\n",
      "        [-1.3562]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0823,  0.4287,  0.0462,  0.9269,  1.5375],\n",
      "        [-0.0447,  0.9970,  1.7832,  0.8623, -0.6561],\n",
      "        [ 0.9866,  0.3315, -0.9231, -0.3186,  0.9943],\n",
      "        [-1.1721,  0.0205,  0.5184, -0.1788,  1.4916]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5429,  1.8157,  2.1562,  1.2084,  2.3729],\n",
      "        [ 0.6333,  0.6271,  1.2093,  0.5161,  1.1515],\n",
      "        [ 0.1719,  0.9051,  0.3688,  0.9617,  1.0709],\n",
      "        [ 1.2899,  0.8192,  0.8404,  0.3516,  1.4919]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0823,  0.4287,  0.0462,  0.9269,  1.5375],\n",
      "        [-0.0447,  0.9970,  1.7832,  0.8623, -0.6561],\n",
      "        [ 0.9866,  0.3315, -0.9231, -0.3186,  0.9943],\n",
      "        [-1.1721,  0.0205,  0.5184, -0.1788,  1.4916]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.7733],\n",
      "        [ 2.4428],\n",
      "        [ 0.8875],\n",
      "        [ 1.1030]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.4625, -0.6194, -0.3145,  0.4815, -0.7435],\n",
      "        [-0.9202,  0.1723,  1.1031, -0.4386,  1.6536],\n",
      "        [ 0.5130, -0.3837, -0.5086,  0.5566,  0.3116],\n",
      "        [ 0.4044,  2.3579,  1.2232, -1.4902,  0.6779]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6182, -0.4757, -0.4882, -1.3241, -1.7765],\n",
      "        [-0.3451, -0.6832, -1.0894, -0.6884, -1.4444],\n",
      "        [-0.2216,  0.7981,  0.2546,  0.0398,  0.2193],\n",
      "        [-0.0036,  1.0398,  0.2814, -0.0203,  0.6216]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.4625, -0.6194, -0.3145,  0.4815, -0.7435],\n",
      "        [-0.9202,  0.1723,  1.1031, -0.4386,  1.6536],\n",
      "        [ 0.5130, -0.3837, -0.5086,  0.5566,  0.3116],\n",
      "        [ 0.4044,  2.3579,  1.2232, -1.4902,  0.6779]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.6538],\n",
      "        [-3.0883],\n",
      "        [-0.4589],\n",
      "        [ 3.2461]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6145, -1.2074,  1.3597,  0.4993,  0.6441],\n",
      "        [-1.5919,  1.0452,  1.4853, -0.1451,  1.2302],\n",
      "        [-0.3170, -1.1863, -0.8581,  0.0200,  0.4006],\n",
      "        [-0.6262,  1.1586,  0.1998,  1.0116, -0.2155]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9336, -1.3569, -1.5750, -2.0623, -3.8431],\n",
      "        [ 1.0699,  0.6818,  0.3789,  0.4694,  1.3543],\n",
      "        [ 0.2702,  0.0640,  0.6657, -0.0598,  0.7545],\n",
      "        [-0.4985, -0.5003, -1.6553, -1.5586, -1.6248]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6145, -1.2074,  1.3597,  0.4993,  0.6441],\n",
      "        [-1.5919,  1.0452,  1.4853, -0.1451,  1.2302],\n",
      "        [-0.3170, -1.1863, -0.8581,  0.0200,  0.4006],\n",
      "        [-0.6262,  1.1586,  0.1998,  1.0116, -0.2155]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.4348],\n",
      "        [ 1.1702],\n",
      "        [-0.4317],\n",
      "        [-1.8246]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9505,  1.0345, -0.3831, -2.5678,  0.8692],\n",
      "        [-0.2578, -0.6376,  2.9860, -0.4043,  1.9712],\n",
      "        [-0.4242, -2.1562,  0.7459, -0.3682, -1.5818],\n",
      "        [ 0.5693, -1.7370,  1.0871,  1.3310,  0.2334]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2094, -0.6776, -0.6836, -0.6890, -0.7935],\n",
      "        [ 0.4579,  0.0659, -0.0796, -0.2877,  0.4162],\n",
      "        [ 0.7000,  0.3379,  0.0284,  0.3546,  0.3239],\n",
      "        [-0.2375,  0.6620, -0.7283,  0.1401,  0.3765]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9505,  1.0345, -0.3831, -2.5678,  0.8692],\n",
      "        [-0.2578, -0.6376,  2.9860, -0.4043,  1.9712],\n",
      "        [-0.4242, -2.1562,  0.7459, -0.3682, -1.5818],\n",
      "        [ 0.5693, -1.7370,  1.0871,  1.3310,  0.2334]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8395],\n",
      "        [ 0.5391],\n",
      "        [-1.6472],\n",
      "        [-1.8024]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2867,  0.4843,  0.5132,  1.2930, -1.3583],\n",
      "        [ 0.5749, -0.0999,  0.7553, -1.1222, -0.6566],\n",
      "        [-1.0581,  0.1741,  1.5613,  0.8174, -0.3114],\n",
      "        [ 0.2782,  0.6022, -0.8565, -0.3139,  1.1342]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6071, -0.4849,  0.2085, -0.3558, -1.4237],\n",
      "        [ 0.3319, -0.3041,  0.1564, -0.3684, -0.2076],\n",
      "        [ 0.4431,  1.6155,  0.2470,  0.3698,  1.0991],\n",
      "        [ 0.8843,  0.6337,  1.0572,  1.2316,  0.8887]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2867,  0.4843,  0.5132,  1.2930, -1.3583],\n",
      "        [ 0.5749, -0.0999,  0.7553, -1.1222, -0.6566],\n",
      "        [-1.0581,  0.1741,  1.5613,  0.8174, -0.3114],\n",
      "        [ 0.2782,  0.6022, -0.8565, -0.3139,  1.1342]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5199],\n",
      "        [ 0.8890],\n",
      "        [ 0.1580],\n",
      "        [ 0.3435]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6220, -1.3878,  0.6376, -0.2598,  0.5680],\n",
      "        [-0.3640,  0.1447,  2.9626, -1.5037, -2.5150],\n",
      "        [-1.4375,  0.8809, -0.4943,  1.5944, -0.9053],\n",
      "        [ 0.1805, -1.3452,  0.4025,  1.3298, -0.6753]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1673, -1.6596, -0.9742, -0.9553, -2.1807],\n",
      "        [-0.0728, -0.4916, -0.9546, -0.1977, -0.6330],\n",
      "        [ 0.8151,  0.8793,  0.7160,  0.5627,  1.1360],\n",
      "        [ 0.2930,  0.3995,  1.2174,  0.4915,  0.8528]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6220, -1.3878,  0.6376, -0.2598,  0.5680],\n",
      "        [-0.3640,  0.1447,  2.9626, -1.5037, -2.5150],\n",
      "        [-1.4375,  0.8809, -0.4943,  1.5944, -0.9053],\n",
      "        [ 0.1805, -1.3452,  0.4025,  1.3298, -0.6753]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5875],\n",
      "        [-0.9834],\n",
      "        [-0.8821],\n",
      "        [ 0.0832]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6503, -0.0658,  2.2361,  0.1640, -0.7335],\n",
      "        [ 0.8014, -1.0530, -0.6492,  0.7382,  0.9094],\n",
      "        [ 0.2177,  1.4143, -0.3086,  2.2140, -0.8349],\n",
      "        [ 0.1788,  0.6516,  1.3647,  0.7745, -0.5018]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2442, -0.9708, -1.3462, -1.4533, -1.5717],\n",
      "        [ 0.2241,  0.4041,  0.5164, -0.2778,  0.2555],\n",
      "        [ 1.0538,  0.8655,  0.8958,  0.6232,  1.8945],\n",
      "        [ 0.2596,  0.3499,  0.1263,  0.6011,  0.5132]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6503, -0.0658,  2.2361,  0.1640, -0.7335],\n",
      "        [ 0.8014, -1.0530, -0.6492,  0.7382,  0.9094],\n",
      "        [ 0.2177,  1.4143, -0.3086,  2.2140, -0.8349],\n",
      "        [ 0.1788,  0.6516,  1.3647,  0.7745, -0.5018]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8731],\n",
      "        [-0.5538],\n",
      "        [ 0.9750],\n",
      "        [ 0.6547]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.8249,  0.8457,  0.8877, -1.0451,  0.6674],\n",
      "        [-0.7171,  1.8750, -1.1156, -0.1923,  0.2655],\n",
      "        [-0.1164,  1.2287, -0.3046,  0.7762, -1.4721],\n",
      "        [-0.5962,  1.5804,  0.6204, -1.2215, -1.3116]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1919, -0.9409, -1.0417, -0.6886, -0.8809],\n",
      "        [ 0.4941,  0.1301, -0.4939, -0.4970, -0.5891],\n",
      "        [ 0.5095,  1.1311,  0.8549,  0.0157,  0.7928],\n",
      "        [ 0.1272,  0.6323, -0.5729,  0.5205,  0.1252]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.8249,  0.8457,  0.8877, -1.0451,  0.6674],\n",
      "        [-0.7171,  1.8750, -1.1156, -0.1923,  0.2655],\n",
      "        [-0.1164,  1.2287, -0.3046,  0.7762, -1.4721],\n",
      "        [-0.5962,  1.5804,  0.6204, -1.2215, -1.3116]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9389],\n",
      "        [ 0.3797],\n",
      "        [-0.0847],\n",
      "        [-0.2320]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9028,  0.8474, -1.8197, -0.9426,  1.2232],\n",
      "        [ 0.8837, -2.0570,  0.5214, -0.4610, -0.1570],\n",
      "        [-0.9281,  0.2992,  0.6359, -1.0233, -0.1830],\n",
      "        [-0.3623,  1.3906,  0.6155,  2.0168,  1.7363]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2454,  0.2921,  0.8878,  0.1325,  0.9853],\n",
      "        [ 0.1848, -0.4513,  0.1465,  0.0027, -0.1860],\n",
      "        [ 0.7507,  0.2753,  0.9919,  0.3839,  0.8879],\n",
      "        [ 0.0390, -0.2832,  0.9008,  0.0330,  0.2434]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9028,  0.8474, -1.8197, -0.9426,  1.2232],\n",
      "        [ 0.8837, -2.0570,  0.5214, -0.4610, -0.1570],\n",
      "        [-0.9281,  0.2992,  0.6359, -1.0233, -0.1830],\n",
      "        [-0.3623,  1.3906,  0.6155,  2.0168,  1.7363]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5093],\n",
      "        [ 1.1960],\n",
      "        [-0.5389],\n",
      "        [ 0.6356]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4296,  0.6339, -1.2895, -2.2241,  0.2914],\n",
      "        [ 0.9833,  1.0147,  1.0867, -0.1418, -0.3046],\n",
      "        [-0.4839,  0.5063,  0.9648,  0.8926,  0.3969],\n",
      "        [ 1.2057, -0.1420,  1.7864,  1.5899,  1.8364]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5143,  0.5756,  0.8040,  0.0977,  0.6383],\n",
      "        [-0.7493, -0.4422, -1.0678, -1.0204, -0.7778],\n",
      "        [ 0.7553,  0.3273,  0.4068,  0.5143,  0.8163],\n",
      "        [ 0.1786,  0.8091,  0.7404,  0.6031,  0.3232]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4296,  0.6339, -1.2895, -2.2241,  0.2914],\n",
      "        [ 0.9833,  1.0147,  1.0867, -0.1418, -0.3046],\n",
      "        [-0.4839,  0.5063,  0.9648,  0.8926,  0.3969],\n",
      "        [ 1.2057, -0.1420,  1.7864,  1.5899,  1.8364]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0321],\n",
      "        [-1.9644],\n",
      "        [ 0.9757],\n",
      "        [ 2.9755]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1745, -2.2701, -2.0369, -1.0539,  1.4590],\n",
      "        [-0.8844,  0.4651, -0.5064, -0.2650, -0.8525],\n",
      "        [ 0.9704,  1.6186,  0.3846, -0.0764, -1.1196],\n",
      "        [-0.3037,  1.0686,  0.8921,  0.3257,  0.2765]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6044, -0.6041, -0.3847, -0.6719, -1.0204],\n",
      "        [ 0.9678,  0.8719,  0.8691,  0.4691,  1.4857],\n",
      "        [ 0.2003,  0.3926,  0.5071,  0.0886,  0.6631],\n",
      "        [-0.6201, -0.7541, -0.9792, -1.2286, -2.2908]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1745, -2.2701, -2.0369, -1.0539,  1.4590],\n",
      "        [-0.8844,  0.4651, -0.5064, -0.2650, -0.8525],\n",
      "        [ 0.9704,  1.6186,  0.3846, -0.0764, -1.1196],\n",
      "        [-0.3037,  1.0686,  0.8921,  0.3257,  0.2765]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6645],\n",
      "        [-2.2815],\n",
      "        [ 0.2757],\n",
      "        [-2.5246]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3400, -0.1170,  0.2847,  0.1843,  0.4139],\n",
      "        [-0.2749, -0.1325,  0.1455,  0.7074, -0.2989],\n",
      "        [ 0.4844,  1.2409,  0.3829, -1.2849,  1.0320],\n",
      "        [ 0.1437, -0.6587,  0.1825, -0.2318,  1.3164]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3901, -0.4776, -0.9908, -1.1528, -0.9344],\n",
      "        [ 1.3555,  1.6998,  0.4805,  0.9444,  1.7334],\n",
      "        [ 0.4500,  0.1720,  0.3342,  0.1573,  0.7418],\n",
      "        [ 0.1085,  0.6851,  0.2699,  0.7027,  0.4336]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3400, -0.1170,  0.2847,  0.1843,  0.4139],\n",
      "        [-0.2749, -0.1325,  0.1455,  0.7074, -0.2989],\n",
      "        [ 0.4844,  1.2409,  0.3829, -1.2849,  1.0320],\n",
      "        [ 0.1437, -0.6587,  0.1825, -0.2318,  1.3164]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6928],\n",
      "        [-0.3780],\n",
      "        [ 1.1230],\n",
      "        [ 0.0215]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9995,  1.6202, -0.2320, -0.9410, -0.7360],\n",
      "        [-0.0049,  0.1235,  1.5391,  1.4245,  2.1184],\n",
      "        [-0.3290,  1.0201, -0.1496, -0.6044,  1.0040],\n",
      "        [ 1.7297, -0.7117,  0.1230, -1.0007,  0.7195]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6453, -0.7570, -0.2535, -1.0285, -1.2401],\n",
      "        [ 1.5594,  1.9706,  1.0209,  0.8775,  1.3697],\n",
      "        [ 0.4871,  0.4980,  0.6724,  0.7714, -0.6089],\n",
      "        [ 0.0269,  0.8641,  0.9425,  0.2315, -0.0670]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9995,  1.6202, -0.2320, -0.9410, -0.7360],\n",
      "        [-0.0049,  0.1235,  1.5391,  1.4245,  2.1184],\n",
      "        [-0.3290,  1.0201, -0.1496, -0.6044,  1.0040],\n",
      "        [ 1.7297, -0.7117,  0.1230, -1.0007,  0.7195]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3579],\n",
      "        [ 5.9586],\n",
      "        [-0.8304],\n",
      "        [-0.7325]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1029, -1.1859,  0.8351, -0.1545,  0.4859],\n",
      "        [-0.6538,  0.3991, -0.6990, -1.1525,  1.4910],\n",
      "        [ 0.0750, -0.1301, -0.4628,  1.4629,  1.0175],\n",
      "        [-0.1140, -0.0697, -0.7206,  0.0824,  0.9948]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8632, -1.1412, -1.3323, -0.9514, -1.3067],\n",
      "        [-0.7009, -1.3844, -1.3197, -1.5263, -3.2911],\n",
      "        [-0.0007,  0.6933,  0.5444,  0.3397,  1.5736],\n",
      "        [ 0.6282,  0.4369,  0.3446,  0.4483,  0.3139]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1029, -1.1859,  0.8351, -0.1545,  0.4859],\n",
      "        [-0.6538,  0.3991, -0.6990, -1.1525,  1.4910],\n",
      "        [ 0.0750, -0.1301, -0.4628,  1.4629,  1.0175],\n",
      "        [-0.1140, -0.0697, -0.7206,  0.0824,  0.9948]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3360],\n",
      "        [-2.3199],\n",
      "        [ 1.7558],\n",
      "        [-0.0012]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5687, -0.4166, -1.2939,  1.3227, -0.8573],\n",
      "        [-1.7770,  0.0993, -2.1788, -1.0087,  0.6800],\n",
      "        [-0.2906, -0.9981, -0.7223,  1.1207, -0.2639],\n",
      "        [-0.2767,  0.0604, -0.8261,  0.8897, -0.0118]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1734, -0.8105, -0.3368, -0.9987, -1.3210],\n",
      "        [-0.0204, -0.5746,  0.5329, -0.1097, -0.1012],\n",
      "        [-0.8703,  0.2389,  0.4136,  0.5232, -0.6340],\n",
      "        [ 1.0951,  0.1586,  1.0854,  0.0153,  0.2508]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5687, -0.4166, -1.2939,  1.3227, -0.8573],\n",
      "        [-1.7770,  0.0993, -2.1788, -1.0087,  0.6800],\n",
      "        [-0.2906, -0.9981, -0.7223,  1.1207, -0.2639],\n",
      "        [-0.2767,  0.0604, -0.8261,  0.8897, -0.0118]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3131],\n",
      "        [-1.1401],\n",
      "        [ 0.4694],\n",
      "        [-1.1795]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4419,  1.4551,  0.7019, -0.3447,  0.8714],\n",
      "        [-0.6300,  1.9115,  1.4878, -0.3306, -0.2883],\n",
      "        [ 1.0313, -0.6243, -0.3341, -1.4585, -1.4128],\n",
      "        [-0.2408, -0.7225,  0.1897,  0.2186, -0.4211]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3823, -1.0274, -0.1134, -0.7784, -1.4510],\n",
      "        [ 0.9993,  0.7042,  0.8100,  0.2663,  1.2299],\n",
      "        [-0.4410,  0.2867,  0.6631,  0.3427,  0.1201],\n",
      "        [ 0.5308,  0.9341,  0.8911,  0.2225,  0.6893]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4419,  1.4551,  0.7019, -0.3447,  0.8714],\n",
      "        [-0.6300,  1.9115,  1.4878, -0.3306, -0.2883],\n",
      "        [ 1.0313, -0.6243, -0.3341, -1.4585, -1.4128],\n",
      "        [-0.2408, -0.7225,  0.1897,  0.2186, -0.4211]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.1219],\n",
      "        [ 1.4789],\n",
      "        [-1.5248],\n",
      "        [-0.8753]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3215,  0.0272, -0.2844,  1.1768,  1.4179],\n",
      "        [-0.4847, -0.0964,  0.8695,  0.1056, -0.4306],\n",
      "        [-0.1768,  1.1933,  2.1853,  0.9073,  0.7925],\n",
      "        [-1.0380,  0.4315,  0.9209,  1.3518, -0.3497]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3509,  0.9789,  1.0239,  0.5769,  1.1226],\n",
      "        [ 0.0164,  0.5696,  0.0285, -0.2990, -0.4557],\n",
      "        [ 0.4774,  1.1949,  1.0588,  0.6080,  1.2361],\n",
      "        [ 0.8959,  0.2288,  0.6753,  0.8356,  1.5055]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3215,  0.0272, -0.2844,  1.1768,  1.4179],\n",
      "        [-0.4847, -0.0964,  0.8695,  0.1056, -0.4306],\n",
      "        [-0.1768,  1.1933,  2.1853,  0.9073,  0.7925],\n",
      "        [-1.0380,  0.4315,  0.9209,  1.3518, -0.3497]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2207],\n",
      "        [ 0.1266],\n",
      "        [ 5.1865],\n",
      "        [ 0.3938]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2215,  0.3522,  1.8636,  3.7534,  1.0561],\n",
      "        [-0.1004,  0.8718, -0.0213,  0.2587,  1.3002],\n",
      "        [ 0.7151, -0.2359,  0.3681, -1.2556,  1.0108],\n",
      "        [ 0.6381,  0.3662, -1.5425, -0.1229, -0.7128]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2025,  0.5664,  0.5111,  0.0985,  1.1263],\n",
      "        [-0.1855,  0.5100,  0.6304,  0.2975,  0.9688],\n",
      "        [-0.8955, -1.6131, -1.5655, -1.8807, -2.5122],\n",
      "        [ 0.9901,  0.1953,  0.7229,  0.1038,  0.8150]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2215,  0.3522,  1.8636,  3.7534,  1.0561],\n",
      "        [-0.1004,  0.8718, -0.0213,  0.2587,  1.3002],\n",
      "        [ 0.7151, -0.2359,  0.3681, -1.2556,  1.0108],\n",
      "        [ 0.6381,  0.3662, -1.5425, -0.1229, -0.7128]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2424],\n",
      "        [ 1.7863],\n",
      "        [-1.0139],\n",
      "        [-1.0055]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6712,  0.0497,  1.1035,  0.5943, -0.3466],\n",
      "        [-1.8928, -1.8027,  0.0477,  0.7941, -0.3866],\n",
      "        [ 0.5166, -0.5166,  0.6011,  0.0122, -0.7004],\n",
      "        [ 0.4330, -0.2483, -1.5102, -1.1439, -1.1993]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2872, -0.1523, -0.2579, -0.2983, -0.5661],\n",
      "        [-0.3740, -0.7461, -0.3284, -0.1380, -0.8381],\n",
      "        [-1.0419, -1.2385, -0.6820, -1.4725, -2.2039],\n",
      "        [ 1.0607, -0.1197,  0.5987,  1.0611,  0.8509]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6712,  0.0497,  1.1035,  0.5943, -0.3466],\n",
      "        [-1.8928, -1.8027,  0.0477,  0.7941, -0.3866],\n",
      "        [ 0.5166, -0.5166,  0.6011,  0.0122, -0.7004],\n",
      "        [ 0.4330, -0.2483, -1.5102, -1.1439, -1.1993]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0805],\n",
      "        [ 2.2518],\n",
      "        [ 1.2172],\n",
      "        [-2.6495]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3023, -0.1016, -0.4113,  1.9855, -0.1378],\n",
      "        [-0.7863, -0.1706, -0.4202,  0.6944,  0.6019],\n",
      "        [-0.0916,  0.6863,  0.7445,  0.4948,  1.2988],\n",
      "        [ 0.6225,  1.9138, -1.2171,  1.4296, -0.8321]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1380, -0.7594,  0.4349, -0.4608, -0.3865],\n",
      "        [-0.7232, -0.4487, -0.7038, -1.5838, -2.3284],\n",
      "        [-1.3327, -1.0048, -1.5254, -1.1801, -2.3434],\n",
      "        [ 1.4807,  0.9645,  1.7993,  1.6382,  2.4790]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3023, -0.1016, -0.4113,  1.9855, -0.1378],\n",
      "        [-0.7863, -0.1706, -0.4202,  0.6944,  0.6019],\n",
      "        [-0.0916,  0.6863,  0.7445,  0.4948,  1.2988],\n",
      "        [ 0.6225,  1.9138, -1.2171,  1.4296, -0.8321]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9216],\n",
      "        [-1.5603],\n",
      "        [-5.3309],\n",
      "        [ 0.8569]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0251, -1.4410,  1.7577,  1.2280, -0.0574],\n",
      "        [-1.8413,  0.4413,  0.9667, -0.3829, -0.0529],\n",
      "        [-0.5068, -0.0337,  2.0235,  0.1940,  0.9705],\n",
      "        [-1.9279, -0.2458, -0.5442,  1.7782,  0.7409]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6705, -0.8466,  0.5657, -0.1188,  0.0808],\n",
      "        [-0.6785,  0.0206, -0.6615, -1.0989, -0.7543],\n",
      "        [ 0.7763,  0.3443,  0.6125,  0.2629,  1.4078],\n",
      "        [-0.2044,  0.4017, -1.1143,  0.1544,  0.0240]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0251, -1.4410,  1.7577,  1.2280, -0.0574],\n",
      "        [-1.8413,  0.4413,  0.9667, -0.3829, -0.0529],\n",
      "        [-0.5068, -0.0337,  2.0235,  0.1940,  0.9705],\n",
      "        [-1.9279, -0.2458, -0.5442,  1.7782,  0.7409]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0469],\n",
      "        [ 1.0795],\n",
      "        [ 2.2515],\n",
      "        [ 1.1941]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6328, -0.7669,  0.0705, -0.2436,  0.2810],\n",
      "        [ 0.6366,  0.2275,  2.5290,  1.2072,  0.5416],\n",
      "        [-0.1282,  2.0858,  0.7121, -0.2943, -0.3458],\n",
      "        [-0.3865, -0.6242, -1.0071,  1.7053, -0.3127]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6662, -1.1712, -1.4954, -0.9867, -1.8505],\n",
      "        [-0.4576, -0.8357, -0.6308, -0.8497, -1.6984],\n",
      "        [-0.0777, -0.5933, -0.1568, -0.4768, -0.9633],\n",
      "        [-0.8863, -0.7606, -0.2599, -0.6789, -1.1674]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6328, -0.7669,  0.0705, -0.2436,  0.2810],\n",
      "        [ 0.6366,  0.2275,  2.5290,  1.2072,  0.5416],\n",
      "        [-0.1282,  2.0858,  0.7121, -0.2943, -0.3458],\n",
      "        [-0.3865, -0.6242, -1.0071,  1.7053, -0.3127]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9348],\n",
      "        [-4.0224],\n",
      "        [-0.8658],\n",
      "        [ 0.2865]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5833, -0.1663,  0.4259, -0.2946, -0.8242],\n",
      "        [ 0.0674,  0.1036, -0.0442,  0.3957,  1.4703],\n",
      "        [-1.5921,  0.4336,  0.4482, -0.4878,  1.2911],\n",
      "        [-1.4905,  0.9960, -0.6041, -0.3256,  1.0689]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5304, -0.6354, -1.3553, -2.0107, -2.4987],\n",
      "        [ 1.2719,  1.0679,  1.2050,  1.1194,  1.7736],\n",
      "        [ 0.3087, -0.6968, -0.3668, -0.7734, -1.9006],\n",
      "        [-0.7215, -0.6976, -0.4805, -0.1566,  0.1246]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5833, -0.1663,  0.4259, -0.2946, -0.8242],\n",
      "        [ 0.0674,  0.1036, -0.0442,  0.3957,  1.4703],\n",
      "        [-1.5921,  0.4336,  0.4482, -0.4878,  1.2911],\n",
      "        [-1.4905,  0.9960, -0.6041, -0.3256,  1.0689]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0201],\n",
      "        [ 3.1936],\n",
      "        [-3.0348],\n",
      "        [ 0.8549]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2760, -0.4237,  1.1633, -1.4357, -0.0137],\n",
      "        [ 0.4031, -1.9524,  0.0071,  0.9860,  0.2786],\n",
      "        [ 0.9184, -1.7699, -0.6387, -0.4819, -0.8093],\n",
      "        [-1.9337, -1.0799, -0.6600, -1.8156,  0.6592]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2693, -2.1656, -2.3522, -1.3847, -3.3843],\n",
      "        [-0.4485, -0.8642,  0.3310, -0.1924, -0.7939],\n",
      "        [ 0.6292,  1.1348,  1.0556,  0.9560,  1.1011],\n",
      "        [-0.7236, -0.8678, -0.2043, -0.5370, -0.7416]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2760, -0.4237,  1.1633, -1.4357, -0.0137],\n",
      "        [ 0.4031, -1.9524,  0.0071,  0.9860,  0.2786],\n",
      "        [ 0.9184, -1.7699, -0.6387, -0.4819, -0.8093],\n",
      "        [-1.9337, -1.0799, -0.6600, -1.8156,  0.6592]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5662],\n",
      "        [ 1.0978],\n",
      "        [-3.4567],\n",
      "        [ 2.9574]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4272,  0.9719, -0.0848, -0.2308, -0.8575],\n",
      "        [ 1.6690, -0.3228, -0.7420, -0.0897,  0.4116],\n",
      "        [-1.4543, -1.6625, -0.2125, -0.2091,  1.0103],\n",
      "        [-1.4301, -0.7724, -0.2057, -1.1097,  1.2527]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3864,  0.1465, -0.1183,  0.1563, -0.4451],\n",
      "        [-0.6017,  0.1632, -0.6149,  0.0205, -1.3490],\n",
      "        [ 1.4011,  1.9978,  1.8643,  1.0389,  2.1864],\n",
      "        [-1.0866, -0.8305, -2.0962, -1.1994, -2.8857]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4272,  0.9719, -0.0848, -0.2308, -0.8575],\n",
      "        [ 1.6690, -0.3228, -0.7420, -0.0897,  0.4116],\n",
      "        [-1.4543, -1.6625, -0.2125, -0.2091,  1.0103],\n",
      "        [-1.4301, -0.7724, -0.2057, -1.1097,  1.2527]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0534],\n",
      "        [-1.1579],\n",
      "        [-3.7636],\n",
      "        [ 0.3424]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1477,  0.5774, -0.7209,  1.2663, -1.5294],\n",
      "        [-0.4033,  1.6189,  0.9029, -0.1735,  0.7138],\n",
      "        [-1.4533, -0.9498,  0.5271, -0.8197, -1.2242],\n",
      "        [-0.8706,  0.7192, -0.0050,  1.5315,  1.2145]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0681, -0.1378, -0.0903, -0.2477, -0.6157],\n",
      "        [-0.1567,  0.0314,  0.6010,  0.0515, -0.4204],\n",
      "        [ 2.7049,  3.5543,  2.7360,  2.6138,  3.4811],\n",
      "        [-1.4465, -0.2637, -1.0626, -1.3912, -3.2047]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1477,  0.5774, -0.7209,  1.2663, -1.5294],\n",
      "        [-0.4033,  1.6189,  0.9029, -0.1735,  0.7138],\n",
      "        [-1.4533, -0.9498,  0.5271, -0.8197, -1.2242],\n",
      "        [-0.8706,  0.7192, -0.0050,  1.5315,  1.2145]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[  0.6034],\n",
      "        [  0.3477],\n",
      "        [-12.2689],\n",
      "        [ -4.9477]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1467,  0.9897,  1.9951, -1.1914,  1.4718],\n",
      "        [-0.5103, -0.2836,  0.7670, -0.1783,  0.5525],\n",
      "        [ 1.4004, -0.1926,  1.0281,  1.3482, -2.2760],\n",
      "        [-1.8684,  0.1904, -0.8417, -0.6667,  0.1564]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0860,  0.8893, -0.0352, -0.0679, -0.2861],\n",
      "        [-0.5071, -0.1874,  0.0727, -0.1566, -0.4869],\n",
      "        [-0.1753,  0.1368, -0.6432,  0.5495, -0.1517],\n",
      "        [ 0.0308,  0.2506,  0.1749, -0.0270,  0.4801]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1467,  0.9897,  1.9951, -1.1914,  1.4718],\n",
      "        [-0.5103, -0.2836,  0.7670, -0.1783,  0.5525],\n",
      "        [ 1.4004, -0.1926,  1.0281,  1.3482, -2.2760],\n",
      "        [-1.8684,  0.1904, -0.8417, -0.6667,  0.1564]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3711],\n",
      "        [ 0.1266],\n",
      "        [ 0.1531],\n",
      "        [-0.0639]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1436, -0.0755, -0.1423, -0.0289,  0.5594],\n",
      "        [-0.1047, -0.0471,  0.2525, -2.1035, -0.9889],\n",
      "        [ 1.6791, -1.2287, -0.5839, -0.6680,  2.4739],\n",
      "        [-0.0090, -0.2452, -0.5364, -0.9604,  0.6162]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4763, -0.5165,  0.0403,  0.5986,  0.4217],\n",
      "        [-0.0348, -0.1136,  0.7517,  0.1942, -0.5841],\n",
      "        [ 0.1145,  0.4090, -0.1968,  0.3682,  0.3564],\n",
      "        [ 0.4826, -0.2132, -0.2734,  0.0577,  0.4357]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1436, -0.0755, -0.1423, -0.0289,  0.5594],\n",
      "        [-0.1047, -0.0471,  0.2525, -2.1035, -0.9889],\n",
      "        [ 1.6791, -1.2287, -0.5839, -0.6680,  2.4739],\n",
      "        [-0.0090, -0.2452, -0.5364, -0.9604,  0.6162]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3202],\n",
      "        [ 0.3679],\n",
      "        [ 0.4405],\n",
      "        [ 0.4077]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6741, -0.3229, -0.4415,  0.3090,  1.4798],\n",
      "        [ 0.2032, -0.7665,  0.4556,  1.0902, -1.0364],\n",
      "        [ 0.4059,  2.1934, -0.7473, -0.1581,  0.4577],\n",
      "        [-1.5146,  1.9486,  0.2139, -0.3976, -0.1818]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4958,  0.7172,  0.2395,  0.0345,  0.0093],\n",
      "        [ 0.3504,  0.0253,  0.0095, -0.0077, -0.3339],\n",
      "        [-0.1308,  0.6464, -0.0320,  0.0783, -0.2257],\n",
      "        [ 0.5503,  0.1134, -0.2676,  0.4232,  0.0788]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6741, -0.3229, -0.4415,  0.3090,  1.4798],\n",
      "        [ 0.2032, -0.7665,  0.4556,  1.0902, -1.0364],\n",
      "        [ 0.4059,  2.1934, -0.7473, -0.1581,  0.4577],\n",
      "        [-1.5146,  1.9486,  0.2139, -0.3976, -0.1818]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6471],\n",
      "        [ 0.3939],\n",
      "        [ 1.2729],\n",
      "        [-0.8524]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3752, -0.5148, -0.4399, -0.0193,  1.8380],\n",
      "        [-0.0349, -2.6140,  1.5508,  0.3199,  1.8592],\n",
      "        [ 0.7075, -0.3217, -0.5815, -0.0360,  0.8071],\n",
      "        [-0.5135,  1.1580, -1.2126, -1.0401,  1.5750]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3123,  0.3602, -0.0242, -0.4289,  1.2621],\n",
      "        [-0.8203,  0.3939,  0.3194,  0.2013,  0.2367],\n",
      "        [-0.3702, -0.5016, -0.6644, -0.5161, -1.1551],\n",
      "        [ 0.3591,  0.0900,  1.4402,  0.2377,  0.0416]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3752, -0.5148, -0.4399, -0.0193,  1.8380],\n",
      "        [-0.0349, -2.6140,  1.5508,  0.3199,  1.8592],\n",
      "        [ 0.7075, -0.3217, -0.5815, -0.0360,  0.8071],\n",
      "        [-0.5135,  1.1580, -1.2126, -1.0401,  1.5750]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.2704],\n",
      "        [-0.0010],\n",
      "        [-0.6279],\n",
      "        [-2.0083]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0109,  0.7976,  1.8723, -1.1483,  1.1064],\n",
      "        [ 1.0738,  0.7563, -0.7030,  0.2788,  0.3099],\n",
      "        [-1.1336,  0.5700, -0.8171,  0.5292,  0.6745],\n",
      "        [ 0.0206,  1.2398,  0.0322,  0.2011,  1.4730]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8744,  0.0005, -0.9898, -0.5305, -1.0981],\n",
      "        [-0.1924, -0.3648, -0.1241,  0.4051,  0.4428],\n",
      "        [-0.1872, -0.8191,  0.2611, -0.2011,  0.4260],\n",
      "        [ 1.0024,  1.1611,  1.7549,  0.7919,  1.3551]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0109,  0.7976,  1.8723, -1.1483,  1.1064],\n",
      "        [ 1.0738,  0.7563, -0.7030,  0.2788,  0.3099],\n",
      "        [-1.1336,  0.5700, -0.8171,  0.5292,  0.6745],\n",
      "        [ 0.0206,  1.2398,  0.0322,  0.2011,  1.4730]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.3426],\n",
      "        [-0.1450],\n",
      "        [-0.2870],\n",
      "        [ 3.6720]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0069,  0.2976,  0.2948, -0.0126, -0.6018],\n",
      "        [-0.2509,  0.8320, -0.9923, -1.0771, -0.9321],\n",
      "        [ 0.9474,  0.6771, -0.9116,  2.3728, -1.1180],\n",
      "        [ 0.4997, -0.0316,  1.2358, -0.5968, -0.4802]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5676,  1.4400,  0.7708,  0.3869,  1.2709],\n",
      "        [-0.6584, -0.0155,  0.0512, -0.7065, -0.4960],\n",
      "        [-0.5482,  0.1063,  0.3184, -0.3338, -0.4540],\n",
      "        [-0.2780, -0.9891, -0.1263, -0.7241, -0.5899]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0069,  0.2976,  0.2948, -0.0126, -0.6018],\n",
      "        [-0.2509,  0.8320, -0.9923, -1.0771, -0.9321],\n",
      "        [ 0.9474,  0.6771, -0.9116,  2.3728, -1.1180],\n",
      "        [ 0.4997, -0.0316,  1.2358, -0.5968, -0.4802]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4575],\n",
      "        [ 1.3247],\n",
      "        [-1.0220],\n",
      "        [ 0.4516]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0248, -0.1761, -0.0299, -0.0117,  1.9350],\n",
      "        [-0.7736, -0.6844, -0.2464, -2.3408, -1.9546],\n",
      "        [-1.8229,  1.7637,  0.6963,  2.0578,  2.1531],\n",
      "        [ 0.3144,  1.0632,  1.9073, -1.5903,  1.8463]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0540, -0.1576,  0.4434, -0.2337,  0.3383],\n",
      "        [-0.1865, -0.5729, -0.9322, -0.2681, -1.1848],\n",
      "        [ 0.8140, -0.0234,  0.3563,  0.8674,  0.5031],\n",
      "        [-0.6366, -0.8313, -0.0542, -0.6483, -1.1412]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0248, -0.1761, -0.0299, -0.0117,  1.9350],\n",
      "        [-0.7736, -0.6844, -0.2464, -2.3408, -1.9546],\n",
      "        [-1.8229,  1.7637,  0.6963,  2.0578,  2.1531],\n",
      "        [ 0.3144,  1.0632,  1.9073, -1.5903,  1.8463]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6706],\n",
      "        [ 3.7095],\n",
      "        [ 1.5912],\n",
      "        [-2.2635]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8802, -1.2840, -0.3198, -0.2178, -0.0142],\n",
      "        [ 0.9300, -1.6762, -0.0909, -0.1661, -1.4290],\n",
      "        [-0.9039, -1.3708,  0.5357, -0.9501,  0.7075],\n",
      "        [-0.4304,  0.8810,  0.7894,  0.3351, -1.0643]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3306,  0.3763,  0.0551, -0.2593,  0.5124],\n",
      "        [-0.5962, -1.6022, -1.6416, -1.6295, -3.7389],\n",
      "        [-0.1061, -0.2717, -0.3188, -0.3464, -0.9505],\n",
      "        [ 0.4587,  0.6924,  0.5308,  0.9749,  1.0807]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8802, -1.2840, -0.3198, -0.2178, -0.0142],\n",
      "        [ 0.9300, -1.6762, -0.0909, -0.1661, -1.4290],\n",
      "        [-0.9039, -1.3708,  0.5357, -0.9501,  0.7075],\n",
      "        [-0.4304,  0.8810,  0.7894,  0.3351, -1.0643]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1700],\n",
      "        [ 7.8938],\n",
      "        [-0.0458],\n",
      "        [ 0.0080]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1956,  1.0756,  0.3489, -1.5882, -0.7909],\n",
      "        [-0.1455, -1.1150, -0.1540,  1.2631,  0.8035],\n",
      "        [ 0.2758,  0.1744, -1.5737,  0.1474,  1.0313],\n",
      "        [ 1.0192,  1.9417, -0.6573,  0.2449,  0.4349]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1370, -0.3713, -0.0158, -0.5147,  0.4197],\n",
      "        [ 0.3426,  0.0198,  0.2727,  0.2576,  0.0738],\n",
      "        [-0.0383, -0.0893,  0.3461, -0.3076, -0.4105],\n",
      "        [ 0.4345, -0.3398, -0.4841,  0.3627,  0.7973]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1956,  1.0756,  0.3489, -1.5882, -0.7909],\n",
      "        [-0.1455, -1.1150, -0.1540,  1.2631,  0.8035],\n",
      "        [ 0.2758,  0.1744, -1.5737,  0.1474,  1.0313],\n",
      "        [ 1.0192,  1.9417, -0.6573,  0.2449,  0.4349]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1075],\n",
      "        [ 0.2708],\n",
      "        [-1.0395],\n",
      "        [ 0.5366]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0149, -1.0876,  1.0160, -0.8304,  0.5195],\n",
      "        [ 0.3610, -0.9153,  1.1452, -1.6565, -0.0414],\n",
      "        [-0.8312, -1.3431,  0.3067,  0.8710, -0.3275],\n",
      "        [-1.3197,  0.6471,  0.3018, -0.0591,  0.6675]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2088, -0.0465, -0.1720, -0.1733,  0.1104],\n",
      "        [ 0.3187,  0.7512,  0.6453,  0.6758, -0.3581],\n",
      "        [ 0.4887,  0.1795,  0.3188,  0.1427,  1.0085],\n",
      "        [ 0.1583, -0.4831, -0.2014, -0.1197,  0.0741]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0149, -1.0876,  1.0160, -0.8304,  0.5195],\n",
      "        [ 0.3610, -0.9153,  1.1452, -1.6565, -0.0414],\n",
      "        [-0.8312, -1.3431,  0.3067,  0.8710, -0.3275],\n",
      "        [-1.3197,  0.6471,  0.3018, -0.0591,  0.6675]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0741],\n",
      "        [-0.9382],\n",
      "        [-0.7554],\n",
      "        [-0.5257]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5337, -0.3588, -0.4768, -0.3875,  1.5091],\n",
      "        [ 0.0317,  2.0154, -0.8670,  1.5421,  0.1204],\n",
      "        [-1.9260, -0.6988, -0.7467,  1.5394,  1.2051],\n",
      "        [-0.5360, -1.4922, -0.4688, -1.1060,  1.7579]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0920,  0.3944, -0.3373, -0.8370,  0.6053],\n",
      "        [ 0.6296,  0.9348,  0.3104,  0.4472,  0.8742],\n",
      "        [ 0.2951,  0.0691,  0.5912, -0.3339,  0.7874],\n",
      "        [ 0.1943,  0.3442,  0.2552, -0.4205, -0.3759]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5337, -0.3588, -0.4768, -0.3875,  1.5091],\n",
      "        [ 0.0317,  2.0154, -0.8670,  1.5421,  0.1204],\n",
      "        [-1.9260, -0.6988, -0.7467,  1.5394,  1.2051],\n",
      "        [-0.5360, -1.4922, -0.4688, -1.1060,  1.7579]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1160],\n",
      "        [ 2.4296],\n",
      "        [-0.6233],\n",
      "        [-0.9331]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1620, -0.1831,  2.2712,  0.4949,  2.0240],\n",
      "        [-0.0033,  1.1384,  1.6958, -1.7296, -1.2888],\n",
      "        [ 1.1474, -0.1009,  0.9271,  0.5352,  0.0552],\n",
      "        [ 0.9449, -1.0399, -0.0406, -0.1310,  1.0093]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3350, -0.3427, -0.2907, -0.4163, -0.4987],\n",
      "        [-0.4875, -0.6479, -1.7164, -0.6037, -1.3810],\n",
      "        [ 0.4558,  0.2826,  0.1483, -0.3891,  0.5910],\n",
      "        [ 0.4095,  0.5334, -0.1311, -0.6323,  0.3580]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1620, -0.1831,  2.2712,  0.4949,  2.0240],\n",
      "        [-0.0033,  1.1384,  1.6958, -1.7296, -1.2888],\n",
      "        [ 1.1474, -0.1009,  0.9271,  0.5352,  0.0552],\n",
      "        [ 0.9449, -1.0399, -0.0406, -0.1310,  1.0093]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8672],\n",
      "        [-0.8225],\n",
      "        [ 0.4564],\n",
      "        [ 0.2817]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8278, -0.5314, -0.4241,  0.1915, -1.3517],\n",
      "        [ 0.2147,  1.1543, -0.1074, -0.5600, -0.0506],\n",
      "        [ 0.1894, -0.1488, -0.0761,  1.4172, -0.1898],\n",
      "        [-1.8309, -0.1933,  0.7307,  0.4954, -0.7883]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6861,  0.7030,  0.6688,  0.5659,  1.2775],\n",
      "        [ 0.3790, -0.6097, -0.3542, -0.2396, -1.0348],\n",
      "        [ 0.8815, -0.5538, -0.6103,  0.0936,  0.2773],\n",
      "        [ 0.4337,  0.3941,  0.1169,  0.2935,  0.0533]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8278, -0.5314, -0.4241,  0.1915, -1.3517],\n",
      "        [ 0.2147,  1.1543, -0.1074, -0.5600, -0.0506],\n",
      "        [ 0.1894, -0.1488, -0.0761,  1.4172, -0.1898],\n",
      "        [-1.8309, -0.1933,  0.7307,  0.4954, -0.7883]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.8435],\n",
      "        [-0.3979],\n",
      "        [ 0.3758],\n",
      "        [-0.6815]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2891, -0.7358,  0.2285,  0.3621,  1.5498],\n",
      "        [-0.4392,  0.8954,  0.7223, -0.3542, -0.7954],\n",
      "        [-0.3017,  0.6975,  0.6480, -0.2199, -0.0489],\n",
      "        [-0.5259,  2.0429, -0.0623,  0.9281,  0.0088]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8605,  1.2641,  1.4799,  2.0772,  2.6046],\n",
      "        [-0.1076, -0.8217, -0.3832, -0.1746, -0.4011],\n",
      "        [ 0.0336, -0.0176, -0.1970,  0.4797, -0.1425],\n",
      "        [ 0.6089,  0.4309, -1.0711, -0.0700, -0.3558]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2891, -0.7358,  0.2285,  0.3621,  1.5498],\n",
      "        [-0.4392,  0.8954,  0.7223, -0.3542, -0.7954],\n",
      "        [-0.3017,  0.6975,  0.6480, -0.2199, -0.0489],\n",
      "        [-0.5259,  2.0429, -0.0623,  0.9281,  0.0088]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0873],\n",
      "        [-0.5844],\n",
      "        [-0.2486],\n",
      "        [ 0.5587]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7286, -0.4684, -0.7999, -0.8523,  0.3068],\n",
      "        [-1.4295,  0.8200, -1.0234, -1.3866,  0.7214],\n",
      "        [-0.9593,  0.4939,  1.6431,  1.4912, -0.5490],\n",
      "        [-0.8969, -0.5946, -0.1581, -0.9365,  1.3923]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4934,  0.5372,  0.8065,  0.3350,  0.7036],\n",
      "        [ 0.0272, -0.4386, -0.3950, -0.1154, -0.1672],\n",
      "        [ 0.3488,  1.0012,  0.6186, -0.1563,  0.1997],\n",
      "        [ 0.3896, -0.2069, -0.5060,  0.2364, -0.3689]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7286, -0.4684, -0.7999, -0.8523,  0.3068],\n",
      "        [-1.4295,  0.8200, -1.0234, -1.3866,  0.7214],\n",
      "        [-0.9593,  0.4939,  1.6431,  1.4912, -0.5490],\n",
      "        [-0.8969, -0.5946, -0.1581, -0.9365,  1.3923]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.6069],\n",
      "        [ 0.0453],\n",
      "        [ 0.8336],\n",
      "        [-0.8813]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3897,  0.1788,  0.5850,  2.4945, -0.4993],\n",
      "        [ 0.4735, -1.5410, -0.1134,  0.6236,  1.1146],\n",
      "        [-1.4902, -2.4200,  0.0887, -0.5302, -0.2384],\n",
      "        [ 1.5927,  0.2764,  1.3854,  0.2568,  0.8604]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6248,  1.1199,  0.1037,  0.7873,  0.3580],\n",
      "        [ 0.2650,  0.0188, -0.1760,  0.1880, -0.0835],\n",
      "        [ 0.2462, -0.0015, -0.8511,  0.7348, -1.6785],\n",
      "        [ 0.6079, -0.0001, -0.1656,  0.1249,  0.7360]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3897,  0.1788,  0.5850,  2.4945, -0.4993],\n",
      "        [ 0.4735, -1.5410, -0.1134,  0.6236,  1.1146],\n",
      "        [-1.4902, -2.4200,  0.0887, -0.5302, -0.2384],\n",
      "        [ 1.5927,  0.2764,  1.3854,  0.2568,  0.8604]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1778],\n",
      "        [ 0.1406],\n",
      "        [-0.4281],\n",
      "        [ 1.4042]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3973,  0.4696, -0.5023,  2.5216,  0.2654],\n",
      "        [ 0.0941, -0.9689, -0.6063,  0.1244, -1.3200],\n",
      "        [ 1.1408,  0.1248,  2.4309,  0.1721,  1.6165],\n",
      "        [ 1.2234,  2.0143,  1.2609, -2.1048,  0.4641]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1668,  0.5936,  0.8032,  0.0239,  0.0444],\n",
      "        [ 0.5982, -0.5160, -0.4222,  0.0783, -0.5445],\n",
      "        [ 0.3173,  0.6538,  0.2332, -0.1730, -1.0779],\n",
      "        [-0.4900, -0.8008, -0.5983, -0.1190, -1.1233]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3973,  0.4696, -0.5023,  2.5216,  0.2654],\n",
      "        [ 0.0941, -0.9689, -0.6063,  0.1244, -1.3200],\n",
      "        [ 1.1408,  0.1248,  2.4309,  0.1721,  1.6165],\n",
      "        [ 1.2234,  2.0143,  1.2609, -2.1048,  0.4641]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0137],\n",
      "        [ 1.5407],\n",
      "        [-0.7617],\n",
      "        [-3.2378]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9547,  0.5654, -0.7419,  2.0360, -1.0218],\n",
      "        [ 0.6419,  1.2709, -1.2908, -2.1502,  1.3696],\n",
      "        [ 0.4239,  0.3555, -1.6804,  1.2350, -1.0979],\n",
      "        [-0.3710,  0.8736, -1.0827,  2.1395,  0.8405]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0437,  0.3121,  0.6274,  0.8235,  0.9339],\n",
      "        [-0.4929, -0.3280, -0.8391, -0.9326, -1.6572],\n",
      "        [ 0.1644,  0.0017, -0.0768,  0.6881,  0.1907],\n",
      "        [ 0.8199,  0.8226,  1.3248,  0.4634,  1.9708]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9547,  0.5654, -0.7419,  2.0360, -1.0218],\n",
      "        [ 0.6419,  1.2709, -1.2908, -2.1502,  1.3696],\n",
      "        [ 0.4239,  0.3555, -1.6804,  1.2350, -1.0979],\n",
      "        [-0.3710,  0.8736, -1.0827,  2.1395,  0.8405]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3917],\n",
      "        [ 0.0855],\n",
      "        [ 0.8398],\n",
      "        [ 1.6278]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.2591, -0.8477,  0.7996, -1.1486,  0.8957],\n",
      "        [-1.7807,  0.8912,  0.8478, -0.9801, -0.7926],\n",
      "        [-1.0303,  0.5210, -1.4639, -1.9951,  0.7901],\n",
      "        [-0.0970, -0.4876, -0.4273,  0.4658,  1.2074]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0066,  0.8962,  0.9277,  0.5255,  1.4300],\n",
      "        [-0.2379, -1.4911, -0.8756, -0.5206, -1.2179],\n",
      "        [-0.1148, -0.2530, -0.4658, -0.1033, -0.2007],\n",
      "        [ 0.4096,  0.6204, -0.1909,  0.8895,  0.9582]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.2591, -0.8477,  0.7996, -1.1486,  0.8957],\n",
      "        [-1.7807,  0.8912,  0.8478, -0.9801, -0.7926],\n",
      "        [-1.0303,  0.5210, -1.4639, -1.9951,  0.7901],\n",
      "        [-0.0970, -0.4876, -0.4273,  0.4658,  1.2074]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6445],\n",
      "        [-0.1721],\n",
      "        [ 0.7159],\n",
      "        [ 1.3107]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6460, -0.3230,  0.8634, -0.4751,  0.1767],\n",
      "        [-0.7322,  2.6830, -0.0920, -0.8255,  1.1674],\n",
      "        [ 0.1973,  1.0228,  1.3936, -1.5890,  0.5493],\n",
      "        [ 0.8721,  0.9906,  0.0645,  1.9727,  0.1571]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3987,  1.2724,  0.6449,  0.6473,  1.2580],\n",
      "        [-0.2354, -0.1594, -0.3438, -0.0225, -0.4862],\n",
      "        [-0.3758, -0.2583,  0.0951,  0.2566, -0.0235],\n",
      "        [ 0.3172, -0.2808,  0.8097, -0.2293, -0.5710]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6460, -0.3230,  0.8634, -0.4751,  0.1767],\n",
      "        [-0.7322,  2.6830, -0.0920, -0.8255,  1.1674],\n",
      "        [ 0.1973,  1.0228,  1.3936, -1.5890,  0.5493],\n",
      "        [ 0.8721,  0.9906,  0.0645,  1.9727,  0.1571]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1970],\n",
      "        [-0.7726],\n",
      "        [-0.6264],\n",
      "        [-0.4914]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 2.4194,  0.6548, -1.4722, -0.9400, -0.0833],\n",
      "        [ 0.0737, -0.8587, -0.0626,  1.5636, -1.1960],\n",
      "        [-0.1168,  0.2599,  2.3508, -1.1176,  0.4136],\n",
      "        [-0.8999, -0.8069,  1.3219, -0.9914,  0.4743]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0196,  0.7686,  1.1285,  0.3422,  1.0281],\n",
      "        [-0.3061, -0.5806, -0.3016, -0.4594, -1.1485],\n",
      "        [ 0.2830,  0.2650, -0.3090, -0.3266, -0.1053],\n",
      "        [ 0.2086, -0.0087,  0.5686,  0.5498,  0.7940]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 2.4194,  0.6548, -1.4722, -0.9400, -0.0833],\n",
      "        [ 0.0737, -0.8587, -0.0626,  1.5636, -1.1960],\n",
      "        [-0.1168,  0.2599,  2.3508, -1.1176,  0.4136],\n",
      "        [-0.8999, -0.8069,  1.3219, -0.9914,  0.4743]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5179],\n",
      "        [ 1.1502],\n",
      "        [-0.3691],\n",
      "        [ 0.4025]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2679,  0.7453, -1.2907, -0.1064,  1.6548],\n",
      "        [ 0.4762, -0.0197,  1.7703,  0.0768,  0.9760],\n",
      "        [-0.0574,  0.5754,  1.1177, -2.5480, -0.5695],\n",
      "        [ 1.0233,  0.4784, -0.3559,  1.8841, -0.8094]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9498,  1.6399,  1.7914,  1.0340,  1.1106],\n",
      "        [-0.1780, -0.5872, -1.0444, -0.3946, -1.0766],\n",
      "        [ 0.2188,  0.1352, -0.0884, -0.2770,  0.2964],\n",
      "        [ 0.2302,  0.1365,  1.3233,  0.5742,  1.4538]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2679,  0.7453, -1.2907, -0.1064,  1.6548],\n",
      "        [ 0.4762, -0.0197,  1.7703,  0.0768,  0.9760],\n",
      "        [-0.0574,  0.5754,  1.1177, -2.5480, -0.5695],\n",
      "        [ 1.0233,  0.4784, -0.3559,  1.8841, -0.8094]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8923],\n",
      "        [-3.0031],\n",
      "        [ 0.5034],\n",
      "        [-0.2649]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5422, -0.2224,  0.7771, -0.3171, -1.8501],\n",
      "        [-1.1301,  0.8331, -1.2736,  0.1873,  1.0700],\n",
      "        [ 0.1947, -1.1195,  1.5535, -0.5435,  1.5946],\n",
      "        [ 0.2212, -0.0172, -2.0496,  0.3218,  3.2983]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1377,  0.9234,  0.7071,  1.1183,  1.5235],\n",
      "        [ 0.7673,  0.8427,  0.7708,  0.2794,  1.1080],\n",
      "        [ 0.2397,  0.1488,  0.2031, -0.0227, -0.1126],\n",
      "        [ 0.2957, -0.3277,  0.8010,  0.0621,  0.2588]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5422, -0.2224,  0.7771, -0.3171, -1.8501],\n",
      "        [-1.1301,  0.8331, -1.2736,  0.1873,  1.0700],\n",
      "        [ 0.1947, -1.1195,  1.5535, -0.5435,  1.5946],\n",
      "        [ 0.2212, -0.0172, -2.0496,  0.3218,  3.2983]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9039],\n",
      "        [ 0.0912],\n",
      "        [ 0.0283],\n",
      "        [-0.6971]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4527, -0.6406, -0.9896,  0.2336, -0.4242],\n",
      "        [ 0.5905,  0.5485, -0.1469,  0.0349,  1.0345],\n",
      "        [ 0.0894, -1.4552,  0.2514, -1.4903,  0.5274],\n",
      "        [ 0.8564,  0.4124,  0.7272, -0.8610,  1.7133]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5540,  2.5164,  0.9954,  1.6505,  2.3564],\n",
      "        [ 0.8803,  0.3029,  0.1667,  0.5722,  0.8960],\n",
      "        [-0.1273, -0.4431,  0.4587,  0.0228, -0.1415],\n",
      "        [ 0.2872, -0.2094,  0.5829,  0.7572,  1.1466]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4527, -0.6406, -0.9896,  0.2336, -0.4242],\n",
      "        [ 0.5905,  0.5485, -0.1469,  0.0349,  1.0345],\n",
      "        [ 0.0894, -1.4552,  0.2514, -1.4903,  0.5274],\n",
      "        [ 0.8564,  0.4124,  0.7272, -0.8610,  1.7133]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.9147],\n",
      "        [ 1.6083],\n",
      "        [ 0.6401],\n",
      "        [ 1.8960]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8388,  0.5534,  0.1590, -0.3184, -0.6450],\n",
      "        [ 0.9325,  1.6491, -1.1509, -0.0803, -1.3170],\n",
      "        [ 1.2920,  0.6742,  0.8913,  0.4508,  0.5243],\n",
      "        [-0.5998,  1.8130,  0.6166,  0.7933,  1.6081]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2059,  0.2852,  0.8665, -0.0018, -0.1379],\n",
      "        [-0.4533, -0.7276, -0.3888, -0.9306, -0.9054],\n",
      "        [-0.1221,  0.4346,  0.5475,  0.3595,  0.1439],\n",
      "        [-0.5332, -0.9406, -0.6107, -0.0817, -1.3425]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8388,  0.5534,  0.1590, -0.3184, -0.6450],\n",
      "        [ 0.9325,  1.6491, -1.1509, -0.0803, -1.3170],\n",
      "        [ 1.2920,  0.6742,  0.8913,  0.4508,  0.5243],\n",
      "        [-0.5998,  1.8130,  0.6166,  0.7933,  1.6081]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5578],\n",
      "        [ 0.0919],\n",
      "        [ 0.8608],\n",
      "        [-3.9857]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5787, -1.1169, -0.1990,  0.0575,  0.1646],\n",
      "        [-0.4407, -1.5640,  0.8352,  1.4476, -2.7419],\n",
      "        [-1.2357,  0.0981, -0.2717,  0.4812, -0.1846],\n",
      "        [-1.0725, -0.0091,  0.1542,  0.6268,  0.0950]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0954,  0.1741,  0.1792, -0.1883,  0.6583],\n",
      "        [ 0.0289, -0.4772, -0.9059,  0.1462, -0.3013],\n",
      "        [-0.3496,  0.3958, -0.1677, -0.3523, -0.7950],\n",
      "        [ 0.9123,  1.5227,  1.5344,  1.3386,  1.8872]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5787, -1.1169, -0.1990,  0.0575,  0.1646],\n",
      "        [-0.4407, -1.5640,  0.8352,  1.4476, -2.7419],\n",
      "        [-1.2357,  0.0981, -0.2717,  0.4812, -0.1846],\n",
      "        [-1.0725, -0.0091,  0.1542,  0.6268,  0.0950]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0773],\n",
      "        [ 1.0148],\n",
      "        [ 0.4936],\n",
      "        [ 0.2626]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0153,  0.6968,  1.3008,  0.3256, -0.1416],\n",
      "        [-0.8585, -1.4545,  1.5591, -0.5086, -0.6192],\n",
      "        [-0.4966, -0.5022,  1.0591,  0.5440,  2.3643],\n",
      "        [ 0.9526,  0.9267, -1.0289,  0.1302,  1.2442]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0411,  0.1246,  0.3717, -0.4332,  0.0678],\n",
      "        [-0.0153, -0.8055, -0.5865, -0.6257, -1.5870],\n",
      "        [-0.0257,  0.4198,  0.4309,  0.2944, -0.5140],\n",
      "        [ 1.4145,  1.7878,  0.8415,  1.1806,  1.9316]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0153,  0.6968,  1.3008,  0.3256, -0.1416],\n",
      "        [-0.8585, -1.4545,  1.5591, -0.5086, -0.6192],\n",
      "        [-0.4966, -0.5022,  1.0591,  0.5440,  2.3643],\n",
      "        [ 0.9526,  0.9267, -1.0289,  0.1302,  1.2442]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4204],\n",
      "        [ 1.5711],\n",
      "        [-0.7967],\n",
      "        [ 4.6954]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1073,  1.0916,  1.6537, -0.1102,  0.5026],\n",
      "        [-0.5543,  0.1447, -1.1978, -0.1032, -0.4336],\n",
      "        [-0.9045, -0.1424, -0.0862, -0.0977, -0.6564],\n",
      "        [ 0.2639,  0.5195,  1.0133, -2.8074,  0.6288]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2277,  0.0964, -0.0392, -0.3571, -0.0284],\n",
      "        [-0.3794, -0.4885, -1.4312, -0.9853, -1.7754],\n",
      "        [ 0.2633,  0.9088,  0.6742,  0.4828,  1.3833],\n",
      "        [-0.7502, -0.1002, -0.6584, -0.6190, -2.3742]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1073,  1.0916,  1.6537, -0.1102,  0.5026],\n",
      "        [-0.5543,  0.1447, -1.1978, -0.1032, -0.4336],\n",
      "        [-0.9045, -0.1424, -0.0862, -0.0977, -0.6564],\n",
      "        [ 0.2639,  0.5195,  1.0133, -2.8074,  0.6288]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4142],\n",
      "        [ 2.7255],\n",
      "        [-1.3809],\n",
      "        [-0.6723]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2996, -1.9215,  1.3805, -1.3197,  0.2067],\n",
      "        [ 0.1764, -0.7080,  0.9057,  0.0937, -0.2873],\n",
      "        [ 0.0235,  0.7811,  1.1047,  0.9671,  0.2769],\n",
      "        [ 1.1986,  0.5661,  1.2128, -0.3269, -1.7236]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6450, -0.2986, -0.1466, -0.4192,  0.3649],\n",
      "        [-1.3101, -1.7523, -2.0050, -1.4337, -3.0094],\n",
      "        [ 1.0202,  0.9702,  1.1722,  0.8140,  1.1050],\n",
      "        [-0.5392,  0.1124,  0.4239,  0.6268, -0.4106]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2996, -1.9215,  1.3805, -1.3197,  0.2067],\n",
      "        [ 0.1764, -0.7080,  0.9057,  0.0937, -0.2873],\n",
      "        [ 0.0235,  0.7811,  1.1047,  0.9671,  0.2769],\n",
      "        [ 1.1986,  0.5661,  1.2128, -0.3269, -1.7236]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1619],\n",
      "        [-0.0759],\n",
      "        [ 3.1698],\n",
      "        [ 0.4342]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6525, -0.9431, -0.7774, -1.1347,  0.4498],\n",
      "        [-0.0849, -1.8015, -0.8569,  1.9240, -1.8036],\n",
      "        [ 0.2746, -0.0100, -2.3415,  1.2092, -1.7768],\n",
      "        [-0.3437, -0.1669,  0.7931, -0.2213, -1.4638]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1169,  0.0094,  0.0666, -0.1750, -0.1786],\n",
      "        [-0.1311, -0.2772, -0.1187, -0.5492,  0.2396],\n",
      "        [-0.3744, -1.3153, -0.5461, -1.3712, -1.8501],\n",
      "        [ 0.1117,  0.7981, -0.1011,  0.4465,  0.4026]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6525, -0.9431, -0.7774, -1.1347,  0.4498],\n",
      "        [-0.0849, -1.8015, -0.8569,  1.9240, -1.8036],\n",
      "        [ 0.2746, -0.0100, -2.3415,  1.2092, -1.7768],\n",
      "        [-0.3437, -0.1669,  0.7931, -0.2213, -1.4638]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0187],\n",
      "        [-0.8766],\n",
      "        [ 2.8182],\n",
      "        [-0.9400]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2480,  0.8926, -0.2385,  0.5592, -0.2958],\n",
      "        [ 0.4466, -1.9090,  1.3664,  1.0153, -0.7444],\n",
      "        [ 1.2031,  1.4887,  0.7823,  0.7714, -0.3455],\n",
      "        [-1.5656, -0.5996,  0.3004,  1.0815,  0.0809]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0156, -0.2001, -0.0213, -0.8353, -0.1254],\n",
      "        [ 0.4189,  0.5050,  0.6259,  0.4069, -0.1670],\n",
      "        [-1.4478, -1.5081, -1.3944, -2.2547, -2.8376],\n",
      "        [ 0.1477,  1.2212,  1.1081,  0.5649,  1.0672]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2480,  0.8926, -0.2385,  0.5592, -0.2958],\n",
      "        [ 0.4466, -1.9090,  1.3664,  1.0153, -0.7444],\n",
      "        [ 1.2031,  1.4887,  0.7823,  0.7714, -0.3455],\n",
      "        [-1.5656, -0.5996,  0.3004,  1.0815,  0.0809]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5841],\n",
      "        [ 0.6156],\n",
      "        [-5.8365],\n",
      "        [ 0.0665]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1169,  0.3957,  0.2618, -0.2994, -1.2993],\n",
      "        [ 0.1229, -0.2899, -0.5399,  0.8566,  0.1899],\n",
      "        [-2.2357, -0.5164, -0.6583,  2.3132,  0.7238],\n",
      "        [-0.9629, -0.4190,  1.3046,  1.0252,  2.3636]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2715, -0.7644,  0.6783, -0.3662, -0.2322],\n",
      "        [-0.2907,  0.0951, -0.5958, -0.0543,  0.7160],\n",
      "        [ 0.5902,  0.1963,  0.1946, -0.1477,  1.1315],\n",
      "        [ 0.6067,  1.0848,  1.2205,  1.4614,  1.3948]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1169,  0.3957,  0.2618, -0.2994, -1.2993],\n",
      "        [ 0.1229, -0.2899, -0.5399,  0.8566,  0.1899],\n",
      "        [-2.2357, -0.5164, -0.6583,  2.3132,  0.7238],\n",
      "        [-0.9629, -0.4190,  1.3046,  1.0252,  2.3636]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3182],\n",
      "        [ 0.3479],\n",
      "        [-1.0716],\n",
      "        [ 5.3485]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6846,  0.0927, -0.2727, -1.9588, -0.5157],\n",
      "        [ 2.2914, -1.4011, -0.1421,  0.5764, -0.3760],\n",
      "        [-1.4091, -0.7032,  0.0962,  1.4171,  0.3055],\n",
      "        [-0.6145, -0.6640,  0.1926,  1.0777,  0.8063]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3140,  0.4896,  0.3232,  0.4433,  0.5356],\n",
      "        [ 0.4951, -0.5441, -0.0963, -0.1962, -0.2692],\n",
      "        [ 0.7123,  1.4666,  0.5623,  0.6907,  1.3121],\n",
      "        [-1.6143, -0.3779, -1.8686, -1.5660, -3.8045]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6846,  0.0927, -0.2727, -1.9588, -0.5157],\n",
      "        [ 2.2914, -1.4011, -0.1421,  0.5764, -0.3760],\n",
      "        [-1.4091, -0.7032,  0.0962,  1.4171,  0.3055],\n",
      "        [-0.6145, -0.6640,  0.1926,  1.0777,  0.8063]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9724],\n",
      "        [ 1.8985],\n",
      "        [-0.6013],\n",
      "        [-3.8721]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3083,  0.0511,  0.2860, -1.1653, -0.6069],\n",
      "        [-0.1240, -0.3998,  1.2498,  0.8221,  0.8436],\n",
      "        [-0.7173,  0.0243, -0.3947, -0.0857,  1.5052],\n",
      "        [ 0.3708, -0.3782, -0.4475, -0.2538, -1.0731]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3536,  0.3045,  0.8239,  0.1765,  0.4376],\n",
      "        [-0.2421, -0.9526, -1.3761, -0.8252, -1.6761],\n",
      "        [ 0.5444,  1.2207,  1.1882,  0.7209,  0.8182],\n",
      "        [-0.7072,  0.4764,  0.6863,  0.3757,  0.3853]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3083,  0.0511,  0.2860, -1.1653, -0.6069],\n",
      "        [-0.1240, -0.3998,  1.2498,  0.8221,  0.8436],\n",
      "        [-0.7173,  0.0243, -0.3947, -0.0857,  1.5052],\n",
      "        [ 0.3708, -0.3782, -0.4475, -0.2538, -1.0731]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3291],\n",
      "        [-3.4013],\n",
      "        [ 0.3401],\n",
      "        [-1.2583]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1991, -2.6364, -0.3510,  0.8051,  3.0176],\n",
      "        [-0.9855,  0.1542,  0.2986, -0.0336,  0.6801],\n",
      "        [-1.2538,  0.0293, -0.1064,  0.9451, -1.4055],\n",
      "        [ 0.0102, -0.4839,  1.2969, -0.2781, -0.1537]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0991,  0.2806, -0.3915, -0.5805,  0.2995],\n",
      "        [ 0.3279,  0.6423,  0.9942,  1.4095,  1.3033],\n",
      "        [ 0.7770,  1.2310,  0.5231, -0.1453,  0.4703],\n",
      "        [ 0.0317,  0.3290,  0.6308,  0.9377,  0.6554]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1991, -2.6364, -0.3510,  0.8051,  3.0176],\n",
      "        [-0.9855,  0.1542,  0.2986, -0.0336,  0.6801],\n",
      "        [-1.2538,  0.0293, -0.1064,  0.9451, -1.4055],\n",
      "        [ 0.0102, -0.4839,  1.2969, -0.2781, -0.1537]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1855],\n",
      "        [ 0.9117],\n",
      "        [-1.7923],\n",
      "        [ 0.2977]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4616,  0.3692,  0.2700, -0.0190, -0.3643],\n",
      "        [ 1.4256, -0.0900,  1.5847,  0.9957,  1.0287],\n",
      "        [-0.8246, -1.4455, -0.0420, -0.2353,  0.1141],\n",
      "        [-0.5353,  1.0188,  0.4356,  0.7549,  0.8880]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0005, -0.5752,  0.0539,  0.1920, -0.6508],\n",
      "        [ 0.2866,  0.1778,  0.1801,  0.4063,  0.5314],\n",
      "        [ 1.3109,  1.6748,  1.1650,  0.9122,  1.7561],\n",
      "        [ 0.6744,  0.4083, -0.0198,  0.3633,  0.4937]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4616,  0.3692,  0.2700, -0.0190, -0.3643],\n",
      "        [ 1.4256, -0.0900,  1.5847,  0.9957,  1.0287],\n",
      "        [-0.8246, -1.4455, -0.0420, -0.2353,  0.1141],\n",
      "        [-0.5353,  1.0188,  0.4356,  0.7549,  0.8880]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0354],\n",
      "        [ 1.6292],\n",
      "        [-3.5650],\n",
      "        [ 0.7590]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1314,  0.6491, -0.5542,  0.4828,  0.6030],\n",
      "        [ 0.3859, -0.5535, -1.1652,  2.3788, -1.6410],\n",
      "        [-0.7495, -0.0159,  1.0152,  0.2400,  1.5395],\n",
      "        [ 1.6472,  0.7509,  0.9713,  0.4746, -0.9242]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0977, -0.0205, -0.2700, -0.6828, -1.2280],\n",
      "        [-0.4269, -1.2817, -0.0049, -0.6542, -1.2715],\n",
      "        [ 1.8557,  2.7066,  2.0697,  1.7463,  3.0843],\n",
      "        [-0.1793,  0.4016,  0.2704,  0.3517,  0.6178]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1314,  0.6491, -0.5542,  0.4828,  0.6030],\n",
      "        [ 0.3859, -0.5535, -1.1652,  2.3788, -1.6410],\n",
      "        [-0.7495, -0.0159,  1.0152,  0.2400,  1.5395],\n",
      "        [ 1.6472,  0.7509,  0.9713,  0.4746, -0.9242]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0443],\n",
      "        [ 1.0808],\n",
      "        [ 5.8348],\n",
      "        [-0.1352]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0632, -0.0359, -0.6040,  2.2083, -0.3313],\n",
      "        [-1.4182, -0.8769,  0.8888, -0.0731, -2.0048],\n",
      "        [-1.6200,  0.3023,  1.6669,  0.9496,  0.2916],\n",
      "        [ 1.1384, -1.7380,  0.4186, -0.9290,  0.1648]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6738,  0.0068,  0.1630,  0.4291,  0.7269],\n",
      "        [-0.2399, -1.0132, -1.2480, -0.9476, -1.4624],\n",
      "        [ 0.5183, -0.0121,  0.3981,  0.4166, -0.1131],\n",
      "        [ 0.0127,  0.2249, -0.0358,  0.8789,  0.7381]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0632, -0.0359, -0.6040,  2.2083, -0.3313],\n",
      "        [-1.4182, -0.8769,  0.8888, -0.0731, -2.0048],\n",
      "        [-1.6200,  0.3023,  1.6669,  0.9496,  0.2916],\n",
      "        [ 1.1384, -1.7380,  0.4186, -0.9290,  0.1648]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6508],\n",
      "        [ 3.1205],\n",
      "        [ 0.1830],\n",
      "        [-1.0863]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6393, -0.9523, -0.2104,  0.4854, -0.1181],\n",
      "        [-0.0274,  0.3940, -0.1192, -0.3201,  1.3474],\n",
      "        [ 0.0684,  0.9202,  0.8482,  0.6815, -0.7955],\n",
      "        [-1.1323,  1.7249, -2.1735,  1.3677,  1.0662]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3064,  0.5437,  0.4926, -0.3018,  0.1652],\n",
      "        [-1.2040, -0.8365, -1.4227, -2.0580, -3.8194],\n",
      "        [ 0.0656, -0.2453, -0.4270,  0.4228,  0.2522],\n",
      "        [ 0.4698,  0.0976,  0.8195,  0.6847,  1.2467]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6393, -0.9523, -0.2104,  0.4854, -0.1181],\n",
      "        [-0.0274,  0.3940, -0.1192, -0.3201,  1.3474],\n",
      "        [ 0.0684,  0.9202,  0.8482,  0.6815, -0.7955],\n",
      "        [-1.1323,  1.7249, -2.1735,  1.3677,  1.0662]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9833],\n",
      "        [-4.6144],\n",
      "        [-0.4959],\n",
      "        [ 0.1211]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1532, -0.1445,  1.1481,  1.9630,  0.5408],\n",
      "        [ 0.4038, -0.0875, -1.0341,  1.8848,  2.1438],\n",
      "        [-1.3200,  0.6512, -0.2842,  1.8091,  0.1014],\n",
      "        [-0.2366,  1.8773, -2.4544,  0.0694,  0.1133]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9456, -0.0845, -0.0995,  0.0344,  0.2127],\n",
      "        [ 0.2769, -0.1619,  0.1870, -0.1571,  0.2518],\n",
      "        [-0.1830,  0.2476, -1.0637,  0.1611,  0.0062],\n",
      "        [ 0.1444,  0.9722,  0.2535,  0.3228,  0.7795]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1532, -0.1445,  1.1481,  1.9630,  0.5408],\n",
      "        [ 0.4038, -0.0875, -1.0341,  1.8848,  2.1438],\n",
      "        [-1.3200,  0.6512, -0.2842,  1.8091,  0.1014],\n",
      "        [-0.2366,  1.8773, -2.4544,  0.0694,  0.1133]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0644],\n",
      "        [ 0.1762],\n",
      "        [ 0.9971],\n",
      "        [ 1.2794]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7018,  0.2808,  0.5206,  1.2007,  1.3010],\n",
      "        [-1.9441,  0.8680, -0.6404,  1.8064, -1.5103],\n",
      "        [ 0.6840, -1.2612,  0.0416, -0.3395, -0.3425],\n",
      "        [-1.0432, -0.4955,  0.0966,  0.8112, -0.5130]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7301, -0.2665, -0.3167, -0.8177, -0.0942],\n",
      "        [ 0.0337, -0.9196, -0.5378, -0.5850, -0.6597],\n",
      "        [ 0.0785, -0.3851, -0.0417,  0.2305, -0.7710],\n",
      "        [ 0.0575, -0.1825,  0.2980, -0.1237, -0.2681]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7018,  0.2808,  0.5206,  1.2007,  1.3010],\n",
      "        [-1.9441,  0.8680, -0.6404,  1.8064, -1.5103],\n",
      "        [ 0.6840, -1.2612,  0.0416, -0.3395, -0.3425],\n",
      "        [-1.0432, -0.4955,  0.0966,  0.8112, -0.5130]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8316],\n",
      "        [-0.5796],\n",
      "        [ 0.7235],\n",
      "        [ 0.0964]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3290,  0.4242,  0.3878,  1.4216, -0.2799],\n",
      "        [ 0.3536, -0.1685, -0.7960,  1.6245,  1.6240],\n",
      "        [-0.0722,  0.8388,  1.9002,  0.7435,  0.4296],\n",
      "        [-1.1454,  0.3084,  1.1388,  0.4974,  0.3018]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3838,  0.1067, -0.3545, -0.1237,  0.5215],\n",
      "        [ 0.0278, -0.3927, -0.4683, -0.9346, -1.1554],\n",
      "        [-0.5943,  0.1348,  0.2094, -0.2301, -0.3382],\n",
      "        [ 0.6111,  0.2135,  1.0694, -0.0095, -0.3789]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3290,  0.4242,  0.3878,  1.4216, -0.2799],\n",
      "        [ 0.3536, -0.1685, -0.7960,  1.6245,  1.6240],\n",
      "        [-0.0722,  0.8388,  1.9002,  0.7435,  0.4296],\n",
      "        [-1.1454,  0.3084,  1.1388,  0.4974,  0.3018]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2878],\n",
      "        [-2.9458],\n",
      "        [ 0.2375],\n",
      "        [ 0.4646]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0942, -0.8055,  0.7019,  1.1736, -0.2290],\n",
      "        [ 0.4768, -0.1034, -1.9772, -1.7795,  1.1315],\n",
      "        [ 0.1796,  0.6449, -0.2680,  0.8333,  2.1641],\n",
      "        [-0.4489, -1.1748,  0.4751, -0.3370,  1.2894]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8891, -0.2125, -0.7120, -0.4828, -0.1940],\n",
      "        [ 1.1577,  1.3133,  0.7296,  0.8714,  0.8330],\n",
      "        [-0.6799,  0.3331,  0.2174, -0.0911, -0.5745],\n",
      "        [-0.3146,  0.6386, -0.1324,  0.6258,  0.7901]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0942, -0.8055,  0.7019,  1.1736, -0.2290],\n",
      "        [ 0.4768, -0.1034, -1.9772, -1.7795,  1.1315],\n",
      "        [ 0.1796,  0.6449, -0.2680,  0.8333,  2.1641],\n",
      "        [-0.4489, -1.1748,  0.4751, -0.3370,  1.2894]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7670],\n",
      "        [-1.6345],\n",
      "        [-1.2848],\n",
      "        [ 0.1360]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3598, -3.0930,  0.8276,  1.8065,  0.4102],\n",
      "        [-1.3092, -1.2856,  0.2663,  0.6377,  0.7330],\n",
      "        [ 0.5183, -0.2704,  0.6419,  0.0096,  0.4176],\n",
      "        [-0.8546, -0.3341, -0.8945,  0.2309,  0.1443]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7775, -1.1055, -0.7101, -0.4726, -0.0347],\n",
      "        [ 1.1667,  1.1429,  0.2155,  0.3647,  2.2573],\n",
      "        [-0.0936,  0.8155,  0.5226,  0.0589,  1.0785],\n",
      "        [-0.1001,  0.1669,  0.8002, -0.0758,  0.6420]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3598, -3.0930,  0.8276,  1.8065,  0.4102],\n",
      "        [-1.3092, -1.2856,  0.2663,  0.6377,  0.7330],\n",
      "        [ 0.5183, -0.2704,  0.6419,  0.0096,  0.4176],\n",
      "        [-0.8546, -0.3341, -0.8945,  0.2309,  0.1443]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6837],\n",
      "        [-1.0523],\n",
      "        [ 0.5174],\n",
      "        [-0.6108]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9530,  0.2643, -0.2219, -0.6361,  0.9785],\n",
      "        [-0.7195,  0.7103,  0.6444, -1.1519,  1.1300],\n",
      "        [-0.5273,  1.2353,  0.6831,  0.9486, -0.0602],\n",
      "        [-0.8030, -0.8779,  0.0780,  0.8723,  0.0645]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6159, -0.5857, -1.3134, -0.9438, -1.4674],\n",
      "        [ 1.5754,  1.0517,  1.4821,  0.6340,  2.3345],\n",
      "        [ 0.1383,  0.4381, -0.3068,  0.3142,  0.6121],\n",
      "        [ 0.3430,  0.0992,  0.6346,  1.0584,  0.1810]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9530,  0.2643, -0.2219, -0.6361,  0.9785],\n",
      "        [-0.7195,  0.7103,  0.6444, -1.1519,  1.1300],\n",
      "        [-0.5273,  1.2353,  0.6831,  0.9486, -0.0602],\n",
      "        [-0.8030, -0.8779,  0.0780,  0.8723,  0.0645]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1118],\n",
      "        [ 2.4764],\n",
      "        [ 0.5199],\n",
      "        [ 0.6218]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6312,  0.6676,  1.2100, -1.5959,  0.4644],\n",
      "        [-0.2349, -1.1006,  0.8079,  1.9570,  1.5970],\n",
      "        [-0.5484, -1.4170, -0.6084,  0.7585,  2.6891],\n",
      "        [-0.2046,  0.8004,  1.8796,  1.9544,  0.2840]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0664, -0.4091, -1.0428, -0.5068, -0.7835],\n",
      "        [ 0.9649,  0.7501,  0.5147,  1.1394,  0.6984],\n",
      "        [-0.2376, -0.0431,  0.2130,  0.3205, -0.1220],\n",
      "        [-0.1160,  0.5883,  0.9133,  0.1388,  1.1235]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6312,  0.6676,  1.2100, -1.5959,  0.4644],\n",
      "        [-0.2349, -1.1006,  0.8079,  1.9570,  1.5970],\n",
      "        [-0.5484, -1.4170, -0.6084,  0.7585,  2.6891],\n",
      "        [-0.2046,  0.8004,  1.8796,  1.9544,  0.2840]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0481],\n",
      "        [ 2.7089],\n",
      "        [-0.0232],\n",
      "        [ 2.8017]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1549,  1.2273,  0.9643,  0.1224,  0.6280],\n",
      "        [-0.1265, -1.2465, -2.0041,  0.3155,  0.9317],\n",
      "        [-3.2104, -0.0916,  0.1094, -0.8109,  0.4176],\n",
      "        [ 0.1722, -0.3954,  1.7415,  0.1002,  0.4384]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7461, -0.4451, -0.3432, -0.1413,  0.0431],\n",
      "        [-0.6113, -0.6039, -1.3425, -0.9102, -1.8405],\n",
      "        [-0.2791, -0.0241,  0.4821,  0.8080,  0.3169],\n",
      "        [-0.4279, -0.6800, -0.6180, -0.3628, -1.7096]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1549,  1.2273,  0.9643,  0.1224,  0.6280],\n",
      "        [-0.1265, -1.2465, -2.0041,  0.3155,  0.9317],\n",
      "        [-3.2104, -0.0916,  0.1094, -0.8109,  0.4176],\n",
      "        [ 0.1722, -0.3954,  1.7415,  0.1002,  0.4384]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7519],\n",
      "        [ 1.5186],\n",
      "        [ 0.4280],\n",
      "        [-1.6668]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5981,  0.1507,  0.8100, -0.6899,  0.2928],\n",
      "        [-0.4940,  0.2823, -1.3446, -0.3370, -0.1816],\n",
      "        [ 0.0919,  0.1547, -1.2226,  1.0543, -0.2243],\n",
      "        [-0.6110,  0.1165, -0.5046,  0.0179,  0.3856]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5235, -0.5258, -0.5831, -0.1934, -0.0157],\n",
      "        [-0.7450, -1.1203, -1.0202, -1.0397, -2.1650],\n",
      "        [ 0.0130,  0.1079, -0.8254,  0.3008,  0.8038],\n",
      "        [-0.4552, -0.2572,  0.1084,  0.5051, -0.2260]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5981,  0.1507,  0.8100, -0.6899,  0.2928],\n",
      "        [-0.4940,  0.2823, -1.3446, -0.3370, -0.1816],\n",
      "        [ 0.0919,  0.1547, -1.2226,  1.0543, -0.2243],\n",
      "        [-0.6110,  0.1165, -0.5046,  0.0179,  0.3856]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1096],\n",
      "        [ 2.1672],\n",
      "        [ 1.1638],\n",
      "        [ 0.1153]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1888,  0.9244, -0.9706,  0.4170, -0.9100],\n",
      "        [-1.6437, -0.9771,  0.8528, -1.2005, -1.4302],\n",
      "        [-0.2835,  0.5926,  0.8879, -0.1795, -0.5374],\n",
      "        [ 1.1117, -0.4018,  1.3015, -0.9496,  0.1125]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3517, -1.0745, -0.3144, -0.7366, -0.3763],\n",
      "        [-0.9187, -1.1869, -1.4583, -1.6334, -3.8801],\n",
      "        [-0.2701,  0.0251,  0.4074, -0.6016, -0.6168],\n",
      "        [ 0.0894,  0.4184, -0.2833,  0.2018,  0.4886]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1888,  0.9244, -0.9706,  0.4170, -0.9100],\n",
      "        [-1.6437, -0.9771,  0.8528, -1.2005, -1.4302],\n",
      "        [-0.2835,  0.5926,  0.8879, -0.1795, -0.5374],\n",
      "        [ 1.1117, -0.4018,  1.3015, -0.9496,  0.1125]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7193],\n",
      "        [ 8.9362],\n",
      "        [ 0.8927],\n",
      "        [-0.5741]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 3.5472,  1.4656,  0.3155, -1.0392,  1.4596],\n",
      "        [ 0.2483,  0.5189, -0.1497,  2.2911, -0.0957],\n",
      "        [ 0.5132, -0.1514,  1.5992, -0.2254,  1.4384],\n",
      "        [ 0.6769, -0.6508, -0.5345, -0.3940,  0.8193]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8361, -0.3995, -0.2702, -0.3126, -0.6677],\n",
      "        [-0.0418,  0.1620, -0.0503, -0.0365,  0.0056],\n",
      "        [ 0.3370,  0.0616, -0.6166, -0.1420, -1.2194],\n",
      "        [-0.3644,  0.2137, -0.5488,  0.2298,  0.3668]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 3.5472,  1.4656,  0.3155, -1.0392,  1.4596],\n",
      "        [ 0.2483,  0.5189, -0.1497,  2.2911, -0.0957],\n",
      "        [ 0.5132, -0.1514,  1.5992, -0.2254,  1.4384],\n",
      "        [ 0.6769, -0.6508, -0.5345, -0.3940,  0.8193]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6455],\n",
      "        [-0.0030],\n",
      "        [-2.5443],\n",
      "        [ 0.1175]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4938,  0.1833,  0.2837,  1.6222, -0.8278],\n",
      "        [-0.3920,  0.4406, -0.6015,  1.0112,  0.6907],\n",
      "        [-0.7520,  1.1199, -0.6161,  0.2154,  0.2503],\n",
      "        [ 0.0636,  1.1974,  0.5027, -0.5482,  2.3262]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1768, -1.1913, -0.5970, -0.5080, -1.5573],\n",
      "        [ 0.1387, -0.4066, -0.3796,  0.4790, -0.6880],\n",
      "        [ 0.2532,  0.7250,  0.3330,  0.7497,  1.6551],\n",
      "        [ 0.1829,  0.2492, -0.1216, -0.5651,  0.3685]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4938,  0.1833,  0.2837,  1.6222, -0.8278],\n",
      "        [-0.3920,  0.4406, -0.6015,  1.0112,  0.6907],\n",
      "        [-0.7520,  1.1199, -0.6161,  0.2154,  0.2503],\n",
      "        [ 0.0636,  1.1974,  0.5027, -0.5482,  2.3262]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1647],\n",
      "        [ 0.0040],\n",
      "        [ 0.9920],\n",
      "        [ 1.4159]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7744, -0.5231,  0.8914,  1.2646, -0.1422],\n",
      "        [ 0.0800,  2.8508,  0.5705,  0.5643, -0.3413],\n",
      "        [ 0.6936,  1.5410, -0.8404,  1.3660,  1.2585],\n",
      "        [-0.8376, -0.1185,  0.3560, -0.5701,  0.4643]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3151, -1.9786, -1.4606, -1.2127, -1.8563],\n",
      "        [ 0.3263,  0.4234,  1.0302,  0.0726, -1.0674],\n",
      "        [ 0.0995,  0.3404,  0.0990,  0.0170,  0.2493],\n",
      "        [-1.2235, -0.7525, -0.0853, -0.2892, -1.0743]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7744, -0.5231,  0.8914,  1.2646, -0.1422],\n",
      "        [ 0.0800,  2.8508,  0.5705,  0.5643, -0.3413],\n",
      "        [ 0.6936,  1.5410, -0.8404,  1.3660,  1.2585],\n",
      "        [-0.8376, -0.1185,  0.3560, -0.5701,  0.4643]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9775],\n",
      "        [ 2.2262],\n",
      "        [ 0.8473],\n",
      "        [ 0.7496]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2753,  0.0987, -0.3974,  0.5954,  0.3384],\n",
      "        [ 0.3207, -0.6545, -0.3096, -0.0300,  0.8901],\n",
      "        [ 0.4008,  1.5447, -0.4429,  2.3459,  0.4167],\n",
      "        [-1.2008, -0.5185, -0.9739, -0.9111, -1.9835]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5698, -0.6280, -0.8832, -0.3437, -1.2297],\n",
      "        [-0.8671, -1.1734, -0.5090, -0.8047, -1.8665],\n",
      "        [-0.0495,  0.4647,  0.3418, -0.0893, -0.3503],\n",
      "        [-0.5656, -0.3396, -0.5593, -0.6596, -1.4761]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2753,  0.0987, -0.3974,  0.5954,  0.3384],\n",
      "        [ 0.3207, -0.6545, -0.3096, -0.0300,  0.8901],\n",
      "        [ 0.4008,  1.5447, -0.4429,  2.3459,  0.4167],\n",
      "        [-1.2008, -0.5185, -0.9739, -0.9111, -1.9835]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0586],\n",
      "        [-0.9899],\n",
      "        [ 0.1911],\n",
      "        [ 4.9287]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.6579, -0.5360, -0.3071,  0.5063,  2.1537],\n",
      "        [-1.5515, -0.1290, -0.4837, -1.0739,  0.5580],\n",
      "        [ 0.5782, -0.7175,  0.9398, -0.2797,  1.5277],\n",
      "        [-0.8915, -0.9292,  2.2039, -0.3872,  0.9412]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3944, -0.7173, -0.4943, -0.2830, -0.2707],\n",
      "        [-0.7504, -0.8821, -0.1554, -0.9244, -0.8745],\n",
      "        [-0.1707,  0.3549,  0.0441,  0.1323,  0.0712],\n",
      "        [-0.9926, -2.0745, -2.0708, -2.4568, -4.6332]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.6579, -0.5360, -0.3071,  0.5063,  2.1537],\n",
      "        [-1.5515, -0.1290, -0.4837, -1.0739,  0.5580],\n",
      "        [ 0.5782, -0.7175,  0.9398, -0.2797,  1.5277],\n",
      "        [-0.8915, -0.9292,  2.2039, -0.3872,  0.9412]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4637],\n",
      "        [ 1.8579],\n",
      "        [-0.2402],\n",
      "        [-5.1606]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7233,  0.3901, -1.1929, -0.5296, -0.1602],\n",
      "        [ 0.6292, -1.2581,  0.5249, -0.0520,  1.1360],\n",
      "        [ 0.6587, -0.9248,  0.8652, -0.9382, -0.3457],\n",
      "        [ 1.7356,  0.8268,  0.1694, -1.8031, -0.5559]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0129, -0.6019, -0.3726, -0.1807, -0.6986],\n",
      "        [-0.5144, -0.5956, -0.9593, -0.4999, -2.0999],\n",
      "        [-0.5208, -0.3935, -0.3810, -0.1462,  0.4999],\n",
      "        [-0.0111,  0.0511, -0.3405, -0.2107,  0.4029]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7233,  0.3901, -1.1929, -0.5296, -0.1602],\n",
      "        [ 0.6292, -1.2581,  0.5249, -0.0520,  1.1360],\n",
      "        [ 0.6587, -0.9248,  0.8652, -0.9382, -0.3457],\n",
      "        [ 1.7356,  0.8268,  0.1694, -1.8031, -0.5559]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4266],\n",
      "        [-2.4371],\n",
      "        [-0.3445],\n",
      "        [ 0.1212]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5088,  1.4631, -0.0828,  1.0589,  0.3979],\n",
      "        [-0.9127, -0.4473, -0.4655, -0.5085,  0.1966],\n",
      "        [-0.2854, -1.8751,  0.2597, -0.2438,  0.1182],\n",
      "        [-0.3198, -0.1012, -0.5369,  1.1172, -0.9427]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3202, -1.1637, -1.2671, -0.7637, -1.0071],\n",
      "        [ 0.1108, -0.6787, -0.4092, -0.0914, -0.0713],\n",
      "        [ 0.1298, -0.3853,  0.5661,  0.0824,  0.4064],\n",
      "        [ 0.1286, -0.4845, -0.0716,  0.0991,  0.0460]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5088,  1.4631, -0.0828,  1.0589,  0.3979],\n",
      "        [-0.9127, -0.4473, -0.4655, -0.5085,  0.1966],\n",
      "        [-0.2854, -1.8751,  0.2597, -0.2438,  0.1182],\n",
      "        [-0.3198, -0.1012, -0.5369,  1.1172, -0.9427]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9700],\n",
      "        [ 0.4254],\n",
      "        [ 0.8605],\n",
      "        [ 0.1137]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1147,  1.2399,  0.6231,  0.3796, -0.3123],\n",
      "        [ 1.4053, -0.3997, -1.3282,  0.7111,  0.9509],\n",
      "        [ 1.6273,  1.2800,  0.7606,  1.2515, -1.5418],\n",
      "        [-0.6319,  1.4564, -0.3861,  0.0204, -0.2142]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8061,  0.6900,  0.8556,  0.2292,  1.0162],\n",
      "        [-0.5383, -0.0170, -0.8184, -1.2134, -0.8358],\n",
      "        [ 0.0535,  0.2213,  0.1579, -0.0590,  0.0784],\n",
      "        [ 0.1221, -0.1259, -0.0898,  0.2623, -0.1564]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1147,  1.2399,  0.6231,  0.3796, -0.3123],\n",
      "        [ 1.4053, -0.3997, -1.3282,  0.7111,  0.9509],\n",
      "        [ 1.6273,  1.2800,  0.7606,  1.2515, -1.5418],\n",
      "        [-0.6319,  1.4564, -0.3861,  0.0204, -0.2142]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2506],\n",
      "        [-1.3203],\n",
      "        [ 0.2958],\n",
      "        [-0.1870]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4218,  0.1092, -0.8783,  0.5397, -0.1036],\n",
      "        [-0.5617,  1.3701,  0.3467, -0.7224,  1.5330],\n",
      "        [-1.0697,  1.5472,  1.9252,  0.8944,  0.9664],\n",
      "        [ 2.0219,  0.3393, -1.1245,  0.5600,  1.1350]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7352, -0.5340, -0.1252, -0.6416,  0.0358],\n",
      "        [ 0.2683,  0.0335, -0.6665,  0.4596, -0.0155],\n",
      "        [ 0.1498,  0.2627,  0.1893, -0.8683,  0.3343],\n",
      "        [ 0.5105,  0.3710, -0.0066,  0.9636, -0.3332]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4218,  0.1092, -0.8783,  0.5397, -0.1036],\n",
      "        [-0.5617,  1.3701,  0.3467, -0.7224,  1.5330],\n",
      "        [-1.0697,  1.5472,  1.9252,  0.8944,  0.9664],\n",
      "        [ 2.0219,  0.3393, -1.1245,  0.5600,  1.1350]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0118],\n",
      "        [-0.6918],\n",
      "        [ 0.1570],\n",
      "        [ 1.3269]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5108, -0.2722, -0.0183, -1.9869,  2.1569],\n",
      "        [-2.0567,  0.7554,  1.0765,  1.4375, -1.0562],\n",
      "        [ 0.8250,  0.5620,  1.0222, -0.4461, -0.4918],\n",
      "        [-0.2331,  1.8087,  0.1736,  0.5617,  0.1204]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3435, -0.2734, -0.5307, -0.2490, -0.0671],\n",
      "        [ 0.5127, -0.4204,  0.1666,  0.0616,  0.0726],\n",
      "        [-0.1476, -0.5374,  0.0167, -0.0501, -0.0565],\n",
      "        [-0.1941, -0.2717, -0.8787,  0.2507, -1.7035]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5108, -0.2722, -0.0183, -1.9869,  2.1569],\n",
      "        [-2.0567,  0.7554,  1.0765,  1.4375, -1.0562],\n",
      "        [ 0.8250,  0.5620,  1.0222, -0.4461, -0.4918],\n",
      "        [-0.2331,  1.8087,  0.1736,  0.5617,  0.1204]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5955],\n",
      "        [-1.1808],\n",
      "        [-0.3565],\n",
      "        [-0.6629]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6261, -1.3677,  0.2568, -1.0371,  1.3629],\n",
      "        [-2.2468, -1.6334,  0.9617, -1.2362,  1.6274],\n",
      "        [-0.8489, -2.0094,  0.0718,  0.2027, -0.5801],\n",
      "        [-0.5544,  1.0314, -0.2966, -0.4869,  1.3428]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8124, -0.1425,  0.0178, -0.0526, -0.1084],\n",
      "        [ 0.3169,  0.2884, -0.4714,  0.1115,  0.7836],\n",
      "        [ 0.4788,  0.3490,  0.3488,  0.4347, -0.4394],\n",
      "        [-0.4460, -0.0213, -0.7500, -0.2586, -0.3974]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6261, -1.3677,  0.2568, -1.0371,  1.3629],\n",
      "        [-2.2468, -1.6334,  0.9617, -1.2362,  1.6274],\n",
      "        [-0.8489, -2.0094,  0.0718,  0.2027, -0.5801],\n",
      "        [-0.5544,  1.0314, -0.2966, -0.4869,  1.3428]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4025],\n",
      "        [-0.4988],\n",
      "        [-0.7396],\n",
      "        [ 0.0400]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5621,  2.5100, -0.9077,  1.0276, -0.5327],\n",
      "        [ 0.1716,  0.1323,  1.5334,  1.1990,  2.0804],\n",
      "        [-0.4959,  2.2844,  1.6166,  0.2809,  0.8215],\n",
      "        [ 0.3569,  0.1239,  0.8790,  1.5850,  2.4038]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8919,  0.3740,  0.8789, -0.1664,  0.6118],\n",
      "        [ 1.0351,  0.3719,  0.2129,  0.1180,  0.3548],\n",
      "        [ 0.2816,  0.2766,  0.0108, -0.0191,  0.5536],\n",
      "        [-0.2328, -0.6096, -0.2174,  0.2911, -0.2881]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5621,  2.5100, -0.9077,  1.0276, -0.5327],\n",
      "        [ 0.1716,  0.1323,  1.5334,  1.1990,  2.0804],\n",
      "        [-0.4959,  2.2844,  1.6166,  0.2809,  0.8215],\n",
      "        [ 0.3569,  0.1239,  0.8790,  1.5850,  2.4038]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8571],\n",
      "        [ 1.4330],\n",
      "        [ 0.9590],\n",
      "        [-0.5809]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1173,  0.8454,  0.2045, -1.2424,  1.5094],\n",
      "        [-0.6995,  0.3140, -0.4969,  2.3043,  0.0454],\n",
      "        [-1.4625,  0.2167,  1.0978,  0.7047, -1.3229],\n",
      "        [-0.0366,  1.3670, -0.2523,  0.2111, -0.7238]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8416,  0.5919,  0.7661,  0.4912,  0.6012],\n",
      "        [-0.2746, -1.2284, -0.9017, -1.2825, -1.3862],\n",
      "        [ 0.0074, -0.1370, -0.8386, -0.1007,  0.0790],\n",
      "        [-0.0362, -0.0946,  0.0669,  0.1847, -0.5207]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1173,  0.8454,  0.2045, -1.2424,  1.5094],\n",
      "        [-0.6995,  0.3140, -0.4969,  2.3043,  0.0454],\n",
      "        [-1.4625,  0.2167,  1.0978,  0.7047, -1.3229],\n",
      "        [-0.0366,  1.3670, -0.2523,  0.2111, -0.7238]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8556],\n",
      "        [-2.7637],\n",
      "        [-1.1366],\n",
      "        [ 0.2710]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0991, -0.5654, -0.0212,  0.3574, -0.0805],\n",
      "        [ 0.2725,  1.7284, -0.1137,  0.0236,  1.2673],\n",
      "        [-1.4719,  0.8101,  0.2637,  0.9251, -0.5504],\n",
      "        [ 0.9369, -0.7400,  0.0953, -0.7863, -0.6603]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2592, -0.4066,  0.3817,  0.1034,  0.2392],\n",
      "        [ 1.1250,  0.6473,  1.2106,  0.0505,  1.3783],\n",
      "        [ 0.3242,  0.6401,  0.6907,  0.6292,  0.7718],\n",
      "        [ 0.1511, -0.2567,  0.2255, -0.0932, -0.7911]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0991, -0.5654, -0.0212,  0.3574, -0.0805],\n",
      "        [ 0.2725,  1.7284, -0.1137,  0.0236,  1.2673],\n",
      "        [-1.4719,  0.8101,  0.2637,  0.9251, -0.5504],\n",
      "        [ 0.9369, -0.7400,  0.0953, -0.7863, -0.6603]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2651],\n",
      "        [ 3.0357],\n",
      "        [ 0.3807],\n",
      "        [ 0.9486]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0619, -0.3302,  0.1013,  2.1908,  1.2477],\n",
      "        [ 0.5860, -0.7911, -0.2280,  0.3145, -0.0558],\n",
      "        [-1.6969,  0.7763,  0.8548, -0.1428,  0.9733],\n",
      "        [-0.0378, -0.3921,  0.3434,  0.2797,  0.3832]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2347, -0.2755, -0.3346,  0.5299,  0.4017],\n",
      "        [-0.3831, -0.7009, -1.1335, -0.8432, -1.9646],\n",
      "        [-0.3694, -0.2485,  0.5026,  0.5697, -0.1386],\n",
      "        [ 0.0177, -0.4920, -0.2394,  0.0700, -0.8715]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0619, -0.3302,  0.1013,  2.1908,  1.2477],\n",
      "        [ 0.5860, -0.7911, -0.2280,  0.3145, -0.0558],\n",
      "        [-1.6969,  0.7763,  0.8548, -0.1428,  0.9733],\n",
      "        [-0.0378, -0.3921,  0.3434,  0.2797,  0.3832]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4698],\n",
      "        [ 0.4329],\n",
      "        [ 0.6472],\n",
      "        [-0.2044]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7809,  0.1804,  0.9508,  0.3146,  0.2004],\n",
      "        [ 0.5968, -0.3023,  0.3309,  0.0547,  1.1396],\n",
      "        [-0.2511, -0.1742,  1.2353, -0.2146, -1.1573],\n",
      "        [ 0.7695,  0.6173, -0.3856,  0.2897, -0.5710]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2968, -0.3324, -1.2633, -0.6161, -1.3778],\n",
      "        [-0.8737, -1.4328, -0.4655, -0.4823, -1.1993],\n",
      "        [-0.5943,  0.1372, -0.4338, -0.0699, -0.3292],\n",
      "        [ 0.3244, -0.1480, -0.0399,  0.5357, -0.7708]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7809,  0.1804,  0.9508,  0.3146,  0.2004],\n",
      "        [ 0.5968, -0.3023,  0.3309,  0.0547,  1.1396],\n",
      "        [-0.2511, -0.1742,  1.2353, -0.2146, -1.1573],\n",
      "        [ 0.7695,  0.6173, -0.3856,  0.2897, -0.5710]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4992],\n",
      "        [-1.6355],\n",
      "        [-0.0145],\n",
      "        [ 0.7689]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2230, -0.6742,  1.2092,  1.2159,  0.1631],\n",
      "        [ 1.3161,  0.2730, -0.6822,  0.3971,  0.9224],\n",
      "        [ 0.5516, -0.0836, -0.5782, -0.8431, -0.8062],\n",
      "        [-0.1464, -1.8444,  0.4132,  1.0441, -0.1363]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4524, -0.4044,  0.4552,  0.2649, -0.2865],\n",
      "        [ 0.1188, -0.2581, -0.9955, -0.4113, -0.2217],\n",
      "        [ 0.2230,  0.6242, -0.2636,  0.0362,  0.3445],\n",
      "        [-0.1711,  0.0045, -0.1072, -0.3441, -1.2866]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2230, -0.6742,  1.2092,  1.2159,  0.1631],\n",
      "        [ 1.3161,  0.2730, -0.6822,  0.3971,  0.9224],\n",
      "        [ 0.5516, -0.0836, -0.5782, -0.8431, -0.8062],\n",
      "        [-0.1464, -1.8444,  0.4132,  1.0441, -0.1363]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6516],\n",
      "        [ 0.3971],\n",
      "        [-0.0851],\n",
      "        [-0.2115]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3132,  0.6899,  0.0354,  0.3969, -1.4942],\n",
      "        [-2.1781, -0.2847, -1.5361, -0.3828,  1.0041],\n",
      "        [-0.1232,  0.4238,  0.1992,  0.1380, -0.3557],\n",
      "        [-0.6781,  1.9925, -0.1885,  0.6918, -1.2813]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0070, -0.6873, -1.3239, -0.1388, -1.0704],\n",
      "        [ 0.1585, -0.4526, -0.7688, -0.6783, -0.3888],\n",
      "        [ 0.0901,  0.3012,  0.2863, -0.2420, -0.1001],\n",
      "        [-0.1622, -0.0053, -0.1530, -0.1374, -0.2797]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3132,  0.6899,  0.0354,  0.3969, -1.4942],\n",
      "        [-2.1781, -0.2847, -1.5361, -0.3828,  1.0041],\n",
      "        [-0.1232,  0.4238,  0.1992,  0.1380, -0.3557],\n",
      "        [-0.6781,  1.9925, -0.1885,  0.6918, -1.2813]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0324],\n",
      "        [ 0.8339],\n",
      "        [ 0.1758],\n",
      "        [ 0.3915]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4449, -1.4979,  1.0379,  0.5967,  1.3686],\n",
      "        [-1.5458, -1.7198,  0.2929,  0.6069, -1.5001],\n",
      "        [ 0.2467, -0.2933, -1.1658, -1.1207, -1.4024],\n",
      "        [-0.1414,  0.3314, -0.4503,  0.7345,  0.3902]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7254, -1.4296, -1.8586, -0.9563, -1.3868],\n",
      "        [-0.2770, -0.9743, -0.3042, -1.0363, -1.4743],\n",
      "        [ 0.0900,  0.1956,  0.1422, -0.1912,  0.4202],\n",
      "        [-0.3165,  0.4203, -0.3530, -0.1773, -0.9450]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4449, -1.4979,  1.0379,  0.5967,  1.3686],\n",
      "        [-1.5458, -1.7198,  0.2929,  0.6069, -1.5001],\n",
      "        [ 0.2467, -0.2933, -1.1658, -1.1207, -1.4024],\n",
      "        [-0.1414,  0.3314, -0.4503,  0.7345,  0.3902]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9335],\n",
      "        [ 3.5975],\n",
      "        [-0.5760],\n",
      "        [-0.1560]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.5350,  1.2700, -1.1357, -0.7493,  1.6279],\n",
      "        [ 1.7990, -1.7250, -0.6204,  0.3522,  0.0841],\n",
      "        [-0.6831,  0.1878, -0.3803,  1.9348, -0.3527],\n",
      "        [-0.4648,  0.7617, -0.1752,  0.5002, -0.6061]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1304,  0.1850, -0.0699, -0.1796, -0.1152],\n",
      "        [-1.0945, -1.3677, -2.4907, -1.8471, -3.1828],\n",
      "        [ 0.2957, -0.1497, -0.3363,  0.1016,  0.0539],\n",
      "        [ 0.0231, -0.4071, -0.1209, -0.6106, -0.3380]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.5350,  1.2700, -1.1357, -0.7493,  1.6279],\n",
      "        [ 1.7990, -1.7250, -0.6204,  0.3522,  0.0841],\n",
      "        [-0.6831,  0.1878, -0.3803,  1.9348, -0.3527],\n",
      "        [-0.4648,  0.7617, -0.1752,  0.5002, -0.6061]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0613],\n",
      "        [ 1.0174],\n",
      "        [ 0.0754],\n",
      "        [-0.4002]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3032, -0.0621,  1.0013,  0.0025,  2.1977],\n",
      "        [-1.2168, -0.2675,  0.5401,  1.4056,  0.9514],\n",
      "        [ 1.8333, -2.4081,  1.6181,  0.1513,  0.5207],\n",
      "        [ 0.9482, -1.1118, -0.6242,  0.4113, -1.3169]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4727, -0.0914, -0.8758, -0.2043, -0.2314],\n",
      "        [ 0.0639, -0.1848, -1.0412, -0.0282, -0.4812],\n",
      "        [ 0.4125,  0.0215, -0.2520, -0.0998, -0.0051],\n",
      "        [-0.4540, -0.3560,  0.2633, -0.3591, -0.1927]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3032, -0.0621,  1.0013,  0.0025,  2.1977],\n",
      "        [-1.2168, -0.2675,  0.5401,  1.4056,  0.9514],\n",
      "        [ 1.8333, -2.4081,  1.6181,  0.1513,  0.5207],\n",
      "        [ 0.9482, -1.1118, -0.6242,  0.4113, -1.3169]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5237],\n",
      "        [-1.0881],\n",
      "        [ 0.2789],\n",
      "        [-0.0929]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9695, -1.5353, -0.7329, -0.0964,  0.9330],\n",
      "        [-0.0444, -0.1124,  0.7883,  1.5786, -1.4733],\n",
      "        [-2.6252,  0.0135,  0.3372, -0.8344, -0.9144],\n",
      "        [-2.3933,  2.0033,  0.9130,  0.2196, -0.5124]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6065,  0.5196, -0.2090,  0.5369,  0.9956],\n",
      "        [ 0.4095,  0.7394,  0.0757,  0.2074,  0.7582],\n",
      "        [ 0.3162, -0.3893,  0.4646,  0.4481,  0.2333],\n",
      "        [ 0.2957,  0.5494,  0.2630, -0.1881, -0.3351]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9695, -1.5353, -0.7329, -0.0964,  0.9330],\n",
      "        [-0.0444, -0.1124,  0.7883,  1.5786, -1.4733],\n",
      "        [-2.6252,  0.0135,  0.3372, -0.8344, -0.9144],\n",
      "        [-2.3933,  2.0033,  0.9130,  0.2196, -0.5124]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3555],\n",
      "        [-0.8312],\n",
      "        [-1.2659],\n",
      "        [ 0.7635]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7130, -1.7127, -0.2873,  1.5787,  0.2278],\n",
      "        [-2.1484,  1.0454, -1.5623,  0.9287,  0.4432],\n",
      "        [ 0.7533,  0.7786,  0.9256,  1.3654,  0.0752],\n",
      "        [ 0.7169,  0.3464,  1.8617, -0.4097, -0.8704]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3814, -0.3123,  0.1003,  0.4709,  0.5040],\n",
      "        [ 0.5551,  0.9901,  0.5962,  0.9314,  0.9962],\n",
      "        [ 0.5256, -0.1219,  0.6416,  0.6043,  0.5850],\n",
      "        [ 0.2279, -0.7485,  0.1452, -0.1209, -0.7377]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7130, -1.7127, -0.2873,  1.5787,  0.2278],\n",
      "        [-2.1484,  1.0454, -1.5623,  0.9287,  0.4432],\n",
      "        [ 0.7533,  0.7786,  0.9256,  1.3654,  0.0752],\n",
      "        [ 0.7169,  0.3464,  1.8617, -0.4097, -0.8704]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.6364],\n",
      "        [ 0.2175],\n",
      "        [ 1.7640],\n",
      "        [ 0.8661]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9521,  1.9161, -0.1845,  2.4128,  2.7957],\n",
      "        [ 0.4066, -1.1772, -0.9462,  0.2595, -0.6341],\n",
      "        [ 2.3761, -0.2217,  0.0768,  0.5924,  0.6353],\n",
      "        [-1.0918,  1.0094, -0.0483,  0.3004,  0.2447]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6420, -0.4312, -0.5255, -0.5601, -0.3506],\n",
      "        [ 0.6162,  0.0234, -0.8694,  0.0036,  0.8068],\n",
      "        [-0.4447,  0.1835,  0.1446, -0.3146, -0.4701],\n",
      "        [-0.7168,  0.1573, -0.4412, -0.1050, -1.0587]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9521,  1.9161, -0.1845,  2.4128,  2.7957],\n",
      "        [ 0.4066, -1.1772, -0.9462,  0.2595, -0.6341],\n",
      "        [ 2.3761, -0.2217,  0.0768,  0.5924,  0.6353],\n",
      "        [-1.0918,  1.0094, -0.0483,  0.3004,  0.2447]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.4496],\n",
      "        [ 0.5351],\n",
      "        [-1.5711],\n",
      "        [ 0.6720]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6081, -0.2925,  1.3822, -0.6414,  1.1751],\n",
      "        [-1.2502, -1.2806,  1.2962,  0.4782,  0.5841],\n",
      "        [ 0.0506,  1.7883, -0.9429, -0.2995, -0.4560],\n",
      "        [ 0.3363,  0.8588,  0.2143, -0.6158, -1.6319]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5301,  0.4606,  0.5790,  1.4237,  1.0588],\n",
      "        [ 0.4570,  0.2624, -0.2141, -0.6653, -0.1117],\n",
      "        [ 0.3691, -0.0261,  0.5529, -0.2696,  0.6699],\n",
      "        [-0.5315, -1.6481, -0.2045, -0.2910, -0.3843]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6081, -0.2925,  1.3822, -0.6414,  1.1751],\n",
      "        [-1.2502, -1.2806,  1.2962,  0.4782,  0.5841],\n",
      "        [ 0.0506,  1.7883, -0.9429, -0.2995, -0.4560],\n",
      "        [ 0.3363,  0.8588,  0.2143, -0.6158, -1.6319]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0661],\n",
      "        [-1.5683],\n",
      "        [-0.7741],\n",
      "        [-0.8317]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2038, -0.7484, -0.4879,  0.3781,  1.1010],\n",
      "        [ 1.9312,  0.9394, -0.4793, -0.6296, -0.6472],\n",
      "        [-1.6627,  0.2773,  0.0973,  0.2884,  0.4554],\n",
      "        [-0.1438, -0.6445,  0.7142, -0.1443,  0.2211]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8480,  0.5666, -0.1318,  0.3203,  0.1788],\n",
      "        [ 1.1700,  0.8209,  1.1478,  0.4571,  1.0310],\n",
      "        [ 0.4676,  0.7891,  1.0935,  0.6046,  0.6187],\n",
      "        [ 0.2186, -0.4831, -0.8001,  0.1327,  0.0362]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2038, -0.7484, -0.4879,  0.3781,  1.1010],\n",
      "        [ 1.9312,  0.9394, -0.4793, -0.6296, -0.6472],\n",
      "        [-1.6627,  0.2773,  0.0973,  0.2884,  0.4554],\n",
      "        [-0.1438, -0.6445,  0.7142, -0.1443,  0.2211]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9790],\n",
      "        [ 1.5256],\n",
      "        [ 0.0039],\n",
      "        [-0.3027]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2891,  0.8940,  0.0355, -1.2435,  0.3103],\n",
      "        [ 1.1177,  1.2613, -1.4711,  1.3792, -0.5066],\n",
      "        [ 0.6051,  0.0573, -1.6304, -0.6752, -1.6955],\n",
      "        [-0.0958,  2.1216,  0.7126, -1.0113, -0.0931]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7715,  0.7226, -0.1889,  0.2262,  0.2512],\n",
      "        [ 0.6587,  0.5661, -0.1799, -0.3709, -0.4897],\n",
      "        [ 0.3675, -0.1360,  0.0786, -0.3144,  0.7358],\n",
      "        [-0.3066, -0.3897, -0.4088, -0.7065, -0.5454]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2891,  0.8940,  0.0355, -1.2435,  0.3103],\n",
      "        [ 1.1177,  1.2613, -1.4711,  1.3792, -0.5066],\n",
      "        [ 0.6051,  0.0573, -1.6304, -0.6752, -1.6955],\n",
      "        [-0.0958,  2.1216,  0.7126, -1.0113, -0.0931]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6590],\n",
      "        [ 1.4514],\n",
      "        [-0.9488],\n",
      "        [-0.3235]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2902,  1.8307, -0.0796,  1.6449,  1.1232],\n",
      "        [-1.1250,  1.9484, -0.6044, -0.2886,  0.3416],\n",
      "        [-1.6053, -1.3516, -0.6569, -1.3692,  0.2263],\n",
      "        [ 1.2144, -0.2718, -1.4102,  0.1713, -1.2261]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5410,  0.3998, -0.5884,  0.5844, -0.4836],\n",
      "        [-0.2249, -0.3255, -0.8262, -0.0845, -1.1743],\n",
      "        [ 0.8683,  0.4815,  0.6976,  0.5086,  1.0398],\n",
      "        [ 0.0124, -0.8256, -0.5761, -0.5527, -1.0477]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2902,  1.8307, -0.0796,  1.6449,  1.1232],\n",
      "        [-1.1250,  1.9484, -0.6044, -0.2886,  0.3416],\n",
      "        [-1.6053, -1.3516, -0.6569, -1.3692,  0.2263],\n",
      "        [ 1.2144, -0.2718, -1.4102,  0.1713, -1.2261]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0399],\n",
      "        [-0.2587],\n",
      "        [-2.9640],\n",
      "        [ 2.2418]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3288,  0.1964, -0.1406, -1.0762,  3.3249],\n",
      "        [-0.4050, -1.2631, -0.5388,  0.0003,  0.5788],\n",
      "        [-1.0862,  1.1151,  0.0447,  1.6844,  2.0705],\n",
      "        [-0.5841,  2.2068,  0.1452, -0.3209, -0.9720]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0030, -0.1027, -0.6370, -0.3677, -0.6344],\n",
      "        [-0.1534,  0.2450,  0.1891,  0.6719,  0.4633],\n",
      "        [ 1.7989,  2.0479,  1.5625,  1.2817,  2.3477],\n",
      "        [-0.7777, -1.4999, -1.1399, -1.0025, -2.0872]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3288,  0.1964, -0.1406, -1.0762,  3.3249],\n",
      "        [-0.4050, -1.2631, -0.5388,  0.0003,  0.5788],\n",
      "        [-1.0862,  1.1151,  0.0447,  1.6844,  2.0705],\n",
      "        [-0.5841,  2.2068,  0.1452, -0.3209, -0.9720]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6451],\n",
      "        [-0.0809],\n",
      "        [ 7.4195],\n",
      "        [-0.6708]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5105, -0.3491,  1.6789,  0.1478,  0.5533],\n",
      "        [-0.0132,  1.6372,  0.3420, -1.1285, -0.7289],\n",
      "        [ 1.6742, -1.8411, -1.1903,  0.7905, -0.5107],\n",
      "        [ 1.5166,  0.4958,  1.1946,  0.9693,  0.2003]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5780,  0.1012, -0.7555,  0.5009,  0.0907],\n",
      "        [ 0.1159, -0.0296,  0.7959, -0.0046,  0.2211],\n",
      "        [-1.2381, -0.4841, -1.2842, -1.7766, -2.6942],\n",
      "        [-0.4657, -1.1066, -0.2728, -1.0494, -1.2310]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5105, -0.3491,  1.6789,  0.1478,  0.5533],\n",
      "        [-0.0132,  1.6372,  0.3420, -1.1285, -0.7289],\n",
      "        [ 1.6742, -1.8411, -1.1903,  0.7905, -0.5107],\n",
      "        [ 1.5166,  0.4958,  1.1946,  0.9693,  0.2003]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.4746],\n",
      "        [ 0.0661],\n",
      "        [ 0.3185],\n",
      "        [-2.8446]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5781,  0.9548,  1.2013,  1.4463,  0.7650],\n",
      "        [-0.5723, -1.5818, -0.3156, -1.2755,  0.8142],\n",
      "        [-0.0312, -1.1734, -0.7208, -0.2454, -0.2774],\n",
      "        [ 1.1051, -0.9776, -1.1274, -0.2309, -1.6088]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4552, -0.3691, -1.0938,  0.0494, -0.3602],\n",
      "        [ 0.2475, -0.1075,  0.5205,  0.4046, -0.1048],\n",
      "        [-1.0556, -0.8615, -1.0701, -1.6598, -3.4060],\n",
      "        [ 0.2291,  0.0548, -0.4480, -0.2615,  0.4608]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5781,  0.9548,  1.2013,  1.4463,  0.7650],\n",
      "        [-0.5723, -1.5818, -0.3156, -1.2755,  0.8142],\n",
      "        [-0.0312, -1.1734, -0.7208, -0.2454, -0.2774],\n",
      "        [ 1.1051, -0.9776, -1.1274, -0.2309, -1.6088]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.5889],\n",
      "        [-0.7372],\n",
      "        [ 3.1672],\n",
      "        [ 0.0236]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.7690,  0.8326,  1.3328, -0.4032,  0.4014],\n",
      "        [ 0.7559,  1.7611,  0.2593, -1.6596, -0.2927],\n",
      "        [ 0.0858,  2.4528,  1.2946,  0.6231,  0.7806],\n",
      "        [-0.1119,  1.1662,  1.8590, -0.1271, -1.3952]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4057,  0.8679,  0.2283,  0.7343,  0.5474],\n",
      "        [-0.1517,  0.3065,  0.8393,  0.4924,  0.8169],\n",
      "        [-0.8712, -2.8865, -2.5330, -3.0165, -5.4724],\n",
      "        [ 1.0485, -0.9485, -1.0060, -0.8380, -0.8996]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.7690,  0.8326,  1.3328, -0.4032,  0.4014],\n",
      "        [ 0.7559,  1.7611,  0.2593, -1.6596, -0.2927],\n",
      "        [ 0.0858,  2.4528,  1.2946,  0.6231,  0.7806],\n",
      "        [-0.1119,  1.1662,  1.8590, -0.1271, -1.3952]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ -0.1728],\n",
      "        [ -0.4136],\n",
      "        [-16.5856],\n",
      "        [ -1.7319]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9879,  1.7787,  1.6935,  0.2516,  0.1524],\n",
      "        [-1.0448, -0.8849, -0.7281,  1.4546,  0.3924],\n",
      "        [ 0.1220,  0.3657,  0.4938, -0.1970,  0.2550],\n",
      "        [-0.1212,  2.0399, -0.7212, -0.3261,  1.2562]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4553, -0.6528,  0.1430,  0.4197,  0.8336],\n",
      "        [ 0.1612, -0.3299,  0.4512,  0.7970,  0.2687],\n",
      "        [ 3.0150,  3.2441,  2.5487,  2.1178,  4.8294],\n",
      "        [ 0.7786,  0.6836,  0.5564,  0.4462,  1.1757]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9879,  1.7787,  1.6935,  0.2516,  0.1524],\n",
      "        [-1.0448, -0.8849, -0.7281,  1.4546,  0.3924],\n",
      "        [ 0.1220,  0.3657,  0.4938, -0.1970,  0.2550],\n",
      "        [-0.1212,  2.0399, -0.7212, -0.3261,  1.2562]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2364],\n",
      "        [ 1.0597],\n",
      "        [ 3.6268],\n",
      "        [ 2.2301]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0739,  1.7139,  1.3266, -0.4932,  1.2121],\n",
      "        [ 0.0306, -0.7327,  0.3268,  0.4883,  0.0677],\n",
      "        [-1.5120, -1.5707, -0.9445, -0.0064, -0.8292],\n",
      "        [ 0.6116,  0.9322, -0.1532, -1.7213, -0.2640]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8287, -0.0248,  0.2576,  1.1910,  1.0491],\n",
      "        [-0.0399, -0.4697, -0.0714, -0.5873, -0.1174],\n",
      "        [ 2.0102,  2.3190,  2.1049,  1.6305,  2.8281],\n",
      "        [-0.4389, -0.8812, -0.3440, -1.1276, -1.4654]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0739,  1.7139,  1.3266, -0.4932,  1.2121],\n",
      "        [ 0.0306, -0.7327,  0.3268,  0.4883,  0.0677],\n",
      "        [-1.5120, -1.5707, -0.9445, -0.0064, -0.8292],\n",
      "        [ 0.6116,  0.9322, -0.1532, -1.7213, -0.2640]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[  1.0447],\n",
      "        [  0.0248],\n",
      "        [-11.0255],\n",
      "        [  1.2908]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7229, -0.4319, -0.4238,  0.6133, -0.2201],\n",
      "        [-1.3318,  0.0831, -0.3689,  0.4008, -0.2141],\n",
      "        [-0.9778, -0.1990, -2.1797,  1.1132,  0.3811],\n",
      "        [-1.4540,  0.7982,  0.3322, -0.8768,  1.2721]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3855, -0.1297,  0.3991,  0.1074, -0.2896],\n",
      "        [ 0.4538,  0.0992, -0.1151, -0.3703,  0.1252],\n",
      "        [-0.0808,  0.4705, -0.2705,  0.7760, -0.0813],\n",
      "        [-0.5811, -1.4507, -1.2532, -2.0372, -2.4711]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7229, -0.4319, -0.4238,  0.6133, -0.2201],\n",
      "        [-1.3318,  0.0831, -0.3689,  0.4008, -0.2141],\n",
      "        [-0.9778, -0.1990, -2.1797,  1.1132,  0.3811],\n",
      "        [-1.4540,  0.7982,  0.3322, -0.8768,  1.2721]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2622],\n",
      "        [-0.7289],\n",
      "        [ 1.4077],\n",
      "        [-2.0870]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8266, -0.0698,  0.2186,  1.3211,  0.2199],\n",
      "        [-0.8650,  0.6542, -0.3760,  1.5203,  0.5873],\n",
      "        [-0.0668, -1.1572, -0.9488,  0.4634, -0.4462],\n",
      "        [-0.0563,  0.6102,  0.8134,  0.2093,  0.0567]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2420,  0.4638, -0.1470,  0.6070,  0.4707],\n",
      "        [ 0.4543,  0.2220,  0.8583,  0.1599,  0.1105],\n",
      "        [-0.6910, -0.2887, -0.1951, -0.6665, -1.0100],\n",
      "        [ 0.0823, -1.1499, -1.0901, -0.6207, -1.7990]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8266, -0.0698,  0.2186,  1.3211,  0.2199],\n",
      "        [-0.8650,  0.6542, -0.3760,  1.5203,  0.5873],\n",
      "        [-0.0668, -1.1572, -0.9488,  0.4634, -0.4462],\n",
      "        [-0.0563,  0.6102,  0.8134,  0.2093,  0.0567]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0409],\n",
      "        [-0.2625],\n",
      "        [ 0.7071],\n",
      "        [-1.8248]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2785, -1.8859,  0.4125, -1.4278,  1.8908],\n",
      "        [-1.2769,  0.9054, -0.4094, -0.5602,  1.4519],\n",
      "        [ 0.0870,  0.7423,  0.8579, -2.6459,  1.2398],\n",
      "        [ 1.0399,  2.1109,  0.2521,  0.2166, -0.2078]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5118,  0.3696,  0.0185,  0.6733,  0.1551],\n",
      "        [ 0.4230,  0.2653, -0.1383, -0.0445,  0.3102],\n",
      "        [-0.2582, -0.6663, -1.1138, -0.5256, -1.6822],\n",
      "        [ 0.7896, -0.1745, -0.3882, -0.3658,  0.3344]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2785, -1.8859,  0.4125, -1.4278,  1.8908],\n",
      "        [-1.2769,  0.9054, -0.4094, -0.5602,  1.4519],\n",
      "        [ 0.0870,  0.7423,  0.8579, -2.6459,  1.2398],\n",
      "        [ 1.0399,  2.1109,  0.2521,  0.2166, -0.2078]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2149],\n",
      "        [ 0.2320],\n",
      "        [-2.1674],\n",
      "        [ 0.2062]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5020,  0.9191,  1.7050, -0.5052,  0.4149],\n",
      "        [ 0.0530,  0.3655, -1.2699, -0.3419, -0.2774],\n",
      "        [ 0.1449, -0.1707, -1.6293, -0.9430,  1.2055],\n",
      "        [-1.8333,  1.4490, -1.3420,  1.2162,  0.8313]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5897,  0.5639,  0.0824,  0.3066,  1.1617],\n",
      "        [ 0.4835,  0.5417,  0.6481,  0.1478,  0.3602],\n",
      "        [ 0.8752,  1.1669,  0.4653,  0.2044,  1.3015],\n",
      "        [ 0.4519,  0.2439, -1.0047, -1.2728, -1.0635]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5020,  0.9191,  1.7050, -0.5052,  0.4149],\n",
      "        [ 0.0530,  0.3655, -1.2699, -0.3419, -0.2774],\n",
      "        [ 0.1449, -0.1707, -1.6293, -0.9430,  1.2055],\n",
      "        [-1.8333,  1.4490, -1.3420,  1.2162,  0.8313]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1001],\n",
      "        [-0.7499],\n",
      "        [ 0.5458],\n",
      "        [-1.5589]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0035, -0.0840, -0.9307,  0.1332,  0.5865],\n",
      "        [ 0.2407, -1.3741,  0.3190,  0.5399, -1.1134],\n",
      "        [-2.1049, -0.9705,  1.2862,  0.5266,  0.4139],\n",
      "        [ 0.1716, -1.1049,  0.7510,  0.0021,  2.3838]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7598,  0.5843, -0.4560,  1.1644,  0.3653],\n",
      "        [ 0.9165,  0.6102,  0.4442,  0.6952,  0.7903],\n",
      "        [ 0.0574, -0.2686,  0.2620, -0.4933, -0.4489],\n",
      "        [ 0.3778,  0.1494,  0.5375,  0.1190,  0.4118]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0035, -0.0840, -0.9307,  0.1332,  0.5865],\n",
      "        [ 0.2407, -1.3741,  0.3190,  0.5399, -1.1134],\n",
      "        [-2.1049, -0.9705,  1.2862,  0.5266,  0.4139],\n",
      "        [ 0.1716, -1.1049,  0.7510,  0.0021,  2.3838]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7473],\n",
      "        [-0.9808],\n",
      "        [ 0.0313],\n",
      "        [ 1.2852]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4916,  0.3403,  1.3555, -0.7049,  2.5858],\n",
      "        [ 0.2128, -0.1270, -1.0976,  0.8089, -0.6603],\n",
      "        [ 1.2159, -1.6350, -2.1399,  0.8961,  0.7869],\n",
      "        [-1.3376, -1.2077, -0.3240, -0.4621, -0.4770]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4846,  0.6893,  0.8230,  0.4058,  1.1389],\n",
      "        [ 0.8094,  0.7169,  1.3746,  0.7801,  0.6954],\n",
      "        [-0.2405, -0.1864,  0.0879, -0.0686,  0.2353],\n",
      "        [ 0.0954, -0.7481, -1.0013, -0.7562, -1.0690]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4916,  0.3403,  1.3555, -0.7049,  2.5858],\n",
      "        [ 0.2128, -0.1270, -1.0976,  0.8089, -0.6603],\n",
      "        [ 1.2159, -1.6350, -2.1399,  0.8961,  0.7869],\n",
      "        [-1.3376, -1.2077, -0.3240, -0.4621, -0.4770]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.2473],\n",
      "        [-1.2557],\n",
      "        [-0.0521],\n",
      "        [ 1.9595]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7638, -0.5963,  0.0151, -0.3832, -0.1702],\n",
      "        [ 0.3280,  0.0763,  2.2536,  0.2503,  0.8814],\n",
      "        [ 0.5495,  0.5487, -0.3990, -0.7195,  2.3024],\n",
      "        [ 0.7846, -0.5927, -2.0753,  0.2566, -0.5853]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2603, -1.6650, -2.2801, -1.2954, -2.4904],\n",
      "        [ 0.6326, -0.2968,  0.3993,  0.6425,  1.5417],\n",
      "        [-0.0551, -0.3187, -0.5814,  0.1143,  0.0908],\n",
      "        [-0.4910, -1.7265, -1.9155, -1.3137, -2.7785]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7638, -0.5963,  0.0151, -0.3832, -0.1702],\n",
      "        [ 0.3280,  0.0763,  2.2536,  0.2503,  0.8814],\n",
      "        [ 0.5495,  0.5487, -0.3990, -0.7195,  2.3024],\n",
      "        [ 0.7846, -0.5927, -2.0753,  0.2566, -0.5853]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.9161],\n",
      "        [ 2.6044],\n",
      "        [ 0.1536],\n",
      "        [ 5.9025]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4945,  0.6210, -0.4305,  0.5454,  0.1505],\n",
      "        [ 1.1152,  1.4162, -0.4213,  1.4158, -1.8801],\n",
      "        [ 0.6953, -0.7296,  0.8839,  0.9336,  0.6721],\n",
      "        [ 0.1213,  0.8169,  0.2797,  0.6263,  1.0007]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.4689, -1.1720, -1.9666, -1.8640, -2.9243],\n",
      "        [ 0.2519, -0.9219, -0.1826, -0.6730, -1.0725],\n",
      "        [ 0.5414, -0.5151,  0.0048, -0.1832, -1.2606],\n",
      "        [ 0.4417, -1.0344,  1.0555, -0.0410, -0.0814]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4945,  0.6210, -0.4305,  0.5454,  0.1505],\n",
      "        [ 1.1152,  1.4162, -0.4213,  1.4158, -1.8801],\n",
      "        [ 0.6953, -0.7296,  0.8839,  0.9336,  0.6721],\n",
      "        [ 0.1213,  0.8169,  0.2797,  0.6263,  1.0007]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8574],\n",
      "        [ 0.1157],\n",
      "        [-0.2618],\n",
      "        [-0.6033]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8017, -0.5026, -0.9066, -0.3230, -1.1194],\n",
      "        [ 0.1147, -0.2509,  0.8518,  1.2511, -0.0397],\n",
      "        [ 1.6223,  0.2674,  1.0331,  0.0103, -0.1767],\n",
      "        [ 1.1302, -1.4653, -0.5809,  0.3797,  0.2399]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8372, -1.3092, -1.6588, -2.1197, -3.9544],\n",
      "        [-0.3934,  0.5595,  0.1406,  0.2412,  0.3048],\n",
      "        [ 0.5262, -0.0742, -0.7426,  0.0086, -0.2536],\n",
      "        [ 0.9832, -1.0271, -0.3347,  0.0546,  0.1912]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8017, -0.5026, -0.9066, -0.3230, -1.1194],\n",
      "        [ 0.1147, -0.2509,  0.8518,  1.2511, -0.0397],\n",
      "        [ 1.6223,  0.2674,  1.0331,  0.0103, -0.1767],\n",
      "        [ 1.1302, -1.4653, -0.5809,  0.3797,  0.2399]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 8.7812],\n",
      "        [ 0.2238],\n",
      "        [ 0.1116],\n",
      "        [ 2.8772]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1203,  0.0879, -0.8731,  1.9491, -0.1432],\n",
      "        [-1.0859, -1.4260, -0.0469, -0.0288,  0.6581],\n",
      "        [ 0.3619,  0.2582,  0.0534,  0.7683,  0.6402],\n",
      "        [-1.3182, -0.7903,  0.8835,  1.7626,  1.4179]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1922,  0.1509,  0.4339, -0.0184, -0.5514],\n",
      "        [ 0.1663,  0.3919,  1.0326,  0.3266,  0.3890],\n",
      "        [ 0.4321, -0.7563, -0.6487, -0.2895, -0.3355],\n",
      "        [-0.7622, -1.0782, -1.5199, -1.4717, -2.4576]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1203,  0.0879, -0.8731,  1.9491, -0.1432],\n",
      "        [-1.0859, -1.4260, -0.0469, -0.0288,  0.6581],\n",
      "        [ 0.3619,  0.2582,  0.0534,  0.7683,  0.6402],\n",
      "        [-1.3182, -0.7903,  0.8835,  1.7626,  1.4179]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2993],\n",
      "        [-0.5412],\n",
      "        [-0.5107],\n",
      "        [-5.5648]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9483,  0.6452, -0.0143,  1.5841, -2.3860],\n",
      "        [ 1.2041, -0.5624,  1.7053,  0.5504, -0.4437],\n",
      "        [-0.2324,  2.6668,  0.4102,  1.0353,  1.2450],\n",
      "        [-0.4341, -1.0243,  1.1044,  1.1557, -0.0653]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6418,  0.3619,  0.1706,  0.0580,  0.0866],\n",
      "        [ 0.1207,  0.1604, -0.0507,  0.0914,  0.9694],\n",
      "        [ 0.2463, -0.4772, -0.1299, -0.6318, -0.7162],\n",
      "        [ 0.3324,  1.7977,  0.9958,  1.1413,  1.7632]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9483,  0.6452, -0.0143,  1.5841, -2.3860],\n",
      "        [ 1.2041, -0.5624,  1.7053,  0.5504, -0.4437],\n",
      "        [-0.2324,  2.6668,  0.4102,  1.0353,  1.2450],\n",
      "        [-0.4341, -1.0243,  1.1044,  1.1557, -0.0653]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7251],\n",
      "        [-0.4111],\n",
      "        [-2.9287],\n",
      "        [ 0.3179]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2517,  0.5907, -0.4182,  2.1615, -0.6200],\n",
      "        [-0.3937,  0.7421,  0.5054, -0.4461,  0.1586],\n",
      "        [ 0.4258, -0.7267,  1.2905,  2.4139,  0.2644],\n",
      "        [-2.0900, -1.0811, -1.3795, -0.3049,  1.1371]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0673, -0.0988,  0.5155,  0.1794, -0.3068],\n",
      "        [ 0.6073,  0.8072, -0.1679,  0.0631,  0.4634],\n",
      "        [ 0.8799,  1.6564,  1.2013,  1.1878,  1.8031],\n",
      "        [ 0.9887,  0.5373,  1.4244,  0.4101,  1.2072]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2517,  0.5907, -0.4182,  2.1615, -0.6200],\n",
      "        [-0.3937,  0.7421,  0.5054, -0.4461,  0.1586],\n",
      "        [ 0.4258, -0.7267,  1.2905,  2.4139,  0.2644],\n",
      "        [-2.0900, -1.0811, -1.3795, -0.3049,  1.1371]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2199],\n",
      "        [ 0.3204],\n",
      "        [ 4.0653],\n",
      "        [-3.3648]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2422,  0.9706,  1.2513,  0.7436, -0.7654],\n",
      "        [ 0.6230, -0.6267, -2.4162,  1.3597, -0.1166],\n",
      "        [ 1.5708,  0.0325, -0.6616,  1.9399,  0.4663],\n",
      "        [-0.5055, -0.6539,  0.4104, -0.4411,  0.1648]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3043,  0.3525, -0.4433, -0.0171, -0.4855],\n",
      "        [ 0.4718, -0.2585,  0.4489,  0.2263,  1.7235],\n",
      "        [-0.1492, -0.9580, -0.2695, -1.1326, -1.4723],\n",
      "        [ 1.4519,  2.7333,  1.8502,  1.6677,  2.6468]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2422,  0.9706,  1.2513,  0.7436, -0.7654],\n",
      "        [ 0.6230, -0.6267, -2.4162,  1.3597, -0.1166],\n",
      "        [ 1.5708,  0.0325, -0.6616,  1.9399,  0.4663],\n",
      "        [-0.5055, -0.6539,  0.4104, -0.4411,  0.1648]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0725],\n",
      "        [-0.5222],\n",
      "        [-2.9709],\n",
      "        [-2.0613]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6145,  1.5101, -0.9422, -0.5540, -0.6018],\n",
      "        [-0.8683,  1.1724,  1.2802,  0.3828,  1.9503],\n",
      "        [ 0.5868,  0.0472, -0.3948, -0.1099,  1.0088],\n",
      "        [-0.1030,  1.7209,  1.7111, -0.3638, -1.4152]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2953, -0.1055,  0.5707, -0.0230, -0.1781],\n",
      "        [ 0.6984,  0.0413,  0.6015, -0.0496,  0.3881],\n",
      "        [ 1.1295,  1.2104,  1.5273,  0.0710,  0.5309],\n",
      "        [ 0.5383, -0.2595,  0.0971, -0.0590,  0.2652]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6145,  1.5101, -0.9422, -0.5540, -0.6018],\n",
      "        [-0.8683,  1.1724,  1.2802,  0.3828,  1.9503],\n",
      "        [ 0.5868,  0.0472, -0.3948, -0.1099,  1.0088],\n",
      "        [-0.1030,  1.7209,  1.7111, -0.3638, -1.4152]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7586],\n",
      "        [ 0.9500],\n",
      "        [ 0.6449],\n",
      "        [-0.6897]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9437,  0.4320,  0.0898,  1.0449,  1.5176],\n",
      "        [-0.0006,  1.5179, -1.5240,  0.0627,  0.2137],\n",
      "        [-0.4780, -1.5692,  1.7504,  1.2678,  0.9955],\n",
      "        [ 0.3267, -1.8035, -0.2442,  1.4235,  2.3619]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4615,  0.0097,  0.6498,  0.6567, -0.1544],\n",
      "        [-0.1162, -0.0163,  0.2923,  0.2533,  0.5010],\n",
      "        [ 0.2904, -0.3558, -0.1437,  0.4120, -0.2520],\n",
      "        [-0.2414,  0.0729, -0.5227,  0.1495,  0.5914]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9437,  0.4320,  0.0898,  1.0449,  1.5176],\n",
      "        [-0.0006,  1.5179, -1.5240,  0.0627,  0.2137],\n",
      "        [-0.4780, -1.5692,  1.7504,  1.2678,  0.9955],\n",
      "        [ 0.3267, -1.8035, -0.2442,  1.4235,  2.3619]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0787],\n",
      "        [-0.3472],\n",
      "        [ 0.4395],\n",
      "        [ 1.5269]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5572, -1.1374,  1.5983,  0.3865, -0.2154],\n",
      "        [-1.4187,  0.3611,  0.7109, -1.1798,  0.3055],\n",
      "        [-0.4996,  3.4124, -1.3701, -0.3589,  0.8803],\n",
      "        [-0.3366, -1.5303, -0.7370,  1.7196,  0.5991]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1932, -0.0748,  0.0659, -0.2590, -0.3893],\n",
      "        [-0.1355,  0.4974,  0.6882,  0.4620,  0.1001],\n",
      "        [ 0.2208,  0.4514, -0.5961, -0.3884,  0.4715],\n",
      "        [-0.4703, -0.9375, -0.4390, -1.2313, -0.9653]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5572, -1.1374,  1.5983,  0.3865, -0.2154],\n",
      "        [-1.4187,  0.3611,  0.7109, -1.1798,  0.3055],\n",
      "        [-0.4996,  3.4124, -1.3701, -0.3589,  0.8803],\n",
      "        [-0.3366, -1.5303, -0.7370,  1.7196,  0.5991]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2818],\n",
      "        [ 0.3467],\n",
      "        [ 2.8011],\n",
      "        [-0.7792]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3910,  0.5595,  2.6196, -0.1172,  0.7488],\n",
      "        [ 0.5473,  0.3604,  1.3279,  0.9990,  0.1589],\n",
      "        [ 1.9098,  0.4410,  0.9957,  0.4087,  0.1582],\n",
      "        [-0.4989,  0.8165,  1.5690,  0.7103, -0.9625]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2939, -0.4514, -0.8706, -0.1505, -0.4347],\n",
      "        [ 0.2429,  0.5285,  0.4304,  0.4222,  0.1856],\n",
      "        [-1.0666, -1.7277, -1.7216, -1.2908, -2.0004],\n",
      "        [ 0.3375, -0.4104, -0.5576, -0.1170, -0.3762]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3910,  0.5595,  2.6196, -0.1172,  0.7488],\n",
      "        [ 0.5473,  0.3604,  1.3279,  0.9990,  0.1589],\n",
      "        [ 1.9098,  0.4410,  0.9957,  0.4087,  0.1582],\n",
      "        [-0.4989,  0.8165,  1.5690,  0.7103, -0.9625]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.7261],\n",
      "        [ 1.3462],\n",
      "        [-5.3571],\n",
      "        [-1.0994]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0761,  0.2600,  0.8224,  0.5920,  1.5230],\n",
      "        [-1.7002,  0.7546, -0.7214,  0.7111, -0.9932],\n",
      "        [ 1.1566,  1.1961,  1.0115,  0.1292,  1.2170],\n",
      "        [ 1.1017, -0.6114, -0.7935, -0.1480, -0.5707]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6063,  1.0092,  1.5863,  1.0077,  1.4957],\n",
      "        [-0.4211, -0.1375,  0.1472,  0.2738, -0.4596],\n",
      "        [ 0.7818,  1.8798,  1.7313,  0.6267,  1.6643],\n",
      "        [ 0.3924, -0.7616, -0.2097, -0.3316,  0.7366]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0761,  0.2600,  0.8224,  0.5920,  1.5230],\n",
      "        [-1.7002,  0.7546, -0.7214,  0.7111, -0.9932],\n",
      "        [ 1.1566,  1.1961,  1.0115,  0.1292,  1.2170],\n",
      "        [ 1.1017, -0.6114, -0.7935, -0.1480, -0.5707]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.7890],\n",
      "        [ 1.1570],\n",
      "        [ 7.0103],\n",
      "        [ 0.6930]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6739, -0.7564,  0.2444,  0.4732,  0.8322],\n",
      "        [ 1.1426, -0.2769,  1.0364, -0.9020,  0.8733],\n",
      "        [ 0.3458,  0.3157, -0.1489,  0.3052,  0.9544],\n",
      "        [ 0.9508,  0.5612,  0.4868, -1.5133,  1.1676]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5343, -1.0080, -0.6624, -0.6223, -1.1157],\n",
      "        [-0.8769, -0.3152,  0.3662, -0.0650, -0.8990],\n",
      "        [-1.5035, -1.6464, -1.9587, -1.2833, -3.0202],\n",
      "        [-0.3703, -0.2729, -0.2779, -0.6292, -0.3782]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6739, -0.7564,  0.2444,  0.4732,  0.8322],\n",
      "        [ 1.1426, -0.2769,  1.0364, -0.9020,  0.8733],\n",
      "        [ 0.3458,  0.3157, -0.1489,  0.3052,  0.9544],\n",
      "        [ 0.9508,  0.5612,  0.4868, -1.5133,  1.1676]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2624],\n",
      "        [-1.2617],\n",
      "        [-4.0221],\n",
      "        [-0.1299]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.6606,  0.5793, -1.8493,  0.1815,  1.0760],\n",
      "        [-0.9512,  2.9309, -0.2928,  1.6388,  0.5312],\n",
      "        [ 0.5581, -1.7308, -0.2368, -0.2802,  1.3490],\n",
      "        [ 0.9381, -0.7627, -1.6662,  0.4576,  1.4305]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2246, -0.1119, -0.5485,  0.2373,  0.2351],\n",
      "        [ 0.0571,  0.0878,  0.4960,  0.9606,  0.4586],\n",
      "        [-0.0385, -0.6206, -0.5005, -0.5780, -0.4980],\n",
      "        [-0.1863, -0.4185, -1.1913, -0.4287, -0.1220]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.6606,  0.5793, -1.8493,  0.1815,  1.0760],\n",
      "        [-0.9512,  2.9309, -0.2928,  1.6388,  0.5312],\n",
      "        [ 0.5581, -1.7308, -0.2368, -0.2802,  1.3490],\n",
      "        [ 0.9381, -0.7627, -1.6662,  0.4576,  1.4305]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0972],\n",
      "        [ 1.8758],\n",
      "        [ 0.6612],\n",
      "        [ 1.7586]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5927,  0.9082, -0.1675, -0.5890, -0.1759],\n",
      "        [-0.5856,  0.3763, -0.5675,  0.2897, -1.7090],\n",
      "        [ 2.6867,  0.8584,  0.1108,  0.0526, -0.2868],\n",
      "        [ 0.2176,  0.7291,  0.3253, -0.4082,  0.5402]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6438, -0.7991, -0.3914, -0.4878, -1.1043],\n",
      "        [-0.3136, -0.2959, -0.5800, -0.5570, -0.9291],\n",
      "        [-0.2850, -0.9417, -0.2961, -0.4008, -1.0488],\n",
      "        [-0.4822, -0.6932, -1.0521, -0.5320, -1.5326]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5927,  0.9082, -0.1675, -0.5890, -0.1759],\n",
      "        [-0.5856,  0.3763, -0.5675,  0.2897, -1.7090],\n",
      "        [ 2.6867,  0.8584,  0.1108,  0.0526, -0.2868],\n",
      "        [ 0.2176,  0.7291,  0.3253, -0.4082,  0.5402]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2030],\n",
      "        [ 1.8279],\n",
      "        [-1.3271],\n",
      "        [-1.5633]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4961, -0.4318,  0.4275, -1.4211, -0.3830],\n",
      "        [-0.3072,  0.9507,  1.9799,  0.9325, -0.5583],\n",
      "        [ 0.1336, -1.2344,  0.1548,  0.7147, -0.8477],\n",
      "        [-0.1000, -0.1305,  0.7935, -0.0819, -1.0141]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0922, -0.4733,  0.0621,  0.0928, -0.0986],\n",
      "        [-1.0119, -1.2764, -0.9977, -1.4788, -2.6944],\n",
      "        [ 0.2744, -0.2597, -0.2078, -0.6314,  0.6733],\n",
      "        [ 0.5796, -0.5758, -0.7507, -0.4090, -0.7074]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4961, -0.4318,  0.4275, -1.4211, -0.3830],\n",
      "        [-0.3072,  0.9507,  1.9799,  0.9325, -0.5583],\n",
      "        [ 0.1336, -1.2344,  0.1548,  0.7147, -0.8477],\n",
      "        [-0.1000, -0.1305,  0.7935, -0.0819, -1.0141]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1826],\n",
      "        [-2.7528],\n",
      "        [-0.6970],\n",
      "        [ 0.1724]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8188, -1.0249,  2.1788, -0.3878, -0.3973],\n",
      "        [-0.0391,  1.4481,  1.5603, -1.1822, -0.8809],\n",
      "        [-0.4174, -1.3554,  0.2724, -0.9467,  1.1094],\n",
      "        [-2.4661, -0.7868,  1.0369,  1.9047, -0.0470]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5118,  0.4464,  0.2962,  0.4097,  0.0060],\n",
      "        [ 0.5044,  0.5911,  1.4670,  0.4140,  1.1334],\n",
      "        [ 0.6380,  0.0200, -0.5598,  0.4794,  0.4785],\n",
      "        [ 0.1848, -1.1796, -0.3655, -0.0961, -0.8838]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8188, -1.0249,  2.1788, -0.3878, -0.3973],\n",
      "        [-0.0391,  1.4481,  1.5603, -1.1822, -0.8809],\n",
      "        [-0.4174, -1.3554,  0.2724, -0.9467,  1.1094],\n",
      "        [-2.4661, -0.7868,  1.0369,  1.9047, -0.0470]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4456],\n",
      "        [ 1.6375],\n",
      "        [-0.3688],\n",
      "        [-0.0482]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5545,  0.4946, -0.0516, -0.0446, -1.2388],\n",
      "        [-0.6544, -0.9949,  0.3299,  0.7580, -0.2731],\n",
      "        [ 0.8576, -0.1095,  0.0932, -1.4523,  0.6997],\n",
      "        [-2.1493, -0.4967, -0.2450, -1.3692, -0.9572]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1307,  0.0045,  0.1125,  0.4197,  0.0639],\n",
      "        [-0.2678, -0.4683, -0.9260, -0.7365, -1.6206],\n",
      "        [ 0.0454, -0.7804, -1.2898, -0.7224, -0.3225],\n",
      "        [-0.1953, -0.7779, -0.7795, -0.3816, -1.1950]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5545,  0.4946, -0.0516, -0.0446, -1.2388],\n",
      "        [-0.6544, -0.9949,  0.3299,  0.7580, -0.2731],\n",
      "        [ 0.8576, -0.1095,  0.0932, -1.4523,  0.6997],\n",
      "        [-2.1493, -0.4967, -0.2450, -1.3692, -0.9572]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0290],\n",
      "        [ 0.2201],\n",
      "        [ 0.8277],\n",
      "        [ 2.6634]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1147, -1.2228, -0.4420, -0.4373, -0.5324],\n",
      "        [-0.7502,  0.5121,  1.5628,  0.8557,  0.7746],\n",
      "        [-0.7888, -1.4002,  2.1997, -0.2067,  1.7138],\n",
      "        [ 0.1347,  0.5263,  0.2024,  1.5911, -0.0734]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5816, -0.1304,  0.3084,  0.2057,  0.4052],\n",
      "        [-0.7157, -0.7841,  0.3297, -0.8030, -0.9637],\n",
      "        [-0.3558, -0.7176, -0.7977, -1.0677, -1.2167],\n",
      "        [-0.8484, -0.5799, -1.7010, -0.8173, -2.5322]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1147, -1.2228, -0.4420, -0.4373, -0.5324],\n",
      "        [-0.7502,  0.5121,  1.5628,  0.8557,  0.7746],\n",
      "        [-0.7888, -1.4002,  2.1997, -0.2067,  1.7138],\n",
      "        [ 0.1347,  0.5263,  0.2024,  1.5911, -0.0734]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3658],\n",
      "        [-0.7829],\n",
      "        [-2.3338],\n",
      "        [-1.8785]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0581, -0.1658, -1.3730, -0.2961, -1.4025],\n",
      "        [ 0.7132, -0.3280,  2.0085,  0.0060,  0.2315],\n",
      "        [-0.3693, -0.0586,  0.3908,  0.0642,  0.6035],\n",
      "        [-0.7662,  0.6189, -1.2988,  0.8378,  0.0193]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5922,  0.3239,  0.8171,  0.0462,  0.2189],\n",
      "        [-0.3210, -0.0187, -0.5096, -0.3394, -0.5454],\n",
      "        [ 0.8167,  0.3878,  1.2365, -0.0897,  1.2969],\n",
      "        [-0.6243, -1.5346, -0.0406, -0.9633, -0.8627]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0581, -0.1658, -1.3730, -0.2961, -1.4025],\n",
      "        [ 0.7132, -0.3280,  2.0085,  0.0060,  0.2315],\n",
      "        [-0.3693, -0.0586,  0.3908,  0.0642,  0.6035],\n",
      "        [-0.7662,  0.6189, -1.2988,  0.8378,  0.0193]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5307],\n",
      "        [-1.3747],\n",
      "        [ 0.9358],\n",
      "        [-1.2425]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5070,  0.7005, -2.6657, -0.0335, -0.5163],\n",
      "        [-0.8509,  0.3881,  0.0542, -0.7705, -0.7513],\n",
      "        [-0.3107,  1.2555,  1.0018, -1.6069,  0.0724],\n",
      "        [-0.6934, -0.5503,  0.2331,  0.4006,  0.6865]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0282,  0.7440,  0.6133,  1.3553,  1.6426],\n",
      "        [ 0.0121, -0.0603,  0.5025, -0.0759,  0.9256],\n",
      "        [-0.1407, -0.7787,  0.3151, -0.4009, -0.9854],\n",
      "        [ 0.3130, -0.0414, -0.6050, -0.6465, -1.4257]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5070,  0.7005, -2.6657, -0.0335, -0.5163],\n",
      "        [-0.8509,  0.3881,  0.0542, -0.7705, -0.7513],\n",
      "        [-0.3107,  1.2555,  1.0018, -1.6069,  0.0724],\n",
      "        [-0.6934, -0.5503,  0.2331,  0.4006,  0.6865]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9928],\n",
      "        [-0.6434],\n",
      "        [-0.0455],\n",
      "        [-1.5731]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4840,  0.0663,  0.8598,  1.8692,  0.7988],\n",
      "        [ 0.0472,  0.4752,  0.9304,  0.8941,  2.1122],\n",
      "        [-0.9418,  1.2262, -2.0601,  0.0419,  0.6051],\n",
      "        [ 0.9208,  1.1447, -1.5868,  0.8151, -0.3125]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2494,  1.7313,  2.1171,  1.6499,  1.7570],\n",
      "        [ 0.2872, -0.1547, -0.1151, -0.3792, -0.2334],\n",
      "        [-0.0746, -0.5085, -0.8632, -0.0411, -1.0125],\n",
      "        [ 0.9274, -0.4341, -0.1604, -0.6420,  0.1464]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4840,  0.0663,  0.8598,  1.8692,  0.7988],\n",
      "        [ 0.0472,  0.4752,  0.9304,  0.8941,  2.1122],\n",
      "        [-0.9418,  1.2262, -2.0601,  0.0419,  0.6051],\n",
      "        [ 0.9208,  1.1447, -1.5868,  0.8151, -0.3125]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 7.0274],\n",
      "        [-0.9991],\n",
      "        [ 0.6106],\n",
      "        [ 0.0425]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4071,  0.1258,  0.1830,  0.4410, -1.2338],\n",
      "        [-1.1322,  1.2911,  0.6408, -0.1551,  3.4276],\n",
      "        [-0.7986,  0.4828,  1.7464, -0.0905,  1.1276],\n",
      "        [-1.4682,  1.2610, -0.2175, -2.8276,  1.3831]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.1439, -1.2540, -1.2194, -1.1885, -2.7650],\n",
      "        [ 0.3218,  0.5484,  0.0383,  0.0876,  0.9250],\n",
      "        [ 0.1267, -0.7923, -0.2943, -1.1585, -1.4472],\n",
      "        [ 0.3035, -1.0753, -0.8235, -0.5635, -1.0935]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4071,  0.1258,  0.1830,  0.4410, -1.2338],\n",
      "        [-1.1322,  1.2911,  0.6408, -0.1551,  3.4276],\n",
      "        [-0.7986,  0.4828,  1.7464, -0.0905,  1.1276],\n",
      "        [-1.4682,  1.2610, -0.2175, -2.8276,  1.3831]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.9721],\n",
      "        [ 3.5251],\n",
      "        [-2.5246],\n",
      "        [-1.5416]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6828, -1.5675,  0.8201,  0.5816,  1.0585],\n",
      "        [-0.8491, -1.0918, -0.9983,  0.5242, -0.6020],\n",
      "        [ 0.5677,  0.3033, -0.6208,  0.4856,  0.6669],\n",
      "        [ 0.6752, -0.4399,  0.8320,  1.1918,  0.1459]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-2.2186, -1.9706, -2.5942, -1.8847, -5.0248],\n",
      "        [-0.5937, -1.1504, -0.8698, -1.1014, -2.4906],\n",
      "        [ 1.1637,  0.9937,  1.0395,  0.0196,  1.3910],\n",
      "        [ 0.8686,  0.2710,  0.1137, -0.3207,  0.4002]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6828, -1.5675,  0.8201,  0.5816,  1.0585],\n",
      "        [-0.8491, -1.0918, -0.9983,  0.5242, -0.6020],\n",
      "        [ 0.5677,  0.3033, -0.6208,  0.4856,  0.6669],\n",
      "        [ 0.6752, -0.4399,  0.8320,  1.1918,  0.1459]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.9388],\n",
      "        [ 3.5507],\n",
      "        [ 1.2538],\n",
      "        [ 0.2380]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.5765,  0.5191,  0.3423,  1.3930,  0.1072],\n",
      "        [-0.5845, -1.3108, -0.4957,  1.5301, -0.6473],\n",
      "        [ 0.0809, -0.9299,  0.6760, -0.9511, -1.2920],\n",
      "        [-1.7619, -1.5864,  1.7931,  0.6154,  0.2351]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4928,  0.5579,  0.3068,  0.1040, -0.3495],\n",
      "        [ 0.0967, -0.0172,  0.5523, -0.0384,  0.0907],\n",
      "        [ 0.4564, -0.2972, -0.8572, -0.7427, -0.8340],\n",
      "        [ 0.4870, -0.6610, -0.7843, -1.1946, -0.8417]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.5765,  0.5191,  0.3423,  1.3930,  0.1072],\n",
      "        [-0.5845, -1.3108, -0.4957,  1.5301, -0.6473],\n",
      "        [ 0.0809, -0.9299,  0.6760, -0.9511, -1.2920],\n",
      "        [-1.7619, -1.5864,  1.7931,  0.6154,  0.2351]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2789],\n",
      "        [-0.4251],\n",
      "        [ 1.5178],\n",
      "        [-2.1488]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0047,  0.9890,  0.4785, -0.7182, -0.0614],\n",
      "        [-0.6075,  0.0368, -0.0006, -1.0035, -0.8681],\n",
      "        [-0.2492, -1.4434,  0.8550, -2.3256,  1.7538],\n",
      "        [ 0.2503,  0.4833,  0.3345, -0.5663,  0.7041]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1213, -0.4649, -0.2716, -0.2897, -0.7594],\n",
      "        [ 0.2693, -0.1584,  0.1375,  0.1812, -0.1093],\n",
      "        [-0.3690, -1.1864, -0.6602, -0.9511, -1.2836],\n",
      "        [ 0.8757, -0.2020,  1.3014, -0.3261,  0.6659]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0047,  0.9890,  0.4785, -0.7182, -0.0614],\n",
      "        [-0.6075,  0.0368, -0.0006, -1.0035, -0.8681],\n",
      "        [-0.2492, -1.4434,  0.8550, -2.3256,  1.7538],\n",
      "        [ 0.2503,  0.4833,  0.3345, -0.5663,  0.7041]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3346],\n",
      "        [-0.2564],\n",
      "        [ 1.2007],\n",
      "        [ 1.2104]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8943, -0.6627,  2.0825, -1.1402, -0.7732],\n",
      "        [ 0.6736, -1.3760,  0.0526,  1.0375,  0.3250],\n",
      "        [ 0.3774, -1.3389,  1.2697,  1.2277, -0.0982],\n",
      "        [-0.0789,  0.1094, -0.4350,  0.6886, -0.1172]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2492,  0.0358,  0.2023,  0.1624, -0.3190],\n",
      "        [ 0.0247, -0.1940, -0.5401, -0.4377, -0.1039],\n",
      "        [-0.4971, -1.8087, -2.0821, -1.0414, -2.4233],\n",
      "        [ 0.0393, -0.8413, -0.8401, -0.7267, -0.4557]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8943, -0.6627,  2.0825, -1.1402, -0.7732],\n",
      "        [ 0.6736, -1.3760,  0.0526,  1.0375,  0.3250],\n",
      "        [ 0.3774, -1.3389,  1.2697,  1.2277, -0.0982],\n",
      "        [-0.0789,  0.1094, -0.4350,  0.6886, -0.1172]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6820],\n",
      "        [-0.2327],\n",
      "        [-1.4502],\n",
      "        [-0.1767]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3509, -0.8479, -0.3139,  0.2288, -2.0521],\n",
      "        [ 0.3053, -2.5550, -1.9154,  0.1444, -0.6759],\n",
      "        [ 0.1888, -0.6526, -0.9237,  1.2469,  1.1031],\n",
      "        [ 0.9770, -0.9116, -0.2137,  0.6711,  0.2784]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1475,  0.1043, -0.2625, -0.2686, -0.0843],\n",
      "        [ 0.2368, -0.0592, -0.0710,  0.1131,  0.5565],\n",
      "        [-0.0141, -0.9640, -0.5544, -1.1248, -1.6347],\n",
      "        [ 1.1909, -1.0974, -0.6108, -1.1044, -0.3038]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3509, -0.8479, -0.3139,  0.2288, -2.0521],\n",
      "        [ 0.3053, -2.5550, -1.9154,  0.1444, -0.6759],\n",
      "        [ 0.1888, -0.6526, -0.9237,  1.2469,  1.1031],\n",
      "        [ 0.9770, -0.9116, -0.2137,  0.6711,  0.2784]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 5.3830e-02],\n",
      "        [-8.1152e-05],\n",
      "        [-2.0670e+00],\n",
      "        [ 1.4686e+00]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1774, -0.0570, -0.0628, -0.7755, -0.4806],\n",
      "        [-1.1790, -1.9842, -1.6548,  1.5378,  1.6963],\n",
      "        [ 0.2336,  0.7753, -1.2175,  2.0805, -1.4581],\n",
      "        [ 0.7280,  0.1387, -0.4050,  0.5981, -0.2219]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0031,  0.3302,  0.2708,  0.3887,  0.4880],\n",
      "        [ 0.8269, -0.3880, -0.0962, -0.4161, -0.2713],\n",
      "        [ 0.6716,  0.1477,  0.3760,  0.7647,  1.1448],\n",
      "        [ 0.1234, -1.7398, -1.2093, -1.0873, -2.0124]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1774, -0.0570, -0.0628, -0.7755, -0.4806],\n",
      "        [-1.1790, -1.9842, -1.6548,  1.5378,  1.6963],\n",
      "        [ 0.2336,  0.7753, -1.2175,  2.0805, -1.4581],\n",
      "        [ 0.7280,  0.1387, -0.4050,  0.5981, -0.2219]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5712],\n",
      "        [-1.1461],\n",
      "        [-0.2647],\n",
      "        [ 0.1345]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.8207,  0.8271, -0.8463, -0.7152,  0.2976],\n",
      "        [-0.0627, -0.9350,  0.0978,  0.2674, -1.0329],\n",
      "        [-0.9265,  0.5055,  1.3322,  0.5809, -2.0059],\n",
      "        [-0.1039,  1.2105, -0.5884,  1.1568,  0.8129]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0657, -0.2276,  0.2344,  0.4569,  0.3458],\n",
      "        [ 0.5091,  0.3518,  0.3919,  0.0983,  0.5551],\n",
      "        [ 0.6155, -0.2836, -0.1466, -0.1011, -0.0308],\n",
      "        [-0.4251, -2.0458, -1.4788, -0.9667, -1.5313]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.8207,  0.8271, -0.8463, -0.7152,  0.2976],\n",
      "        [-0.0627, -0.9350,  0.0978,  0.2674, -1.0329],\n",
      "        [-0.9265,  0.5055,  1.3322,  0.5809, -2.0059],\n",
      "        [-0.1039,  1.2105, -0.5884,  1.1568,  0.8129]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7301],\n",
      "        [-0.8696],\n",
      "        [-0.9058],\n",
      "        [-3.9253]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4430, -0.3165, -1.1758, -0.6995, -0.6871],\n",
      "        [-0.2351, -1.4639,  0.9232,  0.9438, -0.1738],\n",
      "        [ 0.5013,  0.0845,  0.6254,  0.6300,  1.1181],\n",
      "        [ 0.2938,  0.7483,  2.2835,  1.9759, -0.6719]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7391, -0.1328, -0.1492,  0.0959,  0.6924],\n",
      "        [ 0.7996,  0.7354,  0.2107,  0.9394,  1.3262],\n",
      "        [ 0.8791,  0.4223, -0.0721, -0.1209,  0.1326],\n",
      "        [ 1.1664,  1.0413,  1.5663, -0.0242,  1.4309]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4430, -0.3165, -1.1758, -0.6995, -0.6871],\n",
      "        [-0.2351, -1.4639,  0.9232,  0.9438, -0.1738],\n",
      "        [ 0.5013,  0.0845,  0.6254,  0.6300,  1.1181],\n",
      "        [ 0.2938,  0.7483,  2.2835,  1.9759, -0.6719]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0020],\n",
      "        [-0.4139],\n",
      "        [ 0.5034],\n",
      "        [ 3.6893]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3190,  0.9251, -0.2204,  2.5750,  1.2373],\n",
      "        [ 0.1608, -0.7228,  0.4719, -1.6536,  0.8610],\n",
      "        [ 0.6409, -0.3014, -1.4024,  0.2446,  0.1751],\n",
      "        [ 1.0346, -0.8131,  1.0274, -0.9052,  0.5323]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6294, -0.6244,  0.1135,  0.1769,  0.4297],\n",
      "        [-0.0540,  0.7552,  0.6154,  0.8542,  1.3282],\n",
      "        [ 0.5011, -0.7660, -1.2272, -0.7277, -0.9923],\n",
      "        [-0.5674, -1.4933, -1.2483, -1.1617, -2.2785]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3190,  0.9251, -0.2204,  2.5750,  1.2373],\n",
      "        [ 0.1608, -0.7228,  0.4719, -1.6536,  0.8610],\n",
      "        [ 0.6409, -0.3014, -1.4024,  0.2446,  0.1751],\n",
      "        [ 1.0346, -0.8131,  1.0274, -0.9052,  0.5323]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1839],\n",
      "        [-0.5331],\n",
      "        [ 1.9213],\n",
      "        [-0.8167]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5799, -0.2869, -0.7770, -0.4583, -0.4461],\n",
      "        [ 0.7510,  1.1927,  1.0969, -0.8170,  0.2772],\n",
      "        [-0.3891,  0.4276,  0.0794,  0.4641,  0.3777],\n",
      "        [ 0.3102, -0.1068, -0.8887, -0.3708,  0.5161]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0834, -0.1719,  0.4720, -0.1855,  0.0353],\n",
      "        [ 0.3318,  0.1969,  0.2428, -0.0047,  1.4601],\n",
      "        [-0.3918, -2.0779, -1.6450, -1.1421, -2.6649],\n",
      "        [ 0.1766, -0.9898, -0.2700, -1.4436, -0.7180]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5799, -0.2869, -0.7770, -0.4583, -0.4461],\n",
      "        [ 0.7510,  1.1927,  1.0969, -0.8170,  0.2772],\n",
      "        [-0.3891,  0.4276,  0.0794,  0.4641,  0.3777],\n",
      "        [ 0.3102, -0.1068, -0.8887, -0.3708,  0.5161]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2966],\n",
      "        [ 1.1590],\n",
      "        [-2.4032],\n",
      "        [ 0.5651]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7152, -0.9378, -0.6258, -1.7075,  0.7300],\n",
      "        [-0.4369,  0.0661,  0.3209, -0.1644,  2.1940],\n",
      "        [ 1.4899, -0.9359,  0.6177,  0.2885, -0.6954],\n",
      "        [ 0.8999,  0.3043,  0.7972, -0.4051, -0.9002]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0640, -0.2820,  0.1152, -0.1366, -0.1849],\n",
      "        [ 0.2000,  0.0862, -0.2408, -0.0915, -0.4651],\n",
      "        [ 0.4130, -0.1835, -0.5420, -0.1188, -0.4297],\n",
      "        [ 0.1114, -1.1477, -1.2780, -1.1124, -1.8196]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7152, -0.9378, -0.6258, -1.7075,  0.7300],\n",
      "        [-0.4369,  0.0661,  0.3209, -0.1644,  2.1940],\n",
      "        [ 1.4899, -0.9359,  0.6177,  0.2885, -0.6954],\n",
      "        [ 0.8999,  0.3043,  0.7972, -0.4051, -0.9002]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1808],\n",
      "        [-1.1642],\n",
      "        [ 0.7168],\n",
      "        [ 0.8208]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3301, -0.2213,  2.0308, -0.0363,  0.8211],\n",
      "        [ 0.3368,  0.0103,  1.6422,  0.6661, -0.4801],\n",
      "        [-1.8755, -0.1883,  1.3449,  0.6750, -1.1335],\n",
      "        [-0.1629,  0.0214,  1.4564,  1.3701, -1.1534]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1345, -0.3123,  0.1915,  0.4408,  0.1866],\n",
      "        [ 0.3161,  1.0352,  0.6777,  0.4595,  1.3845],\n",
      "        [ 0.2627, -1.0064, -0.6315, -0.5001, -0.8082],\n",
      "        [-0.5135, -0.4630, -1.7913, -1.0825, -0.9609]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3301, -0.2213,  2.0308, -0.0363,  0.8211],\n",
      "        [ 0.3368,  0.0103,  1.6422,  0.6661, -0.4801],\n",
      "        [-1.8755, -0.1883,  1.3449,  0.6750, -1.1335],\n",
      "        [-0.1629,  0.0214,  1.4564,  1.3701, -1.1534]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6397],\n",
      "        [ 0.8714],\n",
      "        [-0.5741],\n",
      "        [-2.9099]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9099,  0.9096,  0.2447, -1.2227,  0.3566],\n",
      "        [-0.9778,  0.1067,  0.0111,  1.9737,  1.9003],\n",
      "        [-0.4085, -0.7832,  0.9277,  0.7226,  0.2334],\n",
      "        [-0.6874,  1.0204,  1.3136, -2.1852, -0.9212]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4928,  0.2914,  0.4779, -0.0392,  0.0447],\n",
      "        [ 0.0678, -0.0102,  0.1406,  0.2430,  0.1078],\n",
      "        [ 0.1705, -0.5234, -0.5087, -0.5854, -0.5006],\n",
      "        [ 0.8072, -0.5227,  0.3992, -0.0084, -0.6029]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9099,  0.9096,  0.2447, -1.2227,  0.3566],\n",
      "        [-0.9778,  0.1067,  0.0111,  1.9737,  1.9003],\n",
      "        [-0.4085, -0.7832,  0.9277,  0.7226,  0.2334],\n",
      "        [-0.6874,  1.0204,  1.3136, -2.1852, -0.9212]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0025],\n",
      "        [ 0.6186],\n",
      "        [-0.6715],\n",
      "        [ 0.0098]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4321, -0.2411,  0.0199, -0.1201,  0.8082],\n",
      "        [ 0.4985,  0.3738,  0.0436, -1.8126, -0.9883],\n",
      "        [ 2.1479,  0.4664, -1.6080,  1.0332,  0.2790],\n",
      "        [-0.8241,  0.1308, -0.2711,  0.3531, -0.2093]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7408, -0.2123,  0.2075, -0.1831, -0.1678],\n",
      "        [ 0.1810, -0.0781, -0.0507, -0.0582, -0.2948],\n",
      "        [ 0.3818, -0.6734, -0.6750, -0.8373, -0.7638],\n",
      "        [ 0.4149, -1.2767, -1.3564, -0.2657, -0.5748]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4321, -0.2411,  0.0199, -0.1201,  0.8082],\n",
      "        [ 0.4985,  0.3738,  0.0436, -1.8126, -0.9883],\n",
      "        [ 2.1479,  0.4664, -1.6080,  1.0332,  0.2790],\n",
      "        [-0.8241,  0.1308, -0.2711,  0.3531, -0.2093]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1191],\n",
      "        [ 0.4557],\n",
      "        [ 0.5132],\n",
      "        [-0.1147]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0942, -0.1721, -1.1525,  0.8030,  0.0419],\n",
      "        [-0.3743,  1.8558,  0.4361,  0.6815,  0.0028],\n",
      "        [ 1.9831,  0.4420, -0.8460,  0.2196,  1.9043],\n",
      "        [-0.9184,  1.4384,  1.9670,  0.1869,  0.6527]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2878,  0.6616,  0.1185,  0.8514,  0.5139],\n",
      "        [ 0.0366,  0.6695,  0.4609,  0.9056,  0.6642],\n",
      "        [ 0.0960, -1.5726, -2.3353, -0.9192, -1.0486],\n",
      "        [ 0.3354, -1.4734, -0.7876, -0.6097, -1.1535]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0942, -0.1721, -1.1525,  0.8030,  0.0419],\n",
      "        [-0.3743,  1.8558,  0.4361,  0.6815,  0.0028],\n",
      "        [ 1.9831,  0.4420, -0.8460,  0.2196,  1.9043],\n",
      "        [-0.9184,  1.4384,  1.9670,  0.1869,  0.6527]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4819],\n",
      "        [ 2.0487],\n",
      "        [-0.7279],\n",
      "        [-4.8433]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7879,  1.3383,  1.2595,  2.2216, -0.2596],\n",
      "        [ 0.7463,  0.8681, -2.4219,  0.5683, -1.3469],\n",
      "        [ 0.2418,  1.9526,  0.1008, -0.7208,  1.5886],\n",
      "        [ 1.2048,  2.1834, -1.6693,  0.7808,  0.9202]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6530,  0.7963,  0.5582, -0.0912, -0.0578],\n",
      "        [-0.1003, -0.5740, -0.3309, -0.7455, -0.7576],\n",
      "        [-0.1448, -0.5106, -0.8068, -0.5946, -0.6300],\n",
      "        [ 1.8370,  1.2431,  1.8473,  1.7870,  2.3927]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7879,  1.3383,  1.2595,  2.2216, -0.2596],\n",
      "        [ 0.7463,  0.8681, -2.4219,  0.5683, -1.3469],\n",
      "        [ 0.2418,  1.9526,  0.1008, -0.7208,  1.5886],\n",
      "        [ 1.2048,  2.1834, -1.6693,  0.7808,  0.9202]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0665],\n",
      "        [ 0.8251],\n",
      "        [-1.6855],\n",
      "        [ 5.4408]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5977,  0.6932, -0.3777,  0.2455, -0.0971],\n",
      "        [ 0.4202,  1.5528,  0.1013,  1.2265,  0.6172],\n",
      "        [ 0.5261, -0.2828, -1.0263,  0.5026, -0.5034],\n",
      "        [-0.1035,  0.1788, -1.1785, -0.0209,  0.8372]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2313,  0.1624, -1.0122, -0.2255, -0.9136],\n",
      "        [-0.4774, -0.5486, -0.5098, -0.6829, -0.8833],\n",
      "        [ 0.4443, -0.2458,  0.5212, -0.2552, -0.0912],\n",
      "        [-0.1710, -1.0614, -1.6906, -1.0338, -2.2254]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5977,  0.6932, -0.3777,  0.2455, -0.0971],\n",
      "        [ 0.4202,  1.5528,  0.1013,  1.2265,  0.6172],\n",
      "        [ 0.5261, -0.2828, -1.0263,  0.5026, -0.5034],\n",
      "        [-0.1035,  0.1788, -1.1785, -0.0209,  0.8372]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3901],\n",
      "        [-2.4869],\n",
      "        [-0.3140],\n",
      "        [-0.0210]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0634,  0.3976, -1.6595,  0.4840,  0.2453],\n",
      "        [ 0.6728, -0.7121,  1.0069,  0.7090, -0.5816],\n",
      "        [-0.4729,  0.7842, -0.7962,  0.0221,  0.1141],\n",
      "        [-1.8502, -1.7061, -0.7786,  1.6192,  0.6800]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1565,  0.5028, -0.0749, -0.1863, -0.1985],\n",
      "        [ 0.7250,  0.7918,  0.7268,  0.6807,  1.0520],\n",
      "        [ 0.5323, -0.7699, -0.8871, -0.2802, -0.3273],\n",
      "        [-0.3000, -0.9308, -1.1571, -0.9805, -1.6925]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0634,  0.3976, -1.6595,  0.4840,  0.2453],\n",
      "        [ 0.6728, -0.7121,  1.0069,  0.7090, -0.5816],\n",
      "        [-0.4729,  0.7842, -0.7962,  0.0221,  0.1141],\n",
      "        [-1.8502, -1.7061, -0.7786,  1.6192,  0.6800]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1953],\n",
      "        [ 0.5264],\n",
      "        [-0.1927],\n",
      "        [ 0.3055]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3423,  0.2836, -0.7630,  0.2183,  0.9310],\n",
      "        [ 0.1126, -0.4959,  2.7961, -0.4335,  0.1056],\n",
      "        [ 0.6667,  0.4216, -0.3575,  1.5363, -0.8624],\n",
      "        [-0.2125,  0.4021, -1.3524, -0.7481, -0.1045]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2008,  0.5368,  0.1961, -0.3237,  0.5099],\n",
      "        [ 0.2261,  1.2677,  0.1962,  0.3862,  0.5434],\n",
      "        [ 0.3882, -1.3882, -1.0950, -0.7639, -0.2039],\n",
      "        [-0.0245, -0.9839, -1.3438, -1.3254, -1.8595]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3423,  0.2836, -0.7630,  0.2183,  0.9310],\n",
      "        [ 0.1126, -0.4959,  2.7961, -0.4335,  0.1056],\n",
      "        [ 0.6667,  0.4216, -0.3575,  1.5363, -0.8624],\n",
      "        [-0.2125,  0.4021, -1.3524, -0.7481, -0.1045]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3379],\n",
      "        [-0.1645],\n",
      "        [-0.9328],\n",
      "        [ 2.6128]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6433, -1.9118, -1.5760, -0.9842,  0.2162],\n",
      "        [-0.4083, -1.0411,  1.6020, -0.0551,  1.3916],\n",
      "        [-0.6713,  0.7580,  0.5275, -0.4292,  0.7686],\n",
      "        [-0.3863,  0.3787, -0.4149,  0.8069,  0.2563]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5327, -0.1470,  0.4623,  0.5742, -0.3287],\n",
      "        [ 0.3392,  0.6008,  0.0949,  0.0640,  0.6908],\n",
      "        [ 0.9803,  0.1181, -0.3753, -0.3504, -0.4896],\n",
      "        [-1.1315, -1.3479, -1.9734, -1.5393, -3.4749]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6433, -1.9118, -1.5760, -0.9842,  0.2162],\n",
      "        [-0.4083, -1.0411,  1.6020, -0.0551,  1.3916],\n",
      "        [-0.6713,  0.7580,  0.5275, -0.4292,  0.7686],\n",
      "        [-0.3863,  0.3787, -0.4149,  0.8069,  0.2563]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7410],\n",
      "        [ 0.3458],\n",
      "        [-0.9925],\n",
      "        [-1.3875]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2014,  0.4913,  0.2390,  0.9046,  1.2365],\n",
      "        [-1.1396, -0.1461, -0.6232,  1.3370,  0.3087],\n",
      "        [ 2.0691, -0.2794,  0.0913, -0.0419, -0.0869],\n",
      "        [-1.6374, -0.0029,  1.0082, -0.3984, -1.4410]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3527,  1.2497,  0.3859,  0.3669,  1.4994],\n",
      "        [ 0.2903,  0.4628,  0.5221, -0.2739,  0.6720],\n",
      "        [ 1.0592, -0.6756, -0.3304, -0.3058,  0.4598],\n",
      "        [-0.3529, -1.8092, -1.6320, -1.5010, -2.0559]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2014,  0.4913,  0.2390,  0.9046,  1.2365],\n",
      "        [-1.1396, -0.1461, -0.6232,  1.3370,  0.3087],\n",
      "        [ 2.0691, -0.2794,  0.0913, -0.0419, -0.0869],\n",
      "        [-1.6374, -0.0029,  1.0082, -0.3984, -1.4410]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.8210],\n",
      "        [-0.8825],\n",
      "        [ 2.3231],\n",
      "        [ 2.4983]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7495,  0.8068,  1.6225, -1.0807,  0.9425],\n",
      "        [-1.6026, -0.9864,  0.8397,  0.6096,  1.0118],\n",
      "        [ 0.0941,  0.7355, -0.5827,  0.2260,  0.2716],\n",
      "        [ 0.0675,  0.5103,  0.4621, -2.3674,  0.1728]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8636, -1.0823, -1.3794, -1.3032, -1.1394],\n",
      "        [ 0.3867,  0.1732,  0.6044,  0.5805,  0.4206],\n",
      "        [-0.6575, -0.8544, -0.9038, -1.1655, -2.0691],\n",
      "        [-0.0062, -0.0353,  0.4033,  0.7441, -0.0171]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7495,  0.8068,  1.6225, -1.0807,  0.9425],\n",
      "        [-1.6026, -0.9864,  0.8397,  0.6096,  1.0118],\n",
      "        [ 0.0941,  0.7355, -0.5827,  0.2260,  0.2716],\n",
      "        [ 0.0675,  0.5103,  0.4621, -2.3674,  0.1728]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1295],\n",
      "        [ 0.4964],\n",
      "        [-0.9889],\n",
      "        [-1.5966]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9158,  0.2915, -2.4318,  0.3154,  0.3062],\n",
      "        [ 0.8245, -0.4487, -0.5866,  0.1892,  0.0010],\n",
      "        [ 0.9386,  1.9958,  0.6195, -0.6343, -0.8564],\n",
      "        [ 0.1418, -0.0332,  0.2259,  1.0379, -0.2348]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2822,  0.4421, -0.1027,  0.1205,  0.1912],\n",
      "        [ 0.3267, -0.0179, -0.0163,  0.3663,  0.6343],\n",
      "        [ 0.3342,  0.3168, -0.6224, -1.3883, -1.2646],\n",
      "        [ 0.2736,  0.9028,  0.8773,  0.6430,  1.4635]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9158,  0.2915, -2.4318,  0.3154,  0.3062],\n",
      "        [ 0.8245, -0.4487, -0.5866,  0.1892,  0.0010],\n",
      "        [ 0.9386,  1.9958,  0.6195, -0.6343, -0.8564],\n",
      "        [ 0.1418, -0.0332,  0.2259,  1.0379, -0.2348]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0655],\n",
      "        [ 0.3569],\n",
      "        [ 2.5239],\n",
      "        [ 0.5308]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3616,  1.2163,  0.8625,  1.4103,  0.2756],\n",
      "        [-2.5310,  0.6060,  0.3376, -1.2752, -0.3049],\n",
      "        [-0.8246,  0.2970,  1.7198, -1.6338,  1.0689],\n",
      "        [-0.1785,  0.9650,  1.3938,  0.8350, -1.2014]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0285, -0.4026, -0.1921,  0.2760, -0.4338],\n",
      "        [-0.6065,  0.0659,  0.2993,  0.5860,  0.7740],\n",
      "        [-0.6963, -1.4601, -2.1045, -1.5131, -3.5674],\n",
      "        [ 0.3466,  0.5439,  0.5412,  0.5359,  0.6550]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3616,  1.2163,  0.8625,  1.4103,  0.2756],\n",
      "        [-2.5310,  0.6060,  0.3376, -1.2752, -0.3049],\n",
      "        [-0.8246,  0.2970,  1.7198, -1.6338,  1.0689],\n",
      "        [-0.1785,  0.9650,  1.3938,  0.8350, -1.2014]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3753],\n",
      "        [ 0.6928],\n",
      "        [-4.8200],\n",
      "        [ 0.8779]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0779,  0.8038,  0.3314,  1.7618,  1.4953],\n",
      "        [-0.4976,  1.8249, -0.5069, -1.1916, -1.1559],\n",
      "        [ 0.0617, -0.2506,  0.8572,  0.1014, -0.6007],\n",
      "        [ 0.4729, -0.6647,  0.1321,  0.4400,  0.8077]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0605, -0.5051, -0.1441,  0.1905, -0.5493],\n",
      "        [ 0.2336,  0.4630,  0.2489,  0.1474,  0.2753],\n",
      "        [ 0.5919, -0.1388, -0.2983, -0.1134,  0.6878],\n",
      "        [ 0.0295,  0.5853,  0.4527,  0.9712,  0.4351]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0779,  0.8038,  0.3314,  1.7618,  1.4953],\n",
      "        [-0.4976,  1.8249, -0.5069, -1.1916, -1.1559],\n",
      "        [ 0.0617, -0.2506,  0.8572,  0.1014, -0.6007],\n",
      "        [ 0.4729, -0.6647,  0.1321,  0.4400,  0.8077]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9348],\n",
      "        [ 0.1087],\n",
      "        [-0.6090],\n",
      "        [ 0.4634]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.0326,  1.5641,  1.4962,  0.7147,  0.5762],\n",
      "        [ 0.6137,  1.4554,  0.2446,  0.6172,  1.2467],\n",
      "        [-0.4351,  1.8839, -1.1221, -0.9611, -1.5939],\n",
      "        [-0.0802, -0.5262,  0.3497,  0.6728,  1.3787]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0790,  0.0950,  0.3347, -0.7343, -0.0606],\n",
      "        [ 0.1781,  0.1545,  0.2874,  0.3514,  0.7465],\n",
      "        [ 1.6921, -0.0827,  0.1454, -0.2965,  1.2016],\n",
      "        [ 0.0380,  0.2021,  0.8912, -0.5133, -0.1438]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.0326,  1.5641,  1.4962,  0.7147,  0.5762],\n",
      "        [ 0.6137,  1.4554,  0.2446,  0.6172,  1.2467],\n",
      "        [-0.4351,  1.8839, -1.1221, -0.9611, -1.5939],\n",
      "        [-0.0802, -0.5262,  0.3497,  0.6728,  1.3787]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0710],\n",
      "        [ 1.5519],\n",
      "        [-2.6857],\n",
      "        [-0.3414]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3952,  0.4864,  0.5725,  0.0818,  1.8699],\n",
      "        [-0.3783,  0.3110, -1.0549, -0.0172, -2.7091],\n",
      "        [-0.4656,  1.5787, -0.5341,  0.9354,  1.7079],\n",
      "        [ 1.0605, -0.6849, -0.8692,  1.6364, -0.7530]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6015, -0.8120, -0.6018, -0.9929, -1.0935],\n",
      "        [-0.5641, -0.5199, -0.5687, -0.1179, -0.2453],\n",
      "        [ 2.2832,  0.8967,  0.8816,  1.3079,  2.1929],\n",
      "        [ 0.6437,  0.3751,  0.5978,  0.1633,  0.7525]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3952,  0.4864,  0.5725,  0.0818,  1.8699],\n",
      "        [-0.3783,  0.3110, -1.0549, -0.0172, -2.7091],\n",
      "        [-0.4656,  1.5787, -0.5341,  0.9354,  1.7079],\n",
      "        [ 1.0605, -0.6849, -0.8692,  1.6364, -0.7530]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.7047],\n",
      "        [ 1.3183],\n",
      "        [ 4.8503],\n",
      "        [-0.3931]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0626,  0.2807,  1.6228, -0.1462,  0.3324],\n",
      "        [-2.3772,  0.0682,  0.5723,  2.3504,  1.0887],\n",
      "        [ 0.3075,  1.6624,  1.5089, -0.0540,  0.9714],\n",
      "        [-0.8288,  0.9406,  1.5709, -0.7931,  0.7763]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1320,  2.2025,  2.0218,  1.2873,  1.7084],\n",
      "        [-0.8183, -0.4422, -0.4383, -1.0431, -2.2102],\n",
      "        [-1.6537, -1.6283, -2.0046, -0.6297, -2.5858],\n",
      "        [ 0.2035, -0.0103,  0.1414, -0.1305,  0.6437]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0626,  0.2807,  1.6228, -0.1462,  0.3324],\n",
      "        [-2.3772,  0.0682,  0.5723,  2.3504,  1.0887],\n",
      "        [ 0.3075,  1.6624,  1.5089, -0.0540,  0.9714],\n",
      "        [-0.8288,  0.9406,  1.5709, -0.7931,  0.7763]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.0759],\n",
      "        [-3.1935],\n",
      "        [-8.7181],\n",
      "        [ 0.6471]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5600,  1.1646, -1.4602, -0.2170,  0.9617],\n",
      "        [-0.5595,  1.3845, -1.5447,  1.3559,  0.4695],\n",
      "        [-0.2864,  0.0988, -0.8680, -0.9694,  0.2917],\n",
      "        [-0.3241,  0.0019, -0.7787, -0.3826,  0.2808]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5577, -0.8113,  0.0249, -0.5886,  0.1670],\n",
      "        [ 0.0933,  0.9816,  0.8567,  0.4428,  1.1270],\n",
      "        [-1.0895, -1.9033, -1.4543, -1.1110, -3.3351],\n",
      "        [ 0.5598,  0.0303,  0.4089, -0.2049,  0.0811]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5600,  1.1646, -1.4602, -0.2170,  0.9617],\n",
      "        [-0.5595,  1.3845, -1.5447,  1.3559,  0.4695],\n",
      "        [-0.2864,  0.0988, -0.8680, -0.9694,  0.2917],\n",
      "        [-0.3241,  0.0019, -0.7787, -0.3826,  0.2808]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0052],\n",
      "        [ 1.1130],\n",
      "        [ 1.4904],\n",
      "        [-0.3986]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7639, -1.2945, -1.6792,  1.0538,  0.4845],\n",
      "        [-0.3628,  0.8997, -1.0674,  1.2316,  2.4501],\n",
      "        [ 0.1103, -0.9426,  2.3141, -0.9205,  0.9203],\n",
      "        [-0.9665, -0.2214,  1.2765,  0.2502,  0.3792]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9326,  0.7164,  0.2620,  0.0802,  1.2135],\n",
      "        [ 0.2006,  0.6755,  0.5118,  0.2896, -0.0226],\n",
      "        [-1.1577, -1.7188, -2.9755, -2.6891, -4.4401],\n",
      "        [ 0.1495,  0.4393,  0.2017,  0.1946,  0.1169]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7639, -1.2945, -1.6792,  1.0538,  0.4845],\n",
      "        [-0.3628,  0.8997, -1.0674,  1.2316,  2.4501],\n",
      "        [ 0.1103, -0.9426,  2.3141, -0.9205,  0.9203],\n",
      "        [-0.9665, -0.2214,  1.2765,  0.2502,  0.3792]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0176],\n",
      "        [ 0.2901],\n",
      "        [-7.0038],\n",
      "        [ 0.1088]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.3601, -2.0738, -0.6512, -0.6920,  1.0853],\n",
      "        [-0.5204, -0.2064,  0.5426, -0.3165, -0.5395],\n",
      "        [-0.7001, -0.6106, -0.1159,  0.5885,  0.1542],\n",
      "        [ 0.4178, -1.8648, -1.9419,  0.6105, -1.4868]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6016,  0.0451,  0.4647, -0.4093, -0.1506],\n",
      "        [-0.1349,  0.3954,  0.1653,  0.4819,  0.2542],\n",
      "        [-0.4185, -0.2355, -0.5543, -0.5612,  0.2996],\n",
      "        [ 0.4136, -0.3357,  0.8797,  0.5557,  0.2325]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.3601, -2.0738, -0.6512, -0.6920,  1.0853],\n",
      "        [-0.5204, -0.2064,  0.5426, -0.3165, -0.5395],\n",
      "        [-0.7001, -0.6106, -0.1159,  0.5885,  0.1542],\n",
      "        [ 0.4178, -1.8648, -1.9419,  0.6105, -1.4868]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5419],\n",
      "        [-0.2114],\n",
      "        [ 0.2170],\n",
      "        [-0.9158]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9247,  0.8784, -0.6749,  1.4512, -0.7659],\n",
      "        [-0.8342,  1.0219, -0.1775,  1.4546, -0.1880],\n",
      "        [-0.6303, -2.1937,  0.8586,  1.3259, -0.8388],\n",
      "        [ 0.3565, -0.2388,  0.6752,  1.2391,  0.4381]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2487,  0.1302, -0.5022, -0.2617, -0.5453],\n",
      "        [-0.3756,  0.2471, -0.3179,  0.2274,  0.4833],\n",
      "        [ 0.2894, -0.1786,  0.2205, -0.0487,  0.5107],\n",
      "        [ 0.0827,  0.5592,  0.7657,  0.5103,  0.8565]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9247,  0.8784, -0.6749,  1.4512, -0.7659],\n",
      "        [-0.8342,  1.0219, -0.1775,  1.4546, -0.1880],\n",
      "        [-0.6303, -2.1937,  0.8586,  1.3259, -0.8388],\n",
      "        [ 0.3565, -0.2388,  0.6752,  1.2391,  0.4381]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2611],\n",
      "        [ 0.8621],\n",
      "        [-0.0943],\n",
      "        [ 1.4204]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1204, -0.2521,  0.1894,  0.6590,  1.1278],\n",
      "        [ 1.3818, -0.7007,  1.2090, -1.2585, -0.0921],\n",
      "        [ 0.6465, -0.7448, -0.1173, -1.0789, -0.6950],\n",
      "        [-1.0164,  1.7096,  2.3837,  0.0452, -0.6140]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2299,  0.5844,  0.5266,  0.0948,  0.2684],\n",
      "        [-0.1552,  0.1638,  0.0442,  0.0831, -0.7543],\n",
      "        [ 0.0057,  0.3322, -0.3088, -0.1660,  0.9851],\n",
      "        [-0.2533, -0.6335,  0.5126, -0.1525, -0.3953]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1204, -0.2521,  0.1894,  0.6590,  1.1278],\n",
      "        [ 1.3818, -0.7007,  1.2090, -1.2585, -0.0921],\n",
      "        [ 0.6465, -0.7448, -0.1173, -1.0789, -0.6950],\n",
      "        [-1.0164,  1.7096,  2.3837,  0.0452, -0.6140]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0599],\n",
      "        [-0.3109],\n",
      "        [-0.7131],\n",
      "        [ 0.6321]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5711, -0.4288, -1.6943,  0.4495, -0.3062],\n",
      "        [-1.4683,  0.9809,  1.4929,  0.3929,  0.3657],\n",
      "        [-0.9694, -1.8374, -0.3082,  1.7731,  0.9105],\n",
      "        [-0.5208,  1.9711, -0.0207,  0.2842,  1.3480]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2721,  0.7377, -0.4202,  0.3214,  0.4254],\n",
      "        [ 0.2623, -0.2474, -0.5170,  0.2465,  0.2692],\n",
      "        [ 0.2341, -0.6582,  0.8178, -0.1601,  0.5115],\n",
      "        [-0.0436,  1.0747,  0.6414,  0.9126, -0.3660]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5711, -0.4288, -1.6943,  0.4495, -0.3062],\n",
      "        [-1.4683,  0.9809,  1.4929,  0.3929,  0.3657],\n",
      "        [-0.9694, -1.8374, -0.3082,  1.7731,  0.9105],\n",
      "        [-0.5208,  1.9711, -0.0207,  0.2842,  1.3480]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5652],\n",
      "        [-1.2043],\n",
      "        [ 0.9124],\n",
      "        [ 1.8939]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6614, -1.4996,  2.5712, -0.9111,  1.7449],\n",
      "        [-0.9997,  1.0249,  3.2844,  0.9856,  0.4423],\n",
      "        [ 1.2304,  0.6009, -0.1785,  0.5398,  0.3093],\n",
      "        [-0.6907,  0.9166, -1.5504, -1.8227,  1.6948]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0242,  0.6110,  0.0989,  0.3705,  0.7271],\n",
      "        [ 0.4071,  0.2645,  0.5280,  0.7636,  0.7060],\n",
      "        [-0.0834, -0.7193, -0.1236,  0.5155, -0.6162],\n",
      "        [-0.7636, -0.6499, -1.4058, -0.6821, -1.8286]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6614, -1.4996,  2.5712, -0.9111,  1.7449],\n",
      "        [-0.9997,  1.0249,  3.2844,  0.9856,  0.4423],\n",
      "        [ 1.2304,  0.6009, -0.1785,  0.5398,  0.3093],\n",
      "        [-0.6907,  0.9166, -1.5504, -1.8227,  1.6948]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.2292],\n",
      "        [ 2.6631],\n",
      "        [-0.4251],\n",
      "        [ 0.2554]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2156,  0.1325,  0.5598, -1.8624, -1.1827],\n",
      "        [-1.7878,  0.9838,  0.5486,  0.1292, -1.9114],\n",
      "        [-1.0005, -0.0569, -0.7547,  0.3911,  1.0489],\n",
      "        [-0.2777, -0.1553,  0.8344, -1.2434,  0.5753]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1226,  0.9681,  0.6340,  0.4902,  0.6884],\n",
      "        [-1.0405, -0.6906, -0.8237, -1.1997, -1.2091],\n",
      "        [ 0.2941,  0.3339, -0.2269,  0.6380, -0.1373],\n",
      "        [-0.8782, -0.4016, -0.0108, -0.4523, -1.2443]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2156,  0.1325,  0.5598, -1.8624, -1.1827],\n",
      "        [-1.7878,  0.9838,  0.5486,  0.1292, -1.9114],\n",
      "        [-1.0005, -0.0569, -0.7547,  0.3911,  1.0489],\n",
      "        [-0.2777, -0.1553,  0.8344, -1.2434,  0.5753]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3930],\n",
      "        [ 2.8849],\n",
      "        [-0.0365],\n",
      "        [ 0.1439]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0255,  0.2568, -1.9853, -0.6487,  1.1585],\n",
      "        [-0.5714, -0.6896, -1.6594,  0.1595, -1.6452],\n",
      "        [ 0.6506,  0.1278, -0.0243,  1.5476, -0.4120],\n",
      "        [-0.0981,  0.3854,  0.9598,  1.5195, -1.0505]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4525,  1.1103,  1.2453,  0.5718,  1.7096],\n",
      "        [-1.1638, -1.6896, -1.4998, -1.6670, -3.2845],\n",
      "        [ 0.0347, -0.3232, -0.2528, -0.1696, -0.5394],\n",
      "        [-0.5835, -0.6413, -0.4924, -0.6083, -1.3697]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.0255,  0.2568, -1.9853, -0.6487,  1.1585],\n",
      "        [-0.5714, -0.6896, -1.6594,  0.1595, -1.6452],\n",
      "        [ 0.6506,  0.1278, -0.0243,  1.5476, -0.4120],\n",
      "        [-0.0981,  0.3854,  0.9598,  1.5195, -1.0505]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0414],\n",
      "        [ 9.4567],\n",
      "        [-0.0528],\n",
      "        [-0.1479]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7605, -0.3556,  1.0003,  0.7373, -0.7345],\n",
      "        [-0.3455, -0.7918,  0.2886, -0.0271,  1.2919],\n",
      "        [-0.7841,  0.8715,  0.6434, -0.1410,  0.1187],\n",
      "        [ 0.5283, -0.0073,  0.1369,  0.3300,  1.2463]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2250,  1.6232,  1.1096,  1.2145,  1.3040],\n",
      "        [ 0.0711,  0.2884,  0.0307, -0.2869,  0.4122],\n",
      "        [ 0.0179, -0.4081, -0.4292, -0.0378, -0.3997],\n",
      "        [-0.6698, -0.6456, -0.3029,  0.3371,  0.2409]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7605, -0.3556,  1.0003,  0.7373, -0.7345],\n",
      "        [-0.3455, -0.7918,  0.2886, -0.0271,  1.2919],\n",
      "        [-0.7841,  0.8715,  0.6434, -0.1410,  0.1187],\n",
      "        [ 0.5283, -0.0073,  0.1369,  0.3300,  1.2463]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8666],\n",
      "        [ 0.2961],\n",
      "        [-0.6880],\n",
      "        [ 0.0209]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7348, -2.4511,  0.4206,  0.0521,  0.7814],\n",
      "        [ 0.0038,  1.2314, -0.8964,  3.0788, -0.2884],\n",
      "        [ 1.2189,  0.5192, -0.9837, -2.3198, -0.6690],\n",
      "        [-0.6281,  0.7969, -0.1989, -0.0337, -1.6085]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6335,  0.5539,  0.9366,  0.7986,  1.4251],\n",
      "        [-0.0473, -0.0276,  0.8191,  0.4187,  0.2820],\n",
      "        [ 0.3898, -0.2910, -0.5409,  0.0954,  0.3559],\n",
      "        [-0.2264, -1.4354,  0.1793, -0.2870,  0.2186]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7348, -2.4511,  0.4206,  0.0521,  0.7814],\n",
      "        [ 0.0038,  1.2314, -0.8964,  3.0788, -0.2884],\n",
      "        [ 1.2189,  0.5192, -0.9837, -2.3198, -0.6690],\n",
      "        [-0.6281,  0.7969, -0.1989, -0.0337, -1.6085]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6571],\n",
      "        [ 0.4392],\n",
      "        [ 0.3967],\n",
      "        [-1.3792]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2507,  0.1406,  0.0841,  0.5777, -0.7623],\n",
      "        [ 0.0458,  1.2083,  0.7484,  1.1069,  0.8323],\n",
      "        [-0.3356,  0.3528, -0.1753, -1.1692,  0.8250],\n",
      "        [ 1.6565, -0.4314,  0.9142,  1.5376,  0.9713]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6099,  1.3142,  0.2904,  0.5457,  1.1681],\n",
      "        [-0.1364, -0.5169,  0.3408, -0.0314, -0.6638],\n",
      "        [ 0.1851,  0.7860, -0.7296, -0.3148,  0.2359],\n",
      "        [ 1.0531,  0.1051,  1.0086,  0.0057,  1.7030]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2507,  0.1406,  0.0841,  0.5777, -0.7623],\n",
      "        [ 0.0458,  1.2083,  0.7484,  1.1069,  0.8323],\n",
      "        [-0.3356,  0.3528, -0.1753, -1.1692,  0.8250],\n",
      "        [ 1.6565, -0.4314,  0.9142,  1.5376,  0.9713]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5188],\n",
      "        [-0.9630],\n",
      "        [ 0.9057],\n",
      "        [ 4.2841]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2947,  0.0796, -0.0398,  0.7196,  0.5532],\n",
      "        [ 0.7999,  1.4066,  0.1166,  1.2452,  1.2179],\n",
      "        [-0.3542, -0.3471,  2.3022, -0.0340,  0.6410],\n",
      "        [ 2.1901,  1.4041,  1.3312, -1.9420, -1.1760]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0118,  0.0905,  1.2785,  1.1599,  1.0875],\n",
      "        [ 0.1967,  0.3877,  0.2300,  0.1477,  1.1828],\n",
      "        [ 0.0583, -0.0647, -0.1424, -0.1026, -0.5769],\n",
      "        [-1.1860, -0.4862, -1.3318, -1.7640, -2.7912]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2947,  0.0796, -0.0398,  0.7196,  0.5532],\n",
      "        [ 0.7999,  1.4066,  0.1166,  1.2452,  1.2179],\n",
      "        [-0.3542, -0.3471,  2.3022, -0.0340,  0.6410],\n",
      "        [ 2.1901,  1.4041,  1.3312, -1.9420, -1.1760]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3891],\n",
      "        [ 2.3538],\n",
      "        [-0.6924],\n",
      "        [ 1.6550]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.0592,  1.1782, -0.4525,  0.1431,  0.1467],\n",
      "        [ 0.3889,  0.7947, -0.7453, -0.7760,  0.9227],\n",
      "        [ 0.7906,  1.1676,  2.2433,  1.0172,  2.6909],\n",
      "        [-0.7095,  2.2501,  1.2198,  1.4933,  1.2717]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0324,  0.4174,  0.4450,  0.2588,  0.3284],\n",
      "        [-0.8410, -0.9305, -0.5300, -0.5059, -1.0513],\n",
      "        [-0.0705,  0.1803,  0.2319, -0.0532, -0.3488],\n",
      "        [ 0.3942,  0.0220,  0.0137,  0.2441, -0.3856]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.0592,  1.1782, -0.4525,  0.1431,  0.1467],\n",
      "        [ 0.3889,  0.7947, -0.7453, -0.7760,  0.9227],\n",
      "        [ 0.7906,  1.1676,  2.2433,  1.0172,  2.6909],\n",
      "        [-0.7095,  2.2501,  1.2198,  1.4933,  1.2717]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4099],\n",
      "        [-1.2491],\n",
      "        [-0.3177],\n",
      "        [-0.3395]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4690,  0.1500,  2.5263, -1.9449, -0.0111],\n",
      "        [ 0.8631, -0.1279, -1.1770, -0.9995,  0.7444],\n",
      "        [-0.0028, -1.9613, -1.0849, -0.4858,  0.0004],\n",
      "        [-0.5926, -0.2255, -0.1709,  2.4847,  0.6599]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1186,  0.3566,  0.7123,  1.2627,  1.0952],\n",
      "        [-0.3556, -0.6578, -0.0630,  0.0880, -0.4556],\n",
      "        [ 0.3120,  0.2873, -0.0055,  0.0356,  0.0544],\n",
      "        [ 0.3609, -0.1800,  0.0624, -0.2504, -0.4942]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4690,  0.1500,  2.5263, -1.9449, -0.0111],\n",
      "        [ 0.8631, -0.1279, -1.1770, -0.9995,  0.7444],\n",
      "        [-0.0028, -1.9613, -1.0849, -0.4858,  0.0004],\n",
      "        [-0.5926, -0.2255, -0.1709,  2.4847,  0.6599]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7892],\n",
      "        [-0.5758],\n",
      "        [-0.5757],\n",
      "        [-1.1323]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2431,  2.4741, -0.0430,  0.0885,  0.7833],\n",
      "        [ 1.3210, -0.1067,  1.5705,  0.9089,  0.5245],\n",
      "        [ 0.8112, -1.9973,  0.9162, -1.7515, -0.8458],\n",
      "        [-1.1389, -0.5375,  0.9283, -0.0098,  0.6238]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 5.9521e-01,  8.6133e-01,  1.4450e+00,  1.2695e+00,  1.1652e+00],\n",
      "        [ 9.2297e-01, -5.8357e-01,  1.4645e-01, -5.6377e-01, -1.5126e-01],\n",
      "        [ 6.0816e-01, -6.2168e-05,  4.1734e-01, -3.3376e-01, -2.1122e-01],\n",
      "        [ 5.6969e-01,  6.3287e-01,  3.8166e-01,  4.8382e-01,  4.2030e-01]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2431,  2.4741, -0.0430,  0.0885,  0.7833],\n",
      "        [ 1.3210, -0.1067,  1.5705,  0.9089,  0.5245],\n",
      "        [ 0.8112, -1.9973,  0.9162, -1.7515, -0.8458],\n",
      "        [-1.1389, -0.5375,  0.9283, -0.0098,  0.6238]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.2385],\n",
      "        [ 0.9197],\n",
      "        [ 1.6391],\n",
      "        [-0.3773]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3018, -1.1140,  0.4317,  0.5902, -0.7107],\n",
      "        [ 1.6294, -0.7461, -0.5510, -0.3095, -0.2974],\n",
      "        [ 0.1145,  0.1473, -1.6060,  1.3186, -0.0899],\n",
      "        [-0.7932,  1.8785,  0.6582, -0.4897, -1.1095]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0732, -0.6247, -0.2794, -1.0798, -0.9754],\n",
      "        [-0.4117, -0.6680, -0.2971, -0.3313, -0.5707],\n",
      "        [-0.2134, -1.3046, -0.2229, -0.2394, -1.2413],\n",
      "        [ 0.5021, -0.1455,  0.6919, -0.2830, -0.1732]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3018, -1.1140,  0.4317,  0.5902, -0.7107],\n",
      "        [ 1.6294, -0.7461, -0.5510, -0.3095, -0.2974],\n",
      "        [ 0.1145,  0.1473, -1.6060,  1.3186, -0.0899],\n",
      "        [-0.7932,  1.8785,  0.6582, -0.4897, -1.1095]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.6092],\n",
      "        [ 0.2635],\n",
      "        [-0.0627],\n",
      "        [ 0.1144]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4747, -0.7775,  1.6341,  0.1343, -0.2967],\n",
      "        [-1.2202,  1.6321,  0.3696,  0.4469,  0.3539],\n",
      "        [-2.3868, -0.1132, -0.0231,  0.0637,  1.1175],\n",
      "        [ 0.0943, -0.4277, -0.2602, -1.6829,  0.1708]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9478, -0.1982, -0.3776, -1.2232, -0.6482],\n",
      "        [-0.3859,  0.1988,  0.1304, -0.3196,  0.1060],\n",
      "        [-0.3420, -1.2034, -0.6265, -0.2720, -1.2084],\n",
      "        [ 0.4958,  0.7898, -0.1455, -0.4442,  0.6365]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4747, -0.7775,  1.6341,  0.1343, -0.2967],\n",
      "        [-1.2202,  1.6321,  0.3696,  0.4469,  0.3539],\n",
      "        [-2.3868, -0.1132, -0.0231,  0.0637,  1.1175],\n",
      "        [ 0.0943, -0.4277, -0.2602, -1.6829,  0.1708]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0151],\n",
      "        [ 0.7382],\n",
      "        [-0.4005],\n",
      "        [ 0.6030]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1543,  2.0088,  1.2374,  0.4186,  1.0710],\n",
      "        [ 0.1101, -0.9241,  0.0449, -0.4288, -1.0979],\n",
      "        [-0.6316,  1.0222,  0.5097, -0.3877, -1.1518],\n",
      "        [-1.5417,  1.2536, -1.7180,  0.7708, -1.3322]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6278, -0.0323, -0.4093, -0.4375, -1.0780],\n",
      "        [-0.5301, -0.7922,  0.6112, -0.5000, -0.7830],\n",
      "        [-0.3065, -0.9518, -1.1463, -0.2766,  0.2720],\n",
      "        [-0.0238, -0.4142, -0.7034, -0.3874, -0.7446]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1543,  2.0088,  1.2374,  0.4186,  1.0710],\n",
      "        [ 0.1101, -0.9241,  0.0449, -0.4288, -1.0979],\n",
      "        [-0.6316,  1.0222,  0.5097, -0.3877, -1.1518],\n",
      "        [-1.5417,  1.2536, -1.7180,  0.7708, -1.3322]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.8122],\n",
      "        [ 1.7751],\n",
      "        [-1.5695],\n",
      "        [ 1.4191]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4519,  0.7329, -1.3028, -0.8287, -1.2809],\n",
      "        [-0.0879, -0.5854,  0.5827, -0.6321,  1.8238],\n",
      "        [ 0.1494,  0.2564,  0.5222,  1.3277,  0.8539],\n",
      "        [ 0.9139,  0.7136,  0.6761,  0.4942, -0.1647]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4639,  0.9484,  0.0568,  0.4643,  0.5447],\n",
      "        [-0.9796, -0.3239, -0.6641, -1.4331, -1.5254],\n",
      "        [ 0.4807,  0.8216,  1.1337,  1.2623,  0.6484],\n",
      "        [-0.5715, -0.7261, -0.1417, -1.1377, -1.8021]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4519,  0.7329, -1.3028, -0.8287, -1.2809],\n",
      "        [-0.0879, -0.5854,  0.5827, -0.6321,  1.8238],\n",
      "        [ 0.1494,  0.2564,  0.5222,  1.3277,  0.8539],\n",
      "        [ 0.9139,  0.7136,  0.6761,  0.4942, -0.1647]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2519],\n",
      "        [-1.9873],\n",
      "        [ 3.1042],\n",
      "        [-1.4016]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.0355, -0.4398, -1.9464, -1.4560,  0.4727],\n",
      "        [-0.4631, -0.7200,  1.2179,  0.7280,  0.3038],\n",
      "        [-1.9605,  0.7402, -0.1750,  2.1766, -1.5120],\n",
      "        [-0.0685, -0.6023,  0.4264, -0.0830,  0.4524]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3388,  0.4938,  0.5257,  0.1415,  1.1045],\n",
      "        [ 0.0174, -0.0758, -0.1470,  0.0381,  0.4531],\n",
      "        [-0.7813, -1.2570, -0.7879, -1.3909, -2.3043],\n",
      "        [-0.0195, -0.1676,  0.3741, -0.3894, -0.9862]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.0355, -0.4398, -1.9464, -1.4560,  0.4727],\n",
      "        [-0.4631, -0.7200,  1.2179,  0.7280,  0.3038],\n",
      "        [-1.9605,  0.7402, -0.1750,  2.1766, -1.5120],\n",
      "        [-0.0685, -0.6023,  0.4264, -0.0830,  0.4524]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2751],\n",
      "        [ 0.0328],\n",
      "        [ 1.1956],\n",
      "        [-0.1520]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.2581,  1.2047, -1.0562,  0.7269, -0.6405],\n",
      "        [ 0.0756,  0.6708,  2.1541, -0.7780, -0.3483],\n",
      "        [ 0.2708,  0.3753,  0.2366,  1.9045, -0.9447],\n",
      "        [ 0.7754, -0.3683,  1.7003,  1.1220,  0.1919]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6545,  1.2681,  1.1944,  0.9397,  1.4611],\n",
      "        [ 0.0485, -0.3421, -0.6098, -0.7386, -0.4894],\n",
      "        [-0.8880, -1.7288, -1.7570, -1.4814, -2.5265],\n",
      "        [ 0.4665,  0.1510, -0.0491, -0.3743, -0.2734]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.2581,  1.2047, -1.0562,  0.7269, -0.6405],\n",
      "        [ 0.0756,  0.6708,  2.1541, -0.7780, -0.3483],\n",
      "        [ 0.2708,  0.3753,  0.2366,  1.9045, -0.9447],\n",
      "        [ 0.7754, -0.3683,  1.7003,  1.1220,  0.1919]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8368],\n",
      "        [-0.7942],\n",
      "        [-1.7395],\n",
      "        [-0.2499]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0961, -0.5924,  1.0228,  1.4008, -0.4707],\n",
      "        [ 0.2331,  0.8732,  0.2124, -0.3770, -0.4525],\n",
      "        [ 0.0468, -0.1625,  1.4426, -0.1295,  1.5240],\n",
      "        [ 0.5915,  1.5862,  0.7782, -0.5882, -0.9277]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4535,  0.6938,  0.2692,  1.6780,  0.9924],\n",
      "        [-0.0207, -0.2935, -0.2902,  0.2246,  0.1696],\n",
      "        [-1.0001, -1.1840, -1.1228, -0.8142, -1.3510],\n",
      "        [ 0.8074, -0.2444,  0.5369, -0.9078, -0.1156]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0961, -0.5924,  1.0228,  1.4008, -0.4707],\n",
      "        [ 0.2331,  0.8732,  0.2124, -0.3770, -0.4525],\n",
      "        [ 0.0468, -0.1625,  1.4426, -0.1295,  1.5240],\n",
      "        [ 0.5915,  1.5862,  0.7782, -0.5882, -0.9277]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7041],\n",
      "        [-0.4842],\n",
      "        [-3.4276],\n",
      "        [ 1.1490]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1047, -0.6944,  0.7791,  2.2679,  1.4638],\n",
      "        [-0.3339,  0.8696,  1.1381, -0.2066,  0.7418],\n",
      "        [ 0.6567, -1.0988,  0.7831, -1.3145,  1.6715],\n",
      "        [ 1.6443,  0.0206, -0.2347, -0.5119,  0.9364]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1833,  0.2077,  0.4019, -0.2684,  0.0518],\n",
      "        [-0.6417, -0.8126, -0.4053, -0.3685, -0.3613],\n",
      "        [ 0.7655,  0.8345,  0.5146,  0.6795,  1.6563],\n",
      "        [ 0.1819, -0.3518,  0.3685,  0.0514, -0.7136]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1047, -0.6944,  0.7791,  2.2679,  1.4638],\n",
      "        [-0.3339,  0.8696,  1.1381, -0.2066,  0.7418],\n",
      "        [ 0.6567, -1.0988,  0.7831, -1.3145,  1.6715],\n",
      "        [ 1.6443,  0.0206, -0.2347, -0.5119,  0.9364]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3832],\n",
      "        [-1.1454],\n",
      "        [ 1.8640],\n",
      "        [-0.4892]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4563,  1.9356,  1.1782,  0.1439,  0.6518],\n",
      "        [-1.7722, -1.0396,  1.4160, -0.7831, -0.2857],\n",
      "        [-1.7039,  0.3019,  0.5490,  0.2284,  0.5021],\n",
      "        [-0.2771, -1.1482,  0.2928, -1.4600,  0.7873]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0057,  0.3916,  0.0774,  0.9544,  0.6590],\n",
      "        [ 0.4459,  0.1156, -0.3121, -0.1548, -0.3302],\n",
      "        [-0.0709, -1.1650, -0.6335, -1.0462, -1.0654],\n",
      "        [ 0.3548, -0.3672, -0.7027,  0.1556, -0.3097]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4563,  1.9356,  1.1782,  0.1439,  0.6518],\n",
      "        [-1.7722, -1.0396,  1.4160, -0.7831, -0.2857],\n",
      "        [-1.7039,  0.3019,  0.5490,  0.2284,  0.5021],\n",
      "        [-0.2771, -1.1482,  0.2928, -1.4600,  0.7873]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4134],\n",
      "        [-1.1368],\n",
      "        [-1.3526],\n",
      "        [-0.3535]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1305, -0.7176,  1.3975, -0.5632,  0.3081],\n",
      "        [-2.4544, -0.7451,  0.8787,  0.3469,  2.3490],\n",
      "        [-0.3218,  0.5569,  0.2639,  0.4496,  0.5275],\n",
      "        [-0.8530, -1.0182, -0.5174, -0.0207, -1.1354]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9288, -0.0834, -0.3043,  0.2047, -0.7476],\n",
      "        [ 0.5126,  0.1522,  0.1073,  0.1033,  0.8974],\n",
      "        [ 0.0938,  0.0262, -0.7970, -0.6551,  0.1084],\n",
      "        [ 0.2611, -0.2566, -0.0290, -0.4763, -0.3658]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1305, -0.7176,  1.3975, -0.5632,  0.3081],\n",
      "        [-2.4544, -0.7451,  0.8787,  0.3469,  2.3490],\n",
      "        [-0.3218,  0.5569,  0.2639,  0.4496,  0.5275],\n",
      "        [-0.8530, -1.0182, -0.5174, -0.0207, -1.1354]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5899],\n",
      "        [ 0.8667],\n",
      "        [-0.4633],\n",
      "        [ 0.4788]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.6627, -0.5863,  0.6998, -0.4079,  1.7893],\n",
      "        [-1.4318, -0.9208, -1.3857, -1.2172, -0.8815],\n",
      "        [-1.2803,  0.7120, -0.1108,  0.0796, -1.6223],\n",
      "        [-1.1428, -0.3644,  0.7317, -0.9612,  0.6543]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1255,  0.5031,  0.4708,  0.6009,  0.6057],\n",
      "        [ 0.0940, -0.7969, -1.2614, -0.5318, -0.9399],\n",
      "        [ 0.9593, -0.3398,  0.0260, -0.2178, -0.7045],\n",
      "        [ 0.1382,  0.1636, -0.8465, -0.1358, -0.0380]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.6627, -0.5863,  0.6998, -0.4079,  1.7893],\n",
      "        [-1.4318, -0.9208, -1.3857, -1.2172, -0.8815],\n",
      "        [-1.2803,  0.7120, -0.1108,  0.0796, -1.6223],\n",
      "        [-1.1428, -0.3644,  0.7317, -0.9612,  0.6543]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.2073],\n",
      "        [ 3.8228],\n",
      "        [-0.3476],\n",
      "        [-0.7313]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5522,  0.8297, -0.2512,  1.2992,  1.0200],\n",
      "        [-0.2828, -0.8256,  1.0438, -1.1463, -0.1658],\n",
      "        [ 1.4274, -1.0921, -1.1914,  0.4081,  0.4960],\n",
      "        [ 1.4000, -0.0566,  2.0026,  0.8821,  0.0453]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1317, -0.0773,  0.2041,  0.2218, -0.1378],\n",
      "        [-1.1807, -1.0129, -1.3364, -2.1977, -3.2596],\n",
      "        [ 0.1473, -0.6101, -1.0757, -0.6206, -1.0819],\n",
      "        [ 0.4535, -0.2274, -0.7229, -0.9173, -0.6302]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5522,  0.8297, -0.2512,  1.2992,  1.0200],\n",
      "        [-0.2828, -0.8256,  1.0438, -1.1463, -0.1658],\n",
      "        [ 1.4274, -1.0921, -1.1914,  0.4081,  0.4960],\n",
      "        [ 1.4000, -0.0566,  2.0026,  0.8821,  0.0453]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0406],\n",
      "        [ 2.8351],\n",
      "        [ 1.3683],\n",
      "        [-1.6375]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7530,  0.7120, -0.9809, -0.1996,  0.8795],\n",
      "        [ 1.5994, -0.8848, -0.2832, -1.9285,  1.2434],\n",
      "        [-0.3203, -0.1389, -0.3558,  1.7006,  0.8604],\n",
      "        [ 1.0513, -0.7334, -0.3615,  0.4388,  0.4409]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2099,  0.5760, -0.2257,  0.2872,  0.4581],\n",
      "        [ 0.2116, -0.1953,  0.3128, -0.5457,  0.8716],\n",
      "        [ 0.3837, -1.3524, -1.1802, -1.6555, -2.3929],\n",
      "        [ 0.8250,  0.0225, -0.0031,  0.3256,  1.8023]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7530,  0.7120, -0.9809, -0.1996,  0.8795],\n",
      "        [ 1.5994, -0.8848, -0.2832, -1.9285,  1.2434],\n",
      "        [-0.3203, -0.1389, -0.3558,  1.7006,  0.8604],\n",
      "        [ 1.0513, -0.7334, -0.3615,  0.4388,  0.4409]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8191],\n",
      "        [ 2.5586],\n",
      "        [-4.3895],\n",
      "        [ 1.7894]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1818,  1.2082, -0.9659,  0.4837, -0.3722],\n",
      "        [ 0.4005,  0.7634,  0.3058,  0.5569,  0.9184],\n",
      "        [-1.0929, -0.5104,  1.1629, -0.6063, -0.4824],\n",
      "        [-0.0254, -0.3535, -0.0119,  1.4850, -0.0468]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3773,  0.4966,  0.5426,  0.1188,  0.2948],\n",
      "        [-0.5149, -1.3805, -0.5416, -2.0923, -1.8059],\n",
      "        [ 1.0676,  0.5217,  0.3637,  0.4782,  1.2273],\n",
      "        [-0.0256, -0.8047, -0.4042, -0.4455, -0.9519]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1818,  1.2082, -0.9659,  0.4837, -0.3722],\n",
      "        [ 0.4005,  0.7634,  0.3058,  0.5569,  0.9184],\n",
      "        [-1.0929, -0.5104,  1.1629, -0.6063, -0.4824],\n",
      "        [-0.0254, -0.3535, -0.0119,  1.4850, -0.0468]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.0921],\n",
      "        [-4.2494],\n",
      "        [-1.8921],\n",
      "        [-0.3272]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0744, -0.0927, -1.1960,  0.5772, -0.3885],\n",
      "        [-1.5153,  0.8199, -0.5769,  0.8400,  0.5467],\n",
      "        [ 1.2475,  0.9578, -1.0135,  0.9684,  1.1980],\n",
      "        [-1.2549,  0.9451, -0.9863, -0.6620, -0.2086]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3530,  0.4777,  0.9475,  0.1275,  0.2181],\n",
      "        [ 1.1977,  0.8089,  0.2757,  0.8208,  0.7636],\n",
      "        [ 1.6749,  1.8031,  1.3711,  0.9176,  2.9044],\n",
      "        [-0.0410,  0.1605, -0.4133,  0.0578, -0.8077]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0744, -0.0927, -1.1960,  0.5772, -0.3885],\n",
      "        [-1.5153,  0.8199, -0.5769,  0.8400,  0.5467],\n",
      "        [ 1.2475,  0.9578, -1.0135,  0.9684,  1.1980],\n",
      "        [-1.2549,  0.9451, -0.9863, -0.6620, -0.2086]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2149],\n",
      "        [-0.2037],\n",
      "        [ 6.7949],\n",
      "        [ 0.7409]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.1566,  1.4717, -0.0373,  0.1424,  0.8999],\n",
      "        [-0.1219,  1.0411, -0.9113, -1.2084,  1.4426],\n",
      "        [-0.9729, -1.1242, -1.6829,  1.8996,  0.8275],\n",
      "        [-0.9139,  0.3654,  0.7553, -1.1043, -1.9145]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4669,  0.9300,  1.3667,  1.2691,  1.0077],\n",
      "        [ 0.5530,  1.3783,  0.2936,  0.5154,  1.2832],\n",
      "        [-0.7135, -0.6875, -0.7559, -1.5172, -2.9366],\n",
      "        [-0.1471, -0.1653,  0.2917,  0.0801, -0.8428]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.1566,  1.4717, -0.0373,  0.1424,  0.8999],\n",
      "        [-0.1219,  1.0411, -0.9113, -1.2084,  1.4426],\n",
      "        [-0.9729, -1.1242, -1.6829,  1.8996,  0.8275],\n",
      "        [-0.9139,  0.3654,  0.7553, -1.1043, -1.9145]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3984],\n",
      "        [ 2.3284],\n",
      "        [-2.5731],\n",
      "        [ 1.8194]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2695,  1.0446,  1.0682, -1.1951,  0.9306],\n",
      "        [ 0.7653,  2.1875, -0.8962, -1.5326,  0.1688],\n",
      "        [ 0.2072,  1.1290, -0.7466,  1.9285,  1.1750],\n",
      "        [-0.0702,  2.3539,  0.2314,  1.3563,  0.1577]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0096,  0.5004,  0.0423,  0.6073,  0.6909],\n",
      "        [-0.4265, -0.2710,  0.2076, -0.1446, -1.3983],\n",
      "        [ 1.1939, -0.7060, -0.5027, -0.3787, -0.4148],\n",
      "        [-0.4929, -0.7350, -0.9070, -1.2041, -2.0224]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2695,  1.0446,  1.0682, -1.1951,  0.9306],\n",
      "        [ 0.7653,  2.1875, -0.8962, -1.5326,  0.1688],\n",
      "        [ 0.2072,  1.1290, -0.7466,  1.9285,  1.1750],\n",
      "        [-0.0702,  2.3539,  0.2314,  1.3563,  0.1577]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4877],\n",
      "        [-1.1197],\n",
      "        [-1.3921],\n",
      "        [-3.8576]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9362,  0.8191, -0.5890,  0.7605, -0.3918],\n",
      "        [ 0.6787,  0.0029,  1.8832, -0.2450, -0.2607],\n",
      "        [ 2.5578, -0.4018, -1.4899,  0.9091, -1.2346],\n",
      "        [-2.4177,  1.4496,  1.6510,  0.5859, -0.1556]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3730,  0.5776,  1.0342,  0.1266,  0.4296],\n",
      "        [ 0.1839, -0.0145,  0.6269,  0.3098, -0.0681],\n",
      "        [ 0.9293,  0.4820, -0.6449, -0.0450,  0.5886],\n",
      "        [ 1.2583,  1.1847,  1.2710,  1.1902,  1.0746]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9362,  0.8191, -0.5890,  0.7605, -0.3918],\n",
      "        [ 0.6787,  0.0029,  1.8832, -0.2450, -0.2607],\n",
      "        [ 2.5578, -0.4018, -1.4899,  0.9091, -1.2346],\n",
      "        [-2.4177,  1.4496,  1.6510,  0.5859, -0.1556]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5142],\n",
      "        [ 1.2471],\n",
      "        [ 2.3766],\n",
      "        [ 1.3036]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9725,  0.4226,  1.5783,  1.0881,  0.6367],\n",
      "        [ 0.3529, -0.7071,  0.4782, -0.5936, -0.7420],\n",
      "        [-1.4817,  1.4017, -1.0216,  0.4768,  1.2526],\n",
      "        [-1.4019,  1.4017, -2.1162,  0.3347, -0.1259]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0607,  0.4207,  1.2497,  0.9931,  0.0109],\n",
      "        [-0.3835, -0.9330, -0.0121, -0.8936, -0.7955],\n",
      "        [-0.3140, -0.5467, -1.2660, -1.7832, -1.6867],\n",
      "        [ 0.7622, -0.4473,  0.1970, -0.0231,  0.8458]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9725,  0.4226,  1.5783,  1.0881,  0.6367],\n",
      "        [ 0.3529, -0.7071,  0.4782, -0.5936, -0.7420],\n",
      "        [-1.4817,  1.4017, -1.0216,  0.4768,  1.2526],\n",
      "        [-1.4019,  1.4017, -2.1162,  0.3347, -0.1259]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.1787],\n",
      "        [ 1.6393],\n",
      "        [-1.9707],\n",
      "        [-2.2266]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4062, -0.5956, -0.6219, -0.0372,  1.2953],\n",
      "        [-2.1203, -0.9585,  0.6522,  1.2529,  0.7377],\n",
      "        [-0.3770,  0.7415, -0.6904,  0.4735,  0.4992],\n",
      "        [-0.4537, -1.5691, -0.6674,  0.5538, -0.4546]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9111, -0.4815, -0.6349, -0.8161, -1.8479],\n",
      "        [-1.1171, -0.2427, -0.4444, -0.2423, -1.8544],\n",
      "        [ 0.1373, -0.2587, -0.3979,  0.0156, -0.0385],\n",
      "        [ 0.8783,  1.6390,  1.0565,  1.1273,  1.5099]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4062, -0.5956, -0.6219, -0.0372,  1.2953],\n",
      "        [-2.1203, -0.9585,  0.6522,  1.2529,  0.7377],\n",
      "        [-0.3770,  0.7415, -0.6904,  0.4735,  0.4992],\n",
      "        [-0.4537, -1.5691, -0.6674,  0.5538, -0.4546]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3115],\n",
      "        [ 0.6398],\n",
      "        [ 0.0193],\n",
      "        [-3.7372]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9485, -1.5201,  0.8910,  1.5488,  0.2574],\n",
      "        [-0.6503,  0.6316,  2.0746, -1.0984, -1.3599],\n",
      "        [ 0.4372, -0.3770,  1.4293,  0.0908, -0.6885],\n",
      "        [-0.9872,  0.0229,  2.6115,  1.1726,  0.9219]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.9078, -0.3125, -0.5091, -0.2397,  0.0463],\n",
      "        [-1.0646, -0.7651, -1.2642, -1.2928, -1.9202],\n",
      "        [ 0.6984, -0.1853,  0.1735, -0.3550, -0.2777],\n",
      "        [ 1.7502,  2.5598,  2.1676,  1.8799,  3.1090]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9485, -1.5201,  0.8910,  1.5488,  0.2574],\n",
      "        [-0.6503,  0.6316,  2.0746, -1.0984, -1.3599],\n",
      "        [ 0.4372, -0.3770,  1.4293,  0.0908, -0.6885],\n",
      "        [-0.9872,  0.0229,  2.6115,  1.1726,  0.9219]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4309],\n",
      "        [ 1.6176],\n",
      "        [ 0.7821],\n",
      "        [ 9.0621]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0394,  0.7025, -0.9047,  0.3312,  0.7105],\n",
      "        [ 1.8456,  0.0966, -1.1901, -2.1115, -0.3666],\n",
      "        [-0.7239,  0.0636, -0.5016,  3.3229, -0.6610],\n",
      "        [ 1.8556,  0.8362, -0.3129,  1.2052, -0.7343]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7310, -0.8829, -0.3235, -0.9688, -1.8314],\n",
      "        [-0.8331, -1.7966, -1.6758, -1.3420, -2.4591],\n",
      "        [-0.2312, -0.6884, -0.8095, -0.8342, -0.9066],\n",
      "        [-1.1371, -1.1210, -1.2512, -1.6204, -2.6074]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0394,  0.7025, -0.9047,  0.3312,  0.7105],\n",
      "        [ 1.8456,  0.0966, -1.1901, -2.1115, -0.3666],\n",
      "        [-0.7239,  0.0636, -0.5016,  3.3229, -0.6610],\n",
      "        [ 1.8556,  0.8362, -0.3129,  1.2052, -0.7343]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9785],\n",
      "        [ 4.0184],\n",
      "        [-1.6429],\n",
      "        [-2.6943]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9566, -1.1350, -0.3045, -0.3488, -0.4155],\n",
      "        [ 1.8039,  0.5196,  1.0894,  0.8961, -0.0145],\n",
      "        [-0.8293,  0.3088, -0.6334, -0.3176,  1.0462],\n",
      "        [-0.3646,  1.3956,  0.4602,  0.6746, -0.6778]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3576,  0.2964,  0.5566,  0.5906, -1.2111],\n",
      "        [ 0.1306, -0.3851,  0.0096, -0.0474,  0.1535],\n",
      "        [ 0.7308,  0.0111,  0.5637, -0.1055,  1.1352],\n",
      "        [ 0.0616,  0.3661,  0.2423,  0.4960,  1.2468]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9566, -1.1350, -0.3045, -0.3488, -0.4155],\n",
      "        [ 1.8039,  0.5196,  1.0894,  0.8961, -0.0145],\n",
      "        [-0.8293,  0.3088, -0.6334, -0.3176,  1.0462],\n",
      "        [-0.3646,  1.3956,  0.4602,  0.6746, -0.6778]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4909],\n",
      "        [ 0.0011],\n",
      "        [ 0.2614],\n",
      "        [ 0.0895]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7198, -0.2995, -0.7114, -0.1304,  1.3174],\n",
      "        [-1.0458,  0.5415,  0.5414,  2.7241,  2.2897],\n",
      "        [ 0.8863,  0.5372, -0.3460, -1.1591,  0.2206],\n",
      "        [ 1.3471,  0.4718,  1.1892, -1.8587, -0.5968]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4815, -0.4313, -0.1761, -0.4520, -0.7501],\n",
      "        [ 0.2585,  0.1994,  0.0031, -0.3556,  0.1371],\n",
      "        [ 0.0747,  0.0590, -0.7058, -0.3848, -0.0239],\n",
      "        [ 0.4388,  1.1428,  0.8766,  1.2060,  1.7596]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7198, -0.2995, -0.7114, -0.1304,  1.3174],\n",
      "        [-1.0458,  0.5415,  0.5414,  2.7241,  2.2897],\n",
      "        [ 0.8863,  0.5372, -0.3460, -1.1591,  0.2206],\n",
      "        [ 1.3471,  0.4718,  1.1892, -1.8587, -0.5968]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1534],\n",
      "        [-0.8154],\n",
      "        [ 0.7829],\n",
      "        [-1.1191]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.9548,  0.1127,  0.7041,  0.0114, -0.5021],\n",
      "        [-1.1100, -0.3692,  0.9837, -0.0650,  0.6700],\n",
      "        [ 0.1335,  0.2645,  1.4475,  1.8641, -0.3224],\n",
      "        [ 0.5960,  0.0732, -1.0910, -0.6468,  0.6339]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5132, -0.2603, -0.3347, -0.1704, -0.1890],\n",
      "        [ 0.3155,  0.2814,  0.3486,  0.9500,  0.7626],\n",
      "        [-0.1583, -0.3664, -1.1101, -0.6658, -0.7804],\n",
      "        [ 0.6366,  1.6466,  2.1830,  1.2064,  1.6840]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.9548,  0.1127,  0.7041,  0.0114, -0.5021],\n",
      "        [-1.1100, -0.3692,  0.9837, -0.0650,  0.6700],\n",
      "        [ 0.1335,  0.2645,  1.4475,  1.8641, -0.3224],\n",
      "        [ 0.5960,  0.0732, -1.0910, -0.6468,  0.6339]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8312],\n",
      "        [ 0.3379],\n",
      "        [-2.7145],\n",
      "        [-1.5945]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1306, -0.1697, -0.1216, -0.0392, -0.7915],\n",
      "        [-0.4695,  0.1298,  0.4250,  1.5740, -0.5396],\n",
      "        [-1.3125,  0.6165, -0.2502,  0.5991,  0.2369],\n",
      "        [-0.9737,  0.8804,  0.2156,  0.2423, -0.6483]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.6393, -0.4971, -0.3101, -0.0977, -0.4642],\n",
      "        [ 0.5070, -0.4405,  0.2888, -0.5317,  0.3567],\n",
      "        [ 1.0667,  0.8018,  2.1561,  0.4064,  0.6425],\n",
      "        [ 1.4989,  1.8869,  1.3376,  1.3110,  1.8473]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1306, -0.1697, -0.1216, -0.0392, -0.7915],\n",
      "        [-0.4695,  0.1298,  0.4250,  1.5740, -0.5396],\n",
      "        [-1.3125,  0.6165, -0.2502,  0.5991,  0.2369],\n",
      "        [-0.9737,  0.8804,  0.2156,  0.2423, -0.6483]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5768],\n",
      "        [-1.2019],\n",
      "        [-1.0495],\n",
      "        [-0.3899]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7501,  0.3642, -0.2591,  0.6127, -0.2260],\n",
      "        [ 0.4801, -0.2651, -0.5364,  2.0927,  0.5840],\n",
      "        [-0.9355, -1.3793, -0.2150,  0.3310,  0.2192],\n",
      "        [ 0.1524, -0.1532, -0.3216, -0.3577,  1.0802]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0589, -0.1777, -0.1329,  0.0893,  0.4179],\n",
      "        [ 0.5707,  0.3496,  0.0875,  0.4467,  1.7690],\n",
      "        [ 1.0646,  1.1525,  1.2460,  1.1740,  1.3351],\n",
      "        [-0.1495, -0.2651,  0.6543,  0.1501, -0.1793]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7501,  0.3642, -0.2591,  0.6127, -0.2260],\n",
      "        [ 0.4801, -0.2651, -0.5364,  2.0927,  0.5840],\n",
      "        [-0.9355, -1.3793, -0.2150,  0.3310,  0.2192],\n",
      "        [ 0.1524, -0.1532, -0.3216, -0.3577,  1.0802]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1730],\n",
      "        [ 2.1022],\n",
      "        [-2.1724],\n",
      "        [-0.4400]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3787, -0.2429, -0.5416, -0.9226,  0.4357],\n",
      "        [-0.5049, -0.3697,  1.4278,  1.2583, -0.2878],\n",
      "        [ 0.2853, -0.7193, -0.4203, -0.5254, -1.0971],\n",
      "        [-1.1845,  0.5664, -0.8984,  0.7849,  1.8638]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2391, -0.0252,  0.3180, -0.0769,  0.4409],\n",
      "        [-0.1946, -0.9102, -1.4271, -0.7199, -1.9583],\n",
      "        [ 0.1966, -0.3655,  0.3433, -0.2394, -0.2234],\n",
      "        [ 0.2317, -0.6084, -0.8935, -0.9190, -0.5940]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3787, -0.2429, -0.5416, -0.9226,  0.4357],\n",
      "        [-0.5049, -0.3697,  1.4278,  1.2583, -0.2878],\n",
      "        [ 0.2853, -0.7193, -0.4203, -0.5254, -1.0971],\n",
      "        [-1.1845,  0.5664, -0.8984,  0.7849,  1.8638]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4266],\n",
      "        [-1.9452],\n",
      "        [ 0.5456],\n",
      "        [-1.6447]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4615,  0.7873,  0.1941,  0.6530, -0.0505],\n",
      "        [-0.0942, -0.2116, -0.8857, -0.7686,  0.2038],\n",
      "        [-0.3236,  0.3138,  2.1947, -1.1465,  0.9305],\n",
      "        [ 0.4182, -0.2444, -0.0477,  0.4730,  1.8684]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6372,  0.5669,  0.5687, -0.4482,  0.3252],\n",
      "        [ 0.9993,  0.6547, -0.0118, -0.0117,  0.7444],\n",
      "        [-0.3772,  0.1447, -0.0686,  0.0201, -0.0257],\n",
      "        [ 0.7856,  0.1029,  1.0426,  0.7075,  1.2523]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4615,  0.7873,  0.1941,  0.6530, -0.0505],\n",
      "        [-0.0942, -0.2116, -0.8857, -0.7686,  0.2038],\n",
      "        [-0.3236,  0.3138,  2.1947, -1.1465,  0.9305],\n",
      "        [ 0.4182, -0.2444, -0.0477,  0.4730,  1.8684]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0465],\n",
      "        [-0.0614],\n",
      "        [-0.0300],\n",
      "        [ 2.9280]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2781, -1.0537, -0.1804,  1.3840, -0.6317],\n",
      "        [-0.0482, -0.1296,  0.5929,  0.7659,  1.5312],\n",
      "        [ 0.6333,  0.4896, -0.9521,  0.3951,  0.2416],\n",
      "        [-0.0887, -1.5822, -2.0208, -1.3172,  2.6364]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0409,  0.2058,  0.0231,  0.4442,  0.3576],\n",
      "        [ 0.4025, -0.6105,  0.1542, -0.2262,  0.4579],\n",
      "        [ 0.4317,  0.3182,  0.1546, -0.6047,  0.0540],\n",
      "        [-0.5782, -0.8268, -0.5941, -1.2745, -1.0331]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2781, -1.0537, -0.1804,  1.3840, -0.6317],\n",
      "        [-0.0482, -0.1296,  0.5929,  0.7659,  1.5312],\n",
      "        [ 0.6333,  0.4896, -0.9521,  0.3951,  0.2416],\n",
      "        [-0.0887, -1.5822, -2.0208, -1.3172,  2.6364]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1564],\n",
      "        [ 0.6790],\n",
      "        [ 0.0562],\n",
      "        [ 1.5150]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5094,  0.2450, -0.6086, -0.5855,  1.5226],\n",
      "        [-0.6166, -2.4898,  1.2195,  0.9902,  0.1824],\n",
      "        [-0.5167, -1.4395,  0.1887, -1.3024,  0.6355],\n",
      "        [ 1.2074, -2.4062, -1.1099, -0.0097, -0.6568]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1440,  0.4806,  0.5212, -0.2814,  0.2428],\n",
      "        [ 0.0161,  0.3854,  0.3347, -0.0201, -0.3388],\n",
      "        [ 0.3447, -0.0287,  0.8647, -0.3469,  0.2160],\n",
      "        [-0.7917, -0.8766, -1.2377, -1.7249, -1.9625]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5094,  0.2450, -0.6086, -0.5855,  1.5226],\n",
      "        [-0.6166, -2.4898,  1.2195,  0.9902,  0.1824],\n",
      "        [-0.5167, -1.4395,  0.1887, -1.3024,  0.6355],\n",
      "        [ 1.2074, -2.4062, -1.1099, -0.0097, -0.6568]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4085],\n",
      "        [-0.6431],\n",
      "        [ 0.6155],\n",
      "        [ 3.8331]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3160,  0.8767, -1.3077, -0.9388,  1.0072],\n",
      "        [ 0.6583,  0.6435,  0.5637,  1.1869,  0.1437],\n",
      "        [-0.6943, -0.0491, -1.1015,  0.2630,  0.3690],\n",
      "        [ 1.5821,  0.2596,  0.5351,  0.2111, -0.9695]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0474,  0.0424,  0.9303, -0.0934,  0.3480],\n",
      "        [ 0.6686, -0.7193, -0.4876,  0.3523, -0.5202],\n",
      "        [ 0.2168,  0.4244,  0.6847,  0.6276,  0.4871],\n",
      "        [-1.6306, -1.9669, -2.3571, -2.6619, -4.8873]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3160,  0.8767, -1.3077, -0.9388,  1.0072],\n",
      "        [ 0.6583,  0.6435,  0.5637,  1.1869,  0.1437],\n",
      "        [-0.6943, -0.0491, -1.1015,  0.2630,  0.3690],\n",
      "        [ 1.5821,  0.2596,  0.5351,  0.2111, -0.9695]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7562],\n",
      "        [ 0.0457],\n",
      "        [-0.5808],\n",
      "        [-0.1753]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5494,  1.0479,  0.0141,  0.8826, -0.6032],\n",
      "        [-0.6782,  0.8586, -1.0058, -0.2193,  0.8251],\n",
      "        [-0.2151,  0.0845,  1.1786, -0.5189, -0.0898],\n",
      "        [-1.2348, -0.1262, -0.4632, -0.4073,  0.8201]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2697,  0.2120, -0.0961,  0.3558,  0.7939],\n",
      "        [ 0.0284, -0.2872,  0.2608,  0.0823, -0.0133],\n",
      "        [ 0.6038,  0.1550,  0.3886, -0.1677,  0.0852],\n",
      "        [ 0.5231, -0.4085, -0.2179,  0.3258,  0.0321]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5494,  1.0479,  0.0141,  0.8826, -0.6032],\n",
      "        [-0.6782,  0.8586, -1.0058, -0.2193,  0.8251],\n",
      "        [-0.2151,  0.0845,  1.1786, -0.5189, -0.0898],\n",
      "        [-1.2348, -0.1262, -0.4632, -0.4073,  0.8201]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0923],\n",
      "        [-0.5573],\n",
      "        [ 0.4205],\n",
      "        [-0.5998]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3995,  0.3388,  1.2532,  0.9019, -0.8007],\n",
      "        [-0.7894, -0.5358, -0.3764,  0.1215, -2.0134],\n",
      "        [-1.0034, -0.9044,  1.7371, -1.8309,  0.0345],\n",
      "        [-0.1424,  1.4467,  0.4998, -0.3128, -0.0890]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4714, -0.0193, -0.1276,  0.1965,  0.2608],\n",
      "        [-0.0196, -0.3155, -0.4305, -0.7651, -0.3556],\n",
      "        [ 0.2554, -0.3180,  0.5006,  0.6934,  0.5565],\n",
      "        [-0.4519, -0.2232, -0.2274, -0.0220, -0.3500]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3995,  0.3388,  1.2532,  0.9019, -0.8007],\n",
      "        [-0.7894, -0.5358, -0.3764,  0.1215, -2.0134],\n",
      "        [-1.0034, -0.9044,  1.7371, -1.8309,  0.0345],\n",
      "        [-0.1424,  1.4467,  0.4998, -0.3128, -0.0890]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.3863],\n",
      "        [ 0.9697],\n",
      "        [-0.3494],\n",
      "        [-0.3341]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-2.8503, -1.7023,  0.5882,  0.3605,  0.6692],\n",
      "        [ 0.4146,  1.3832, -0.3911,  1.1902,  0.2435],\n",
      "        [ 1.0289, -0.2613,  1.5308, -1.3131, -0.3651],\n",
      "        [-0.1433,  0.3995,  0.0758,  0.2089,  1.1468]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4517, -0.1063, -0.3154, -0.1025,  0.6614],\n",
      "        [-0.0128, -0.4312, -0.2981, -0.6921, -0.5949],\n",
      "        [-0.1680, -0.1291, -0.1495, -0.4964,  0.3451],\n",
      "        [ 0.2645, -0.7915, -0.4119, -0.0248, -0.3996]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-2.8503, -1.7023,  0.5882,  0.3605,  0.6692],\n",
      "        [ 0.4146,  1.3832, -0.3911,  1.1902,  0.2435],\n",
      "        [ 1.0289, -0.2613,  1.5308, -1.3131, -0.3651],\n",
      "        [-0.1433,  0.3995,  0.0758,  0.2089,  1.1468]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8865],\n",
      "        [-1.4536],\n",
      "        [ 0.1579],\n",
      "        [-0.8488]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4282,  0.1851,  0.7652,  0.8774,  1.5251],\n",
      "        [ 0.0170,  0.2196, -0.6693, -0.5376,  0.6132],\n",
      "        [-0.5769,  0.6713, -0.8909, -0.2959, -0.3574],\n",
      "        [ 1.7830, -0.5146,  0.7200,  2.6504, -0.8663]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5681,  0.2543,  0.5269,  0.0578,  1.3941],\n",
      "        [ 0.5173,  0.0269,  0.0847, -0.2414,  0.2245],\n",
      "        [ 0.0196,  0.4362,  0.4802, -0.0452,  0.2687],\n",
      "        [ 0.3963, -0.1257,  0.6163, -0.0603,  0.9084]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4282,  0.1851,  0.7652,  0.8774,  1.5251],\n",
      "        [ 0.0170,  0.2196, -0.6693, -0.5376,  0.6132],\n",
      "        [-0.5769,  0.6713, -0.8909, -0.2959, -0.3574],\n",
      "        [ 1.7830, -0.5146,  0.7200,  2.6504, -0.8663]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 3.4384],\n",
      "        [ 0.2254],\n",
      "        [-0.2290],\n",
      "        [ 0.2682]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.9128, -0.7047,  1.6146, -0.0530,  0.6020],\n",
      "        [-0.6554,  0.2166, -0.3847,  0.1117,  1.1331],\n",
      "        [-0.7801, -0.1811,  1.0833, -0.1401,  0.5113],\n",
      "        [ 0.3210, -1.5922,  0.3953,  1.7281,  0.4515]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8888, -1.7310, -1.3125, -1.9384, -2.8348],\n",
      "        [ 0.3877,  0.0656,  0.3478,  0.3994,  0.0908],\n",
      "        [ 0.2013,  0.9615, -0.2057, -0.1553,  0.1573],\n",
      "        [ 0.0811, -0.9667, -0.0427,  0.4409, -0.0676]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.9128, -0.7047,  1.6146, -0.0530,  0.6020],\n",
      "        [-0.6554,  0.2166, -0.3847,  0.1117,  1.1331],\n",
      "        [-0.7801, -0.1811,  1.0833, -0.1401,  0.5113],\n",
      "        [ 0.3210, -1.5922,  0.3953,  1.7281,  0.4515]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.3145],\n",
      "        [-0.2262],\n",
      "        [-0.4518],\n",
      "        [ 2.2798]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5704,  0.2352, -1.1966,  0.6917,  0.6928],\n",
      "        [ 0.8397, -0.2640,  1.1628, -1.9395,  0.0881],\n",
      "        [-0.1110, -0.5581,  0.2952,  1.6805, -0.5781],\n",
      "        [ 1.7205, -1.1545, -0.5229, -1.1977, -0.0904]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7653,  1.0621, -0.2438, -0.3259,  0.9011],\n",
      "        [-0.2957, -0.2559,  0.0676, -0.2341, -0.9162],\n",
      "        [ 0.1097, -0.0811, -0.4082,  0.0996,  0.2609],\n",
      "        [-0.9450, -1.1725, -0.9971, -1.1560, -1.9431]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5704,  0.2352, -1.1966,  0.6917,  0.6928],\n",
      "        [ 0.8397, -0.2640,  1.1628, -1.9395,  0.0881],\n",
      "        [-0.1110, -0.5581,  0.2952,  1.6805, -0.5781],\n",
      "        [ 1.7205, -1.1545, -0.5229, -1.1977, -0.0904]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3768],\n",
      "        [ 0.2711],\n",
      "        [-0.0708],\n",
      "        [ 1.8092]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8922, -0.6291, -0.3769,  1.2975,  1.8187],\n",
      "        [ 0.7771,  0.1396,  1.3287,  0.0459,  1.6751],\n",
      "        [ 0.0585,  1.3687, -0.6137,  1.1753,  1.0842],\n",
      "        [-1.6334,  0.8425, -0.7036,  0.5846, -0.9358]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2407, -0.0300, -0.2277, -0.0327, -1.3317],\n",
      "        [ 0.2793, -0.0052, -0.0247, -0.5826,  0.9439],\n",
      "        [ 0.3458,  0.0379, -0.5623, -0.7930,  0.5410],\n",
      "        [-0.7799, -1.5927, -1.3857, -1.4340, -2.4501]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8922, -0.6291, -0.3769,  1.2975,  1.8187],\n",
      "        [ 0.7771,  0.1396,  1.3287,  0.0459,  1.6751],\n",
      "        [ 0.0585,  1.3687, -0.6137,  1.1753,  1.0842],\n",
      "        [-1.6334,  0.8425, -0.7036,  0.5846, -0.9358]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.1449],\n",
      "        [ 1.7378],\n",
      "        [ 0.0718],\n",
      "        [ 2.3616]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4492,  1.5582, -0.3316,  0.4162,  0.2954],\n",
      "        [-1.5819,  1.3543,  0.9965,  1.3529,  0.4595],\n",
      "        [ 0.3083,  0.3114,  1.4246, -0.0037,  1.8082],\n",
      "        [ 1.0215,  0.1263,  0.1820, -0.9989, -0.3792]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.7392,  0.2471,  0.7283, -0.1426,  0.8265],\n",
      "        [-0.6116, -1.1365, -0.6927, -1.5866, -1.9260],\n",
      "        [ 0.3211,  0.0344,  0.1729, -0.0090, -0.2117],\n",
      "        [ 0.3043,  0.2193, -0.0328,  0.2569, -0.2445]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4492,  1.5582, -0.3316,  0.4162,  0.2954],\n",
      "        [-1.5819,  1.3543,  0.9965,  1.3529,  0.4595],\n",
      "        [ 0.3083,  0.3114,  1.4246, -0.0037,  1.8082],\n",
      "        [ 1.0215,  0.1263,  0.1820, -0.9989, -0.3792]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0037],\n",
      "        [-4.2935],\n",
      "        [-0.0268],\n",
      "        [ 0.1687]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3074, -0.5145, -0.5001, -1.0551,  0.1541],\n",
      "        [-0.0766,  0.5903,  0.2728, -0.4387,  0.2098],\n",
      "        [-0.2616,  0.3827,  0.6787,  0.7492,  0.3028],\n",
      "        [ 0.6138, -0.1156,  0.7955, -0.0410, -0.1626]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0600, -0.4695, -0.0356, -0.1482,  0.1113],\n",
      "        [ 0.8684,  1.7079,  0.8080,  1.0228,  1.0752],\n",
      "        [-0.2253, -0.1427, -0.7324, -0.1563,  0.2502],\n",
      "        [ 0.4809,  0.0012,  0.5554,  0.0721, -0.0746]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3074, -0.5145, -0.5001, -1.0551,  0.1541],\n",
      "        [-0.0766,  0.5903,  0.2728, -0.4387,  0.2098],\n",
      "        [-0.2616,  0.3827,  0.6787,  0.7492,  0.3028],\n",
      "        [ 0.6138, -0.1156,  0.7955, -0.0410, -0.1626]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.4144],\n",
      "        [ 0.9388],\n",
      "        [-0.5341],\n",
      "        [ 0.7460]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0913, -0.2899, -0.1856,  0.8184,  0.6996],\n",
      "        [-1.8318, -0.0036,  0.1176,  0.7849,  0.4863],\n",
      "        [ 0.3794, -1.2914,  0.3563, -1.4107,  0.7033],\n",
      "        [ 0.3725, -0.5782,  1.3154, -1.9059, -1.2333]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2971,  0.5120,  0.3782, -0.8924, -0.3434],\n",
      "        [ 1.0282,  0.5965,  0.9190,  0.9438,  1.3584],\n",
      "        [ 0.0565, -0.4038,  0.8557,  0.4538,  0.3859],\n",
      "        [ 0.4973,  0.1419,  0.8650,  0.4363, -0.4426]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0913, -0.2899, -0.1856,  0.8184,  0.6996],\n",
      "        [-1.8318, -0.0036,  0.1176,  0.7849,  0.4863],\n",
      "        [ 0.3794, -1.2914,  0.3563, -1.4107,  0.7033],\n",
      "        [ 0.3725, -0.5782,  1.3154, -1.9059, -1.2333]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2164],\n",
      "        [-0.3761],\n",
      "        [ 0.4789],\n",
      "        [ 0.9553]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4958,  0.2581,  0.7914, -0.1730, -1.4595],\n",
      "        [ 0.6444,  0.8248, -0.8941,  0.0661, -1.8230],\n",
      "        [ 0.9862,  0.1459, -0.5030, -0.2232, -0.3551],\n",
      "        [ 1.2575, -2.0307,  1.5891,  0.6063,  0.7404]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6436,  0.2555, -0.5216,  0.2159,  0.4227],\n",
      "        [ 1.0283,  0.9781,  1.2438,  1.0846,  1.5198],\n",
      "        [ 0.3964,  0.2902,  0.1666, -0.0166, -0.1688],\n",
      "        [-0.3330, -0.7896, -0.0292, -0.4230, -0.6236]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4958,  0.2581,  0.7914, -0.1730, -1.4595],\n",
      "        [ 0.6444,  0.8248, -0.8941,  0.0661, -1.8230],\n",
      "        [ 0.9862,  0.1459, -0.5030, -0.2232, -0.3551],\n",
      "        [ 1.2575, -2.0307,  1.5891,  0.6063,  0.7404]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.9638],\n",
      "        [-2.3416],\n",
      "        [ 0.4130],\n",
      "        [ 0.4201]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4606,  0.5047, -1.4283,  1.3723, -0.0620],\n",
      "        [ 1.0691, -1.3438, -0.3672, -0.1907,  0.4082],\n",
      "        [-0.6987, -0.3471,  1.2377, -0.9858, -0.1144],\n",
      "        [-0.7280,  1.1736,  1.7940, -0.5903, -0.1760]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.2750,  1.3380,  0.6820,  0.8287,  1.8201],\n",
      "        [ 1.7499,  2.0890,  1.5209,  1.6066,  1.7877],\n",
      "        [ 0.1358,  0.1745,  0.4942,  0.4683,  0.1904],\n",
      "        [ 0.0272,  0.8421,  0.2282,  0.7815, -0.7062]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4606,  0.5047, -1.4283,  1.3723, -0.0620],\n",
      "        [ 1.0691, -1.3438, -0.3672, -0.1907,  0.4082],\n",
      "        [-0.6987, -0.3471,  1.2377, -0.9858, -0.1144],\n",
      "        [-0.7280,  1.1736,  1.7940, -0.5903, -0.1760]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1383],\n",
      "        [-1.0717],\n",
      "        [-0.0272],\n",
      "        [ 1.0409]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7404,  0.1437,  0.6029, -1.1926,  0.4555],\n",
      "        [ 0.6524,  0.1090, -0.4150,  0.8507,  0.2473],\n",
      "        [-1.3853,  0.1014,  0.6238,  1.2785,  0.9577],\n",
      "        [ 0.8200,  0.2537,  0.8102, -0.7041,  0.3779]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.3052,  0.8177,  0.6066,  0.9610,  1.4763],\n",
      "        [ 0.2352, -0.0827, -0.5112,  0.2812, -0.5802],\n",
      "        [ 0.2637,  0.4000,  0.5293,  0.4654, -0.0607],\n",
      "        [-0.4299, -0.7258, -0.7245, -0.7597, -0.9531]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7404,  0.1437,  0.6029, -1.1926,  0.4555],\n",
      "        [ 0.6524,  0.1090, -0.4150,  0.8507,  0.2473],\n",
      "        [-1.3853,  0.1014,  0.6238,  1.2785,  0.9577],\n",
      "        [ 0.8200,  0.2537,  0.8102, -0.7041,  0.3779]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.9567],\n",
      "        [ 0.4523],\n",
      "        [ 0.5424],\n",
      "        [-0.9489]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1398, -0.2078, -0.6443,  0.2923,  0.4512],\n",
      "        [ 0.1375, -1.0291,  0.4053, -2.2535,  0.0707],\n",
      "        [ 0.5124, -0.9642,  0.6021, -0.1800,  0.4832],\n",
      "        [-2.0203,  0.0227,  2.0651, -0.2913, -0.4868]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0621,  1.3185,  2.2415,  1.4308,  1.8333],\n",
      "        [ 0.4104, -0.3436,  0.1661, -0.7254,  0.4901],\n",
      "        [-0.2261, -0.3463,  0.7462, -0.1724, -0.0197],\n",
      "        [ 0.2792,  0.7188, -0.2407,  0.7458,  0.2894]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1398, -0.2078, -0.6443,  0.2923,  0.4512],\n",
      "        [ 0.1375, -1.0291,  0.4053, -2.2535,  0.0707],\n",
      "        [ 0.5124, -0.9642,  0.6021, -0.1800,  0.4832],\n",
      "        [-2.0203,  0.0227,  2.0651, -0.2913, -0.4868]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6835],\n",
      "        [ 2.1469],\n",
      "        [ 0.6889],\n",
      "        [-1.4028]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.5820, -0.8350, -0.4461, -1.6143,  2.3950],\n",
      "        [ 0.2632,  0.2314, -0.0546,  0.5371,  0.5070],\n",
      "        [-0.5692, -0.1894,  0.8821,  0.3177,  2.0144],\n",
      "        [ 0.7032,  1.4388, -1.1774, -0.1259,  1.5403]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4334,  0.4124, -0.2031,  0.5005, -0.3019],\n",
      "        [-0.9524, -0.0822, -0.8805, -1.2424, -1.7221],\n",
      "        [-0.2831,  0.0398, -0.2535,  0.0437,  1.0548],\n",
      "        [ 0.4329,  0.8330,  0.9235,  0.6571,  0.7889]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.5820, -0.8350, -0.4461, -1.6143,  2.3950],\n",
      "        [ 0.2632,  0.2314, -0.0546,  0.5371,  0.5070],\n",
      "        [-0.5692, -0.1894,  0.8821,  0.3177,  2.0144],\n",
      "        [ 0.7032,  1.4388, -1.1774, -0.1259,  1.5403]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.5325],\n",
      "        [-1.7620],\n",
      "        [ 2.0688],\n",
      "        [ 1.5479]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3617,  0.3916, -1.6145,  0.0180, -0.8208],\n",
      "        [ 0.5727,  0.5349,  0.5864, -0.7943, -1.5513],\n",
      "        [-0.0912, -0.7274, -0.9952, -0.5741,  0.7965],\n",
      "        [-0.2371,  0.5395, -0.4926, -0.5134,  2.1568]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.1570,  0.3096,  1.4075,  0.5019,  1.4684],\n",
      "        [-0.1334, -0.7611, -0.3087,  0.4293,  0.3917],\n",
      "        [-0.4390, -0.7194, -0.9596, -0.3798, -1.5022],\n",
      "        [-0.1617, -0.5119,  0.7108,  0.6585, -0.6371]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3617,  0.3916, -1.6145,  0.0180, -0.8208],\n",
      "        [ 0.5727,  0.5349,  0.5864, -0.7943, -1.5513],\n",
      "        [-0.0912, -0.7274, -0.9952, -0.5741,  0.7965],\n",
      "        [-0.2371,  0.5395, -0.4926, -0.5134,  2.1568]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-2.9289],\n",
      "        [-1.6131],\n",
      "        [ 0.5399],\n",
      "        [-2.3000]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2430,  0.6684, -0.1695,  0.5171,  0.4782],\n",
      "        [-0.5756,  0.8183, -0.8025,  1.5618, -0.1068],\n",
      "        [ 0.1278,  1.9334,  1.7772, -0.0982,  1.6327],\n",
      "        [-1.4872, -1.2940, -0.2510, -0.4246,  0.6614]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0782,  1.9935,  1.7830,  1.6696,  2.4355],\n",
      "        [ 0.8183,  0.8981,  0.7031,  0.8445,  1.2426],\n",
      "        [-0.5154, -0.5960, -0.3229, -0.7491, -1.4797],\n",
      "        [ 0.9145,  0.5488,  1.4027,  0.0605,  1.9324]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2430,  0.6684, -0.1695,  0.5171,  0.4782],\n",
      "        [-0.5756,  0.8183, -0.8025,  1.5618, -0.1068],\n",
      "        [ 0.1278,  1.9334,  1.7772, -0.0982,  1.6327],\n",
      "        [-1.4872, -1.2940, -0.2510, -0.4246,  0.6614]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7181],\n",
      "        [ 0.8860],\n",
      "        [-4.1343],\n",
      "        [-1.1697]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0945,  0.8536, -0.3963, -0.8844,  0.0851],\n",
      "        [ 0.8986, -0.7615, -0.2341,  0.8765,  0.9196],\n",
      "        [-1.3255, -1.1906, -0.9649,  1.2891,  0.3297],\n",
      "        [-1.0183, -1.5467,  0.3947,  0.0393,  0.0594]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4471,  1.0875,  1.8865,  1.3168,  1.6254],\n",
      "        [-0.0468, -0.3115, -0.2377, -0.3966, -0.3947],\n",
      "        [ 0.4281,  1.4861,  1.4002,  1.2298,  1.5875],\n",
      "        [ 0.9332,  1.6556,  1.8000,  1.2771,  1.9818]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0945,  0.8536, -0.3963, -0.8844,  0.0851],\n",
      "        [ 0.8986, -0.7615, -0.2341,  0.8765,  0.9196],\n",
      "        [-1.3255, -1.1906, -0.9649,  1.2891,  0.3297],\n",
      "        [-1.0183, -1.5467,  0.3947,  0.0393,  0.0594]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7090],\n",
      "        [-0.4597],\n",
      "        [-1.5790],\n",
      "        [-2.6326]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3132, -0.4171, -2.1088,  0.4575, -1.0960],\n",
      "        [ 0.2063,  0.4313, -0.8535, -0.0558,  0.4257],\n",
      "        [ 1.4300, -2.2455,  0.4899,  1.4056, -0.8576],\n",
      "        [-0.3961, -0.3132,  1.4501,  1.1899,  1.1457]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3172,  0.1404, -1.0517, -0.2622, -0.6112],\n",
      "        [ 0.4168, -0.6874, -0.0942, -0.0827, -0.7333],\n",
      "        [ 1.3661,  1.5158,  1.1006,  1.1554,  1.5823],\n",
      "        [ 1.8945,  2.3426,  2.1812,  1.7926,  2.2000]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3132, -0.4171, -2.1088,  0.4575, -1.0960],\n",
      "        [ 0.2063,  0.4313, -0.8535, -0.0558,  0.4257],\n",
      "        [ 1.4300, -2.2455,  0.4899,  1.4056, -0.8576],\n",
      "        [-0.3961, -0.3132,  1.4501,  1.1899,  1.1457]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.8086],\n",
      "        [-0.4376],\n",
      "        [-0.6440],\n",
      "        [ 6.3324]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0366,  0.9289, -0.1317, -0.0282, -0.0997],\n",
      "        [-1.7763,  2.4264,  0.9290, -2.0600, -1.4524],\n",
      "        [-0.7873,  2.0090, -0.3113, -0.9979,  0.8740],\n",
      "        [ 0.6295,  0.6711,  2.8785,  0.7218, -0.2153]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.7284, -1.3742, -1.0493, -1.1726, -2.4703],\n",
      "        [ 0.0322, -0.3088, -0.4932, -0.3112, -0.1137],\n",
      "        [ 1.4690,  1.9502,  1.9141,  1.3092,  2.4036],\n",
      "        [-0.3369,  0.0015,  0.4025,  0.2422, -1.3351]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0366,  0.9289, -0.1317, -0.0282, -0.0997],\n",
      "        [-1.7763,  2.4264,  0.9290, -2.0600, -1.4524],\n",
      "        [-0.7873,  2.0090, -0.3113, -0.9979,  0.8740],\n",
      "        [ 0.6295,  0.6711,  2.8785,  0.7218, -0.2153]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.8322],\n",
      "        [-0.4586],\n",
      "        [ 2.9599],\n",
      "        [ 1.4098]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.5513, -0.9699,  0.1855,  0.1882, -1.1521],\n",
      "        [-1.7738, -0.1062, -0.3244,  0.8098, -0.8849],\n",
      "        [ 0.9359,  1.3241, -0.3629,  1.2666,  0.8891],\n",
      "        [ 0.1231,  0.7014, -0.9763, -0.1299,  1.2186]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2810, -0.8810, -0.1958, -1.3168, -1.7428],\n",
      "        [ 0.4760, -0.4603, -1.2338, -0.3298, -1.0332],\n",
      "        [ 0.3749,  1.0058,  0.4680,  0.5133,  0.9478],\n",
      "        [-0.9152, -0.4073, -0.6775,  0.0089, -1.0544]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.5513, -0.9699,  0.1855,  0.1882, -1.1521],\n",
      "        [-1.7738, -0.1062, -0.3244,  0.8098, -0.8849],\n",
      "        [ 0.9359,  1.3241, -0.3629,  1.2666,  0.8891],\n",
      "        [ 0.1231,  0.7014, -0.9763, -0.1299,  1.2186]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.7331],\n",
      "        [ 0.2521],\n",
      "        [ 3.0057],\n",
      "        [-1.0230]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7823,  0.3635,  0.5144,  0.3773,  1.2628],\n",
      "        [-1.5249, -0.7655, -0.1043,  0.1181, -0.5214],\n",
      "        [ 2.1857,  0.1700,  1.8375,  0.1923,  0.7408],\n",
      "        [-0.6858,  0.1550,  0.2669, -0.6431,  1.1048]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8670, -1.6980, -1.6782, -2.1223, -3.5528],\n",
      "        [ 0.2886, -0.2704, -0.0912,  0.0013, -0.2796],\n",
      "        [-0.3134, -0.7040,  0.0254, -0.5710, -1.5983],\n",
      "        [ 0.0896,  0.7473,  0.8008,  1.0326,  0.8225]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7823,  0.3635,  0.5144,  0.3773,  1.2628],\n",
      "        [-1.5249, -0.7655, -0.1043,  0.1181, -0.5214],\n",
      "        [ 2.1857,  0.1700,  1.8375,  0.1923,  0.7408],\n",
      "        [-0.6858,  0.1550,  0.2669, -0.6431,  1.1048]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-6.0895],\n",
      "        [-0.0776],\n",
      "        [-2.0518],\n",
      "        [ 0.5127]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0272,  2.2068, -0.7172,  0.3620, -0.1204],\n",
      "        [-0.7722, -1.3284, -1.1437,  1.1786, -0.8346],\n",
      "        [ 0.2054,  0.5278,  0.8447,  0.2965, -1.2188],\n",
      "        [ 0.1936, -0.4477, -1.5427,  0.8242,  0.7150]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6508,  0.6106,  0.0017,  0.6866,  1.5231],\n",
      "        [ 0.4577, -0.3428, -0.7370,  0.8589, -0.5645],\n",
      "        [-0.0321,  0.6347,  1.1734,  1.3126,  0.7505],\n",
      "        [-0.3627,  0.9380,  0.5161,  1.1516,  0.8686]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0272,  2.2068, -0.7172,  0.3620, -0.1204],\n",
      "        [-0.7722, -1.3284, -1.1437,  1.1786, -0.8346],\n",
      "        [ 0.2054,  0.5278,  0.8447,  0.2965, -1.2188],\n",
      "        [ 0.1936, -0.4477, -1.5427,  0.8242,  0.7150]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4292],\n",
      "        [ 2.4283],\n",
      "        [ 0.7941],\n",
      "        [ 0.2838]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7825, -1.0378,  0.8373, -1.3956, -0.9441],\n",
      "        [ 0.6948,  1.4376,  1.8420,  0.6506, -0.3389],\n",
      "        [-0.8081,  0.2316,  0.0020,  2.0091,  0.5952],\n",
      "        [-0.5421, -0.8987,  1.2383,  1.1423,  0.5886]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.0018, -0.3113, -0.8299, -0.8656, -0.3723],\n",
      "        [-0.9679, -1.2594, -1.1931, -1.5653, -2.6259],\n",
      "        [ 0.1378,  1.5002,  0.5757,  0.1721,  0.9334],\n",
      "        [ 0.1252,  1.0526,  1.3640,  0.7387,  1.1851]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7825, -1.0378,  0.8373, -1.3956, -0.9441],\n",
      "        [ 0.6948,  1.4376,  1.8420,  0.6506, -0.3389],\n",
      "        [-0.8081,  0.2316,  0.0020,  2.0091,  0.5952],\n",
      "        [-0.5421, -0.8987,  1.2383,  1.1423,  0.5886]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1892],\n",
      "        [-4.8091],\n",
      "        [ 1.1385],\n",
      "        [ 2.2166]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4195,  0.9997,  0.6988, -0.5432, -0.8251],\n",
      "        [-0.6562,  0.8957,  1.3253,  0.6542,  0.6709],\n",
      "        [-0.5737, -0.4698,  0.8392, -0.0466, -0.0308],\n",
      "        [ 0.4682, -0.8093, -0.0413,  0.5222, -0.3481]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2836, -0.9653, -1.7704, -0.8073, -0.9375],\n",
      "        [ 1.1923,  1.3503,  0.6054,  0.5711,  0.6162],\n",
      "        [-0.0965,  0.6675,  0.4074,  0.7351,  0.5392],\n",
      "        [-0.1713,  0.2612,  0.1630,  0.3602, -0.5998]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4195,  0.9997,  0.6988, -0.5432, -0.8251],\n",
      "        [-0.6562,  0.8957,  1.3253,  0.6542,  0.6709],\n",
      "        [-0.5737, -0.4698,  0.8392, -0.0466, -0.0308],\n",
      "        [ 0.4682, -0.8093, -0.0413,  0.5222, -0.3481]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.1090],\n",
      "        [ 2.0164],\n",
      "        [ 0.0328],\n",
      "        [ 0.0985]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.6515, -0.2811, -0.6735,  0.6088,  1.1451],\n",
      "        [-0.1715,  0.5826, -0.2782,  1.2342,  0.9529],\n",
      "        [-1.5181, -0.1595,  0.8947, -0.4708, -0.1600],\n",
      "        [-1.5444,  1.0780,  2.6807, -0.2564, -0.5816]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1638, -1.1312, -1.9502, -1.4038, -1.3468],\n",
      "        [ 0.2275, -0.2016,  0.1130,  0.1863, -0.3662],\n",
      "        [ 0.2061,  1.3752,  0.7487,  1.0428,  1.1806],\n",
      "        [-0.3427,  0.1055,  0.6749,  0.5942,  0.1222]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.6515, -0.2811, -0.6735,  0.6088,  1.1451],\n",
      "        [-0.1715,  0.5826, -0.2782,  1.2342,  0.9529],\n",
      "        [-1.5181, -0.1595,  0.8947, -0.4708, -0.1600],\n",
      "        [-1.5444,  1.0780,  2.6807, -0.2564, -0.5816]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.4946],\n",
      "        [-0.3070],\n",
      "        [-0.5423],\n",
      "        [ 2.2287]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8241, -0.6354,  0.1728,  0.3625, -0.4806],\n",
      "        [ 0.3279,  1.2549, -0.4372,  0.3787,  1.9439],\n",
      "        [-1.9820, -1.0329,  1.8576,  1.0771,  0.6118],\n",
      "        [-1.5292, -0.1272, -0.4629,  0.0511,  0.2943]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2041, -1.0069, -0.8017, -0.9381, -1.4147],\n",
      "        [ 0.1558,  0.0107, -0.2928,  0.1541, -0.2764],\n",
      "        [ 0.1712,  1.4279,  1.6953,  0.8914,  0.8947],\n",
      "        [-0.3501, -0.3812, -0.0904, -0.8376, -1.5650]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8241, -0.6354,  0.1728,  0.3625, -0.4806],\n",
      "        [ 0.3279,  1.2549, -0.4372,  0.3787,  1.9439],\n",
      "        [-1.9820, -1.0329,  1.8576,  1.0771,  0.6118],\n",
      "        [-1.5292, -0.1272, -0.4629,  0.0511,  0.2943]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.0093],\n",
      "        [-0.2863],\n",
      "        [ 2.8424],\n",
      "        [ 0.1224]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.2025, -1.0175,  1.7303, -0.9925,  0.5573],\n",
      "        [ 0.5971, -0.8146, -1.3925,  0.9093,  0.6275],\n",
      "        [ 0.2149, -0.1937,  0.3597,  2.0349,  1.6337],\n",
      "        [ 1.8811, -1.0745,  2.7441, -0.2363,  0.0309]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.0066, -1.2489, -1.5066, -1.3557, -1.4673],\n",
      "        [ 0.2278,  0.2564, -0.5423, -0.4806, -0.2337],\n",
      "        [-0.7641,  0.1644, -0.1820,  0.3304, -0.9533],\n",
      "        [-1.0037,  0.2474, -0.3556, -1.0829, -0.6644]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.2025, -1.0175,  1.7303, -0.9925,  0.5573],\n",
      "        [ 0.5971, -0.8146, -1.3925,  0.9093,  0.6275],\n",
      "        [ 0.2149, -0.1937,  0.3597,  2.0349,  1.6337],\n",
      "        [ 1.8811, -1.0745,  2.7441, -0.2363,  0.0309]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0122],\n",
      "        [ 0.0987],\n",
      "        [-1.1466],\n",
      "        [-2.8943]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.7747, -0.1925,  1.0834, -1.7253,  0.7134],\n",
      "        [ 2.2682, -0.6029, -0.2179, -0.5231, -1.6615],\n",
      "        [-0.6066, -0.0309, -0.9628,  0.3491, -0.5444],\n",
      "        [ 0.2654,  0.0264,  1.4713, -1.8700,  1.5158]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.5485, -1.5020, -0.6620, -1.0451, -1.2837],\n",
      "        [ 0.1618, -0.0823,  0.0498, -0.6082, -0.0299],\n",
      "        [-0.2064,  0.9479,  0.1899,  0.9957, -0.2370],\n",
      "        [-0.3569,  0.9967,  0.3147,  0.7621,  0.1224]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.7747, -0.1925,  1.0834, -1.7253,  0.7134],\n",
      "        [ 2.2682, -0.6029, -0.2179, -0.5231, -1.6615],\n",
      "        [-0.6066, -0.0309, -0.9628,  0.3491, -0.5444],\n",
      "        [ 0.2654,  0.0264,  1.4713, -1.8700,  1.5158]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5142],\n",
      "        [ 0.7736],\n",
      "        [ 0.3897],\n",
      "        [-0.8451]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.9871,  0.4301,  0.4610, -0.5368, -0.2431],\n",
      "        [-1.6858,  0.0413,  1.1227, -0.2256, -1.3456],\n",
      "        [ 0.4369, -0.1521, -0.1857,  1.0272, -0.3785],\n",
      "        [ 0.3966,  0.0967,  0.1657,  0.7640,  0.9409]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1804, -2.0224, -0.3997, -1.2590, -1.4546],\n",
      "        [-0.1178, -0.4257, -0.3142, -0.4362, -0.7083],\n",
      "        [-0.1658,  0.7483,  0.7459,  0.7922,  0.2221],\n",
      "        [-0.2816,  0.8097,  0.7414,  0.5119,  1.1077]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.9871,  0.4301,  0.4610, -0.5368, -0.2431],\n",
      "        [-1.6858,  0.0413,  1.1227, -0.2256, -1.3456],\n",
      "        [ 0.4369, -0.1521, -0.1857,  1.0272, -0.3785],\n",
      "        [ 0.3966,  0.0967,  0.1657,  0.7640,  0.9409]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2028],\n",
      "        [ 0.8798],\n",
      "        [ 0.4049],\n",
      "        [ 1.5227]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4834,  0.2413, -0.1095,  1.2099,  1.0159],\n",
      "        [ 1.2656,  0.4289, -2.1083,  0.5595,  0.1693],\n",
      "        [ 1.0415, -0.8280, -2.4954,  0.0552,  0.1279],\n",
      "        [ 0.8299,  0.2786,  0.2885,  0.3086, -0.0115]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0437, -1.6038, -0.9398, -1.6845, -0.8029],\n",
      "        [-0.3226,  0.3615, -0.7736, -1.0315, -1.0831],\n",
      "        [ 0.1458,  0.7410,  0.2926,  0.3874,  1.1615],\n",
      "        [-0.5163,  0.3891, -0.4063,  0.5522,  0.1887]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4834,  0.2413, -0.1095,  1.2099,  1.0159],\n",
      "        [ 1.2656,  0.4289, -2.1083,  0.5595,  0.1693],\n",
      "        [ 1.0415, -0.8280, -2.4954,  0.0552,  0.1279],\n",
      "        [ 0.8299,  0.2786,  0.2885,  0.3086, -0.0115]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.1589],\n",
      "        [ 0.6174],\n",
      "        [-1.0219],\n",
      "        [-0.2690]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7238, -0.7778,  0.7703,  0.1667,  1.1780],\n",
      "        [-0.8714,  0.0534,  0.6154,  0.5788, -0.0850],\n",
      "        [-1.7820, -0.9326, -0.2486, -1.0438,  2.5668],\n",
      "        [ 0.4944,  2.5115, -1.7705,  0.3909,  0.6601]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9169,  0.6056,  0.0845,  0.1779,  1.2134],\n",
      "        [ 0.0123, -0.0398, -0.1517, -0.5128, -1.2326],\n",
      "        [ 0.3247,  0.8663,  1.2426,  1.3228,  2.1338],\n",
      "        [-0.8002,  0.6016,  0.4998,  0.2610, -0.2156]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7238, -0.7778,  0.7703,  0.1667,  1.1780],\n",
      "        [-0.8714,  0.0534,  0.6154,  0.5788, -0.0850],\n",
      "        [-1.7820, -0.9326, -0.2486, -1.0438,  2.5668],\n",
      "        [ 0.4944,  2.5115, -1.7705,  0.3909,  0.6601]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.3894],\n",
      "        [-0.2983],\n",
      "        [ 2.4009],\n",
      "        [ 0.1900]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0918,  1.4024,  0.2087,  1.3354,  0.4040],\n",
      "        [-0.7345,  0.6232,  1.4173,  0.5750,  0.6196],\n",
      "        [-1.4211,  0.3227, -3.1131,  0.6055,  2.0695],\n",
      "        [ 0.6443, -0.1585,  0.6336,  1.5578, -0.7686]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.0531, -0.0138,  0.2975, -0.7226,  0.7770],\n",
      "        [ 0.3849,  0.2937, -0.1025, -0.7577, -0.1431],\n",
      "        [-0.9159,  0.6847, -0.0662,  0.6113,  0.4536],\n",
      "        [ 0.1152, -0.1584,  0.2741,  0.4083, -0.2179]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0918,  1.4024,  0.2087,  1.3354,  0.4040],\n",
      "        [-0.7345,  0.6232,  1.4173,  0.5750,  0.6196],\n",
      "        [-1.4211,  0.3227, -3.1131,  0.6055,  2.0695],\n",
      "        [ 0.6443, -0.1585,  0.6336,  1.5578, -0.7686]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.5116],\n",
      "        [-0.7693],\n",
      "        [ 3.0374],\n",
      "        [ 1.0765]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.1054, -0.1233,  0.2673, -0.3586,  0.8400],\n",
      "        [ 0.2837,  0.4035, -0.3570,  0.0908,  1.9804],\n",
      "        [ 1.2832,  0.1410,  1.4510,  1.1941,  1.2901],\n",
      "        [ 0.1572,  0.0159,  1.8124, -0.0152,  1.3369]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6985, -0.7109,  0.3352,  0.5380,  0.2480],\n",
      "        [ 0.4698, -0.0200, -0.4559, -0.5666,  0.0179],\n",
      "        [-1.1972, -0.9733, -0.5566, -1.2750, -2.2408],\n",
      "        [-0.2438,  0.7864, -0.2314, -0.1054, -0.5339]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.1054, -0.1233,  0.2673, -0.3586,  0.8400],\n",
      "        [ 0.2837,  0.4035, -0.3570,  0.0908,  1.9804],\n",
      "        [ 1.2832,  0.1410,  1.4510,  1.1941,  1.2901],\n",
      "        [ 0.1572,  0.0159,  1.8124, -0.0152,  1.3369]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1190],\n",
      "        [ 0.2721],\n",
      "        [-6.8945],\n",
      "        [-1.1574]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1514,  1.1868,  2.4365,  0.2618, -0.2038],\n",
      "        [-0.8762,  0.9680,  0.2371, -1.2121,  0.5445],\n",
      "        [-0.3681,  1.8567, -0.4294,  1.0156, -1.8026],\n",
      "        [-1.2358,  0.3150,  1.3986,  1.5172,  0.7919]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1196, -1.8408, -0.5151, -0.9442, -0.7336],\n",
      "        [-0.5377, -0.2398, -0.4901, -0.6089, -0.4584],\n",
      "        [ 0.8966,  2.7197,  2.0916,  1.5665,  2.6186],\n",
      "        [ 0.0593,  0.1205,  0.8050,  0.7521,  0.6559]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1514,  1.1868,  2.4365,  0.2618, -0.2038],\n",
      "        [-0.8762,  0.9680,  0.2371, -1.2121,  0.5445],\n",
      "        [-0.3681,  1.8567, -0.4294,  1.0156, -1.8026],\n",
      "        [-1.2358,  0.3150,  1.3986,  1.5172,  0.7919]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-3.3998],\n",
      "        [ 0.6113],\n",
      "        [ 0.6922],\n",
      "        [ 2.7511]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1115, -0.5177,  1.3781,  0.6249, -0.7378],\n",
      "        [ 0.4150,  0.9117,  0.7196, -0.9236,  1.3024],\n",
      "        [-0.4576,  1.3846,  1.6011,  0.2939, -0.6597],\n",
      "        [-2.4226, -1.4382,  0.0782, -0.3522, -0.2099]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.6687,  1.1757,  1.6059,  0.3406,  1.1030],\n",
      "        [-0.1278,  0.0726, -0.2716,  0.5199, -0.0855],\n",
      "        [ 1.3390,  1.6530,  2.1680,  1.8594,  1.9502],\n",
      "        [-0.6967, -0.9554, -1.0545, -0.1105, -1.7098]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1115, -0.5177,  1.3781,  0.6249, -0.7378],\n",
      "        [ 0.4150,  0.9117,  0.7196, -0.9236,  1.3024],\n",
      "        [-0.4576,  1.3846,  1.6011,  0.2939, -0.6597],\n",
      "        [-2.4226, -1.4382,  0.0782, -0.3522, -0.2099]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.1893],\n",
      "        [-0.7739],\n",
      "        [ 4.4074],\n",
      "        [ 3.3774]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3731,  1.3403,  0.6428,  0.2932,  0.9776],\n",
      "        [-0.1564, -0.8742, -0.1394, -0.4486,  1.3239],\n",
      "        [-0.2881, -1.2274, -0.1322,  0.0707,  0.6381],\n",
      "        [ 0.1160, -0.1987, -0.6976,  1.0892,  0.9899]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.4456,  0.2002,  0.7647,  0.5266,  0.9997],\n",
      "        [-0.1225,  0.1240, -0.4683, -0.2272, -0.3718],\n",
      "        [-0.6749,  0.5449,  0.0923, -0.5838, -0.2842],\n",
      "        [ 0.1433,  0.0905, -0.2856, -0.0135, -0.4583]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3731,  1.3403,  0.6428,  0.2932,  0.9776],\n",
      "        [-0.1564, -0.8742, -0.1394, -0.4486,  1.3239],\n",
      "        [-0.2881, -1.2274, -0.1322,  0.0707,  0.6381],\n",
      "        [ 0.1160, -0.1987, -0.6976,  1.0892,  0.9899]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3523],\n",
      "        [-0.4143],\n",
      "        [-0.7093],\n",
      "        [-0.2704]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.7964, -0.1672,  0.3247,  0.5099,  1.4491],\n",
      "        [-0.0234,  0.3174,  0.7946, -0.8283,  0.7356],\n",
      "        [-0.2517, -0.6364, -0.1046,  1.0708, -1.2382],\n",
      "        [ 2.7432, -0.7715,  0.2367,  0.0392, -0.0961]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3640, -0.2897, -0.9754, -0.9401, -0.6388],\n",
      "        [-0.0847, -0.9855, -0.1302, -0.4858, -0.1850],\n",
      "        [-0.2138,  0.3672,  0.0246,  0.3148,  0.9341],\n",
      "        [ 0.2897,  0.3731,  0.4059,  0.1842,  0.1589]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.7964, -0.1672,  0.3247,  0.5099,  1.4491],\n",
      "        [-0.0234,  0.3174,  0.7946, -0.8283,  0.7356],\n",
      "        [-0.2517, -0.6364, -0.1046,  1.0708, -1.2382],\n",
      "        [ 2.7432, -0.7715,  0.2367,  0.0392, -0.0961]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.3833],\n",
      "        [-0.1479],\n",
      "        [-1.0019],\n",
      "        [ 0.5949]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8322, -0.6850,  0.6864,  0.5926,  0.9775],\n",
      "        [-0.3109,  0.5802, -1.2328,  0.8974,  0.4638],\n",
      "        [-0.3892, -0.4668,  0.6094, -0.0123, -0.7894],\n",
      "        [-0.6142,  0.8600,  1.9657, -0.7879, -0.4290]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6197,  0.0833, -0.3161,  0.6810,  0.5449],\n",
      "        [-0.2546, -0.2853, -0.3478, -0.9050,  0.2123],\n",
      "        [ 0.3131,  0.4945,  0.8429,  0.8400,  1.1670],\n",
      "        [ 0.2082, -0.3195,  0.3377,  0.3519, -0.2109]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.8322, -0.6850,  0.6864,  0.5926,  0.9775],\n",
      "        [-0.3109,  0.5802, -1.2328,  0.8974,  0.4638],\n",
      "        [-0.3892, -0.4668,  0.6094, -0.0123, -0.7894],\n",
      "        [-0.6142,  0.8600,  1.9657, -0.7879, -0.4290]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1464],\n",
      "        [-0.3713],\n",
      "        [-0.7707],\n",
      "        [ 0.0745]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.0615, -2.1652, -0.3148, -0.3802, -1.9875],\n",
      "        [ 0.4911, -2.1349,  0.7932,  1.4392, -0.7950],\n",
      "        [ 0.2834,  0.0617, -3.1232, -1.2183,  0.0733],\n",
      "        [ 0.0804,  0.4727, -0.3721,  0.4973, -0.6197]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5868, -1.0223,  0.4576, -0.0631, -0.0234],\n",
      "        [ 0.5842, -0.9327, -0.3309, -0.7811, -0.5080],\n",
      "        [ 0.0813,  1.5193,  1.4359,  0.7205,  1.7358],\n",
      "        [ 0.1431,  0.0498,  0.3916,  0.0131,  0.2963]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.0615, -2.1652, -0.3148, -0.3802, -1.9875],\n",
      "        [ 0.4911, -2.1349,  0.7932,  1.4392, -0.7950],\n",
      "        [ 0.2834,  0.0617, -3.1232, -1.2183,  0.0733],\n",
      "        [ 0.0804,  0.4727, -0.3721,  0.4973, -0.6197]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.1038],\n",
      "        [ 1.2955],\n",
      "        [-5.1184],\n",
      "        [-0.2877]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3459, -0.3077,  0.5146, -0.8124,  0.6322],\n",
      "        [ 0.7560, -0.2764,  0.3855,  1.4390,  2.1462],\n",
      "        [-1.7194,  1.7632,  0.4969,  0.8325, -0.1253],\n",
      "        [ 0.5448,  1.2931,  0.4320, -0.0885, -0.2632]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.4371, -0.9788, -1.7008, -1.0803, -1.4403],\n",
      "        [-0.1400, -1.0495, -0.9020, -1.4142, -1.7844],\n",
      "        [ 1.9706,  3.0010,  2.1749,  2.3921,  3.8815],\n",
      "        [ 0.3973, -0.0050,  0.0648, -1.5592, -0.2222]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3459, -0.3077,  0.5146, -0.8124,  0.6322],\n",
      "        [ 0.7560, -0.2764,  0.3855,  1.4390,  2.1462],\n",
      "        [-1.7194,  1.7632,  0.4969,  0.8325, -0.1253],\n",
      "        [ 0.5448,  1.2931,  0.4320, -0.0885, -0.2632]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7583],\n",
      "        [-6.0283],\n",
      "        [ 4.4886],\n",
      "        [ 0.4344]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3294, -0.8109,  0.5540, -1.7896, -0.9283],\n",
      "        [-1.7323, -0.7715, -0.7674, -0.0686,  0.5636],\n",
      "        [-0.0459,  0.5606, -1.4284, -0.4958, -1.0947],\n",
      "        [ 0.0587,  1.0689,  0.0848, -0.1963,  0.4521]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3569, -0.8484, -1.0546, -0.3711, -0.7542],\n",
      "        [ 1.6967,  1.9969,  2.3750,  1.8551,  1.7580],\n",
      "        [-0.2349,  1.0893, -0.5215, -1.6735,  0.0599],\n",
      "        [-0.0476, -0.4957, -0.3444, -0.5101,  0.3832]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3294, -0.8109,  0.5540, -1.7896, -0.9283],\n",
      "        [-1.7323, -0.7715, -0.7674, -0.0686,  0.5636],\n",
      "        [-0.0459,  0.5606, -1.4284, -0.4958, -1.0947],\n",
      "        [ 0.0587,  1.0689,  0.0848, -0.1963,  0.4521]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.3504],\n",
      "        [-5.4388],\n",
      "        [ 2.1305],\n",
      "        [-0.2885]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2242, -0.0449, -0.8477, -0.8702,  0.5133],\n",
      "        [-0.4476,  0.2847, -0.3678,  0.0311,  2.0218],\n",
      "        [ 0.1641,  0.5155,  0.3495, -0.1987,  0.5238],\n",
      "        [-0.8364,  0.6634,  0.3135, -0.7206,  0.5103]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.2801, -0.4960, -2.0365, -0.9971, -1.7582],\n",
      "        [ 0.2892,  0.5013,  0.2952, -0.1845, -0.1721],\n",
      "        [-1.0270, -1.4155, -0.6178, -0.8659, -2.2464],\n",
      "        [ 0.6886,  0.4971, -0.1170, -0.5354,  0.5817]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2242, -0.0449, -0.8477, -0.8702,  0.5133],\n",
      "        [-0.4476,  0.2847, -0.3678,  0.0311,  2.0218],\n",
      "        [ 0.1641,  0.5155,  0.3495, -0.1987,  0.5238],\n",
      "        [-0.8364,  0.6634,  0.3135, -0.7206,  0.5103]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.7764],\n",
      "        [-0.4490],\n",
      "        [-2.1189],\n",
      "        [ 0.3997]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7264,  1.2626,  0.6822,  1.3191,  2.1702],\n",
      "        [ 0.9793,  0.1062,  1.0483, -0.7637,  1.0497],\n",
      "        [-1.3142, -1.6009,  0.2598, -0.2650, -1.5971],\n",
      "        [ 1.0763,  1.0968, -0.5339,  0.6221,  0.6271]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.8131, -1.3268, -1.9308, -1.2867, -2.9261],\n",
      "        [ 0.0589, -0.2944,  0.4321, -0.2917, -0.0398],\n",
      "        [ 0.1652, -0.4321, -0.7017, -0.5279,  0.0688],\n",
      "        [-0.6461, -0.0509,  0.1607,  0.4893,  0.5381]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7264,  1.2626,  0.6822,  1.3191,  2.1702],\n",
      "        [ 0.9793,  0.1062,  1.0483, -0.7637,  1.0497],\n",
      "        [-1.3142, -1.6009,  0.2598, -0.2650, -1.5971],\n",
      "        [ 1.0763,  1.0968, -0.5339,  0.6221,  0.6271]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-9.6364],\n",
      "        [ 0.6603],\n",
      "        [ 0.3225],\n",
      "        [-0.1952]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.3759,  3.1162, -0.4602, -0.5715,  1.1112],\n",
      "        [ 1.1293,  0.2737, -0.8587, -0.4801,  0.3828],\n",
      "        [-0.0313,  0.6014, -1.9001,  0.4617,  0.5257],\n",
      "        [-1.3994, -1.9829,  0.5188,  0.2807, -1.1715]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 2.6724,  2.6722,  1.9919,  2.1618,  3.6245],\n",
      "        [ 0.4268,  0.0486, -0.5654, -0.1013,  0.1800],\n",
      "        [ 0.0617,  0.3709, -0.3906, -0.0699, -0.2091],\n",
      "        [ 0.2224,  0.3304,  0.1559, -0.0801,  0.2226]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.3759,  3.1162, -0.4602, -0.5715,  1.1112],\n",
      "        [ 1.1293,  0.2737, -0.8587, -0.4801,  0.3828],\n",
      "        [-0.0313,  0.6014, -1.9001,  0.4617,  0.5257],\n",
      "        [-1.3994, -1.9829,  0.5188,  0.2807, -1.1715]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 11.2070],\n",
      "        [  1.0984],\n",
      "        [  0.8210],\n",
      "        [ -1.1686]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8064,  0.7563,  0.6833,  0.2464,  1.7917],\n",
      "        [-1.8761, -0.5516,  2.2392,  0.8534, -0.8267],\n",
      "        [ 0.9152, -1.3166,  0.0954, -0.7866, -2.8626],\n",
      "        [-0.5882,  0.1307,  0.6887,  0.3630,  1.5854]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-1.2706, -1.7333, -1.6818, -1.7292, -3.2780],\n",
      "        [-0.1390, -0.5749, -0.6471, -0.1056, -1.5269],\n",
      "        [-0.3323,  0.1103, -0.4463, -0.4455, -0.9892],\n",
      "        [ 0.8028,  0.5956,  0.8883,  0.6623,  1.1200]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8064,  0.7563,  0.6833,  0.2464,  1.7917],\n",
      "        [-1.8761, -0.5516,  2.2392,  0.8534, -0.8267],\n",
      "        [ 0.9152, -1.3166,  0.0954, -0.7866, -2.8626],\n",
      "        [-0.5882,  0.1307,  0.6887,  0.3630,  1.5854]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-7.7345],\n",
      "        [ 0.3012],\n",
      "        [ 2.6903],\n",
      "        [ 2.2335]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.4530,  2.1335, -0.5078,  0.9005, -0.1486],\n",
      "        [-0.6476,  0.4461,  1.7046,  1.6714,  0.0340],\n",
      "        [-2.8114, -0.9232, -0.2078,  0.8582, -0.2745],\n",
      "        [-1.3237,  0.5212, -0.0245,  0.5641, -1.0718]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 1.5820,  1.6973,  1.4818,  0.8662,  2.1174],\n",
      "        [ 0.0641, -0.8716,  0.0834, -0.5873, -0.4500],\n",
      "        [-0.9108, -1.5914, -1.1578, -1.6520, -2.6284],\n",
      "        [-1.2118, -0.3462, -0.7598, -0.6997, -2.0194]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.4530,  2.1335, -0.5078,  0.9005, -0.1486],\n",
      "        [-0.6476,  0.4461,  1.7046,  1.6714,  0.0340],\n",
      "        [-2.8114, -0.9232, -0.2078,  0.8582, -0.2745],\n",
      "        [-1.3237,  0.5212, -0.0245,  0.5641, -1.0718]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 4.0507],\n",
      "        [-1.2851],\n",
      "        [ 3.5737],\n",
      "        [ 3.2118]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.8254,  1.4325,  1.6937,  1.4511,  0.2094],\n",
      "        [-2.2135,  1.9515, -2.1558,  0.5380,  0.2328],\n",
      "        [ 0.4082, -1.0348,  0.9860,  0.8137,  0.4265],\n",
      "        [ 0.4859,  1.5640,  1.9651,  0.1366, -1.1513]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.1379, -0.1703,  0.0145,  0.2933,  0.2178],\n",
      "        [ 0.2882,  0.5058,  0.6310,  0.2919,  0.9501],\n",
      "        [-1.6082, -1.7747, -2.3795, -2.3239, -4.5497],\n",
      "        [-1.4373, -1.3269, -1.1881, -1.7272, -3.4320]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.8254,  1.4325,  1.6937,  1.4511,  0.2094],\n",
      "        [-2.2135,  1.9515, -2.1558,  0.5380,  0.2328],\n",
      "        [ 0.4082, -1.0348,  0.9860,  0.8137,  0.4265],\n",
      "        [ 0.4859,  1.5640,  1.9651,  0.1366, -1.1513]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1379],\n",
      "        [-0.6331],\n",
      "        [-4.9979],\n",
      "        [-1.3930]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1895, -0.4211, -0.8991,  0.0499, -0.4156],\n",
      "        [-1.6042, -0.8491,  0.0005,  0.9122,  0.2004],\n",
      "        [ 0.6086, -0.6627, -1.0164,  1.4038, -0.2844],\n",
      "        [-0.8083,  0.1008,  0.2922,  0.3058, -0.2221]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.2870, -0.5264, -0.0725, -0.1553, -0.5418],\n",
      "        [ 0.2076,  0.8267,  0.6272,  0.2457,  1.0240],\n",
      "        [ 0.4037,  0.1861,  0.3740,  0.0406,  0.3327],\n",
      "        [-0.1857, -0.8938, -1.8060, -1.5169, -1.6696]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1895, -0.4211, -0.8991,  0.0499, -0.4156],\n",
      "        [-1.6042, -0.8491,  0.0005,  0.9122,  0.2004],\n",
      "        [ 0.6086, -0.6627, -1.0164,  1.4038, -0.2844],\n",
      "        [-0.8083,  0.1008,  0.2922,  0.3058, -0.2221]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.1629],\n",
      "        [-0.6052],\n",
      "        [-0.2954],\n",
      "        [-0.5608]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.3744,  1.8117,  0.6512,  0.7333,  0.5007],\n",
      "        [-0.0306,  0.4042,  0.1793,  0.2199,  0.3687],\n",
      "        [ 0.1333, -1.1150,  0.9622, -1.3849, -0.0252],\n",
      "        [-1.1497,  0.7183, -1.0413, -1.2831,  0.9699]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0663,  0.2968,  1.0888,  0.0662,  0.1966],\n",
      "        [ 0.3032, -0.0076,  0.8533, -0.5104,  0.8027],\n",
      "        [ 0.4059, -0.5846,  0.1444, -0.1050, -0.7466],\n",
      "        [ 0.0064, -1.2771, -1.4625, -1.3149, -1.6791]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.3744,  1.8117,  0.6512,  0.7333,  0.5007],\n",
      "        [-0.0306,  0.4042,  0.1793,  0.2199,  0.3687],\n",
      "        [ 0.1333, -1.1150,  0.9622, -1.3849, -0.0252],\n",
      "        [-1.1497,  0.7183, -1.0413, -1.2831,  0.9699]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.4185],\n",
      "        [ 0.3244],\n",
      "        [ 1.0091],\n",
      "        [ 0.6570]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.1873, -1.3784,  1.0242,  1.4302, -1.3481],\n",
      "        [-0.9468,  0.6606,  0.3464,  0.5888,  0.9773],\n",
      "        [-0.7109,  0.8252,  1.0353,  0.9836,  1.0998],\n",
      "        [-0.9891, -1.9858, -0.1247,  0.7156, -0.2159]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.0022, -0.1456, -0.9315, -0.8207, -0.8843],\n",
      "        [ 0.2951, -0.5842, -0.3656, -0.2606,  0.1726],\n",
      "        [ 0.0509, -0.3340, -0.1919, -0.6419, -1.1509],\n",
      "        [ 0.3607,  0.5229,  0.0041,  0.8013,  0.0889]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.1873, -1.3784,  1.0242,  1.4302, -1.3481],\n",
      "        [-0.9468,  0.6606,  0.3464,  0.5888,  0.9773],\n",
      "        [-0.7109,  0.8252,  1.0353,  0.9836,  1.0998],\n",
      "        [-0.9891, -1.9858, -0.1247,  0.7156, -0.2159]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.7377],\n",
      "        [-0.7767],\n",
      "        [-2.4077],\n",
      "        [-0.8414]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2423,  1.8976,  0.1933, -0.1597,  0.1430],\n",
      "        [-0.8217, -0.8249,  0.1194,  1.9878, -0.2810],\n",
      "        [ 0.7738,  1.1763,  0.5733,  0.1330,  1.1947],\n",
      "        [-0.5596, -0.4116, -0.3272,  1.1587, -0.3510]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.9607, -0.3503,  0.1720,  0.3884,  1.3642],\n",
      "        [ 0.5898, -0.2923, -0.5000, -0.2027,  0.3235],\n",
      "        [ 0.5101,  0.7304,  0.4165,  0.8410,  1.5965],\n",
      "        [ 0.5871,  0.4472, -0.4057,  0.3339,  0.5354]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2423,  1.8976,  0.1933, -0.1597,  0.1430],\n",
      "        [-0.8217, -0.8249,  0.1194,  1.9878, -0.2810],\n",
      "        [ 0.7738,  1.1763,  0.5733,  0.1330,  1.1947],\n",
      "        [-0.5596, -0.4116, -0.3272,  1.1587, -0.3510]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6919],\n",
      "        [-0.7970],\n",
      "        [ 3.5119],\n",
      "        [-0.1809]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.1844, -0.5402, -0.2733,  1.7032, -0.0701],\n",
      "        [ 0.4703, -0.6102, -0.5921,  1.9523,  0.8179],\n",
      "        [-0.2906, -0.4243, -1.2098,  0.7657, -0.0694],\n",
      "        [-1.0921, -0.8560, -1.0369,  0.3022,  0.4534]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8222,  1.1179,  0.7066,  0.8595,  1.5120],\n",
      "        [ 0.8599,  0.6957,  0.2694,  0.0971,  0.6715],\n",
      "        [-0.3084, -0.6120, -1.0750, -0.3634, -1.7466],\n",
      "        [ 0.9454,  0.5171,  0.2855, -0.1778, -0.4503]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.1844, -0.5402, -0.2733,  1.7032, -0.0701],\n",
      "        [ 0.4703, -0.6102, -0.5921,  1.9523,  0.8179],\n",
      "        [-0.2906, -0.4243, -1.2098,  0.7657, -0.0694],\n",
      "        [-1.0921, -0.8560, -1.0369,  0.3022,  0.4534]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.7124],\n",
      "        [ 0.5592],\n",
      "        [ 1.4930],\n",
      "        [-2.0290]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.8609,  1.1306,  0.6218,  0.0319, -0.3140],\n",
      "        [-0.4004, -1.5532,  0.1220, -0.0494, -0.0070],\n",
      "        [-0.3807,  3.0361,  0.1259, -0.7633, -0.2786],\n",
      "        [-0.8627,  1.6110, -0.4062, -1.0050,  0.0934]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4648,  0.4381,  0.0238,  0.5362,  0.2673],\n",
      "        [ 0.1467,  0.3005,  0.0493, -0.5253, -0.6674],\n",
      "        [-0.9238, -1.0982, -1.3286, -1.5521, -1.6737],\n",
      "        [ 1.1512,  0.4200,  1.1080,  0.6999,  1.1183]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.8609,  1.1306,  0.6218,  0.0319, -0.3140],\n",
      "        [-0.4004, -1.5532,  0.1220, -0.0494, -0.0070],\n",
      "        [-0.3807,  3.0361,  0.1259, -0.7633, -0.2786],\n",
      "        [-0.8627,  1.6110, -0.4062, -1.0050,  0.0934]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.8435],\n",
      "        [-0.4888],\n",
      "        [-1.4988],\n",
      "        [-1.3654]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 0.0321,  1.2534,  0.7244, -1.2356, -0.3396],\n",
      "        [ 0.8663, -1.7069,  0.6528, -0.5789,  0.9052],\n",
      "        [-1.7682,  0.2187, -0.2824,  0.2810,  2.2118],\n",
      "        [ 0.9811, -0.6466, -0.1264, -0.9836,  2.1139]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8338,  0.6316,  0.3875,  1.1059, -0.3470],\n",
      "        [ 0.3458, -0.2477, -0.0436, -0.2945, -0.0332],\n",
      "        [-0.6512, -0.5957,  0.2253, -0.5030, -1.1512],\n",
      "        [ 1.6082,  1.2565,  1.6035,  0.7106,  2.2914]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 0.0321,  1.2534,  0.7244, -1.2356, -0.3396],\n",
      "        [ 0.8663, -1.7069,  0.6528, -0.5789,  0.9052],\n",
      "        [-1.7682,  0.2187, -0.2824,  0.2810,  2.2118],\n",
      "        [ 0.9811, -0.6466, -0.1264, -0.9836,  2.1139]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.1496],\n",
      "        [ 0.8344],\n",
      "        [-1.7301],\n",
      "        [ 4.7075]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.7441, -1.7153,  0.3377,  0.2550,  0.5067],\n",
      "        [ 0.8135,  0.2016,  1.1272, -0.6591,  1.1939],\n",
      "        [-0.3824,  0.5120,  0.2137, -0.1472,  0.2362],\n",
      "        [ 0.5498, -1.0827, -1.2679, -0.7827,  0.8037]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5145, -0.0997,  0.4517,  1.3077,  0.3040],\n",
      "        [ 0.0712, -0.3356, -0.4105, -0.4278, -1.4905],\n",
      "        [ 0.1689,  0.2012,  0.4699,  0.4572,  0.0594],\n",
      "        [-1.0561, -1.6040, -0.6901, -1.0418, -1.3709]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.7441, -1.7153,  0.3377,  0.2550,  0.5067],\n",
      "        [ 0.8135,  0.2016,  1.1272, -0.6591,  1.1939],\n",
      "        [-0.3824,  0.5120,  0.2137, -0.1472,  0.2362],\n",
      "        [ 0.5498, -1.0827, -1.2679, -0.7827,  0.8037]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.0862],\n",
      "        [-1.9701],\n",
      "        [ 0.0856],\n",
      "        [ 1.7447]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.6570, -0.5689, -0.8239,  0.1194, -0.4128],\n",
      "        [-0.3351,  1.2903, -1.4261,  1.5003,  0.2115],\n",
      "        [-0.0841,  1.3477,  1.2700, -1.0121,  0.1757],\n",
      "        [ 1.5521,  0.2833, -1.8427, -0.9303,  0.2641]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5495,  0.4399,  0.5163,  1.3697,  0.3135],\n",
      "        [ 0.5752,  0.6484,  0.4015,  0.2614,  1.3819],\n",
      "        [ 0.4926, -0.2342,  0.0349, -0.2096, -0.5104],\n",
      "        [-0.4455, -1.1297, -1.3075, -1.1552, -2.2503]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.6570, -0.5689, -0.8239,  0.1194, -0.4128],\n",
      "        [-0.3351,  1.2903, -1.4261,  1.5003,  0.2115],\n",
      "        [-0.0841,  1.3477,  1.2700, -1.0121,  0.1757],\n",
      "        [ 1.5521,  0.2833, -1.8427, -0.9303,  0.2641]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.0024],\n",
      "        [ 0.7557],\n",
      "        [-0.1903],\n",
      "        [ 1.8781]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.4188,  0.4503,  0.8442,  0.3059,  2.3425],\n",
      "        [ 0.3540,  1.0025,  0.3181,  0.7472, -0.4220],\n",
      "        [-0.7948,  0.8331, -0.6079,  0.5308,  0.3796],\n",
      "        [-0.5041, -0.1810,  2.0215, -0.8114, -0.9381]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.8037,  0.5656,  0.6162,  1.3827,  0.4904],\n",
      "        [ 0.4828,  0.2253,  0.2504, -0.4144,  0.8929],\n",
      "        [ 0.4769,  0.0342, -0.6837, -0.6001, -1.1288],\n",
      "        [-0.8014, -1.3073, -1.7781, -2.2359, -3.3404]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.4188,  0.4503,  0.8442,  0.3059,  2.3425],\n",
      "        [ 0.3540,  1.0025,  0.3181,  0.7472, -0.4220],\n",
      "        [-0.7948,  0.8331, -0.6079,  0.5308,  0.3796],\n",
      "        [-0.5041, -0.1810,  2.0215, -0.8114, -0.9381]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0100],\n",
      "        [-0.2101],\n",
      "        [-0.6821],\n",
      "        [ 1.9938]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.2513,  0.1090, -0.0399, -0.7069,  2.1533],\n",
      "        [ 0.0991, -0.2510,  0.2908,  1.3666, -1.0837],\n",
      "        [-0.5664,  0.3832,  0.7095, -0.1807, -0.3008],\n",
      "        [ 1.8796, -1.5048, -1.8019,  0.4192, -0.3938]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3252,  0.6474, -0.3170,  0.7760, -0.3843],\n",
      "        [ 0.3337,  0.2279,  0.1153,  0.3129,  0.6082],\n",
      "        [ 0.1153, -0.4037, -0.2311, -0.2381, -0.9349],\n",
      "        [-0.1056, -0.6111, -0.3110,  0.4854, -0.1178]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.2513,  0.1090, -0.0399, -0.7069,  2.1533],\n",
      "        [ 0.0991, -0.2510,  0.2908,  1.3666, -1.0837],\n",
      "        [-0.5664,  0.3832,  0.7095, -0.1807, -0.3008],\n",
      "        [ 1.8796, -1.5048, -1.8019,  0.4192, -0.3938]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.2112],\n",
      "        [-0.2221],\n",
      "        [-0.0598],\n",
      "        [ 1.5313]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4956,  1.4175,  0.8041,  0.6901,  0.2886],\n",
      "        [ 0.3962, -0.5223, -0.2993, -0.9405, -0.6114],\n",
      "        [ 1.4631,  0.2974,  0.5636,  1.2979,  1.1370],\n",
      "        [ 0.8546, -1.8040, -0.5043, -2.1510,  0.3951]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.6678,  1.3788,  0.5251,  0.6473,  0.6179],\n",
      "        [ 0.1377, -0.4313, -0.2307,  0.2691, -0.0343],\n",
      "        [ 0.2146, -0.5780, -0.5473, -0.3634, -1.2218],\n",
      "        [-0.5225,  0.0098, -0.4718, -0.9237, -2.2413]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4956,  1.4175,  0.8041,  0.6901,  0.2886],\n",
      "        [ 0.3962, -0.5223, -0.2993, -0.9405, -0.6114],\n",
      "        [ 1.4631,  0.2974,  0.5636,  1.2979,  1.1370],\n",
      "        [ 0.8546, -1.8040, -0.5043, -2.1510,  0.3951]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 2.0029],\n",
      "        [ 0.1167],\n",
      "        [-2.0273],\n",
      "        [ 0.8751]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.2674, -0.7899,  1.2186, -0.8266,  0.7204],\n",
      "        [-0.0260,  0.0091, -0.1307, -0.4713, -1.1515],\n",
      "        [ 1.1634, -0.7612,  0.4112,  0.3256,  0.5849],\n",
      "        [-0.1833, -2.2714,  0.1640, -0.2055, -0.7992]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.1742,  0.4896, -0.6569,  0.0581, -0.9057],\n",
      "        [ 0.2352,  0.4504, -0.0571,  0.1306, -0.4190],\n",
      "        [ 0.6834,  0.4203,  0.8470, -0.0290,  1.4904],\n",
      "        [-0.9523, -1.2537, -0.5735, -0.9672, -1.4982]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.2674, -0.7899,  1.2186, -0.8266,  0.7204],\n",
      "        [-0.0260,  0.0091, -0.1307, -0.4713, -1.1515],\n",
      "        [ 1.1634, -0.7612,  0.4112,  0.3256,  0.5849],\n",
      "        [-0.1833, -2.2714,  0.1640, -0.2055, -0.7992]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-1.6670],\n",
      "        [ 0.4264],\n",
      "        [ 1.6858],\n",
      "        [ 4.3244]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.1191,  0.3122,  1.0329, -0.0696,  0.6728],\n",
      "        [-1.0186,  1.7512,  0.0553,  0.0433,  0.4571],\n",
      "        [-2.0136,  0.3094,  0.3187,  0.5411,  0.4452],\n",
      "        [-0.5022,  1.2030,  0.7085,  0.6944,  2.7273]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.3241,  0.6869,  1.0750,  0.7547,  1.0350],\n",
      "        [ 0.8070,  0.2772, -0.2008, -0.6161, -0.5441],\n",
      "        [-0.1856, -0.8649, -0.9121, -0.4404, -1.1511],\n",
      "        [-2.4477, -2.3199, -2.2607, -2.9042, -4.5878]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.1191,  0.3122,  1.0329, -0.0696,  0.6728],\n",
      "        [-1.0186,  1.7512,  0.0553,  0.0433,  0.4571],\n",
      "        [-2.0136,  0.3094,  0.3187,  0.5411,  0.4452],\n",
      "        [-0.5022,  1.2030,  0.7085,  0.6944,  2.7273]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[  1.6059],\n",
      "        [ -0.6230],\n",
      "        [ -0.9353],\n",
      "        [-17.6925]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-0.7207, -0.5733, -0.1342, -0.5912,  1.3819],\n",
      "        [-1.6549,  0.7215,  1.9850, -0.3760, -0.9756],\n",
      "        [-0.0572, -0.0215,  0.0515, -0.7083,  1.1836],\n",
      "        [-1.5671,  0.5331, -1.3193, -0.6299, -0.3779]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[-0.3781,  1.2724, -0.5247,  0.6763,  0.3800],\n",
      "        [ 0.5264,  0.2626,  0.2224,  0.5206, -0.3721],\n",
      "        [-0.0618, -0.3081, -0.5249, -1.0175, -0.2169],\n",
      "        [ 3.5264,  3.9174,  4.8230,  2.5409,  5.8442]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-0.7207, -0.5733, -0.1342, -0.5912,  1.3819],\n",
      "        [-1.6549,  0.7215,  1.9850, -0.3760, -0.9756],\n",
      "        [-0.0572, -0.0215,  0.0515, -0.7083,  1.1836],\n",
      "        [-1.5671,  0.5331, -1.3193, -0.6299, -0.3779]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ -0.2612],\n",
      "        [ -0.0729],\n",
      "        [  0.4471],\n",
      "        [-13.6095]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[ 1.4823,  0.3894, -1.8414,  0.4250,  0.6930],\n",
      "        [-2.3906,  2.6077,  2.5091, -1.4584,  0.8570],\n",
      "        [ 0.1392, -1.0945,  1.4246, -0.4133, -0.5786],\n",
      "        [-2.2847,  2.0494,  0.9011,  1.1739,  0.7321]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4700,  0.7577,  0.0100,  0.5589,  0.4478],\n",
      "        [ 0.0740,  0.0628,  0.6724,  0.6030,  0.3510],\n",
      "        [ 0.1472, -0.9281, -0.6903, -1.2752, -0.8946],\n",
      "        [ 0.4346,  0.2427,  0.6497, -0.7309,  0.2599]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[ 1.4823,  0.3894, -1.8414,  0.4250,  0.6930],\n",
      "        [-2.3906,  2.6077,  2.5091, -1.4584,  0.8570],\n",
      "        [ 0.1392, -1.0945,  1.4246, -0.4133, -0.5786],\n",
      "        [-2.2847,  2.0494,  0.9011,  1.1739,  0.7321]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 1.5212],\n",
      "        [ 1.0955],\n",
      "        [ 1.0976],\n",
      "        [-0.5779]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.3915,  0.7681, -1.6004, -0.3525,  0.8510],\n",
      "        [-1.7954,  0.3492,  1.8414, -0.0671, -0.5123],\n",
      "        [ 0.3709, -0.5117, -2.0899,  0.6923,  2.1525],\n",
      "        [ 0.5024,  0.7329,  0.9367,  0.1534,  0.1176]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.4487,  0.3394, -0.4289, -0.4120, -0.8893],\n",
      "        [ 0.3838, -0.4634,  0.1021, -0.1067, -0.7591],\n",
      "        [-0.7138, -1.0023, -1.1482, -1.2373, -1.6310],\n",
      "        [ 0.0404, -0.1083, -0.1043, -0.5101,  0.8242]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.3915,  0.7681, -1.6004, -0.3525,  0.8510],\n",
      "        [-1.7954,  0.3492,  1.8414, -0.0671, -0.5123],\n",
      "        [ 0.3709, -0.5117, -2.0899,  0.6923,  2.1525],\n",
      "        [ 0.5024,  0.7329,  0.9367,  0.1534,  0.1176]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[-0.2887],\n",
      "        [-0.2669],\n",
      "        [-1.7195],\n",
      "        [-0.1381]], device='cuda:0') torch.Size([4, 1])\n",
      "weights:     tensor([[-1.4984,  0.4848, -0.1983, -0.0516,  1.3353],\n",
      "        [-0.7251,  0.2267,  2.3193,  0.7635,  0.1896],\n",
      "        [-0.4975,  0.4577, -0.8026,  0.2545, -0.5089],\n",
      "        [-0.8352, -0.4702, -0.9673,  0.9127, -0.4847]], device='cuda:0')\n",
      "length  5\n",
      "policy list:     dict_values([<ppo.PPO object at 0x7fcc761370f0>, <ppo.PPO object at 0x7fcc6fba9c50>, <ppo.PPO object at 0x7fcc6fba9e48>, <ppo.PPO object at 0x7fcc6fa257b8>, <ppo.PPO object at 0x7fcc6fbe6ef0>])\n",
      "length:  5\n",
      "multi-policy multi-env action tensor: \n",
      "tensor([[ 0.5534,  0.7998,  0.7978,  1.2810,  0.9131],\n",
      "        [ 0.1739, -0.0412, -0.3844, -0.1067,  0.1081],\n",
      "        [ 0.3173, -1.1472, -0.8022, -0.8684, -0.2026],\n",
      "        [ 0.3191, -0.0471,  0.2030, -0.1494,  0.0945]], device='cuda:0') torch.Size([4, 5])\n",
      "ensemble weights:\n",
      "tensor([[-1.4984,  0.4848, -0.1983, -0.0516,  1.3353],\n",
      "        [-0.7251,  0.2267,  2.3193,  0.7635,  0.1896],\n",
      "        [-0.4975,  0.4577, -0.8026,  0.2545, -0.5089],\n",
      "        [-0.8352, -0.4702, -0.9673,  0.9127, -0.4847]], device='cuda:0') torch.Size([4, 5])\n",
      "action:\n",
      "tensor([[ 0.5535],\n",
      "        [-1.0879],\n",
      "        [-0.1571],\n",
      "        [-0.6229]], device='cuda:0') torch.Size([4, 1])\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'dim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-150-cdd359ff4335>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#***************************************************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0menv_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_weighting\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_itrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mavg_rew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-150-cdd359ff4335>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0;31m#***************************************************************************************\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0menv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0menv_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_env\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mensemble\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbaseline_weighting\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_itrs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m         \u001b[0mavg_rew\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-5c206dd2f44b>\u001b[0m in \u001b[0;36mtest_env\u001b[0;34m(self, env, ensemble_net, weight_net, comp_model)\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 158\u001b[0;31m             \u001b[0msample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m             \u001b[0;31m#state = torch.FloatTensor(state).unsqueeze(0).to(device)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-145-5c206dd2f44b>\u001b[0m in \u001b[0;36mtest_action\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mtest_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweight_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0mweights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mensemble_net\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweighted_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/school/276C-final/final/ppo.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0mmu\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0mstd\u001b[0m   \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_std\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexpand_as\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    490\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 491\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    492\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/pytorch-0.4/lib/python3.5/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/environments/pytorch-0.4/lib/python3.5/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0;34m-\u001b[0m \u001b[0mOutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0m_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m     \"\"\"\n\u001b[0;32m--> 990\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdim\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m         \u001b[0;31m# fused op is marginally faster\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'dim'"
     ]
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "ppo_updates = 0\n",
    "save_interval = 5\n",
    "#Plotting Flags\n",
    "indvplots=0\n",
    "rewplots=1\n",
    "stdplots=1\n",
    "which_plts = [indvplots,rewplots,stdplots]\n",
    "\n",
    "while baseline_weighting.frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    #collect data\n",
    "    log_probs, values, states, actions, rewards, masks, next_value = baseline_weighting.collect_data(envs)\n",
    "    \n",
    "    #compute gae\n",
    "    returns = baseline_weighting.compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    #update policy\n",
    "    baseline_weighting.ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, values)\n",
    "    \n",
    "    #plot\n",
    "    avg_rew = []\n",
    "    std = []\n",
    "    \n",
    "    #Environment testing and data logging\n",
    "    #***************************************************************************************\n",
    "    for env in tests.envs:\n",
    "        env_rewards = ([tests.test_env(env, ensemble, baseline_weighting) for _ in range(test_itrs)])\n",
    "        avg_rew.append(np.mean(env_rewards))\n",
    "        std.append(np.std(env_rewards))\n",
    "\n",
    "    test_avg_rewards.append(avg_rew)\n",
    "    test_stds.append(std)\n",
    "\n",
    "    if avg_rew[training_env_index] > threshold_reward and EARLY_STOPPING: #avg_rew[0] is testing on the non edited environment\n",
    "        tests.plot(baseline_weighting.frame_idx, test_avg_rewards, test_stds, which_plts, 1)\n",
    "        early_stop = True\n",
    "    else:\n",
    "        if ppo_updates and ppo_updates % save_interval == 0:\n",
    "            baseline_weighting.save_weights(baseline_dir + env_name + '_weights' + str(ppo_updates/save_interval))\n",
    "            tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, 1, str(ppo_updates/save_interval))\n",
    "        else:\n",
    "            tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, 0)\n",
    "            \n",
    "    ppo_updates = ppo_updates + 1 #Loop counter\n",
    "    #***************************************************************************************\n",
    "\n",
    "baseline_weighting.save_weights(baseline_dir + env_name + '_endweights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compensator\n",
    "Train a PPO compensator policy.\n",
    "\n",
    "### Environments:\n",
    "\n",
    "#### InvertedPendulum modified environment:\n",
    "<img src=\"./notebookImages/invertedpendulum.png\" width=\"300\">\n",
    "\n",
    "#### Halfcheetah modified environment:\n",
    "<img src=\"./notebookImages/halfcheetah.png\" width=\"300\">\n",
    "\n",
    "#### Ant modified environment:\n",
    "<img src=\"./notebookImages/ant.png\" width=\"300\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from gym import envs\n",
    "\n",
    "VISUALIZE = False\n",
    "COMPENSATION = False\n",
    "EARLY_STOPPING = True\n",
    "logging_interval = 10\n",
    "num_envs = 4\n",
    "env_key = \"v10\" #Unique identifier for custom envs (case sensitive)\n",
    "\n",
    "#env_name = 'InvertedPendulumModified-multi-v10'\n",
    "#env_name = \"Pendulum-v0\"\n",
    "#env_name = \"HalfCheetah-v2\"\n",
    "#env_name = 'FetchReach-v1'\n",
    "env_name = 'InvertedPendulumModified-multi-v10'\n",
    "\"\"\"\"\n",
    "['InvertedPendulum-v2',\n",
    " 'InvertedPendulumModified-base-v10',\n",
    " 'InvertedPendulumModified-mass-v10',\n",
    " 'InvertedPendulumModified-inertia-v10',\n",
    " 'InvertedPendulumModified-friction-v10',\n",
    " 'InvertedPendulumModified-tilt-v10',\n",
    " 'InvertedPendulumModified-motor-v10',\n",
    " 'InvertedPendulumModified-multi-v10']\n",
    " \"\"\"\n",
    "\n",
    "env_ids = [spec.id for spec in envs.registry.all()]\n",
    "test_env_names = ['InvertedPendulum-v2'] + [x for x in env_ids if str(env_key) in x] #Returns a list of environment names matching identifier\n",
    "training_env_index = test_env_names.index(env_name)\n",
    "\n",
    "#test_env_names = [x for x in env_ids if str(env_key) in x] #Returns a list of environment names matching identifier\n",
    "\n",
    "#Training envs (all the same)\n",
    "envs = [make_env(env_name) for i in range(num_envs)]\n",
    "envs = SubprocVecEnv(envs)\n",
    "\n",
    "#Plotting Results and figures, save weights\n",
    "script_dir = os.getcwd()\n",
    "time_stamp = datetime.datetime.now().replace(microsecond=0).isoformat()\n",
    "results_dir = os.path.join(script_dir, 'plotting_results/' + env_name + time_stamp + '/')\n",
    "baseline_dir = os.path.join(script_dir, 'baseline_weights/' + env_name + time_stamp + '/')\n",
    "\n",
    "if not os.path.isdir(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    \n",
    "if not os.path.isdir(baseline_dir):\n",
    "    os.makedirs(baseline_dir)\n",
    "    \n",
    "#Testing on original and new envs\n",
    "max_frames = 2000000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compensator object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class compensator:\n",
    "    def __init__(self, num_inputs, num_outputs, policy, hidden_size=64, lr = 3e-4, num_steps = 2048,\n",
    "                 mini_batch_size = 64, ppo_epochs = 10, threshold_reward = 950, action_appended = False):\n",
    "        \n",
    "        if action_appended:\n",
    "            num_inputs += num_outputs\n",
    "            \n",
    "        self.ppo_compensator = PPO(num_inputs, num_outputs, hidden_size = hidden_size)\n",
    "        self.policy = policy\n",
    "        self.action_appended = action_appended\n",
    "        \n",
    "    \n",
    "    def sample_action(self, state):\n",
    "        #return self.ppo_compensator.model.sample_action(state)\n",
    "        if self.action_appended:\n",
    "            policy_action = self.policy.model.sample_action(state).squeeze(dim = 0)\n",
    "            compensator_action = self.ppo_compensator.model.sample_action(np.hstack((state, policy_action)))\n",
    "            return policy_action + compensator_action\n",
    "\n",
    "        else:\n",
    "            return self.policy.model.sample_action(state) + self.ppo_compensator.model.sample_action(state)\n",
    "        \n",
    "    def collect_data(self, envs):\n",
    "        if self.ppo_compensator.state is None:\n",
    "            state = envs.reset()\n",
    "\n",
    "        #----------------------------------\n",
    "        #collect data\n",
    "        #----------------------------------\n",
    "        log_probs = []\n",
    "        values    = []\n",
    "        states    = []\n",
    "        actions   = []\n",
    "        rewards   = []\n",
    "        masks     = []\n",
    "        entropy = 0\n",
    "        counter = 0\n",
    "\n",
    "        for _ in range(self.ppo_compensator.num_steps):\n",
    "            \n",
    "            policy_action = self.policy.model.sample_action(state).detach().squeeze(dim = 0)\n",
    "            \n",
    "            if self.action_appended:\n",
    "                state = np.hstack((state, policy_action))\n",
    "                \n",
    "            state = torch.FloatTensor(state).to(device)\n",
    "            dist, value = self.ppo_compensator.model(state)\n",
    "            action = dist.sample()\n",
    "            #action_sum = dist.sample()\n",
    "            action_sum = action + policy_action\n",
    "            \n",
    "            #print(action.shape)\n",
    "            #print(self.policy.model.sample_action(state).detach().shape)\n",
    "            next_state, reward, done, _ = envs.step(action_sum.cpu().numpy())\n",
    "\n",
    "            log_prob = dist.log_prob(action)\n",
    "            entropy += dist.entropy().mean()\n",
    "\n",
    "            log_probs.append(log_prob)\n",
    "            values.append(value)\n",
    "            rewards.append(torch.FloatTensor(reward).unsqueeze(1).to(device))\n",
    "            masks.append(torch.FloatTensor(1 - done).unsqueeze(1).to(device))\n",
    "\n",
    "            states.append(state)\n",
    "            actions.append(action)\n",
    "\n",
    "            state = next_state\n",
    "            self.ppo_compensator.frame_idx += 1\n",
    "\n",
    "            \n",
    "        if self.action_appended:\n",
    "            next_state = np.hstack((next_state, policy_action))\n",
    "\n",
    "        next_state = torch.FloatTensor(next_state).to(device)\n",
    "        _, next_value = self.ppo_compensator.model(next_state) #should this be the sum? nono.\n",
    "\n",
    "        return log_probs, values, states, actions, rewards, masks, next_value\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train compensator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABI4AAAHiCAYAAACOZYcfAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAIABJREFUeJzs3Xl8VNXd+PHPmZlM9gWyEkJAtoTFEBIQEESE2LqBDyDURysgFtpqH35Sxa3aWhQffaT6uPVRpCWKdSuoQNVaLSBgBdkCYhNkEQKEAAlJyDqZ5fz+uJNJZjIJIQkQ4Pt+veZF7rn3nnvuTYDMd77ne5TWGiGEEEIIIYQQQgghfJnO9wCEEEIIIYQQQgghRMckgSMhhBBCCCGEEEII4ZcEjoQQQgghhBBCCCGEXxI4EkIIIYQQQgghhBB+SeBICCGEEEIIIYQQQvglgSMhhBBCCCGEEEII4ZcEjoQQQgjRYkqpbKXUk+6vxyilDp/vMbWGUkorpXq3sY9gpdQqpVSZUuqv7TU2IYQQQoiORAJHQgghhGhEKbVWKVWilAo832PpwG4B4oForfWU8z0YX0qpXymltiilbEqpbD/7xyml8pRSVUqpNUqp7g32BSql/qyUOqWUKlRK/brBvv7ufkvcry+UUv0b7H9cKWVXSlU0ePV07+urlFqhlDqhlDqplPpMKZVylh+FEEIIIdpAAkdCCCGE8KKU6gFcBWhgQjv1aWmPfjrYdbsD32utHefh2i1RADwJ/Nl3h1IqBvgAeAzoDGwB3mtwyONAH4x7vAZ4QCl1XYN+b3GfFwOsBN71ucR7WuuwBq/97vYo9/EpGEG3b4AVbbtNIYQQQpxNEjgSQgghhK9pwEYgG5je2k7c08HuUUrtAfa421KVUp+7s012K6WmutsvU0qVKqVM7u3XlVLHG/S1VCl1r/vrO5VSuUqpcqXUfqXUzxscN0YpdVgp9aBSqhBY4m6fp5Q6qpQqUErN9BnnDUqpf7v7O6KUur8F9/Z74LfAT9wZNXcppWYopb5SSj2vlCoGHldK9VJKrVZKFSulipRSf1FKRTXo54B7bDuVUpVKqT8ppeKVUp+6x/OFUqpTg+OHK6X+5X5WO5RSY5oao9b6A631R0Cxn92TgO+01n/VWtdgBIoGKaVS3funA09orUu01rnA68AMd7+lWusDWmsNKMAJtGjan9b6G631n7TWJ7XWduB5IEUpFd2S84UQQghx7kngSAghhBC+pgF/cb9+rJSKb0Nf/wEMA/orpUKBz4G3gTjgVuCPSqn+WusfgFPAYPd5o4EKpVQ/9/bVwJfur48DNwERwJ3A80qpjAbXTMDIhukOzHZnytwPXIuRRZPlM8Y/AT/XWocDA4HVp7sprfXvgKeoz6z5k3vXMGA/RjbNAozAyn8DiUA/oBtGkKahye6x9QXGA58CjwCxGL+rzQFQSnUFPsbIIursvqflSqnY043XjwHAjgb3UwnsAwa4A1VdGu53fz2gYQdKqVKgBnjJ/SwaGu8ODn6nlPplM+MYDRRqrf0Ft4QQQgjRAUjgSAghhBAeSqlRGAGX97XWWzGCCbe1ocv/dmeXVGMEew5orZdorR1a6+3AcqCuPtCXwNVKqQT39jL39mUYQaIdAFrrj7XW+7ThS+AfGFPr6riA32mtbe7rTgWWaK13uQMkj/uM0Y4R2IpwZ9hsa8P9FmitX3LfX7XWeq/W+nP3WE4Az2EEwRp6SWt9TGt9BFgPbNJab3dnAn1IfTDtp8AnWutPtNYurfXnGFPMbmjFOMOAMp+2MiDcvQ+f/XX7PLTWUUAk8Ctge4Nd72MEyWKBWcBvlVL/6TsApVQS8Arwa999QgghhOg4JHAkhBBCiIamA//QWhe5t9+mDdPVgEMNvu4ODHNPsyp1Z6zcjpEhBEbgaAxGFso6YC1GkOVqYL3W2gWglLpeKbXRndFSihE4iWlwnRPuoEudRJ9xHPQZ42R3HweVUl8qpUa0+m69r4N72tm77ilwp4C3fMYKcKzB19V+tusCOd2BKT7PbxRGdtCZqsAIxjUUAZS79+Gzv26fF3cg7lXgTaVUnLvt31rrAq21U2v9L+AFjJpIHu4sqX8Af9Rav9OK8QshhBDiHDnfRRuFEEII0UEopYIxsnPM7vpAAIFAlFJqkNZ6R9NnN0k3+PoQ8KXW+tomjv0SeBY47P56A0ZQosa9jTJWeVuOMZ1uhdbarpT6CGNKmL9rAhzFmCJWJ9lrgFpvBm5WSgVgZM+873P8mfC99lPutsu11ieVUv8BvNzKvg8BS7XWs1p5fkPf0SAg6J5G2Auj7lGJUuooMAhjaiHur79roi8TEAJ0xZhG6KuuFlLdtTphBI1Waq0XtPE+hBBCCHGWScaREEIIIer8B0ah4/5AuvvVD2P61LR26P9vQF+l1B1KqQD3a2hdHSOt9R6MDJufYgSYTmFk30ymvr6RFSOYdQJwKKWuB350muu+D8xQxjLyIcDv6nYopaxKqduVUpHuYs2nMKa61e3XzRWgboFwjAyeMneNonlt6OstjNpBP1ZKmZVSQe5i4En+DlZKWZRSQYAZIxgYpOpXevsQGKiUmuw+5rfATq11nnv/m8CjSqlO7oLZszCKpaOUulYpNdg9hgiM6XclQK57/83u85RS6gqMGk0r3PsigM+Ar7TWD7XhWQghhBDiHJHAkRBCCCHqTMeoBZSvtS6se2FkyNyu2ri8vNa6HCPIcyvGku6FwDMYgaA6XwLFWutDDbYVsK1BH3MwgkElGPWXVp7mup8C/4tR9HovjYtf3wEccE8l+wXG9DmUUt0wpmd9e+Z36/F7IAOjRtDHwAet7cj9TG7GKJx9AiMDaR5N/z73KEYg7iGMYFy1uw13vaXJGAW8SzCKet/a4NzfYdS3Oog7E0xr/Xf3vijgHfc97cPIVLquwfTAWzGeczlGAOoZrfUb7n0TgaHAne7V6OpeXllgQgghhOg4lLGSqhBCCCGEaEgp9VNggNb64fM9FiGEEEKI80UCR0IIIYQQQgghhBDCL5mqJoQQQgghhBBCCCH8ksCREEIIIYQQQgghhPBLAkdCCCGEEEIIIYQQwi8JHAkhhBBCCCGEEEIIv9q0rO7ZFhMTo3v06HG+hyGEEEIIIYQQQghx0di6dWuR1jq2Jcd26MBRjx492LJly/kehhBCCCGEEEIIIcRFQyl1sKXHylQ1IYQQQgghhBBCCOGXBI6EEEIIIYQQQgghhF8SOBJCCCGEEEIIIYQQfnXoGkdNcblcHD58mMrKyvM9FCHEORQaGkpSUhImk8S8hRBCCCGEEOJcuCADR0VFRSilSElJkTeQQlwiXC4XR44coaioiLi4uPM9HCGEEEIIIYS4JFyQUZfS0lLi4+MlaCTEJcRkMhEfH09ZWdn5HooQQgghhBBCXDIuyMiL0+kkICDgfA9DCHGOBQQE4HA4zvcwhBBCCCGEEOKScUEGjgCUUud7CEKIc0z+3gshhBBCCCHEuXXBBo46kh49erBr165zcq3s7Gy+//77Vp37+OOPc//993v6iYqKIj09nf79+zN58mROnjzZnkNFKUVFRUWzxxw4cICYmJh2vW5LuVwuJk+eTEpKCoMGDeLaa69l375952UsQgghhBBCCCFERySBowuI0+lsU+DIV1ZWFjk5OezatQulFE8++WS79HshmT59Orm5uezYsYObb76Z2bNnn+8hCSGEEEIIIYQQHcYFuapanSWmcefkOne6/tmi48aMGcPQoUP5+uuvKSgoYOrUqTz99NNs2LCB//qv/2L79u2eY4cMGcIf/vAHrr76at544w3++Mc/4nA4iIyM5P/+7/9ISUkhOzubt956i/DwcPbs2cNdd93Fli1bmDNnDo8++igLFy4kKyuLZ555huXLl+NwOOjatSuvv/46CQkJlJWVcdddd7Fr1y4SEhLo1q0b8fHxjcZtMpkYO3YsH3/8MQC7d+/m3nvvpaioiNraWu69917uvPNOwMgiWrBgAR9++CHFxcU8++yzTJ48GYAPPviARx55hKCgIE8bGFlFQ4YMoaioyO92S46r+3rWrFn8/e9/p7q6mr/85S+8+uqrbNq0ieDgYFasWEFCQkKj++vTpw/Lli1j0KBBALz88sts3bqVJUuWMGHCBM9xI0aM4H//939b9L0WQgghhBBCCCEuBZJx1M7y8/NZt24d27dvZ/HixezZs4dRo0ZRUVHBzp07Afj2228pKSlh9OjRrF+/nvfff59169axdetW5s2bx8yZMz39bdy4kYULF7Jr1y7mzp3LkCFDePHFF8nJySErK4u33nqLffv2sXHjRrZt28YNN9zAfffdB8D8+fOJiIggLy+PZcuW8eWXX/ods81mY+XKlQwePBiHw8Ftt93G888/z+bNm9mwYQNPP/00eXl5nuMjIiLYvHkzS5cuZc6cOQAcO3aMWbNmsWLFCnJycggMDGz3Z1tcXMyoUaPYvn07d911F+PGjeOee+5h586dZGZm8vLLL/s9b/r06bzxxhue7SVLlngCYQ29/PLLXoEkIYQQQgghhBDiUndBZxx1RFOmTMFkMhEZGUm/fv3Yt28fffr0Yfr06WRnZ/Pcc8+RnZ3N9OnTUUqxatUqduzYwbBhwwDQWlNSUuLpb9SoUfTq1avJ661cuZItW7aQkZEB4MlaAlizZg0vvfQSADExMUyaNMnr3C+++IL09HQARo4cycMPP8z3339Pbm4ut956q+c4m81Gbm4uqampAJ59w4cPp6CggJqaGjZt2kRGRgYpKSkAzJ49mwcffLD1D9KPsLAwbrzxRgAyMjJISkryjD8zM5PPP//c73nTpk1j2LBh/M///A+5ubmUlpZy1VVXeR1Tt2/16tXtOmYhhBBCCCGEEOJCJoGjdhYUFOT52mw2e5YOnzZtGsOHD+epp57inXfe4euvvwaMQNHMmTOZP3++3/7CwsKavZ7WmkcffdQrS6mlsrKyWLZsWaP+YmJiyMnJafK8uns0m80Ap10e3WKx4HK5PNs1NTWtOq5hFpPZbG7yWU+cOJEffvgBgPXr15OcnMyAAQP49NNPWbt2LTNmzPBaneull17i7bffZvXq1YSEhDR7L0IIIYQQQgghxKXktIEjpdSfgZuA41rrge62zsB7QA/gADBVa12ijHfjLwA3AFXADK31Nvc504FH3d0+qbV+gzZqae2hjiA5OZn+/fszZ84c+vfvT/fu3QEYP34806ZNY/bs2SQlJeF0OsnJySEzM9NvPxEREZSVlXm2J0yYwAsvvMDEiRPp1KkTNpuNvLw8Bg0axNixY1myZAkjR46kuLiYDz/8kClTpjQ7zpSUFEJCQli6dCl33HEHAHl5eSQmJhIREdHkecOHD2fmzJns2bOHPn36sHjxYs++hIQE7HY7e/fupXfv3rz99tt++2jpcafz4YcfNmqbMWMGixcvZvPmzWzcuNHT/tprr7Fo0SJWr15N586dW3U9IYQQQgghhBDiYtWSjKNs4GXgzQZtDwH/1Fo/rZR6yL39IHA90Mf9Ggb8HzDMHWj6HTAE0MBWpdRKrXUJl5AZM2Zwxx13sHTpUk/b6NGjWbBgARMmTMDpdFJbW8uUKVOaDBzNnj2b++67j2effZaFCxdyxx13UFRUxNVXXw0YS8zffffdDBo0iMcee4yZM2eSmppKQkICo0ePPu0YLRYLq1at4t577+XZZ5/F6XQSHx/P+++/3+x5cXFxLFq0iPHjxxMcHOxVHNtisfDCCy9w7bXXEhsb65lu5u/aLTmuNSZNmsQ999zD0KFDSU5OBqC8vJxf/vKXdO/enWuvvRYwspo2bdrUbtcVQgghxMXt1L4CSr7d36596vIyVHkJ1i5xBPfoRkjPbgREhHplTJ9Rf85qqD0J9gqMX8XbMjiNvbwSW2ERtceLcZRXQFgkyto+9S211YWO1WBul+7qeoWaapwH7dj3t+4ZnisBKWBOkDK0FwLtcqEdGu1woZ0utMMFrtOf57cvoMRswqIhwtXKTgBlUhCgUGYzyqJQFlOr/90QzTPrTsRm3Hy+h3HOKK1P/5+HUqoH8LcGGUe7gTFa66NKqS7AWq11ilLqNffX7zQ8ru6ltf65u93ruKYMGTJEb9mypVF7bm4u/fr1a+k9CiEuIvL3XwghREfx/Z8+4atZf2jXPrv1tHBZnwDjDaCbdmlqa8HhMuM0W9GBIRAShjmqE5bozgR0iSO4RywhXUMIiYKAYBsmfcoIFtWWgLO6XcfYnjRQbbVSGR5JZWAgNa19591CnSrN2J/dgH3v8bN6nTNl7Z+I+f4rKbXaz/dQxHmwoziAHyoCAEjrVEvPiOZLgYjzL7zaRFLimZeL6UiUUlu11kNacmxraxzFa62Pur8uBOrWeO8KHGpw3GF3W1PtQgghhBBCXHCqCorY9P9eabf+LBZIHRRIdFzjVBtlUgQGQ0i4mYCYACzRJgKinViiKwiIsRMQXYYpyJ31ZHO/OrBas5nKwEAqQ0KpCgjA5YmRnd2gEUBJqJPO/30dnXJKKHnra+wHi876NZsT2DeBqJ+OoCw1lNLa8vM6FnF+lNUqT9AI4LvSAJJCHVjbNetOiLZpc3FsrbVWSrUx57WeUmo2MBvwTCkSQgghhBCiI9n22BIcVf4X/DhTYRGK/oMDCQ4xpihZYgIJ7hWBJSaQgOggAqIDsUQHYrpA30k6laLKaqUyKIjKwCDslvN7HydtZbgGRdF12E+p2vA9JW9/jf3wua2gYe0ZS6efXknIFT05UnmMcgkaXbL2lwd4bTu14ocKCymRknUkOo7WBo6OKaW6NJiqVpfreQTo1uC4JHfbEYzpag3b1/rrWGu9CFgExlS1Vo5PCCGEEEKIs+Lkjn3syf7Mqy3+qssJ7Bx+Zh1pTbirmGjnEZS79lBI/yjipvX2mqrWVi67C0exDUepDe1on1+vtQaNCZNqnCVkV4FUR4RTGx2EPToIR6QVzuB+rMqCpY0ztpRPLadqiwtXg7bS2lO4tIvE0SmEjupL6dq9HH9nO7WFZzeAE5gcRfztmUSOvAyXdnGo4iiVDu+phGYUQQ6pc9ThmRQoBSZ3HaG67TNgc8LhysbtP5RbSY+2Yj7Tfwa0BpdGuzRoF7i00SbananMCYnnexTnTmsDRyuB6cDT7j9XNGj/lVLqXYzi2GXu4NJnwFNKqU7u434EPNz6YQshhBBCCHHuaa355r7/83ozFpmazHVfLMQU0PJfrV01NRS98kcqVu/wtKkAE9ETu7cqaOSyObEX23AU1Rh/Fht/2otsOE/VtrgmthMzLnMgOigUFR5u1FGKiyUwMZ6g5K4E90wmsEs8ymLca8W69Rz/w3PoMDOuy+OM18BYiGh5sWyzMhEaEEqoJYHQgD4EmLoArQucVWz4iuPPLgSHd7ZG7P8bSVFmNM4Gz/aUvQJXpYuuofF0GteXTuNSgASMhaODWnX9plUBP1D3ebtTOzlUUUi1wztrzeJykUw/AmOvaufri3PDBTh8Xs4m2745fgynPtaolxonnKpNJC06FqNSvMXn1VSbBBzPmdjzPYBz67T/uyml3sHIFopRSh3GWB3taeB9pdRdwEFgqvvwT4AbgL0Y/zreCaC1PqmUegLY7D5uvtb6ZDvehxBCCCGEEGfd4Y83cnT1dq+2oc/+/IyCRrWHD3PsqaexHzzo1R5xVTyWSGvTJ5oCwRoN1k5g7QyBnSHA+NpkCcVaXY3p5ElMxcVw+CjOQwWowmOoomKcZaVQU4MKCcUUFYUlNgZrghEMsiYmYImOxty5E6bAlgd8bM4SKodYcS26Bbv1zApwh1iC3MGieILMPVGq7g1y65X97WOKX32tUYZFxPib6DTuZ4QUvMOh4Eoc5vrrVNirOFRxlKSwLpiVCTiKUcI1EegOtHW1uGrggLtPg8Pl5FBFATXOWq8jrQ4H3WpjCUgc1cZrivPHBFjdr+Zprdla1PSKjJtOVHJ556tlVTTRIZz2fzit9X82sWucn2M1cE8T/fwZ+PMZjU4IIYQQQogOwmV3sHnea15tiVkZJN0wrMV9VKzfwIkXXkRX+0xPig6j04+6Y2QiuIWnQER/I0hk7QTm4GbfRKqQEKwhIViTkggZNKjFYzpTWrsortnBiZptgG7Je2SspgBCA4IJtYQQGhCDSXXFWF+nBSefdjyakqVvUfre+432dZ4+jcgpt6CUIjBxMsk//IlDkRq7pf5tUJWjhvzyArqFdcFiMhv3xBGMIFJdAOlMx1mD8fn6URqme9ldDvLLC6h1ec/FC7Tb6VZhxtJjggQKLhH7ygsora3wbJuVCZfWaPfPS1FNGfvLj9Ir4hKaDyU6rDYXxxZCCCGEEOJSkPfa3yjb3WChYKUYuvAXLXqjr+12ipdkc2rFykb7Arp1o8uvx6DsefWN5mDoejPK3N5Tptqm1nmKgsq1VDubX87ehMkIFLmDRQEEo8yJGFPBQtttPNrppOilVyj//HOfAZiInfNfhF+b5WlS5mCsSVPofmAJhzpHYguoL0pc47SRX36EbuGJBJjq3iK5MBaDLsAo0ZoMeBcybsyGETAqwHd+YK3TTn5FAXaX9zS64NpakkorMV/2M5TpdP2Li8XWE997bfeL6o5TO8ktzfe0fXM8VwJHokOQSZDtoEePHuzateucXCs7O5vvv//+9Af68fjjj3P//fd7+omKiiI9PZ3+/fszefJkTp5s39mDSikqKiqaPebAgQPExMS063XXrl2LUop58+Z5tY8ZM6ZFY/LVcIwFBQVcc801nn0fffQR/fr1Y/DgwezevZv09HSqq88sVRuaf1aPP/44tbX1qcy//e1vee+99zz76r6nOTk5vP9+40/aWiInJ4eRI0cSEhLCLbfc0mj/E088Qa9evejVqxdPPPFEq64hhBBCXMhsJeXk/P4Nr7a+M6+nc1qv057rKCqi4KFH/AaNwsZcTddnHsFs3+29I/aqDhU00lpTavuOH04tbzJoFGQOJCaoE93DE+kb1YNEU2cs6w5R9OhHHJy2CNseF+0ZNHLV1HDsiScbBY1UYCAJv33MK2jk2RcUjyXhRpKLigiq9Z4qZnPZOVheQK3TtzK3C8gHvsaoU+SvcnctRrWOjRjZSt5BoxqnjYPljYNGITYb3YqLMXediLJ2QlwaTtrK2V9+1KstM7Yvw+L6ebUdqDhGYZVUeBHn3wWdcbT/xvHn5Do9P151Tq5zOk6nk+zsbGJiYujbt2+b+8vKymLZsmW4XC6mTp3Kk08+yXPPPdcOIz3/UlJS+Oijj3j66acxm83s37+fyko/SxacocTERNasWePZfu2115g/fz5TpkwBjABMe/v973/P/fffj9VqpEjPnz/f73E5OTn87W9/Y+rUqX73NycuLo7nnnuOnJwcPvf55WvdunX89a9/9QRHhw0bxtVXX83o0aPP+DpCCCHEhWrHgr9gKz7l2baEBjH4iTtPe17V9u0c/5+FuE6d8t5hsRAzexbhN1wPh97HK9Bg7QSdhrbTyNuqCofrCEercqiwlzXaa1Ym4oKjCQ8IxWwyagdpZySlyzdS+vbnaFt9oKTgoUeI/83DhGRktHlUzrIyCuc/gS3PO+Bmiggn4Xe/Iyg1pclzVdTlmKsL6Fb8DUc6d6aqQV0nu8vOwfJjJIcnEdio5JITo17RYYyFpJMwvm+H3G1O3xMAqHa4OFRxDKf2DhqFVVeTWFKCKe4aVFjvlty2uEhsK/JOBOgSEk1iSDQAyaFx5FfWB2e/OZHHhO5XntPxCeFLMo7a0ZgxY5g3bx6jRo2iZ8+ePPTQQwBs2LCBwYMHex07ZMgQvvzySwDeeOMNhg0bRmZmJmPHjmX3buM/wOzsbLKyspg4cSIDBw7kxRdfZMuWLcyZM4f09HS++OILAJ555hmuuOIKMjIyGD9+PIWFRvG9srIybrnlFlJTUxkzZgz79u3zO26TyeR13d27d3P99dczdOhQBg0axJIlSzzHKqV46qmnGDp0KD179mT58uWefR988AGpqamkp6d7ZaX4ZhU1lWXU3HF1Xz/88MMMHjyY1NRUtm7dyqxZs0hLS2PYsGGe+wYICwvjyiuv5LPPPvM842nTpnldb/PmzYwYMYK0tDRGjBjB5s2bPfteeeUVevfuTUZGBn/605/8jmnu3LmsX7+eBx980JOF1DBzqLnn2NSz8nXPPUbJsCuvvJL09HRKS0uZMWMGL7/8stdxxcXF/Pa3v+WLL74gPT2dOXPmNOrrySefZO7cuV7nxMTEUFlZSWJiIsOGDSPQT0HM9957j2nTphEcHExwcDDTpk3zZDwJIYQQl4JT+wrIfelDr7a0h24jJKFzk+dol4uSt9+h8LHfNQoaWWJjSXz2GSJuvAGq8qHcJ9sobhzK1LZC0a2ngXKMzJpvqLCvZv+pDX6DRmEBIfSM6EZUYARmUyTQC7gSZc4g6j9mEzrK+0MmXVND4ePzKV+9plFfZ8J+/DgFDzzYKGhkiY8j8dn/aTZo5JGQhTk4maTiYsJqvFc2c2gbB8sPU+2IxX9tIwfG89mIkYV0EP9Bo0Aq7THklx/Cqb2zlCKqquhaUoIpPBVipBj2paTWaWdnsXdR7CEx9UkBV8Sleu3LLTnIqdq2fwAu2sbhclJiq8CpXed7KOeFBI7aWX5+PuvWrWP79u0sXryYPXv2MGrUKCoqKti5cycA3377LSUlJYwePZr169fz/vvvs27dOrZu3cq8efOYOXOmp7+NGzeycOFCdu3axdy5cxkyZAgvvvgiOTk5ZGVl8dZbb7Fv3z42btzItm3buOGGG7jvvvsAIzMlIiKCvLw8li1b5glU+bLZbKxcuZLBgwfjcDi47bbbeP7559m8eTMbNmzg6aefJi+vfs59REQEmzdvZunSpZ4AxbFjx5g1axYrVqwgJyfHbwCirYqLixk1ahTbt2/nrrvuYty4cdxzzz3s3LmTzMzMRsGUGTNm8MYbb6C15t133+W2227z7KutrWXy5Mk8+eST7Ny5kyeeeILJkydTW1vLzp07WbBgAV999RXbtm2juLjY73ief/55z/ejYRYS0OxzPJNn9corrwDwr3/9i5ycHKK4Pdh/AAAgAElEQVSiovweFx0dzfz588nKyiInJ4cXX3yx0THTpk3j3XffxeFenvbtt99mwoQJhIY2nzKen59P9+7dPdvJyckcOnSomTOEEEKIi8uWhxbhstdni4R2i2PArxtP7a7jLCuj8He/p+QvbwMaFWzFHBNGQPdoIiZdQ9c//p6gvpFofRhsmyGuB3TpDV1T4LJMiKgGtgLfAfsxCiyXYtTP0U1et/W0u/+6qVZbcOn9FFYd4FBFIU7tHRRRKBJCYkgK7YXF1AsY6n4lU7cKmbJYiJ17L1FTfJ6T08mJPzxH6fIPWjVS2w8/UHDfPOyHj3i1Wy+7jMSFz2JNSmpRP0qZodtkTJZwup48SURVlfcwtY388q1U2XtgBMT81R6y4z9gZAX6UF6byKGKLbjwzjSKqqykS2kpyhpt1LGSYtiXlO9KDmBrUBw9xBJIalSyZ7t3RFc6B0Z4tl1otpxoXakS0X4Kq0/yau5Knt3xHq989xGf5G8630M6py7oqWod0ZQpUzCZTERGRtKvXz/27dtHnz59mD59OtnZ2Tz33HNkZ2czffp0lFKsWrWKHTt2MGyYsRqH1pqSkhJPf6NGjaJXr6bnzq9cuZItW7aQ4U75dTgcREZGArBmzRpeeuklAGJiYpg0aZLXuXXZKQAjR47k4Ycf5vvvvyc3N5dbb73Vc5zNZiM3N5fUVCP6Xbdv+PDhFBQUUFNTw6ZNm8jIyCAlxfiEZ/bs2Tz44IOtf5B+hIWFceONNwKQkZFBUlKSZ/yZmZmNpliNGTOGu+++m48++oiBAwcSHR3t2bd7926sVivjxhmLA2ZlZWG1Wtm9ezdr167lxhtvJD4+3nMvZ1o7qLnnaDabz/qz8ic5OZkBAwbwySefMGHCBLKzs3n++efP+nWFEEKIC1nh+p0cXL7eqy3zqbuwBAdi1L4pBCowslAcuKrKcZYVEvOrwZhCh2MKCUSZfAMDewFQCugcDUT77D/l82dDJiC4iVcgLf9c2IURLDoBFGHU6DFUO2ooqDzeaOUvgCBzCImh6QSaewAhzV5BKUXnGdMxd+5E8aLFoOuDXif/vATnyZN0vmsmytSyMVfv/JbCJ55E+wR5gtLSSHj0EUyn+TCs0fgsYehuU1AH3qBLaSkmrSlt0IcLO/kV/yApLIuwgOEY09EOgU8gqF4AxgpsiZTVHqCgci2+gb7o8nJiystRJiskT0WZ2//DVtFxaa3Z6jNNLT26t3s1P4NSimFxqXx66BtPW07xXkYmDCTQLMXTz5cyd9aXRnPKXkW103aeR3RuXdCBo45Se6ihoKD6IoZms9mT3TFt2jSGDx/OU089xTvvvMPXX38NGP94zJw5s8m6NWFhYc1eT2vNo48+6pWl1FJ1NY58+4uJiWm2Vk/dPZrNxj9wdffYFIvFgstVn9JX45MO3NLjGmbmmM3mJp91HaUUU6dOZdasWV7TxM6F5p7jypWNC2PWWbJkCS+88AIA8+bN4/bbb2/1GPz1VZeFddlll1FWVsZVV1112n6Sk5M5ePCgZzs/P59u3bq1elxCCCHEhUK7XGy+/1WvtpihKfT8z7Hurf0YgYR6phCwJjc9ha3tXECl++VLAUH4DyrV/d50EiNYVIxvAERrTXFNKSdq/BXjVcQEXU5M0BCUOrNJC5ETJmCO6sTxPzwHDX5fK/toBY6SEuLm3osKaP4NccWGrzj+7EKv8wFCrxpF3H2/Pu35TVEhSeiE61BHPya+rAyTy8XJ8HDPfo2TQxWf0zX0GiKslwFdaVzTyIKRbZUEmCmx5VFYtaHRtWJPnSK6bjGUrjejAmMbHXOo4jg2l53LwrtgPsPnLDq+Q5XHOVFTP+1ToRgc3bi+1cBOl/Hl0R1UOYzghM1lZ0fxXq7wKZ4tzp0yn+mCkdb2K/R/IZB/jc6R5ORk+vfvz5w5c+jfv79n6s/48eN58803OXz4MGAUwN66dWuT/URERFBWVv+PzYQJE/jjH//oyVKy2Wzs2LEDgLFjx3oCJsXFxXz44YeNO/SRkpJCSEgIS5cu9bTl5eVxyrego4/hw4ezfft29uzZA8DixYs9+xISErDb7ezda3y69vbbb/vto6XHnYnZs2fzwAMPcP3113u1p6SkUFtb65litnr1aux2OykpKYwZM4ZPPvmE48eNonQNaxy1VHPPsblndeedd5KTk0NOTo4naBQeHu71PW+K78+Gv74mTZrEunXr+MMf/sCMGTNalBo9ZcoU3nzzTaqrq6murubNN99sVQFuIYQQ4kKz/53VFG32rqNzxR9+2SBD5gRaa7Q+G9PHWkMD1RjBoSMYmU3fAt8A64D1wC7gGL5Bo1qnsaKYv6BRgCmCHuHjiQ2+4oyDRnXCRl9Fl/mPo4KDvdorv1xH4ePzcflkETVUtupvHH/6mUZBo4jxNxH3wLxWB408OmVA1GAUEFteTmyj33tdHKlcTaltN0ZWUU9gBNAXSHF/3R0wU1yzs3HQSGviS0vrg0YxI1ERjQMA/zr2HW/t/YK/7v+SD35Y34F+rkR72eoz5axvZBIRfgIQFpOZzBjvxZA2n9jdIerruLSLGmft6Q+8yJTVeq+AHWltPsHjYiOBo3NoxowZvP7668yYMcPTNnr0aBYsWMCECRMYNGgQAwcOZMWKFU32MXv2bObPn+8pjn3HHXdw++23c/XVV5OWlkZmZiZfffUVAI899hglJSWkpqYyefLkFq2CZbFYWLVqFe+++y5paWkMGDCAu+++22s5eH/i4uJYtGgR48ePZ/DgwV7ZQhaLhRdeeIFrr72WK664wpOp5O/aLTnuTHTt2pUHHngAi8U7uc5qtbJ8+XIeeeQR0tLS+M1vfsOyZcuwWq2kpaXxyCOPMHLkSDIzM5usK9Sc5p5jc8/Kn/vuu4+xY8d6imM3Zdy4cVRWVjJo0CC/xbEBQkJCuPnmm1m6dKlXsfADBw6QlJTEr3/9az755BOSkpI8AbMxY8YwadIkBgwYwIABA5g0aRJXX331GT8TIYQQ4kLiqKphy8OLvdq6T76K+FGXu7c0VY5S9pQdZHfpD5yoPtnMG30TRt2bECAC7YpEnzqJPnkUXXQIfewAuswO9AMuBwYD6UAqRkAiDgin7ZMFGo9Pa02p7RQ/nDpEtbPx7yRR1hR6Rkwk2BLXxmtD8KBBJD7zNOZO3svOV+fkUPDQwzgalGuoG9vJN96k+NXXvKa5AXSePo3on89u8TS35iiloMv1EJyIAqIrKohv9DuX5mjVek7W7HJvB2BkHyUCFrTWHK/ewvHqb3xO0ySWltKpLjAW2hPirmk0Bq013xyvrym699QRdpz0v7CNuDCdqq1kd9lhrzbf4FBDGTF9sKj690On7FXkleaftfG1xKGK47yw6wP+99vlrC1o/9WkO7JSn4yjqEss40h15Ej2kCFD9JYtWxq15+bm0q+fpOkJcSmSv/9CCCHOhR0L/sK2x/7s2TYFWJj47yVE9Ep0t9g5UP4e1Y76YIv1uxKCPvkB7BB1622EZl6BEezxDm7own9A8cb6BlMg9PkVynK6NyIaI1OoCqhx/1nd4NW4JlFzHC7F0aqTVNgbZxmZVRBdQq4i3Nrdz5ltYy8spPCx32EvKPBqtyQk0GX+7wnomoh2ODjx8itUfP6F98kmE7Fz/ovwa7PafVzaXgb7XgenEeQpCw7maFSUuxhVvZigDGKCBnsyt7XWHKv+mhLbv72OU1qTePIk4TZ3LZSASOg5C2VpXBuqrLaSP/7b+8PjILOVn/e7iRBLUKPjxYXny6M7+Nex7zzbsUGR3JVyQ7MzAP5+6Bu2F+/1bCcEd2JG3+vOS0H1KoeNxXkfU9ng37w7+15HQsjZnJrbcbyau4oSW7ln+66U64kL7tTMGR2fUmqr1npIS46VjCMhhBBCCCEaqCo8yc6nvafM9/uviQ2CRqC1DZvDuzhq7YBOVP3oMmLufZjQzFEYWUY+QaPaEjjpk5USM7IFQSMwahgFAJFAPHAZ0B/IBEYBVwFDgAEY06m6AFHUrXRmCASSqLAnsP/UEb9Bo7CAbvSMmHRWgkYAAQkJJC78HwL79vFqdxQWcmTeA1R/+y2FTyxoFDRSgYEk/O6xsxI0AlABkdBtMsZzhsjqarqWlKB8PmgvqtnG8epN7mmKLo5WrWsUNDK5NEnFxfVBI2WBblP9Bo0ACqsafx9qnLWsvsSyOi5WDpeTnAYBIDCyjU4XABoam+q1XVhdQn7F8XYfX0t8fmSLV9AI8ApqXcy01pxqVONIpqoJIYQQQghxydr+2BIclfVvkAKjIxj06E+9jnG6KnD5mfrlGNSJ45F5uLS/ZdqBY6uhYZ2SgAiIHtYu4zaym8IxprZ1x5jqNhi4EhgNjMSlh1JYdZxDFV/h1NVeZyssJISMJCn0R1hMza+Y1lbmyEi6PLWA4CGZXu2usjKOPvQI1T6zDkwR4XR5agEhQ1r04XirqdDLIL4+MBVeU0PSyZMon2/1Sdsujlat50jlaspq93iP1QXdiosIbVjqIfFGVHCXJq97rLrEb/u3J/dz6DwFCkT7ySvN9xS6Bgg0BTCgU4/TnhcdFEHfyCSvtk0nctt7eKe1u/QQ/y452Kj93yUHsDnPLNPxQlRhr/aqLxVktl5yK9xJ4EgIIYQQQgi3kzv28f2fP/VqS//ddAKjvD9drq0+1mQfFfaDHK74By7ts2pZ1WE49Z33wXFjUaZz8QbETLWjjB9OfUSJrfEbzyBzLJdFTKRTYL9zNg3GFBxMwmOPEjZubLPHWeLjSFz4LEGpKedkXEQPh4j+ns1Qm41uRScwae+3TmW131NuP+DVZtZmkouOE2xv8Ga681BU1KBmL1lY7W8lO8PfD2/G6WoiECkuCFuLvItip0X3xNrCwMOwWO8SDftOFVBUc/qFc9pLlaOGvx/e7HdfrcvhN6B0sfFdUe1Sq28EEjgSQgghhBACcBcovv9Vr0LMkSndSP35TY2OtdUUNdtXpeMI+RV/x6lrPX1z7HPvg4ISIPJyP2e3L61dFFVv50D5Smpdvm84FTFBg+kRPp5Ac+RZH4svZbEQO/deoqbc4mmrDQrAZTKCV9bLLiNx4bNYu3Y9d2NSChInQGCspy3Ebif5xDHMWJs8z0Ig3Y8XENRw9bfgJIj/0Wmv2VTGEUBRTRnfnNjd5H7RsRVUFVNQVezVlhHTp4mjG+saGkNiSLRXW8NC6mfbPw5vpcrR9GI+OcV7mtx3sSi9xFdUAwkcCSGEEEIIAcDhTzZx9J/bvNqGPvtzTAGNVzOzO8q9toNKwWryXom12lFIfvmnOF01UL4bqg55dxKfddaze2qdpzhY/jdO1GzFd1W1AFME3cNvIjY4E6XO39sCpRSdZ0yn889/xsYpI/jwsVv4ZO5NVI8aSuIz/42l87kvvqvMVug21Shc7hbksNO9qBiLajyNz6rC6H78KFZng8wgSxh0m4IyNb9ScKW9mgp7/bRBszI1msa0ofBbSm0ViAvP1hPe2UY9w7vQOTCixecrpRgW5511tKvkB6+fmbNld+khcku9M4oGde7ltV1YXcJRn8DYxcY34yhSMo6EEEIIIYS49LjsDjbPe82rrcu4DJJuHO73eIfy/gTeWhVA9/AbCTR7ZwbUOE9wsPxjHMf/6d1BWG9UWM+2D7wJWmtKbHnsP/UB1c7GNXKirCn0jJhIiCX+rI3hTFWOG8nB9B7G153D+Oo/BuMKPn8riqnAaEia6NVmrS2ne0kVAab6N/6Bps4kF50kwCsrwwTdbkEFhJ/2OoU+2UaxQVFkdc0gyFyf3eTQTj4/srV1NyLOmypHTaPAS2Zs3zPup29kktf0KKd2NZr+1t6MKWrehfzjgzvx425D6RWR6NW+vejiLpItgSMJHLWLHj16sGvXrnNyrezsbL7/vnX/SDz++OPcf//9nn6ioqJIT0+nf//+TJ48mZMnm55b3RpKKSoqmv9k5MCBA8TExLTrddeuXYtSinnz5nm1jxkzpkVj8tVwjAUFBVxzzTWefR999BH9+vVj8ODB7N69m/T0dKqrzzz635pxnYns7GxuucVIAT9w4ACLFi3y2n/DDTewb9++M+rTZrNx3XXXERMT4/d7uGrVKlJTU+nduzc/+clPqKqqav0NCCGEEGfZ7tc/piwvv75BKa5Y+IsmM4IcJu+CsBZXCBZTMN3DbiDYHOe1z+YqIT/SjN1U96u38irA3N4crmoOV35OYdUGNN51lswqiKTQa+kSehUm1bGKu/rWbSm2lbOucOd5Go1BhfeF2NFebQFVBfSoDCIueBhxwcPpXmbHYvOZupjwY1RIcouu4TtNLSGkEyGWIMYmDvZq33vqCN+XHT7zmxDnTU7xPq+iylHWMHqFJzZzhn8mZWq0wtq2oj3UOh1NnNF2/zi8xaugt0mZuCl5OGZlYnB0b69j/1168KIuki2BIwkcXVCcTmebAke+srKyyMnJYdeuXSilePLJJ9ul344gJSWFjz76CKc7XXj//v1UVlae5qzTS0xMZM2aNZ7t1157jfnz57N9+3ZSUlLIyckhODi4zdc5m/wFjj755BN69erVxBn+mc1m7r//fr744otG+yoqKpg1axarVq1i7969hIeHs3DhwjaNWwghhDhbbKUVbH/8Da+2PndeR+dBTf/f6DB7FysOsBjT1MymQLqFX0eIxXsFrVqLhfyYGGrNZohKRwV5B5faS3ntQfafWk6FPb/RvrCAbvSMmES4tftZuXZb+Zt6s+l4LocrT5yH0TQQezWEeb9RNpfk0LnaTufyUkzlPvWHItOgc8tXfztW5f3hbXywMTUvrXNPuoZ6fzj3+eEt1F7Eb9AvJi7tYnuRd/2fzJg+rZ6emta5p1cWWo2zlm9P7m/TGJuSV5pPbqn3vyEj4wcQF9wJgF4RiYQH1L/nsbscfFdy4KyMpSMokxpHF3bgqPb5yefk1VJjxoxh3rx5jBo1ip49e/LQQw8BsGHDBgYP9v7EYMiQIXz55ZcAvPHGGwwbNozMzEzGjh3L7t3Gfz7Z2dlkZWUxceJEBg4cyIsvvsiWLVuYM2cO6enpnjfszzzzDFdccQUZGRmMHz+ewsJCAMrKyrjllltITU1lzJgxTWaUmEwmr+vu3r2b66+/nqFDhzJo0CCWLFniOVYpxVNPPcXQoUPp2bMny5cv9+z74IMPSE1NJT09nSeeeMLT7ptV1FSWUXPH1X398MMPM3jwYFJTU9m6dSuzZs0iLS2NYcOGee4bICwsjCuvvJLPPvvM84ynTZvmdb3NmzczYsQI0tLSGDFiBJs3168W8Morr9C7d28yMjL405/+5HdMc+fOZf369Tz44IOeLKSGmUPNPcemnpU/SikWLFjgeeb//Oc/Pc9h4MCB5OYaK6M0zCryt13nnnvu4d///jfp6eme/U1lzTX3s2uxWMjKyiIqKqrReZ9++ilDhgyhTx+j8N8vfvEL3nvvvWbvUwghhDhfdj71F2xF9dkultAgMp64s9lz7MrltR0YUp9FYFZWuoX9mFCL9zLadnfwyBaT0Q6j9ubSdo5Wrudw5ec4tfc0OoWFhJBRJIX+CIupcX2ejqKyiQK8f8vfiN119jIrTkcpZUxZs3by3nH0Uzi+xrstKAESbzyj4IDvVLV495tzpRTXJV2Bor6vU/YqNhw7NzMdRNvsKTvCKXt9xr1Fmbm885l9UNuQ1RzQqKj2NyfycGlXE2e0TpWjhs98VlFLCO7EiPgBnm2TMjWqdZRTvNdYBOAio7WmzO49c0IyjkSb5efns27dOrZv387ixYvZs2cPo0aNoqKigp07jVTbb7/9lpKSEkaPHs369et5//33WbduHVu3bmXevHnMnDnT09/GjRtZuHAhu3btYu7cuQwZMoQXX3yRnJwcsrKyeOutt9i3bx8bN25k27Zt3HDDDdx3330AzJ8/n4iICPLy8li2bJknUOXLZrOxcuVKBg8ejMPh4LbbbuP5559n8+bNbNiwgaeffpq8vPrK/REREWzevJmlS5cyZ84cAI4dO8asWbNYsWIFOTk5BAYG+r1WWxQXFzNq1Ci2b9/OXXfdxbhx47jnnnvYuXMnmZmZvPzyy17Hz5gxgzfeeAOtNe+++y633XabZ19tbS2TJ0/mySefZOfOnTzxxBNMnjyZ2tpadu7cyYIFC/jqq6/Ytm0bxcX+i709//zznu9HwywkoNnn2JpnFRUVxebNm3nmmWe4+eabGTlyJNu3b2fatGksWLDgjJ7jK6+8Qv/+/cnJyWHZsmXNHtvcz25z8vPz6d69/tPM5ORkDh061MwZQgghxPlRvr+Af7/4oVfb5Q/cSkiX6CbOAIezChfeb9aCYryzeEzKQlLgMMKrvYMhDrOZ/Oo11Djar5hsleMY+099SGlt45W3gsyx9IyYSKfA1LNeiLutKpso9ltiK2dtwY5zPBpvyhxsFMtuOL1PO/EqOG4OdhfDbvkUwBpHrdeKTQpFXHD9h3JxwVFc4TNFafPxPE5Ul57xPYhzy7cG0cDOPQi2NL0qX0tkxvTF3KCQfWltBXvKjrSpT1+f+ZmidmPyCK/rAgyK7uUV1Dx2kRbJrrBXewXngsxWAs0da5rvuSCBo3Y2ZcoUTCYTkZGR9OvXz5PlM336dLKzswEjE2T69OkopVi1ahU7duxg2LBhpKen89BDD3m9wR41alSzU4hWrlzJF198QUZGBunp6bzyyiscOHAAgDVr1nDXXXcBEBMTw6RJk7zO/eKLL0hPT2fYsGH06tWLhx9+mO+//57c3FxuvfVW0tPTueqqq7DZbJ6sFoBbb70VgOHDh1NQUEBNTQ2bNm0iIyODlJQUAGbPnt22B+lHWFgYN954IwAZGRkkJSWRnp4OQGZmJnv3ehdlGzNmDDt37uSjjz5i4MCBREfX/wK4e/durFYr48aNA4xpe1arld27d7N27VpuvPFG4uPjW30vzT3H1jyrn/zkJ577Vkpx0003NXnf7a2pn10hhBDiYrDl4cW4auun/oR0jWHgfVOaPcdW5j2FI0CbMYc2nrqgTqwjseQkET51/py6hoMVH1PtaFy0+kxo7eJ49RYOlv8Nu+uU79WJCcqgR/h4rObINl3nXKloZsnvLUW7ya84dg5H05gKioeu45vaC0mTUL5ZSafhW98oOiiCAJP3Kn6jEgYSEVCfKeZC8/fDmy/K7I6LxYnqUg76/LxmxJx5UWxfYQHBjVbc23Q81//BrZBbcpA8nylqo+IHegUz60RYQ+kV4T0ld3vxxVck27e+UdQlmG0EEjhqd0FB9Ss/mM1mHA4jrXbatGm8++671NTU8M477zB9+nTASH2bOXMmOTk55OTksGPHDvLz6/+yhoU1P39Sa82jjz7qOX/Xrl189dVXLRprXY2jnJwcXnnlFUJDQ9FaExMT42nPycnhwIEDTJxYv6JE3T2azcbSonX32BSLxYLLVR+lranx/0vB6Y5rmJljNpubfNZ1lFJMnTqVWbNmMWPGjGbH2N5a8hz9WbJkCenp6aSnp/OXv/zF097wmfs+h7r7bulzbs63337ruf7cuXOBpn92m5OcnMzBg/UrSOTn59OtW7czHo8QQghxNh37ahcH/uqdkZ351M+whDS/kpet8qjXtsXZ+AMVXVMIpTtQQJfSUqJ8ai26dC0Hyz+h0l7QqrHbnKUcKF9JcU0OXlkvgNUUQY/w8cQGZ6DUhfPrvm+No0CfzJ2P8zee9/o+KnIgRPtZaS/uGlTYmU9DOlbtW9+oceDJag7g2q6ZXm2HK0+w8yzVtxFtt9WntlG30Fi/39vW8M1AO1JV1C51wCrtNXx2eItXW0JwZ0bE92/ynPRo76lzuaUHqXHWtnksHUmp1DcCwHL6Qzou69zlpz+og0hOTqZ///7MmTOH/v37e6bxjB8/nmnTpjF79mySkpJwOp3k5OSQmZnpt5+IiAjKyurn4E+YMIEXXniBiRMn0qlTJ2w2G3l5eQwaNIixY8eyZMkSRo4cSXFxMR9++CFTpjT/CVpKSgohISEsXbqUO+64A4C8vDwSExOJiIho8rzhw4czc+ZM9uzZQ58+fVi8eLFnX0JCAna7nb1799K7d2/efvttv3209LgzMXv2bEJDQ7n++usb3WdtbS1r1qzhmmuuYfXq1djtdlJSUtBa88wzz3D8+HHi4uK8ahy1VHPPsblndeedd3Lnnc3XVGhK79692blzJzabDaUUy5Yt81t/yPdnqKHLL7+cnJwcr7amfnabc9111/GrX/3Kc4+vvvoqU6dObdV9CSGEEGeDdrn45r7/82qLzuxLr9vHnfZcu917mpDZ7h2c0VpD4eeebQXE11hRnQdQYvuu/jgcHKr4jKSwLMICWvYBi9aaElsux6s3oXE22t8psB9xwVd0uBXTTkdrTaXDO3B0U/Jwlh9Y79kura1kzdEcfpw09FwPz1t8FthOQIW7fmjkQIgZ2aqufOsbJbgLY/vqE5lE74hE9p6qDzSuKcihT2QSIZb2LxEhWq/GWcuukh+82jJjU9qt/9jgKHqGd2F/eX0A+5vjeSRdFtvqPrXWfHZ4M9XO+ilqZvcqaqZmgs+9IroQERDiqeVkdzn5ruQAme2QXdVRyIpqhgvnI4iLwIwZM3j99de9sl9Gjx7NggULmDBhAoMGDWLgwIGsWLGiyT5mz57N/PnzPcWx77jjDm6//Xauvvpq0tLSyMzM9GQcPfbYY5SUlJCamsrkyZNPW5cGjKyVVatW8e6775KWlsaAAQO4++67qa1tPnIcFxfHokWLGD9+PIMHD/bKdrFYLLzwwgtce+21XHHFFZ5MJX/XbslxZ6Jr16488MADWCzeMVKr1cry5ct55JFHSEtL4ze/+Q3Lli3DarWSlpbGI488wsiRI8nMzPQbfDmd5p5jc8+qLYYPH05WVhYDBgwgKyuLfv36+T0uLS2NlHt0sdkAACAASURBVJQUBg4c6Ld4tj/+fnYBhg4dyogRIygpKSEpKYmf/exnAISHh7No0SJuuukmevfuTVlZGffff3+b7k8IIYRoT/vfXUPRN3lebVf84Zco0+l/PXbgPfXM4vQJ0lTsg0rvN44qfhzxwcOJCfJedELj5FDF55yq9T7eH7urkkMVn3Gs+l+NgkZmFUy3sB+TEDLyggsaAdS6HNhd9fdkUWb6RCY1egO6rWgPB8oLfU8/p5QyQfJ/QrcpkHwrdJ3Y6mn8vlPVmspKUUpxbdchWFT978fVThtrC3L8Hi/On29P/uBVzD0sIJi+kUnNnHHmhsV5/56/u+wQJ23lre4vt/Qgu8u865GOSricWD9T1BoyKRODon2KZBddXEWyJXBkUB35mzpkyBC9ZcuWRu25ublNvikWQlzc5O+/EEKItnJU2/ggdQaVh+prDHWfdBVjlz3eovMP5mdTFV7/xjDmVGdiuxu1JLV2wb5FYGtQvyi0B3S/wxNcKK7ZyfHqb3x6VSSGjCYysA/+nKr9gcKqDTi1rdG+8IAeJISMwmJqfopdR3bSdorXcv/m2Y60hnJ3/5updTr48+5PKGkwXSQiIIT/z96Zh0dVpfn/c6sqe0gCCUnYAoRAAoakskBAYogQpxsQaAnQjCggNIw/nWaa1ihot2OzODgy0tLitIoNiO1Cg83S40rLrmgIKUI0hEVD2JeQrbLWcn9/hNzkVmUjW1XgfJ6H5+Gce+45557cWu5b3/d7fhUxqcsb1JqsZv4n62/I9VINl0ROx70JA+Vvrn7Pvstqo/BHwx6gr3fr1SaC9kOWZd46+Q9VEOe+4OEkBg9v93E2nvpMFXiMDRjcKjVemamCt09+olIb9fLswZzB/9Kk2qiW0upy1v+wU3Ufzxn8L/Txst9FuyvywZmvyDPWBaunD0xicDsHAh2FJEkZsizHt6StUBwJBAKBQCAQCO4qfvjjdlXQSOOiI371whafb3ZRq31cNPXS+YuOq4NGAEEPqBQp/u5RBHnca9OrzKXy/RRWqY1uLXI1l8r2c7Hsn3ZBIw0u9PJMoo/X+C4dNAIwmtQKbG+dBwCuWh0PhoxWHSsxlfPPi8c6bW4dxbWKItXDtp+rd5NBI6jxtwlwV5udf3bhOyztvCW7oHX8VHpFFTTSSBr0/mHtPo4kSXZeR1kFP6p2Q2sJslxjtG6foja6RUEjgG6unoT59FbV3Ukm2cXC4wgQgSOBQCAQCAQCgZORt/0A2yPmsm3wo+yd+QeyVn/AxS+PUnXTduew26fi6k2O/5faR3Hov/8Cn7A+Le7D7GJjRu1R88u6bK2Ga3vVjX2HI3modx4C6OE+jF6eSYA6xelK+WEKKk8AUG66zE8lH1NcfdrufA9dEAN9puHnNuSO2O20zMYY28ulLhDW17un3UPy8ZtnOVvSOmNxZ8E2TS24BebJWo3WTlVyvbKYo9dz23VugtaRceOUqhzh1w9vF48OGWto9/50q9e3WbaQecP+vaIpfig6x6niC6q6+4KH2wUnm0MfoA6O5RSeo9Lc9U2yZVmm2KROTb5bU9W6tDm2QCAQCAQCgeDOwlRWwaH5r2AqrfmyXnr2EnnbDijHvQf2IiBuMP5xQwiIG4J/3BDcundrcf/HXtiE2VgXpHDr0Y3o3z3S4vMt1RVYbWwY3XxuBZ1uHAFzvV+nJS0E3t9oX35uQ9BIOi6W7aX+zmjXKr6lzHSBMvPFBs7S0NMjDn+34V1qx7TmMNoYY3vp1A/bSb2iOFtyiYKquuDhp+e/5Vfhk5pV6TgrV8ptdlTzbNgY25YQ70CGdx/IiXoGzAevZBHhF3LXPtQ6A4VVRs6UqF+zHWkSrZU0jOgZwVeXMpW6ozdOkRA4FJ2mea9Yo6mCL2x2Uevl6W/nn9QSQrupTbLNsoXswp+Ib0dTcEdgNFVgrafmc9e6dvkU2dYiAkcCgUAgEAgEAqehOCdfCRo1hPGnyxh/uqwKJnUL7YV/3BD8YwcTEB+Of+zgBoNJN0/8yOl3PlXV6f9z7m0FnioLzkO9rDCdpEPr6oNsMkLBYXXjHglIrk2by/q4hiJJOi4a/6kyvG4oaOSm6U5vr2Tcdf4tnm9Xocw2Vc1FnXrnotExKWQUW05/qaR3lZoq2HMxgwf7q1PZugotNcZuiHF9YjhdclHZ+txktbDnYgapA5vfDEfQMdiqfYI9etDHs2N9fqL9B3Hoygmqb5lxl5sr+b4wz86w2hZZlvnsfLpy/0DLdlFrjNqUvANXspQ6Q8EZ4gK6tiLS1hjb7y4OzIrAkUAgEAgEAoHAaTDm3f6OWaU/Xqb0x8vk/W2/UtcttBf+8eEExA5WgkrpT/8Z2Vr367HPkL5EPD75tsaqLLoMwXVlF40OcIHre8Bqqjug9YCeiS3qs5tLCP28f8Z54xfImBts08Mtkp4e8WikO/Pru9FsGziyT+/p4xXAqMChfHPtB6XuROFPhPv163JmtRbZyvXKIlVdS1LVavHUuXN/bz2fnq8zWT9VfIHTxRe63FrcCZisZo7fPKuq64ygibvWFb1/GN9dr9sh8ttrOUT1CG1y7O8L8zhdok5RSwqOuu0UtfpE+Ydy8MoJJbB7vbKYi+U36OvVdY3bi4S/kcKd+ckjEAgEAoFAIOiSlOZdVZUDx0TiO6QvBcdOU5j9E7KlZSbASjBp675G24z4739D43J7X4erTTdUZRdZg1xVAIU2Zs09k5C0LTes9nLpTUi3CZw3fo5VrlMB6CQvensl4eXScg+mrojR1uNI17AvTGLwcE6XXORGZbFS9+n57+jj1RNPnVuHzrE9uVFZrDK09nbxwOs2vXCiewwiq+BHLpbX3ZNfXsigv3cwrlrxmNeZ/FB4TqXe8dC6MbR7SKeMHd8znPTruUrApqCqhLMllwjzbfg9w2iq4MuLGaq63p7+jAyMaLB9S+nm4slg3z4qz6TMG2e6dODIVnF0N6eC3jmJ0QKBQCAQCASCLo+t4ihk6hgS30ljauZbPFLyDx785nVGvb6YwfN+TveoUCRt677OBt+vp9/k209xMqP+BVpr0cHVf1LfowjXHtC9RTscq/DUBdHfeyIe2kA0uODrOoRQn2l3fNAI7M2xbVPVatFptDwYMhqpnql4mbmSL228Wpydqzb+RrejNqpFkiR+1m+Eai2KTWV8fTW7zfMTtBxZlu1MsaP9B91SI3Y8vq5edkGq+gqk+siyzKfnv2u3FDVbbHeQO1mUT0UXNskWgaM6ROCoHRgwYADZ2Z3zBr1p0yZOnTrVfMMGePHFF3n66aeVfvz8/NDr9QwbNozU1FRu3rzZTA+3hyRJGI3GJtvk5eURENC+ub/79u1DkiTS0tJU9cnJyS2aky3153jp0iXuv7/O5HLHjh0MHTqUmJgYcnNz0ev1VFRUNNZVozQ1r9b22RQGg4GtW7d26DhffPEF8fHxuLm5KfddLRaLhSeffJJBgwYRFhbGhg0b2m1cgUAgEHRtyvLViiPv/kHK/3UebvRMGMrQJ6aS+Jc0fmF4m0dK/sGkr//EqD/9uiaYNHxg88EkSWLkmsdblUZidlE/BLmghVKb72ZB45FaYE7bEO66AAb4TGGI36P09kpCq+k6Kpq2YJuq1pjiCKCXZw/uDbpHVfdD0TlOFuV3yNw6git2/kYtM8a2JcijOyNsDIi/vZbD9YqiRs4QtDcXy26o/KokJGJsAigdTUJPtaH1OeNVLpfbP9tlF+bZGXiP7RWNfxtS1OoT2q2XKrhSY5L9Y7v07QhE4KiOLq1hlL9f3injSPe80CnjNIfFYmHTpk0EBAQwZEjbHfpTUlLYtm0bVquVmTNnsnLlSl599dV2mKnjCQ8PZ8eOHaxevRqtVsuPP/5IWVlZ8yc2Q+/evdm7t26b3TfffJPly5czY8YMoCYg0960d59msxmDwcA//vEPZs6c2WHjhIaGsmHDBrZt20ZlpfrL4F//+lfOnDnD6dOnKSgoICYmhpSUFAYMGNCucxAIBAJB18M2Vc17QFAjLWvQebgROGoYgaOGKXXm8kpuZv3IjaOnKDh2ioKMUxR9f07xN4pd/hj+MYNbNT+LuxWoCwq5SiZ1A4++0K1tKR/AHbVjWnNYZSvltoGjRhRHtYwJuoczJRdVD+yfX0gnxDsQT13LUwQdha0xdmsUR7UkBg8np+gcpbdUW1ZkPr9wlNlh47u0MXFX4eiNXFU5zKcPfm6d64UT7NmD/t5BnDPWvX9+dy2HqQPGKOVSUzl7LqqVeX08A+wCj21BkiT0/mHsv3xcqTMUnCU+ILxL3ovFwuNI4e75ROoEkpOTSUtLIzExkdDQUJYuXQrAoUOHiImJUbWNj49n//4aA8fNmzeTkJBAXFwc48aNIze35s1n06ZNpKSk8NBDDxEZGcm6des4evQoixcvRq/Xs2fPHgBefvllRo4cSWxsLJMnT+bKlRqJd3FxMdOnTyciIoLk5GTOnlUbttWi0WhU4+bm5jJhwgRGjBhBdHQ0GzduVNpKksRLL73EiBEjCA0NZfv27cqxjz/+mIiICPR6PStWrFDqbVVFjamMmmpX+/9ly5YRExNDREQEGRkZLFy4kKioKBISEpTrBvD29ubee+/l888/V9Z4zpw5qvHS09MZPXo0UVFRjB49mvT0dOXY+vXrCQsLIzY2lnfeeafBOS1ZsoSDBw/y7LPPKiqk+sqhptaxsbVqiPp9DhgwgBdeeIHRo0czYMAAXn/9daVdc3+3F198kREjRrBkyRJeeOEF9uzZg16vZ/HixXbjPP3000o/48eP59y5cw3O7Ve/+hWvvfaaUs7OziY0NBRZlgkLC0Ov16PT2cenP/roIxYuXIhGo6Fnz5784he/4G9/+1uT6yAQCASCOx9Zlu1S1boNCG6kdePoPN0JHDWMYf/+C+77yzP84vgGHinZzeT0/yX19Bain5/d6jla3NUPP642AQ+CH+iSD0iOpMxmDT20bmibCZxpNVom2aTXlJur+Ox8OrIsN3Gm47HKVvsd1TxbpzgCcNO6kNInTlV3vuwaJwp/anWfgpZRaiont+i8qi6+Z9t/4G8Nth5FOUX5imKmLkWtLtCtk+xfQ+1BVI9QNPXSJ29UFnOh7Hq7jtEZyLJMsUm9w+fdrDgSgaN2Jj8/nwMHDpCZmcmGDRs4ffo0iYmJGI1GsrJqtic8ceIEhYWFJCUlcfDgQbZu3cqBAwfIyMggLS2N+fPnK/0dOXKENWvWkJ2dzZIlS4iPj2fdunUYDAZSUlJ47733OHv2LEeOHOHYsWNMnDiRp556CoDly5fj4+PDyZMn2bZtmxKosqWqqopdu3YRExOD2Wzm4YcfZu3ataSnp3Po0CFWr17NyZN1ebI+Pj6kp6ezZcsWJehw9epVFi5cyM6dOzEYDLi5tb+suqCggMTERDIzM1mwYAHjx4/nySefJCsri7i4OFUQBWDevHls3rwZWZb58MMPefjhh5Vj1dXVpKamsnLlSrKyslixYgWpqalUV1eTlZXFqlWrOHz4MMeOHaOgoKDB+axdu1b5e9RXIQFNrmNb16q8vJxvvvmGffv2sXTpUoxGY4v+bh4eHqSnp/OnP/2J5cuXk5KSgsFgYN26dXZjLF26lPT0dI4fP86//uu/8uyzzzY4l9o1rmXjxo3Mmzev2S/M+fn59O/fXymHhIRw/vz5Js4QCAQCwd1A1c0SzMa6tGmdpztuAe2TQqHzdCcgbgg+g3q3ug9LRRlWN/VnnM5cbxc0n6FInv1a3f/dSpmp+R3VGiLIozuJQZGqutzi8+QUNfyDl7NQWGXEZK27b9y1rvi4eLapz3DffgzyUd/bX13MpNxc1aZ+BU1juHEGaz1/M383H/p7N62S7CgGdeuNv5uPUpaROXq9RhhwovAnzpZcUrUf2ysKf3cf2htvFw+7nf0yC860+zgdjdFUgbWegb271hU3rYsDZ+RYROConZkxYwYajQZfX1+GDh2qqHzmzp3Lpk2bgBol0dy5c5Ekid27d3P8+HESEhLQ6/UsXbpU9QCdmJjIoEGDGh1v165d7Nmzh9jYWPR6PevXrycvLw+AvXv3smDBAgACAgKYNm2a6txaxUlCQgKDBg1i2bJlnDp1ipycHGbNmoVer+e+++6jqqqKnJwc5bxZs2YBMGrUKC5dukRlZSXffvstsbGxhIfXSB0XLVrUtoVsAG9vbyZNmgRAbGwsffv2Ra/XAxAXF8eZM+o3pOTkZLKystixYweRkZH4+/srx3Jzc3F1dWX8+PFATdqeq6srubm57Nu3j0mTJhEUFNTqa2lqHdu6VrXrP2DAALp3786FCxda9HebO3dui8f49NNPGTVqFJGRkaxZs6bRNLbExERKS0s5ceIEZrOZDz744LbGEQgEAoGgPsYG0tScSb1TeT0f6s1HK2nR1Dd+DUzu/EndARjNNjuqNZOmVp/RQcMItvEH+vzCUbtd2pyJKxW2xtg92nyfS5LEA33i0Ul1aZQVlir2X25/GwVBDRarxS4gEhcwxGHvWZIkkRCo9joyFJzhRmUxey6od1Hr69WT+HZMUbOlIZPsrhbEtPU38ruL1UbQxT2OnMV7qD7u7nUfdFqtFvOtX6HmzJnDqFGjeOmll/jggw/45ptvgBoJ3Pz581m+vGG/Jm/vpvMoZVnmd7/7nUql1FJqPY5s+wsICGjS76b2GrXamg8mc/1f2hpAp9NhtdZFa239blrarr4yR6vVNrrWtUiSxMyZM1m4cKEqbaszaGodd+3a1eh5GzduVFK/0tLSmD3bXkrf0HVLktTs3625e6mWc+fOsWTJEtLT0xk4cCBff/21otZatWqVklK2du1a7r//fiUompyczNChQ1VKosYICQnh3LlzjBgxArBXIAkEAoHg7sQ2Tc27FWlqHUll8WXoVld20eigNnAk6cC1fTf8uFuwUxzdhkeR5taOUBtPfaZsb19pqebT898xfWCSUwUea7FPU2u9v1F9urt5MyY40s5fZniP0C69Jbqzklt8XpVm6arREdljoANnBPd0H8D+y8eVeVVbzWw5/SVVVnWK2sR+Ce2eolafgd2C8XP1puiWR5BFtpJ98ye7dDpnpkj4G6kQiqNOIiQkhGHDhrF48WKGDRumPCRPnjyZd999lwsXLgA1BtgZGRmN9uPj40NxcbFSnjJlCm+88QaFhTUfQFVVVRw/XvNhMW7cOCVgUlBQwN///vdm5xkeHo6npydbtmxR6k6ePElJSUmT540aNYrMzExOnz4NoNolKzg4GJPJpCiC3n///Qb7aGm722HRokU888wzTJgwQVUfHh5OdXW1kmL21VdfYTKZCA8PJzk5mU8++YRr164BqDyOWkpT69jUWj322GMYDAYMBkODQaPWjNcQtvdRfUpKSnB1dSU4OBir1cqf//xn5djzzz+vzK/W12nOnDl88MEHbNiwgccee6xF850xYwZvv/02VquV69evs2PHDqZPn97SyxUIBALBHYrxnI3iKMQxKR+NUV12Q1V21biA6VbgyMXXKYMUXQFbdVBLU9Vq6enhR1JwlKruTMlFsp3U4+dqua3iqH0CRwAJPSNU6UoAn59PV6XcCNqHjBunVeXhPUIdnsqk02iJC1B7LFVa1DtB1uyi1v4pavWpMclWZ80YCs44vf9YfcSOampE4KgTmTdvHm+//Tbz5s1T6pKSkli1ahVTpkwhOjqayMhIdu7c2WgfixYtYvny5Yo59qOPPsrs2bMZO3YsUVFRxMXFcfjwYQB+//vfU1hYSEREBKmpqSQlJTU7R51Ox+7du/nwww+Jiorinnvu4YknnqC6urrJ8wIDA3nrrbeYPHkyMTExKrWQTqfjtdde44EHHmDkyJGKUqmhsVvS7nbo06cPzzzzjJ1Bs6urK9u3b+e5554jKiqK559/nm3btuHq6kpUVBTPPfccY8aMIS4uDj8/v9set6l1bGqtWsvt/t3Gjx9PWVkZ0dHRik9VLcOHD2fGjBkMGzaMhIQEBg5s+peT2qDovn37VOmQhw4dom/fvrz66qu8+eab9O3bVzErf/TRRwkNDWXw4MGMGjWKF154odlxBAKBQHDn01CqmjNRbVH/IKNSHLm0jxfT3Yhdqpru9gJHUGMM3NvTX1X35YUMSqvLGznDMciyzBVbxZFH642xbdFqtPys7whV3bXKIsXrRtA+XCm/aWf4HBfQup0a25uYgMGqlMX61KSodY5593Abk+yCqhLOdyGTbBE4UiM5c9QvPj5ePnr0qF19Tk4OQ4cObeAMgUBwpyNe/wKBQHDnsmfq7zi/+xulnPzh7xk4M9lxE7Lh7HevUz24Lo0qyL0H3X/Mrin4xSD1meygmXVtPv7pILnFdR6fU/vfy7DuA267n4LKEv6S+ylm2aLUhXbrxczQZKdRgxVXl/HGD3U/ErtqdPx2+Ix2n9/uc1+TXZinlF00OhZFTMLnLn/4bS/+L/8IWTd/VMoDvIP517BxDpyRms8vpHPMRhGlk7QsiJhID7dujZzV/vz9p4OcrPfaHta9P1P7j+m08dvCB2e+Is9Ylz49feBYBvv2ceCM2h9JkjJkWY5vSVuhOBIIBAKBQCAQOAXO7nFk9VT/4OpS/wdYV6E4ai31fWKgdYojAH93H8b2ilbV/Vh6WfWA72hs09QCPbp3SFBrXO9Y3OulTZmsZvZcPNbu49yNlJur+KFQvXNfZ6l4WsrInvZeQsm9ozs1aASgt1Fh5Radp9zc9myLzsDW4+huN8cWgSOBQCAQCAQCgcORZdmpU9VkWcbqrU7/cLHU841xuf3UdkEN9h5HLTfHtmVEz3D62RhB77mYYZd24ijs09Taz9+oPl4u7oztpVfV5Rafp6CyYY9LQcs5XnBWpWrzdfFikE9vB87Inu5u3YitF7QJ7daL+ICO20WtMQZ4B+FXz1TaIls5cdM5vcfqY5WtlJjUaa53u1qvS++qJhAIBAKBQCC4M6guLMVUWvdFXevhhntP5wnGWEpLsHZTG9+6mC31CkJx1BpkWabM1uPoNs2x6yNJEpNCRvFO7ieYrDV/n2qrmU/yjzBr0DiHp6xdreg4Y2xbYvzDOF5wliv1xswrvYq/u7hXW4IsyxRVG7laUaj8u1ZRSKlNoDM2YHCH7lDWWh7oE8fAbr0wWy2E+/Z1yL0vSRIx/mHsvVy387Oh4Awje0Y4/LXYFEZTpcpQ3kPr5nDjc0cjAkcCgUAgEAgEAodTapemFuRUDxZVN85D97r5aCUNGnPdFtcicNQ6qq1mJcADNT4sbpq2PaB1d+vG/b1i+OJinVdqnvEqmQVnVCoMR2CnOPJsP2NsWyRJYlj3/qrA0fmya8Q5WVqVM2C2WrhRWWwXJKq2mps8TydpibLZPcxZ0Egahvj2dfQ0GN4jlP1XspRAzM2qUvKN1+jfzXkUpbYU26Sp3e3G2CACRwKBQCAQCAQCJ6Ds3DVV2bu/cz1UVBZdhnriENWOakjg0rHbW9+pNJSm1h4Bw9iAweQWn+ecsS798atLmQzs1ovubt5NnNlxlJkqVNerlTQEdLD6xzZtL994DVmWnSoo29lUmKu5ZhMgulFZjJXb3zQqyj8UT51bB8zyzsHLxZ1w377kFOUrdZkFZ5w8cCR2VLNFBI4EAoFAIBAIBA7Hzhi7v3MZY1eX31CVXTQuYL71cOHig+SEqSpdgfYyxrZFkiQm9kvgndxPFNWIyWpm32UDDw1IbJcxbhdbtVFPdz+0HXzfBHv2wEWjw3RrDcrMldysKsXf/e4IdJqtFn4qvcKVips1waLyQopNbfO70koaerr7EurTm3uD7mmnmd7Z6P3DVIGj3OIak2xPXev9zDoSETiyR3zCtQMDBgwgOzu7U8batGkTp06datW5L774Ik8//bTSj5+fH3q9nmHDhpGamsrNmzeb6eH2kCQJo9HYZJu8vDwCAgLaddx9+/YhSRJpaWmq+uTk5BbNyZb6c7x06RL333+/cmzHjh0MHTqUmJgYcnNz0ev1VFRUNNZVozQ1r9bMuaOZPXs2vXv3bnBuR44cITo6miFDhvAv//IvXLt2rZFeBAKBQCCoo6FUNWei2lyiKtcojm6lqok0tVZjrzhqn8ARgJ+bN+P7xKrqzhRfVIIonc1Vm8BRsGfH+RvVopE09PVSf9c+X3Z3fDerspjYeOoztv20n0NXTnCq+MJtB43ctS6EeAcyomc4D4aMYkH4BJ6Kmslj4RMY2yu65n1A0Cz9vYPoXm9HN6tsdardDm0RgSN7ROCoC2GxWNoUOLIlJSUFg8FAdnY2kiSxcuXKdunXGQgPD2fHjh1YLDU58z/++CNlZW3fTaN3797s3btXKb/55pssX76czMxMwsPDMRgMeHi03xceZ2XBggUYDAa7eqvVyiOPPML69es5deoUSUlJLF261AEzFAgEAkFXw3jOdkc151IcmTXqAIcqVU0EjlqNvTF2+yoQonsMwsfFUymbZQt5pVebOKPjuFqu/pE2yKPj/I3q088rUFXON94dgaMj137gxm3sIufr4sVgn74kBkWSOjCJJ4ZN5TeR05kdlkJKnziG9wgl0KN7h6vE7kRqTbLrY7hxBlm+/fTAzsDe48gx6a3ORBe/6/d20r+WkZycTFpaGomJiYSGhioPzIcOHSImJkbVNj4+nv379wOwefNmEhISiIuLY9y4ceTm5gI1qqCUlBQeeughIiMjWbduHUePHmXx4sXo9Xr27NkDwMsvv8zIkSOJjY1l8uTJXLlS84tdcXEx06dPJyIiguTkZM6ePdvgvDUajWrc3NxcJkyYwIgRI4iOjmbjxo1KW0mSeOmllxgxYgShoaFs375dOfbxxx8TERGBXq9nxYoVSr2tqqgxlVFT7Wr/v2zZMmJiYoiIiCAjI4OFCxcSFRVFQkKCct0A3t7e3HvvvXz++efKGs+ZM0c11tImHwAAIABJREFUXnp6OqNHjyYqKorRo0eTnp6uHFu/fj1hYWHExsbyzjvvNDinJUuWcPDgQZ599llFhVRfgdPUOja2Vo3xyiuvoNfrCQ8PV6357NmziY+PZ/jw4Tz00EMUFhYqY48ePZro6GgiIyNZs2YNANXV1aSlpTFy5Eiio6N59NFHG1Qz5efnExwcjMlUZ/o5ffp0Nm/eDMC4ceMIDAy0Oy8jIwN3d3cSE2sk4I8//jhbt25t9voEAoFAILBNVevmZIEji5tFVXbRuIBJBI7aitFkm6rWvoEjSZII8+2jqjtTcrFdx2gpdsbYHbijWn1CvNXf2c7fBYGjMlMF6ddzGzwmIdHT3Y/I7gMZ3zuWhweN5zeR03ninqlMD03ivl5RDPHti6+r113tBdXeDO8xUBV0K6w2qjzInIkiG8WRn1AcdfXAkfORn5/PgQMHyMzMZMOGDZw+fZrExESMRiNZWVkAnDhxgsLCQpKSkjh48CBbt27lwIEDZGRkkJaWxvz585X+jhw5wpo1a8jOzmbJkiXEx8ezbt06DAYDKSkpvPfee5w9e5YjR45w7NgxJk6cyFNPPQXA8uXL8fHx4eTJk2zbtk0JVNlSVVXFrl27iImJwWw28/DDD7N27VrS09M5dOgQq1ev5uTJk0p7Hx8f0tPT2bJlC4sXLwbg6tWrLFy4kJ07d2IwGHBza3+TuIKCAhITE8nMzGTBggWMHz+eJ598kqysLOLi4nj99ddV7efNm8fmzZuRZZkPP/yQhx9+WDlWXV1NamoqK1euJCsrixUrVpCamkp1dTVZWVmsWrWKw4cPc+zYMQoKChqcz9q1a5W/R30VEtDkOrZmrbRaLQaDgV27drFo0SIl/eu1117j6NGjnDhxgnvuuYeXX34ZgDfeeIMpU6Zw/PhxsrOzWbBgAQD//d//ja+vL9999x3Hjx+nd+/e/Nd//ZfdeCEhIURGRvLpp58qa79v3z6mT5/e5Dzz8/Pp37+/Ug4ICMBqtbZ7GqRAIBAI7ixkWcaYZ6s4cp5UNVmWsfqoU1J0kgZqU55c/BwwqzuDsg5MVaslzMc+cNTZSodKczVF9VQMEhKBHp1z3/Ty9Fc9sJeYyu1Sce40vr76vSol0VPnzsR+CTw25Oc8HTWTX0VMZHL/0YwMjKB/tyA8dK4OnO3dgafOnSG+/VR1hoIzDppN41hlKyWmclWdjwgcCXPs9mbGjBloNBp8fX0ZOnQoZ8+eZfDgwcydO5dNmzbx6quvsmnTJubOnYskSezevZvjx4+TkJAA1HwxqVWNACQmJjJoUONbPO7atYujR48SG1uTv202m/H1rfnVa+/evfzpT38Cah7gp02bpjp3z5496PV6AMaMGcOyZcs4deoUOTk5zJo1S2lXVVVFTk4OERERAMqxUaNGcenSJSorK/n222+JjY0lPDwcgEWLFvHss8+2fiEbwNvbm0mTJgEQGxtL3759lfnHxcXx5ZdfqtonJyfzxBNPsGPHDiIjI/H391eO5ebm4urqyvjx44GatD1XV1dyc3PZt28fkyZNIigoSLmW21XNNLWOWq32tteqNvATHh5ObGwsR44cYcqUKbz77rv89a9/pbq6mrKyMoYMqdleNSkpiWeeeYby8nLuv/9+RRG1a9cuSkpK2LZtmzKn6OjoBsecN28emzZtYsqUKbz//vtMmTIFLy/xpikQCASC9qe6yIippO5BVuvuintg56gxWoK58Cayv1oJ42KtF3hwFYqj1mLsIHPs+vT3DlIZRBtNFVytKCTYs3NSxcDe38jf3afT/HF0Gi19PAPIr+dtlG+8yvAeoZ0yfmdTXF1Gpk1AYkzQPUT7N/5MJegcYvzDyCk6p5Rziy9QZqps9xTVtmA0VWKVrUrZQ+uGm9bFgTNyDkTgqJ1xd6+76bVaLWZzzQfUnDlzGDVqFC+99BIffPAB33zzDVATKJo/fz7Lly9vsD9v76bzKWVZ5ne/+51KpdRSUlJSlABC/f4CAgIa9K+ppfYatVotgHKNjaHT6bBa6158lZWVrWpXX5mj1WobXetaJEli5syZLFy4UJUm1hk0tY67du1q9LyNGzfy2muvAZCWlsbs2bMbbXvw4EH+93//l6+//pqePXvy/vvv89ZbbwGQmprK6NGj+eKLL1i9ejV/+ctfeO+995BlmTfeeINx48ap+iooKFCCaOHh4Xz00UdMmzaNJUuWUFBQwKZNm/jjH//Y7HWHhIRw7lzdh8GNGzfQaDT06NF5X8wEAoFA0PWw8zfqH+RUKSLV1y5AkFYpa5DQmOvSuUWqWuvpDMWRTqNlYLdgThVfUOpOl1x0aOCos9LUaunnHagKHJ03Xr9jA0cHr5zAUu/B39fFC72Nv47AMYR4B9LDzYebVTWbDdSaZI8OGubgmdVh728kfjiHLp+qdn8n/Ws7ISEhDBs2jMWLFzNs2DAlnWfy5Mm8++67XLhQ80FmsVjIyMhotB8fHx+Ki+tM3qZMmcIbb7yhqJSqqqo4fvw4UONDUxswKSgo4O9//3uz8wwPD8fT05MtW7YodSdPnqSkpKSJs2rUR5mZmZw+fRqADRs2KMdqvXLOnKmJ/L///vsN9tHSdrfDokWLeOaZZ5gwYYKqPjw8nOrqaiXF7KuvvsJkMhEeHk5ycjKffPKJkg5W3+OopTS1jk2t1WOPPYbBYMBgMKiCRrV/x9OnT5OZmcmoUaMoKirC19cXf39/qqqq+Mtf/qK0P3PmDMHBwcybN4///M//5LvvvgNq7pdXX31V2fmttLSUnJwc/P39lXE/+ugjADw9PZk6dSrLli2jpKSE++67r9nrjouLo6KigkOHDgHw5z//mRkzZtz2+gkEAoHg7sLW38jZjLEriy+ryi4aFySLCBy1B7aKI+8O2p7bLl2tuHN9jq5UqNP2gzvJGLsWW5+j/Dt0Z7UblcVk3/xJVZcYPBydRtvIGYLOpMYkW638MhQ4l0m22FGtYbp44KhrMW/ePN5++23mzZun1CUlJbFq1SqmTJmiGBnv3Lmz0T4WLVrE8uXLFXPsRx99lNmzZzN27FiioqKIi4vj8OHDAPz+97+nsLCQiIgIUlNTSUpKanaOOp2O3bt38+GHHxIVFcU999zDE088QXV1dZPnBQYG8tZbbzF58mRiYmJUaiGdTsdrr73GAw88wMiRIxWlUkNjt6Td7dCnTx+eeeYZdDq1uM7V1ZXt27fz3HPPERUVxfPPP8+2bdtwdXUlKiqK5557jjFjxhAXF4ef3+3nnze1jk2tVWOYzWZiYmJ48MEHefPNNwkMDOTnP/85gwYNYsiQIYwdO1ZJVwTYunUrw4cPJyYmhl//+teKimnp0qVER0czYsQIoqKiSExMJCcnp9Fxa+/ZuXPnquqnTZtG3759gZog2c9+9jOgxmh9y5Yt/L//9/8YPHgw+/fvZ/Xq1be9fgKBQCC4u7DzN+rvPP5GANXlN1RlF229HdW0XkgakcbQGqyylXKbwJFnB6WshPn0VpWvVNyk1MbHpCNxtOKot2cAGupUfIVVpRht1F53AgcuZyFTF4Twd/MhsscAx01IYEekjUl2UbWRPOOVJs7oXETgqGEkZ4ru2RIfHy8fPXrUrj4nJ4ehQ4c6YEYCgcDRiNe/QCAQ3Hl8+5v1/LDuY6Uc99KviFr6rw6ckZpzX75BeXydeW53Nx+CSsvgxnnw6I0U+isHzq7rUmoq5/XvdyhlT50b/xGZ2mHjbTr1OZfL6zY9mdBvZKekMJmsZv4n62+qgMaSyOm4d7Ih8+ZTn3Op3vVP7T+GYd37N3FG1+JSeQGbT32uqps24D7C/fo1cobAUew8d5gfCuvsLSJ8+/HQwOazGzqDT/K/5fjNut3IH+gTR3zPcAfOqOOQJClDluX4lrQViiOBQCAQCAQCgUMxnrNNVXMuxZFZo1bFuGjqKY5EmlqrKTN1vDF2fQbbpKud7qR0tWsVRaqgkZ+rd6cHjQBCvNWvq/N3WLra/kvHVeVenj0Y4tvXQbMRNEWM/2BV+VTxBadRwNl7HDXtOXy3IAJHAoFAIBAIBAKHYpeq5mQeRxY39QYcLhoXEThqB2wfFL07eGelMF914Civ9Ipqy/aOwjZNLbiT09Rq6efdU1U+b7xzAkd5pVfs0p3G9tI7lcm+oI5+Xj3xd/NRylZksm7+6MAZ1VFkk6rmJ1LVABE4EggEAoFAIBA4mIZ2VXMWZIsFq4/aK1EojtqHMnPnKo4C3f3wcfFUymbZwrnSq02c0T5cKVcbYwd14m5u9enr1ROpns/R9cpiO4+progsy+y7rFYb9fcOYmA35wpAC+qQJMkuTfS4E5hkW2UrJTbeZz4icASIwJFAIBAIBAKBwIFUFRmpLqpLDdC6ueAR5BhFRkOYblxHDlAHNNSBo9vfRENQQ2crjiRJYpDt7molHZ+u5mhj7Frcta4Eeajv1wvG6w6ZS3tyuviCyrsKYGyvaAfNRtBShtuZZJfxU+nlJs7oeIymSqyyVSl7aN1w04rND0AEjgQCgUAgEAgEDqTMRm3k1T8ISeM8X1Grr14Cj7oHBwkJDRowm2oqhOKo1RjN6sBRRyuOAAb72geOOlLlYJGtXK8sUtU5KlUNoJ93oKqc38V9jqyylf1XslR1Q3z70scrwEEzErQUD50bEX4hqjpDwdlGWncO9v5GQm1Ui/N8KgsEAoFAIBAI7jpK82yNsZ0rvaSyRP0LuItGh2Sp54vjKgJHrcXWHLujFUdQk8LkotEq5VJThZ0iqD25UVmMpZ6CwdvFAy+Xjg+QNUY/L3XgqKv7HH1fmMeNymJVXVJwlINmI7hdYmzS1Rxtkl1s428kAkd1iMCRQCAQCAQCgcBhGG0DR07kbwRQXX5DVValqWlcQdPxwY47FVvFkXcnBFR0Gi0DvHup6joyXe2qjb+RI9VGYG+QfbWiiEpLtYNm0zYsVgsHr5xQ1UV2H0hPD5E+2lXo69WTAPe64LuMTF7plSbO6FhE4KhxROCoHRgwYADZ2dmdMtamTZs4depUq8598cUXefrpp5V+/Pz80Ov1DBs2jNTUVG7evNlMD7eHJEkYjcYm2+Tl5REQ0L5S0n379iFJEmlpaar65OTkFs3JlvpzvHTpEvfff79ybMeOHQwdOpSYmBhyc3PR6/VUVNx+lLw183IGDAYDY8aMwdPTk+nTp9sdX7FiBYMGDWLQoEGsWLHCATMUCAQCgbNju6NaNydTHJnMJaqyekc1P7FrUxuwVRx1RqoaNJyu1lFcsfM3cowxdi2eOne7B/WLZTeaOMN5ySw4o3rQ10ga7gse7sAZCW4XSZIYbOM71pEKwOYQgaPG0TXfxHnJKdzQKeMM7f6rThmnOSwWC5s2bSIgIIAhQ4a0ub+UlBS2bduG1Wpl5syZrFy5kldffbUdZup4wsPD2bFjB6tXr0ar1fLjjz9SVlbW/InN0Lt3b/bu3auU33zzTZYvX86MGTOAmkDK3URgYCCvvvoqBoOBL7/8UnXswIED/O1vf1OCqgkJCYwdO5akpCRHTFUgEAgETorxnHOnqpk1lUBdQMNFo4PqqlsFkabWWmRZtktJ8eqEVDWAQT69VeXL5Tcxmio6RPFk+xDsaMUR1KSr1U/vyjdes1sTZ6faYubrq9+r6vT+g/Bz83bQjAStxdYs/ppDA0e2HkfifqpFKI7akeTkZNLS0khMTCQ0NJSlS5cCcOjQIWJiYlRt4+Pj2b9/PwCbN28mISGBuLg4xo0bR25uLlCjCkpJSeGhhx4iMjKSdevWcfToURYvXoxer2fPnj0AvPzyy4wcOZLY2FgmT57MlSs1X8CKi4uZPn06ERERJCcnc/Zsw2ZjGo1GNW5ubi4TJkxgxIgRREdHs3HjRqWtJEm89NJLjBgxgtDQULZv364c+/jjj4mIiECv16vUJbaqosZURk21q/3/smXLiImJISIigoyMDBYuXEhUVBQJCQnKdQN4e3tz77338vnnnytrPGfOHNV46enpjB49mqioKEaPHk16erpybP369YSFhREbG8s777zT4JyWLFnCwYMHefbZZxUVUn3lUFPr2NhaNYQkSaxatUpZ83/+85/KOkRGRpKTkwPAlStXuP/++4mLi+Oee+7hmWeeUfrYuXMnw4cPR6/XExkZyb59+wD4wx/+oMwjJiaGoqIiu/FXrlzJkiVLlHJBQQEBAQGUlZXRu3dvEhIScHNzszvvo48+Ys6cOXh4eODh4cGcOXP46KOPmrxWgUAgENx9GBswx3YmLO4WVVm9o5oIHLWWaqsZs1y3tjpJi5umc3Yv8nbxoJenWvnTEaojWZbtd1TzdKziCCDEu+v7HB29kUuZuU6x5qLRMiYo0oEzahuybMWan4X10knkep5YdwOBNoGjqxVFHWpY3xRFNoojP6E4UhCBo3YmPz+fAwcOkJmZyYYNGzh9+jSJiYkYjUaysmoc/0+cOEFhYSFJSUkcPHiQrVu3cuDAATIyMkhLS2P+/PlKf0eOHGHNmjVkZ2ezZMkS4uPjWbduHQaDgZSUFN577z3Onj3LkSNHOHbsGBMnTuSpp54CYPny5fj4+HDy5Em2bdumBKpsqaqqYteuXcTExGA2m3n44YdZu3Yt6enpHDp0iNWrV3Py5EmlvY+PD+np6WzZsoXFixcDcPXqVRYuXMjOnTsxGAwNBhLaSkFBAYmJiWRmZrJgwQLGjx/Pk08+SVZWFnFxcbz++uuq9vPmzWPz5s3IssyHH37Iww8/rByrrq4mNTWVlStXkpWVxYoVK0hNTaW6upqsrCxWrVrF4cOHOXbsGAUFBbZTAWDt2rXK36O+Cgloch1bs1Z+fn6kp6fz8ssvM3XqVMaMGUNmZiZz5sxh1apVSpvdu3eTkZGBwWDg6NGjfPbZZwC88MILvPXWWxgMBo4fP05sbCw3b95k7dq1ZGZmYjAYOHDgAN7e9lH1OXPm8OGHH2I21xiBvv/++0yZMgUvr6bfSPPz8+nfv79SDgkJ4fz5881eq0AgEAjuLuxT1ZwncGStqsLqoxbou2h1YLoVOBLG2K3GVm3k7eLeqWl/YTbpMWeKL7X7GDerSjFZ64zU3bWu+Lh4tvs4t4utz9Hl8gLVPJ2dCnMVR67+oKqLDwjvFI+sjsLy2Z8wb/8D5o+ex7T5P7B8/xWyxeToaXUK3d28awLyt6iwVFHqAINsq2ylxFSuqhOpanWIwFE7M2PGDDQaDb6+vgwdOlRR+cydO5dNmzYBNUqiuXPnIkkSu3fv5vjx4yQkJKDX61m6dKnq4ToxMZFBgwY1Ot6uXbvYs2cPsbGx6PV61q9fT15eHgB79+5lwYIFAAQEBDBt2jTVuXv27EGv15OQkMCgQYNYtmwZp06dIicnh1mzZqHX67nvvvuoqqpSVC0As2bNAmDUqFFcunSJyspKvv32W2JjYwkPDwdg0aJFbVvIBvD29mbSpEkAxMbG0rdvX/R6PQBxcXGcOXNG1T45OZmsrCx27NhBZGQk/v7+yrHc3FxcXV0ZP348UJO25+rqSm5uLvv27WPSpEkEBQW1+lqaWsfWrNUvf/lL5bolSeLBBx+0u26LxUJaWhrR0dHExcWRnZ2tpM6NGzeOJUuW8Morr5CTk4OPjw++vr6EhYUxZ84c3n77bYxGIzqdffZqSEgI99xzD5988glQc//OmzfvttdEIBAIBAJbqouNVBeWKmWNqwsewY5XZNRivnYNOUD9oG/rcSRoHWVm2zS1zn3otw0c5RkvY7ZaGmndOq5U2Bpj93AKT6xuLp50r5eCY+1iPkffXsuhyloXVHHXupAQOMyBM2obctEVrCcP1FUUXsLyxXpMG/8dy7F/INt4gd1paCQNge7q91JH+BwZTZVY66m9PLRuuGo7RwXZFejSHkfO4j1UH3f3utxsrVarqDTmzJnDqFGjeOmll/jggw/45ptvgBoJ6/z581m+fHmD/TWkAKmPLMv87ne/U6mUWkqtx5FtfwEBAU169dReo1Zbs5Vp7TU2hk6nw2qtexFWVjb85tdcu/rKHK1W2+ha1yJJEjNnzmThwoWqNLHOoKl13LVrV6Pnbdy4kddeew2AtLQ0Zs+eDajX3HYdaq/71VdfpbCwkG+//RZ3d3cWLVqkrOHatWs5ceIEX331FTNmzOC3v/0tCxcu5MiRIxw+fJivvvqKuLg4PvvsMzIyMuzmUKveGjhwIMXFxdx3333NrkFISAjnzp1Tyvn5+fTr16/Z8wQCgUBw92CbpubdPxBJ4zy/a1ZfuwShrqo6naQVqWrtgNHmYdi7k4yxawny6E43F09KbykMTFYLecYrdgGltmCfpuZ4f6Na+nkHUnizzs8l33iNAd2cy1+sIYymCtKv56rqRgUOw0Pn2sgZzo/1XCPPXaU3sOzfiOXbbWhjJqLRT0By79a5k+skAj26c7HeDpbXKgrtTOw7Gnt/I6E2qo/zfDLf4YSEhDBs2DAWL17MsGHDlBSeyZMn8+6773LhwgWgRjWSkZHRaD8+Pj4UF9eZ2U2ZMoU33niDwsKaD6aqqiqOHz8O1KhMagMmBQUF/P3vf292nuHh4Xh6erJlyxal7uTJk5SUlDRxVo36KDMzk9OnTwOwYUOdcXlwcDAmk0lRxrz//vsN9tHSdrfDokWLeOaZZ5gwYYKqPjw8nOrqaiXF7KuvvsJkMhEeHk5ycjKffPIJ167V5HvX9zhqKU2tY1Nr9dhjj2EwGDAYDErQqKUUFRXRq1cv3N3duXjxIjt37lSO5ebmMnz4cP7jP/6DRx55hPT0dEpLS7l+/Tpjx47lD3/4A5GRkWRnZzc4h2nTpnHgwAH+53/+h3nz5rXo17IZM2bw7rvvUlFRQUVFBe+++y4zZ868rWsSCAQCwZ2NbZqasxljVxZdVpVdNLqaz0AROGozjjLGrkWSJMJsDKHbO13tarmt4sh5Akd2PkdlXcPn6PDVbJU3lpfOnfie4Q6cUdux5jWzuU5lKZZvPsK04XHMBzYjG9t3J2xnwNYg2xGKI9sd1YS/kZourTjqasybN49HH31UFUxISkpi1apVTJkyBYvFQnV1NTNmzCAuLq7BPhYtWsRTTz3FK6+8wpo1a3j00Ue5ceMGY8eOBcBqtfLEE08QHR3N73//e+bPn09ERATBwcEt2s1Kp9Oxe/dufvOb3/DKK69gsVgICgpi69atTZ4XGBjIW2+9xeTJk/Hw8CA1NVXV52uvvcYDDzxAz549lXSzhsZuSbvboU+fPiqT6FpcXV3Zvn07ixcvpqysDC8vL7Zt24arqytRUVE899xzjBkzBh8fHyZOnHjb4za1jk2tVVtYvHgxM2bMIDIykr59+yppeABLly7l9OnT6HQ6/Pz8eOeddyguLiY1NZWKigqsViuxsbF26Yy1eHp6MnXqVDZu3MhPP/2k1Ofl5ZGYmEh5eTmVlZX07duXP/zhDyxYsIDk5GSmTZvGPffcA9So7mrvU4FAIBAIAIx5Njuq9XeuwFF1pdrnUPHhMFeDpAWd2HGntdQ3NobOVxxBTbpaZkGd1cHZkovIcny7pJPJsswVW8WRh/OkYfbzUgeOLpUVYLZa0Gm0DppR8xRWGTHcUFtTjAmOVPnjdDVkiwn5/AlVnWbIGKxn08FSrW5sqsSasQur4RM0Q5PRxk9F6t61dsNrDFs1njMEjsSOamokRzmWt4T4+Hj56NGjdvU5OTkMHTrUATMSCASORrz+BQKB4M7hu9++wfd/rNuhNXbFfKKfvz3FbUdybudaypPqUkN8XbvRy6075B4B1x5Ig//dgbPr2uw+9w3ZhXU/Rk3oNxK9f1inzsFkNfPHE9tVCpb5Qya0S0pZcXUZb/xQp/521ej47fAZTuFxBDWBrTd+2KkyA34kLIV+NkokZ2LXua/5vjBPKfu5erEo4kG0Thzsag7r+ROYt71YV+Hph8uiDVBejCXz/7Ae/wyqyxs+WdKgGTwKzYiH0ASGdsp8OwqT1cz/ZP0NmbrYxJLh03HXdl4K4v/lHyHr5o9K+V/6xBPXc0inje8IJEnKkGU5viVtRaqaQCAQCAQCgcAhGPPV6THeTrSjGoBZo1bFuGh0Ik2tnbA1x3aE4shFo2Ogja/PmZKL7dK3bZpaoEd3pwkaQU2qnm2Q6HzZdQfNpnmuVRSpgkYA9wVHdemgEdinqWkG6JEkCcnLD13ibFx+9We0iY+AZwNG/LIV66mvMf81DdPfV2K98L3DtrFvKy4aHf7uPqq66xVFnToHe8WRSFWrjwgcCQQCgUAgEAgcgl2qmpN5HFncraqyCBy1H2U25tidvataLWE2Bryn2ylwZJ+m5jz+RrXY+hzlG6820tLxHLh8XFXu6e7LsO79HTSb9kO2McaWBsSoy25eaEc8hMuCN9COWwg+DSvC5LxMzH97AfNHz2M9m44sWxts58wEejh2ZzUROGoaETgSCAQCgUAgEDgE+8CR8yiOrGVlWP3UWzG7aFxE4KidMNopjjrXHLsW213ULpcXUGZj3N0arlY4rzF2LbY+RxfKbqi2I3cWLpRdtwvoje0VjUbq2o+ysvEm8vW8ejUSmpDoBttKOje00T/H5bHX0U74DVJASMN9Xs7FvGs15i1PYck5gGy1NNjOGXGkQbZVtlIiAkdN0qZXmyRJSyRJ+l6SpGxJkj6QJMldkqSBkiR9K0nSGUmSPpIkyfVWW7db5TO3jg9ojwsQCAQCgUAgEHQ9qkvKqLpZqpQ1Ljo8e/k7cEZqTFevIQd4qupUiiPXBlJHBC3CIlspN1ep6jw7eVe1WrxdPAi2Ma0+U9L23dXsFEeezmOMXUsPt2541QvYmaxmu3k7GlmW2W+jNurjGWAX8OuKWG3VRsFhSB7dGml9q41GizbiPnSPvIpu6jKkXg3vKCcX5GP57DVMG/8dy/HPukQAyTZwdK0T70WjqQJrPX8lD60brlqXJs5xo+9tAAAgAElEQVS4+2h14EiSpD7AYiBeluVIQAvMAl4G1sqyHAYUAgtunbIAKLxVv/ZWO4FAIBAIBALBXYjxnDotxqt/EJLGeRQEpmuXwE8dzBCpau1DuU2amqfODa0D1SO26Wpt9TkqM1VgrKda0koaAtyd735p0OfIeK2R1o7hp9Ir5NvMaWyvaKfyi2otdmlq/fUtPleSJDSh8bjMegndjBXYprgplFzD8tXbWD5Z24aZdg6B7urA0fXKYiydpIATaWrN09Z3aB3gIUmSDvAELgPjgG23jm8GfnHr/1Nvlbl1fLx0J7ziBQKBQCAQCAS3jW2aWjcnSlMDqCxSz08naWseVk2mmgoROGo1RrM6cOQIY+z6DLZRr/xUegVzGxQatqqdnu5+Dg2MNUU/r56qsm2QxpE0pDYa0C2Y/t2c672iNchWC9Zz6mvTNBb8aQZN32G4PPQ7dLPXoAkfAw3ca9bT3yDbvKc5G14u7njX8zqzyFYKKos7Zewim8CRnwgc2dHqdzBZli8Ca4B8agJGxUAGUCTLsvlWswtA7TtxH+D8rXPNt9o7jx65DQwYMIDs7OxOGWvTpk2cOnWqVee++OKLPP3000o/fn5+6PV6hg0bRmpqKjdv3mymh9tDkiSMRmOTbfLy8ggICGjXcfft24ckSaSlpanqk5OTWzQnW+rP8dKlS9x///3KsR07djB06FBiYmLIzc1Fr9dTUXH7efGtmdftsG/fPr744osO67+Wqqoqfv7znxMQENDg33X37t1EREQQFhbGL3/5S8rLG9leVCAQCAR3PMZz6gdUrxDnehisrrihKrvUpi2YqwEJXHzsTxK0CFsPIS8HpanVEuTRXfXAarKaOdcGo2hbb5ZgT+fzN6olxFv9urtQds1pdubKLT7PFRuvqOReDXsAdTXkq2ehst53fzdvpOCwNvWpCRyIbuJvcZm3Ds3wB0CrUx23nvq6Tf13Bo7yOSquVj+H+bp6d8q4XYm2pKp1p0ZFNBDoDXgBP2/rhCRJWiRJ0lFJko5ev+68W0I6AovF0qbAkS0pKSkYDAays7ORJImVK1e2S7/OQHh4ODt27MBiqfm16Mcff6SsrKyZs5qnd+/e7N27Vym/+eabLF++nMzMTMLDwzEYDHh4OPZXs4ZoS+Codg1bglar5emnn2bPnj12x4xGIwsXLmT37t2cOXOGbt26sWbNmlbNSSAQCARdH2ffUc1kKVWVXTS3HsLM1aDrhiR17W3AHYmtMbaXgxVHkiTZeea0JV3tark62BHk4Xz+RrX0dPfFXeuqlCstJq5Vdu426A1hla3sv5ylqgv37UcvzztCd2CXpqYJGY6kaZ/3FMmvF7qUx9EmzVPVi8BR44hUteZpi2YyBfhJluXrsiybgI+BMYDfrdQ1gL5A7bvuRaAfwK3jvkCBbaeyLL8ly3K8LMvxPXv2tD2s4r8M73fKv5aSnJxMWloaiYmJhIaGsnTpUgAOHTpETIxaehgfH8/+/fsB2Lx5MwkJCcTFxTFu3Dhyc3OBGlVQSkoKDz30EJGRkaxbt46jR4+yePFi9Hq98nD+8ssvM3LkSGJjY5k8eTJXrtR8ESsuLmb69OlERESQnJzM2bNnG5y3RqNRjZubm8uECRMYMWIE0dHRbNy4UWkrSRIvvfQSI0aMIDQ0lO3btyvHPv74YyIiItDr9axYsUKpt1UVNaYyaqpd7f+XLVtGTEwMERERZGRksHDhQqKiokhISFCuG8Db25t7772Xzz//XFnjOXPmqMZLT09n9OjRREVFMXr0aNLT05Vj69evJywsjNjYWN55550G57RkyRIOHjzIs88+q6iQ6iuHmlrHxtaqISRJYtWqVcqa//Of/1TWITIykpycHKXtyy+/TGRkJJGRkTz22GMYjUZOnDjBn//8Z9599130ej2rV68G4N1332X48OFERUXx0EMPce1azS+/tvfdiRMnVPNp6n7W6XSkpKTg52dvGPrpp58SHx/P4MGDAXj88cf56KOPmrx2gUAgENy5GM85d6qaSaNOp1IFjlxFmlpbKLPxOKqv9nEUdoGj4outVt7YGWM74Y5qtUiSZJeu5gw+Rydu/sTNqhKlLCGR1CvKgTNqX6x5Nv5GrUxTawrN4FGqtDX5+k/IhW03fu9I7A2yOyeIKQJHzdOWwFE+MEqSJM9bXkXjgR+AvcD0W23mAjtv/X/XrTK3jn8lO4sOsh3Jz8/nwIEDZGZmsmHDBk6fPk1iYiJGo5GsrJqo+YkTJygsLCQpKYmDBw+ydetWDhw4QEZGBmlpacyfP1/p78iRI6xZs4bs7GyWLFlCfHw869atw2AwkJKSwnvvvcfZs2c5cuQIx44dY+LEiTz11FMALF++HB8fH06ePMm2bduUQJUtVVVV7Nq1i5iYGMxmMw8//DBr164lPT2dQ4cOsXr1ak6ePKm09/HxIT09nS1btrB48WIArl69ysKFC9m5cycGgwE3N7d2X9uCggISExPJzMxkwYIFjB8/nieffJKsrCzi4uJ4/fXXVe3nzZvH5s2bkWWZDz/8kIcfflg5Vl1dTWpqKitXriQrK4sVK1aQmppKdXU1WVlZrFq1isOHD3Ps2DEKCuzimwCsXbtW+XvUVyEBTa5ja9bKz8+P9PR0Xn75ZaZOncqYMWPIzMxkzpw5rFq1CqgJzGzZsoWvv/6aEydOYLFYWLFiBcOHD+fxxx9nzpw5GAwGli5dSnZ2NkuXLuWLL74gKyuLyMhIfv3rXyvj1b/v9Hq1UV9T93NT5Ofn079/f6UcEhLC+fPnm712gUAgENyZGPPUqUDOpDiSZRmLu1px66JxqdmZyGoR/kZtxF5x5NhUNYAB3YLQ1VORlZjKud4K5U2luZqiemkvEhKBHs69A5+zGWSbrRYOXVH/cDm8x0CnNBhvDXJlKfKV06o6zYCWG2O3FMmrO1LfYao6Z1cdBTagOOqMkIEIHDVPWzyOvqXG5PoYcOJWX28BzwK/lSTpDDUeRrVyjXcA/1v1vwWWtmHeTsuMGTPQaDT4+voydOhQReUzd+5cNm3aBNQoOubOnYskSezevZvjx4+TkJCAXq9n6dKlqofpxMREBg0a1Oh4u3btYs+ePcTGxqLX61m/fj15eXkA7N27lwULaja1CwgIYNq0aapz9+zZg16vJyEhgUGDBrFs2TJOnTpFTk4Os2bNQq/Xc99991FVVaVStcyaNQuAUaNGcenSJSorK/n222+JjY0lPLxmS8hFixa1bSEbwNvbm0mTJgEQGxtL3759laBGXFwcZ86cUbVPTk4mKyuLHTt2EBkZib9/nbQ1NzcXV1dXxo8fD9Sk7bm6upKbm8u+ffuYNGkSQUFBrb6WptaxNWv1y1/+UrluSZJ48MEH7a57z549zJo1Cx8fHyRJYtGiRQ2mjEHNvTFx4kR69eoFwL/927+p2jZ33zV2PwsEAoFA0FKcOVXNWlyM3KOBHdVMYke19uD/s3fecW6Ud/7/PDMjbdFWb/HaXq+97nXdwcYFY+zQTQuEw8E2BMgl5MhxAQIELoQAgbv84MwdJMEkpoVAzgQwFwiJQ8eG2MYV3Hvb4vU2aaXVlOf3h1bSPDOStmhXZf19v1682HmmPRqvtJrPfL6fbyo6jhySgqGW0OW9TV0vV7OW1hRl5oXdailKhUU4OuKpS2rO0Zen9qJZDedgykzCnLKJSZtPT2Mc3gaYuoWxogqwnN4pwZNGzRbPveezXjlPT1HozIHT9H7x6X7hd6E3MLiBZhKOOiSuTzHO+U8B/NQyfADAWRG29QG4Jp7zpQOZmeEvGbIsQ9MCOeFLly7FzJkz8eijj+IPf/gD1q9fDyDwROumm27CQw89FPF4OTmxg7k457j//vsFl1JnWbhwIVavXi2Mcc5RXFyMLVu2RNkr/BplOfBUJvgao6EoCgwj/OHo8/m6tZ3ZmSPLctRrHYQxhmuvvRa33HKLUCaWCGJdxzVr1kTdb9WqVVixYgUA4K677sKSJUsAiNfceh06uv7dwfx7t337dtxwww0AgPPOOw9PPvlk1N/nWFRUVAjOrCNHjmDw4ME9PneCIAgi9VFbWtFWHy5DkRwKsgakTg6MWl0DXiSKGQ5JAfztThlHajtIUh2r4ygnyeHYQUbklWNfc7iUZ1/zccwum9ClY1iFo1QuUwvSP6sQTkmB3wh8p2zVfDjd1oyiJDh82nQV62q+EsamFI3sUzfy1nwj1gtuoyDSiLOhv78yJFTxU0fA64+BFZX32jnjgTGG0qxCHPOEs45rvQ29+u/vVr0wEBZKs+QMOIPNEIgQqS1/d8C9k6/veKMUoaKiAuPGjcPtt9+OcePGhUp2LrvsMixduhS33norysvLoes6tmzZgmnTpkU8Tl5eHpqawm0JFy9ejBUrVuDKK69EYWEh2trasGvXLkyaNAkLFizAqlWrMHv2bNTX1+ONN97ANdfE1u5Gjx6N7OxsvPTSSyGxYNeuXRg4cCDy8qJ3D5k5cyZuuukm7N27FyNHjsRzzz0XWldWVgZVVbFv3z6MGDECr7wSOTeqs9t1hVtvvRUulwsXXXSR7XX6/X588MEHOO+88/D+++9DVVWMHj0anHM8/vjjqK2tRWlpqZBx1FliXcdY1+rGG2/EjTfe2K3XunDhQtx999344Q9/iJycHDz33HNYtGgRgMDvzfHj4adm5513Hn7xi1+guroaZWVlWLlyZWhbKxMnTrQJYNF+n2Nx4YUX4gc/+EHodf/617/Gtdde263XShAEQaQ37sNimZprcAkkOXXCptWaamBCBOFII8dRT2DrqpbkcOwgI/IGCssnWuvhUX1d6vpm7QJWlsLB2EEkJmGQqwQHW06Gxo6465IiHG2o2wWv3hZadkgKzuk/PuHz6C0457Z8I6kX8o2CsOx8sMETwI+Eg8aNPesgz0rd7+ClWQWCcFTjbcDI/N4TuqhMrXPEk3FEdJHly5dj5cqVWL58eWhs3rx5eOSRR7B48WJMmjQJEyZMwFtvvRX1GLfeeiseeuihUDj2DTfcgCVLluDcc89FVVUVpk2bhs8+C1gQH3jgATQ0NGDMmDG4+uqrO8ygAQKun7fffhuvvvoqqqqqMH78eHz/+9+H3++PuV9paSmeffZZXHbZZZgyZYrgFlIUBStWrMCiRYtw1llnhZxKkc7dme26wqBBg3D33XdDUUSN1Ol04vXXX8d9992Hqqoq/OQnP8Hq1avhdDpRVVWF++67D7Nnz8a0adMiBj13RKzrGOtaxcNFF12Eb3/725g1axYmTgzYee+//34AwJVXXokNGzaEwrEnTJiAxx57DIsWLUJVVRW2bt0acjp1lki/zwAwY8YMzJo1Cw0NDSgvL8fNN98MAMjNzcWzzz6LSy+9FCNGjEBTUxPuvPPO+F84QRAEkXZYhaNUKlMDAF9TNSCFS7BlJkNiUlg4onDsbsM5hzsFS9UAINeZjTKLQ2h/F7urpaPjCLCXqx31JD7nqFVrwxe1O4Wxs0rGdEm4S3V4/RHAYxIXlQywgWN79ZzpVq6W6M5qjRbhqICEo4iwVM6nnj59Ot+4caNtfOfOnRg7tnffYARBpCb0/icIgkh/dj79Jj7/l/8OLY+88ULM+e1dSZyRyPHXnkHzN8ItyjPlDFTmlYPXHAROHQPG3gMmOWMcgYiGT/fjye3hqASFybiz6tqUyUr85OQ2fFqzI7Q8On8wrqqc26l9VUPD/9v2v+Cmspc7JnwTmUrq/64cc9fhpX1/Cy3nOrJx27jLE/rv8v7xzfiiLiwcZcpOfG/cYmTKqX/9Oou+8S3on7wYWmaV0+C44r5ePSf3tkD9zU1CrpJyw5OQiit69bzd5WTraTy/5y+h5XynC98fd3mvne/T6u34xBTGfnbJWCwY1HsusFSCMbaJcz69M9uS44ggCIIgCIJIKKncUQ0ANL1ZWA6FG2t+QM4m0SgOIgVjp4poBAAjLCUxB1tOQjP0KFuL1HobBdGowJmTFqIRAJRl9xO6yrWorbYSnt6kxd+KTaf2CGPn9B/fp0QjADAObRaWe6ObmhWWlQtWUSXOI4VdRyWZ+WAIfyY0+T3wabGrX+KBStU6BwlHBEEQBEEQREJxH07djmoAoMqiuOGQ2oNSVT/lG8WJxxKMnWplSGVZhcgxZS75DQ1H3DUx9ghjLamxlr2lMookY5CrWBg7ksBytc9qdkDjYYEux5GFqcUjE3b+RMD9XvATYileb+YbCecZbS1XW5fUznmxUCQZxZlitm6tr/fK1Ug46hwkHBEEQRAEQRAJxe446h9ly8TDdR16pnhDJTiOSDiKC7clGDsnRYKxgzDGMCJfDMk2d1qLRXWrGIzdPzv1g7HNDHaVCMtH3YkRjhrb3Nhav18Ym9N/Qvh910fgx74CdFM35Pz+YAUDEnJuafhZgPl6NpwAP3U4IefuDqUJzDki4ahzkHBEEARBEARBJJSWQ6nrONJPnwYvEl0wDjkoHKkUjB0n1mDsVHMcAcCIvEHC8r6m451yZ6RrMHaQwZaA7CMJEo4+rdkhtEMvcOagqmh4Qs6dSIzDieumZoVl5oANmSTOJ4XL1ewB2Y29ch6DG2gm4ahTkHBEEARBEARBJAzV40XbqabQMlNkZA8sSuKMRNSaGvDibGHMISng3AB0FXB0vdsqEcZaqpZqjiMAGJpbJuT9NKke1PmaYuwB6NxAnU+8uU2nUjUAGOQqDnQPbKfR70aLv7VXz1nva8aO0weFsTllEyGzvnebass3GtL7+UbC+UadIyyncrmaVTiq7SXHkVv1CqJllpwBp+zolXOlO33vHUkQBEEQBEGkLJ7DYpmaa3ApJFmOsnXiUaurgSJRzHBIjoDbCKBStTixOo5yHKknHDkkBUNyxfLJfc3HY+5zytcE3dS1KseRBVcKvrZYOCQFAyzldb2dc/RJ9XYhULwoIw/jC4f06jmTAW88CTSanJaSAjZ4QkLnIA2fAcimcrXGavDag9F3SCKlWaJAX+drgt7JkPquYC1TK8ggt1E0SDgiCIIgCIIgEkZLCucbAUBbYzWghL8iy0wKuB+CXX1IOIoLtzUcW0m9UjUAGBmhXC0WNZZ8o3RzGwWpcInlar2Zc1TrbcDORjFnZ+6AKsH11FcwDollamzQGDBnYoVFluECGyKWx6VquVq2kolcR9j5aXADp3zNMfboHo22MrWcHj9HX6HvvSuTwNChQ7Fjx46EnOv555/Hnj17Ot4wAg8++CDuvPPO0HEKCgowefJkjBs3DldffTVOnz7dwRG6BmMMbrc75jaHDh1CcXFxzG26yocffgjGGO666y5hfP78+Z2akxXzHE+cOIHzzjsvtO7NN9/E2LFjMWXKFOzevRuTJ0+G1+uNdqioxJqX9RxW1qxZY3utVhobG/Ef//EfwtjNN9+MTz75pMtz7Q7Hjx/Heeedh/z8fEyfPt22fuXKlRgxYgSGDx+OH/zgBzAMI8JRCIIgiL6A25JvlDskWr6RBqDnnzB3hN8nfh9SgoGyalA4olK1ePDYMo5S05Uz3CIcHW89ZZu7mWpbvlF6BWMHseYcHfXU9dq5PqneLiyXZhZgTP7gXjtfMrHlGw1JXL6RcN40LlfrjYDsJr94/0X5RtFJ66j6ec+8l5DzfPz9CxJyno7QdR3PP/88iouLMWrUqLiPt3DhQqxevRqGYeDaa6/Fww8/jCeeeKIHZpp8Ro8ejTfffBOPPfYYZFnGgQMH4PF4Ot6xAwYOHIgPPvggtPyb3/wGDz30EK655hoAwJYtW6Lt2m2s5zCjaRoWL16MxYsXxzxGUDi6++67Q2PPPfdcj881Gjk5OXjooYfQ3NyMn/70p8K6gwcP4mc/+xk2b96MoqIiXHTRRXj55ZexdOnShM2PIAiCSBxW4Siy4+gwgAMIfFUdDyBxN+Gq1gwg/KTbIbXnXWh+QHICcmo6ZNIFW8ZRCoZjA0CeMxv9swqFm9X9LSdQ1W9YxO2tN7Xp6jgqd5WAgYXKx075mtCq+ZDdw86wk6312NN0TBibN6AKjLEePU8qwDUV/KhoMmBDE5tvFEQaNh267AjktQFAcy14zX6wshFJmU8s+mcVCiWivSMcUTB2ZyHHUQ8yf/583HXXXZgzZw6GDRuGe+65BwDw6aefYsoUUVWePn06PvroIwDACy+8gLPPPhvTpk3DggULQq6S559/HgsXLsSVV16JCRMm4KmnnsLGjRtx++23Y/LkyVi7di0A4PHHH8dZZ52FqVOn4rLLLkN1deALWVNTE775zW9izJgxmD9/PvbvF9tcBpEkSTjv7t27cdFFF2HGjBmYNGkSVq1aFdqWMYZHH30UM2bMwLBhw/D666+H1v3pT3/CmDFjMHnyZPz85z8PjVtdRdFcRrG2C/587733YsqUKRgzZgw2bdqEW265BVVVVTj77LNDrxsICBXnnHMO3nvvvdA1tgoRGzZswKxZs1BVVYVZs2Zhw4YNoXVPP/00RowYgalTp+K3v/1txDndcccd+OSTT/DjH/845EIyO4diXcdo18pKtHM8+OCDmDFjBn72s5/h+eefxze/+c3QPr/73e8wadIkTJo0CTNmzEBNTQ1uu+02NDY2YvLkyTjnnMCThvnz5+P//u//AAA1NTW48sorUVVVhYkTJ+LFF18MHW/o0KH493//d8yaNQtDhw7F//zP/0Sc68svv4wrr7wytKxpGgYOHIiDBw8iPz8fc+fOhctl/zBevXo1rrjiCpSUlECSJNxyyy147bXXol4TgiAIIr1xH7aWqlkdR34AwdwNDQEBKXFosugqcQYdR5ofcOT3yRvbRKFzA61amzCWqqVqQOTuapHgnNs7qmWnp+MoQ3bYnB5H3T3vOvr45DZheUB2ke169xX4iV2A2a3mKgQrTk6OE8vIBqucKoylarlaIgKySTjqPCQc9TBHjhzBxx9/jM2bN+O5557D3r17MWfOHLjdbmzbFviA3L59OxoaGjBv3jx88skn+OMf/4iPP/4YmzZtwl133YWbbropdLzPP/8cv/zlL7Fjxw7ccccdmD59Op566ils2bIFCxcuxMsvv4z9+/fj888/x5dffomLL74YP/rRjwAADz30EPLy8rBr1y6sXr06JFRZaWtrw5o1azBlyhRomobrr78eTz75JDZs2IBPP/0Ujz32GHbt2hXaPi8vDxs2bMBLL72E22+/HUBAeLjlllvw1ltvYcuWLcjIyOjxa1tfX485c+Zg8+bN+M53voPzzz8ft912G7Zt24Zp06bZBI3ly5fjhRdeAOccr776Kq6//vrQOr/fj6uvvhoPP/wwtm3bhp///Oe4+uqr4ff7sW3bNjzyyCP47LPP8OWXX6K+vj7ifJ588snQv4fZhQQg5nXsyrWKdo6srCxs2LDBJjp9+OGHePTRR/Hee+9h69at+OCDD5Cfn4+nn34aBQUF2LJlC9atW2c7z+23344JEyZg27Zt+Otf/4p77rlHKL9sbW3F+vXr8eGHH+Kee+6JWFZ31VVX4ZNPPsGpU6cAAO+++y7GjBmDysrKqK8PCLxnhgwJ//GsqKjA0aNHY+5DEARBpC92x5FVOHID4JblxJQwc1WFLjZUg8MiHBHdp9VS6pWtZKZ0ns2IfFHIONhyElqEgN7TbS1QDS20nCk7kefItm2XLgzOKRGWj/RwztFRdy0OtJwUxs7to24jIFKZ2uSkvtZ0KVcrjVCq1tV5cotQbcUWjk0ZR1FJ3U/qNOWaa66BJEnIz8/H2LFjQy6fZcuW4fnnnwcQcBItW7YMjDG8/fbb2Lp1K84++2xMnjwZ99xzj3DTPGfOHAwfPjzq+dasWYO1a9di6tSpmDx5Mp5++mkcOnQIAPDBBx/gO9/5DgCguLgYV111lbDv2rVrMXnyZJx99tkYPnw47r33XuzZswc7d+7Eddddh8mTJ2Pu3Lloa2vDzp07Q/tdd911AICZM2fixIkT8Pl8+OKLLzB16lSMHj0aAHDrrbfGdyEjkJOTg0suuQQAMHXqVJSXl2Py5IDNc9q0adi3b5+w/fz587Ft2za8+eabmDBhAoqKwq1+d+/eDafTifPPPx9AoGzP6XRi9+7d+PDDD3HJJZegf//+3X4tsa5jT1yrZcuWRRz/85//jKVLl6KsLPAlPCcnB5mZHT/JW7t2Lb773e8CAAYMGICLL75YEKqC/+ZDhw5FYWEhjh07ZjtGdnY2rrjiCrzyyisAAr/ny5cv79LrIgiCIPo+NsfREGupmrX9NwfQ9fzA7qDV1oEXi5k7iiAcUb5RPFiDsXNS2G0EAAOy+gmOKL+h4WiELmM1Xmswdr+0FkEqcsT3ZKTX3F045za30WBXCYbmRMs6S3/4oc3CMhuanHyjIFLlNEBxhgdaToFX703ehKJQ4HQhI1gqDKDNUG1CTzS4pwHqq/dC/e/rob72Exi1dueqwQ00W46X5yDHUTTSOuMoVbKHzJhv0mVZhqYFnj4sXboUM2fOxKOPPoo//OEPWL9+PYDAh+dNN92Ehx56KOLxcnJiq56cc9x///2CS6mzBDOOrMcrLi6OmdUTfI1ye+vc4GuMhqIoQtixzxc5WLCj7czOHFmWo17rIIwxXHvttbjllluEMrFEEOs6rlmzJup+q1atwooVKwAAd911F5YsWRJxu45+L3qaSNf6vffew49//GMAwJIlS3DXXXdh+fLl+OEPf4glS5bgo48+wksvvdThsSsqKnD4cLijxpEjRzB4cN8MRiQIgjjT0Vp98NU2hpaZLCF7kLV83SocAYAHQO9/oVdrqoEi0SniDN64qH4gixxH8eC2BWOntnDEGMOIvEHYejoc97C36TgqcwcI29mCsbPTM98oSLlLdBzVeBvg0/zINIsN3eSwuwZHLELUvAGT0lpoiwV3nwY/Ze4cxyBVVCVtPgDAnFlgldPA964PjRl7PoM0IJ0FXA8AACAASURBVP4M3Z6EMYbSrAIhoL3W24CCjA7uj7U2aGseD4lh/MQuaK/8GNKUSyDP+laom51b9cIwuVuzlQw45bSWR3oVchwliIqKCowbNw633347xo0bFyrNueyyy/Diiy+GHBy6rmPTpk1Rj5OXl4empqbQ8uLFi/HMM8+goSHwB6utrQ1bt24FACxYsCAkmNTX1+ONN97ocJ6jR49Gdna2cMO/a9cuNDfHbn84c+ZMbN68GXv3Bt6g5uDlsrIyqKoacgQFHSlWOrtdV7j11ltx991346KLLhLGR48eDb/fH3LVvP/++1BVFaNHj8b8+fPxzjvvoLY28EfNnHHUWWJdx1jX6sYbb8SWLVuwZcuWqKJRLC655BK8+OKLqKkJPM11u93w+XzIy8tDa2trVJFv4cKFWLlyJQCguroa77zzDhYsWBDzXBdccEForsGubnPmzEFzczPuvfdeXHHFFcjO7timffXVV+PNN99EXV0dDMPAypUrce2113blZRMEQRBpgtVt5BpcCkmRLVtFeqIcf4OLzqDWVNscR0KpmpOEo3iwBWMrqdlRzYy1XG1f83FbuUxNq9VxlN7CUbaSgZJM8Xf9WA90V+Oc46OTW4WxobllqLB0cutLWMvUWNkIsKzcJM0mjByxXC31uhpby9WsIq0Vzjn0vz5td1BxA8aXb0N98V9hHAjca1O+Udcg4SiBLF++HCtXrhTKd+bNm4dHHnkEixcvxqRJkzBhwgS89dZbUY9x66234qGHHgqFY99www1YsmQJzj33XFRVVWHatGn47LNAwNkDDzyAhoYGjBkzBldffTXmzZvX4RwVRcHbb7+NV199FVVVVRg/fjy+//3vw+/3x9yvtLQUzz77LC677DJMmTJFcAspioIVK1Zg0aJFOOuss0JOpUjn7sx2XWHQoEG4++67oSiieux0OvH666/jvvvuQ1VVFX7yk59g9erVcDqdqKqqwn333YfZs2dj2rRpKCjoui091nWMda3iZf78+bj33nuxcOFCTJo0CQsWLEBTUxP69euHJUuWYOLEiaFwbDNPPfUUtm7diqqqKixatAiPPfYYxo8f3605LFu2zPZ7rus6ysvLcc0112Dbtm0oLy/Hgw8+CAAYNmwYHnjgAcycORMjR47EsGHD8O1vf7tb5yYIgiBSm851VIvkOIo01vP4T9cAGeHvDBJYOINHVynjKE7cqigcuRypLxwNzSmDbMphavJ7cMoXfojLObc7jrLSMxjbzGCLmGN1CXWH/c0ncKJVzA6dV5Zc901vww9ZhKMkl6kFYZXTAMWUs+o+DX5iT/ImFIWuBmQbn/8vjN0xwr5bTkF761Go//dLNDSLDzLyKd8oJiwVg7CCTJ8+nW/cuNE2vnPnTowdOzYJMyIIItnQ+58gCCJ92fWrNVh/24rQ8ojlF2Du7+42baEC+DTCni4AZ/Xy7IDjv/0lmq8KPzDKkJwYlj8YXFOB3Z8Do+4AcyTfLZCuvHdsA748FXYCLBw0FTNKxiRxRp3jjwc+xP7mE6Hl+QMmYVb/wAO2Jr8Hz3wdfujrlBT828Rr0r70amfDYbx5OHwDPjC7CMtGdT8mhHOOVXv+InSfG5E3CNcMOzeueaYy3NCh/vomoC3cVEa57hcpUxKm/fkJoaOaNPliKOd9J4kzslPdehqr9vwltJznyMZt46+IuK2++zPo7zwhDhYMAAwNaLY75tYPr8K6ipGh5bNLx2LBwNQQ9hIFY2wT53x6Z7YlxxFBEARBEASREFqsjqMh1kDcaM6iViSis5qqtwjLDtlUpsZkQKEn0vFgcxylQakaAFub+L3Nx0M/W8vUSrMK0140AuyOo+rW0/DrarePt7vpqCAaAWeA26hmvyAaISMHrH/0pkeJxtZdbe968AhdA5NJcWY+JITfT81qK7wROqUZJ/dAf0/ssI2sPDiuvB+Opf8FadpiwNLBscnhEJapVC02JBwRBEEQBEEQCcHeUc2abRJNOEpMZzVVFm9IhHwjR16fEASSiccSjp2TBqVqADAib6CwfNxzCq1a4LXYy9TSO98oSI4jC4UZYXedAY7jrae6dSyDG/ikerswNqagIu1DxDvCWqYmDakCk+KP4ugpWOUUwBxQ72kAP7EreROKgCLJKLbkbdV6G4Vl3lwHbc3jgG6KVpEUKJfeBVZQBubIhDJvGZTrHxeEu+ZMMYs1d+8/wFW7KEUEIOGIIAiCIAiCSAgeq3A01Oo4ihWC3bs5R4bXCyNH/GocEo5UP+Ub9QDWcGxzq/tUJs/psglCwdK1Gm/fCsY2U+Gy5By5u5dz9HXDYSEXioFhbtnEuOaWDhiHNwvLqZJvFIQpGZCGzxDGjD3rkjSb6FgDss3ONe73QlvzGNAqiknywn+GVD5OGJNKh0G57heQ598EODLRnCk6jHK3rYX60h22QHMiQNoKR6mczUQQRO9A73uCIIj0xh6O3dlSNaC3O6tpNbVAsfgE2iG1lzJofsDR9WYZRBjOOdxp6jgCgOEW19G+pkC5ms1xlJ3+wdhBrOVqR91d76xmcAOfWtxG4wuH2FwkfQ3ubQGv3ieMSUMmJWk20bGXq32ecuVqVtE2KBxxQ4f27grwukPCemn6FZDHnxfxWEySIU+5BPLSJ9FicRzl+VqBphpof/o5tHf/C9zTGPEYZyppKRxlZmaivr6ebiIJ4gyCc476+npkZqbH00mCIIh4UT1eNHx1CHpb7M6m6YLmbYO3JnyTzWQJrvISy1bJE47UmhrwYlHIsJaqEd2nzVCh8fANqUOS4ZSUGHukFiMtOUcHWk6i2d8q5DbJTOpTgkiFRTg60XoKWhdFhe2nD6LBH875YWCYcya4jY5sA0zt7VlxBVhOURJnFBk2ZDLgNAkorY3gx79O3oQiEK2zmv7p78EPbBDWseFnQZ6zpMNjujNdMEylx1l+Hxym321j1ydQX/gh9O1rwXnv5+ulA+nzaW2ivLwcx44dQ11d11VvgiDSl8zMTJSXlyd7GgRBEL2O53gd3pn7r3Afqkb+2Apcuu6/4cxP72Bma76Rq7wEkmLO+9ARO8eot4Wjk+DTrI4jk3CUSY6jePBECMZOp8yoAdlFcCmZ8LRnG/kNDRvrdgvblGQWQGZp+Vw+IvlOF/IdLjSpgfeezg2caK23CUrR0Azd5jaaVDRMyE7qq3BLuRMbklplakGY4oQ0fAaMnR+Fxozd6yANTh1xrzRL/Ow95WtC246/g216SxhnpZVQLvohWCfeg01+8e9JvuQEwBDI02unzQ197a9g7PwQyvn/DFZ0Zt+DpKVw5HA4UFlZmexpEARBEARB9Ap7V70XKutq2nkEB155H2O+tzjJs4qPjsvUrKKRAkCzrDfQW4Z5f30tkB3ussPAILN2YUujjKN4SecyNQBgjGF43kBsO30gNPZl/R5hm7I+GPY8OKcETQ3hm+yj7tpOC0dbT+9Hsxp2EcpMwjn9J/T4HFMNzjmMQ2K+kZRi+UZmpFGzReFo3+fgC25OmSDvLCUDeY7s0O+SAY7aL15Df/NGrkIoi+8Fc3SuMqHJ5IIDgILioVD+6THoa39lK33jx3dCfflHkGZcAfmsq8EUZxyvJn3pO5I4QRAEQRBEH6Fp9xFhueGrQ8mZSA/iPmQNxu5v2cLqKMoDYP6CbgDwobfw++qFZYekhB0xmh9wknAUD+kajG3GWq6mWsq2+mf1nXyjINaco84GZKuGhnXVXwljk4tGnBEtz/mpw4DHlH2lZIANHJO8CXUAG1IFZJjclt5m8KM7kjehCFjfW7XZJgeu7ISy+B6w3M6XAtocR04XpLIRUK7/D8hzlwJKhriDocH4YjXUl38E46joojtTIOGIIAiCIAgixWg9Jra9bjl4Mkkz6TmspWo5FVbhyJpvlN3+n5neK1dT9RZh2WHO31H9gELCUTxYHUeuTjoDUomhuWUxS9GsWSx9Aau76HhrHfROZL5sPrUXbpNYqDAZ5/Qf3+PzS0VsZWqDJ4ApjihbJx8mOyANP1sYS7Xuav0t5Wp1ueFl5cJ/gVQ2okvHiyQcAe3h2dMvh2PpfyFiF7yGE9BWPwjtvf8G9zZ36ZzpDglHBEEQBEEQKYbnqPhU332gLwhHXe2o5mr/z0zvCEecc2hymzAWFI64rgGyK2XKNtIVq+MoR0mvUjUAcMoODMmxCp4BGJgti6UvUOjMFdxhqqGjuvV0zH38uop1NWLA8rSSUWlXnthdjEOicJTKZWpBpNGzhWVj3+eBz74UodQSOl+bE1iWZ11n6wzXGaIJR0FYfimUK34C+eJ/A7Lt72vj6w+hPn87eGO1bV1fhYQjgiAIgiCIFIIbBjw2x1E1uJHenV3sGUcdlaplwy4cxeq61n2MlhbwfDH60yG1OwQo36hHcFvCsdNVRBiRPyjieFFmnuhS6yMwxmyuo6Oe2OVqG0/tgVcPC7FOScHM0rG9Mr9Ug/u94Md3CmPS0MlJmk3nYYMnAhmm8i+fGzyFSrKKt/5NWK7LKQAbMxfS2d/s1vFsGUdOe/MJxhjk0bPhWPYUpKpv2NcXVwD5kYXkvggJRwRBEARBECmE71QTDL8qjBl+Fa0n6qPskR7YM47MjiMOezh24krVtJoa8KIYHdVIOIobt5b+pWoAMCIvsnDUF8vUgnQl58in+fFFreg2mlEyBtlpmGnVHfjRHYBhcurkl4EVDEjehDoJkxVII1OzXE3f8g5yNr+LDNUfGvMrDrjPXdqtzowGN9DsFx9C5DmiZ2+xTBeU878L5VuPgBUNDgzKCpTzv5tWnSHjhYQjgiAIgiCIFMJztC7ieEsal6tp3jZ4q8PlLUyS4CovMW0R7JgWxIFAMHYkxxFHT6NW14CXWIQj2SQcUTB23HisXdXSsFQNCJS0lGbaS1fK+mAwdpAKlygcHXPXwYiSc/SPul3w6WHhO1N24KyS1A2G7mmMw9YytdR3GwWRRlnL1b4A19UoWycG49Bm6B+uAgNQ4m4S1tVYXEOdpUX1wjD9HclWMuCUO3YLSgPHQFnyn5BnL4E86zqwfpFF5L4KCUcEQRAEQRAphDXfKEjLgRMJnknP4Tkivqbs8mJIDvMX9UjB2EBAPDKHyvZOZzWtpga8WBQyQqVqqh9w9L3smkRjLVVLV8cRELlcrS87jooz85Ephzscthkq6rxNtu1aNR821O0Sxs4uHYfMM6h9eTrmGwVhgycAmbnhgTYP+JFtSZsPrz8K7c9PAO0iZam7UVhf422ItFuHdJRvFAsmOyCfdRXkGVd269zpDAlHBEEQBEEQKYQ13yhIOjuO7PlG1mBsawma+Yt875er+etrgFyx/bLC2sOwNZVK1eJE54aQeQNACFxONyKVq/Vl4YgxhsEW19ERT41tu89rd8JvKtPKkjMwvXhUr88vVeCNJ4Em02edpICVp08nOSbJkEbOFMaM3ckpV+PeZqhv/QIwlZSVWhxHtd0WjkSnUn6EfCPCDglHBEEQBEEQKURUx9HBNBaODlvyjYZYA0WjOY6ARHRW83vF/CiHpISzKyjjKG6sZWrZSiakGG3tU52B2UUoMXV5GppT1uddNbaAbLdYUutWvdhUt0cYO6f/ODjl1G1D39NY3UZs0FgwZ3qVZNrK1fb/A1xLbLka11Rob/8H0CT+3SgbNVdYrvGKDqTOEo/j6EwmfT+xCYIgCIIg+iCtx6M4jvanb6mazXEUl3DU853VVF18Ai10x9KoVC1ePJqlo1oau42AgAPnyqFzMLGwElX9huGiwWcle0q9TqSAbM7DOTHrar6CxvXQco6ShSnFIxM2v1TAOLRZWE6nfKMgrHwckG0Syv2t4Jbcpt6Ecw7977+2d6absBAlky8TBOcWtRWtWtdLl63CUaSOaoQdEo4IgiAIgiBSiOgZR33IcWTrqBarVK13HUfcMKApYhlVKN8IAAwGJvdtN0lv41atHdXSy4URiaLMfFw6ZBYuqZiJgoy+f+PZP6sATpOg6tXbUN/WDCBwI76lfp+w/Tll40UBto/DNTXQUc0EG5KGwpEkQxo5SxhLZHc1Y+ObML7+UJxT+XjIC26GIiuC0w8AarvhOiLHUfcg4YggCIIgCCKFiNZVzVfbCNVtbVmfHrTEzDjyA9BNyzIAc95QpIyjnuuspjc0gBeK+UbCDS9Lf5Ej2fQ1x9GZiMQklLtKhLEj7oDIva5mB3RTl7V8hwuT+g1P6PySDT+xE9BMArSrEKx4SPImFAfSqHOE5UC5WluUrXsOY98X0D/9vThYUAblsrvA2kseSy1ZYt0JyLZnHJFw1BlIOCIIgiAIgkgRuGFELVUD0jfnyH1IdBzlDjWXqkUqU2OmZScAs3OhZzurRe6oFjgfNwxAzo20G9EFrB3VcvqA4+hMxJ5zVIvTbS3YWn9AGJ9dNgGKJCdyakknUje1UE5amsEGjgGyTeW5qg/8UO+Wqxm1B6C9uwLCQ4EMFxyX3wdm6vTWP0ssG+6qcGRwA81+8W8OCUedg4QjgiAIgiCIFMFb2whD1aKuT8dyNc3nh/dkOHyaSRKyy83OBWvpmdVhxNCbOUdqdQ14sXjOkOOI8o16BHupGjmO0hFbzpGnFp9Wbwc33ewXOnMwsV9loqeWdPhhMd8oHcvUgjBJtruO9nzWa+fjDSegvfWY6NhiEpRLfgTWT+xgaO1e2FXhqEX1wjD9vmYrmWdUSWU8kHBEEARBEASRIkTLNwriTkPhyHNEdBtlDSyC7DR3WooVjB2k93KOtJoa8BKrcNQ+P+qo1iPYS9XIcZSODMjqB4WFnURu1YuvGg4J28wdUJXWHfO6A3fXg586Eh5gEqQhVcmbUA9gE44ObAJXe7ZcjXMOfcffob58J+AWO1vK590Macgk2z6lmaJwVO9rhmpEf9hihfKNus+Z9a4mCIIgCIJIYVqPRS9TA9LTcWQNxhbL1IDYwdhBIuUc9Qz+uhqgQHTACI4jJwlH8WItVesL4dhnIrIkY5CrOOr64sx8jC2oSOCMUgPj8FZhmZWNEMqr0hE2cDSQ0y88oPrAD37ZY8fnPjf0P/8/6H97RnQaAZCmXAJ50gUR98tUnCgwiT0cHKd8TZ0+rzXfqICEo05DwhFBEARBEESKYHUcZfQTbz7SMePImm8kBmMDyXYc+b3ik26FyeFsEnIc9QgeS8tsF4Vjpy3WcjUz88rOPLcRAPBDYpmaNGRKkmbSczAmQRrZO+VqxrGvob78Ixh719vWSWPmQp63LOb+8QRkk+Oo+5x572yCIAiCIIgUxdpRrf88sdwhLR1H1o5qQ8zCkYZAV7UgDEAkN0qkjKOe6aymGeIT6FCZGkAZRz0A5xweS8YRhWOnLxWuyMJR/6xCjMovT/Bskg83dBiHtwljbGj65huZsZWrHdwE7u9+Z0+ua9DW/QHa6p8CLRZ3rZIBeeH3IF/4Q7AOgtXjyTmyC0c5nd73TIeEI4IgCIIgiBTBc1wUjsrmiRkP7oMnA52+0ghrqVqOUKpmdQ5lIfLXU2tnNR1A/HkbXNOgOVRhzCGbzqPpgEwiRzy06So0roeWHZIMJ4XRpi0DXUURXUXzBlSlbRexeODV+4A2k/ickQPWf3jyJtSDsAGjgFxTaaLmh9HNcjXeWA3tfx+A8cVqgIt/w1hpJRxL/hPyxIWd+h2yCke13sZOz4McR92HhCOCIAiCIIgUweo4KpxYCWdhuFxNb1PRerLeultKY3McCaVq1jK1aF/iGXoj50irOwVeFCXfCADgPCNvhnsSazC2S8mia5rGOCQFA7OLhLFB2cUYnjswSTPqGK5r4LxnHIpWjMNim3ppyKQOHTPpAmOsR7qr6Ts/hvr7O8FP7rGtk6YthvKtX9i6p8XCWqpW623o9L+vNeOIhKPOQ3I/QRAEQRBEimDNOHKVlyB32ADUb2oJjbUcOAnXoBLrrilLbOHIKv5EyjcK4gLQbNm3KMq2nUOrqQEvitJRDQAYZfHEi5vK1PocYwsqcMwTFrnPHTAppcRAzg3wmv0wDmwEP7AJvO4gWL9ySDOvgTTqHLAezGGy5hv1lTK1INKoc2BsWhNa5gc3g/u9YM6O38e8rRX6+yth7PrYvjK7AMqFt0fsnNYReY5sZMpO+PRAmbPf0NDgd6NfRuxAcoMbaPaLDytIOOo8JBwRBEEQBEGkAIauo/WE6CZyDQ4KR+EntS0HTqJsbnq0etbb/Gg9eTo8wBhcg82iV2eCsaOts+7bddSaavAS8QZIcBwxuqmIF7fNcURiXLoztXgkvFobTrTWY2K/SgzJtXZKTDzc7wU/sg3GgY2BcqpWsXyJnz4G/Z0nYWx4E/LsfwIbOjVusYt7WwKlaiakIX1LOGL9RwB5pUBz+0MN3Q/jwEbIY+bG3M84uQfaO0+G9zMfs3IalG/cBpbdvcYDjDH0zyrEYXe4DLrG29ChcNSiemGYsvGylUyLw5SIBV0pgiAIgiCIFMBX0wCuhbNgMvrlQsnORG7lAGE7dxoFZHuO1gGmEoLsgUWQnSZHT6dL1SKt64FSteoa8GFWx1Hg6zHnHJDz4j7HmQ4FY/c9JCZh7oDki9e8uRbGgU0BZ9GxHYCudbxP3UFobz4KNnAs5NnXQyof1+3zG0e2wRzSz4qHgJlb2PcBguVqxsY3Q2PGns+iCkfc0GFseAP6+tdsWUaQHZDnLYM06cK4RbvSrAJBOKr1NmBsQUXMfahMLT5IOCIIgiAIgkgBPMfELjPZ5QFnTu4wUThqOZg+wlHsMjUdgLVDT0elamY8CNy0df8GxF9bAxSKT71DT6A1FXCkT0lgqkKOI6Kn4IYOXr0PxsGN4Ac2gp860v1jndgJ7X8fABs6JSAglQ7r+jFsZWpTuj2fVMYqHPFDm8HbPGAZ4mcyb66D9penwI9/bTsGK6qAfPEdkIpjizudpX+WKNB1prNaoyUYu4CEoy5BwhFBEARBEEQKYMs3GhxFOEojx5FdODKXtFhFowwAsUJlg+uDriwdgL99vHuo3tOAHA5alZkc7hil+QFn90opiDAeVfx3JscR0RW43wt+eEu4BM3b3PFOAODMAhsyGdKw6WD9ymF8uQbGbnuwMz+0GdqhzZBGzYZ8znVghZ0L+eacw7AIR32tTC0IKx0G5JcBTe2f57oGY/8GyOPmh7Yx9qyHtvZXQJvdCSpNvhjy3G+DKd3/rLZi7azWGeGIOqrFBwlHBEEQBEEQKYC1o5qrvBRAegtHLYdqhOWcIbGCsTv6Eh/srNZiGvMgLuHIEEsXhLwLzQ9kFnT72EQAazi2y0GOIyI2vKnGVIL2FWB0XIIGAMjvD2nYdEiV08DKx4HJ4bJY6eJ/gzH9SujrXgGP0FLe2PMZjL3rIY1fAHnmNWDmNvSR5njqsJij5MgEGzimc/NMM0Llahv+FBoz9qyDPG4+uN8L/aNVMHb83b5jVh6Ub9wGadj0Hp9TUWYeZCZBby+Hc6teeFRfzM8Xu3CU0+Pz6suQcEQQBEEQBJECeI5ZhaOA48g1uBRMlsD1wBdkb/VpqB4vHK7Ud264D4uOo1zBcRQ7GFs1PJCZExIzZyK5YBeOupcpYrS1Qc/QhTFROFIBhW4s4sVjKVXLUVL/95ZIPJxzGF+9D+PLt8Hrj3ZuJyaBDRwNqXJ6QJzoNyhmdo5UWgnpip/AOL4T+me/Bz++0zIJA8aOtTB2fgRp0gWQZ1wVNcCZH9oiTmXwBDDFEXHbvoBVOOKHt8I4sg3a+yuBhhO27dmQSVC+8S9gOYW2dT2BzCSUZOaj2uQ0qvU1oNIxIOo+lHEUHyQcEQRBEARBpAA24ai9VE1yKHBV9IfblG3kPliNwgmVCZ1fd3BbHEeuIbGEo/CX+FrvBtT7tgJgKM9ZhFxHhW2bAN0PyNZqasGLY3RUM1iPtu0+U7E6jqhUjYiEvv5VGF+s7njDjGxIQ6aADZsOaegUsKzYnbQiIQ0aC3bNz8EPbYb+2SvgdQctk1FhfPl/MLavhTRtMeSpl4FliMK2cdhSptZH842CsJKhQOHAsEhkaNBe/5l9Q0mBPGcJpKmX9vrnZ2lWoSAc1XgbUJkbSziiUrV4IOGIIAiCIAgiBbBmHAXDsYFAuZpZOGo5cDI9hKPDllK1obFK1QI3ZprRinrftvYxjpOej+HKvw4SU2AXjqziU+fRamrAi60d1UyOAU5fk+NFN3R49TZhLLsHc06IvoG+5Z3YolFBWaAEbdh0sIFjweT435uMMbDKqWBDJ8PYsx76+lftzhnVB+PzP8LY8i7ks66CNOkCMCUjkLt0fJewaV/NNwoSKleL9e9UOBDKxXd0K2i8O9hyjlqj5xwZ3ECzX/x7QcJR16C/iARBEARBEClAq6WrWtBxBAC5lQNgTjZKh5wj3a+i9bj4mnIqStt/4ojWUc2n18Pc4lrnPjS27UK/zAmwd13rfmc1NaJwZPpqzCiLJ148migauZTMcPg4QQDQd38G/YPfiYNMAhs0NpxX1G9Qr52fMQny6NmQRs6E8dUH0D//I+CuFzfytUD/+AXoX74Neea1YFl5Yu5SQRlYQRn6OtKo2VGFI2niIsjnLgdLYIZZVwKyW1QvuOnvSraSKX7eEx1CV4sgCIIgCCLJGLqO1hMW4cjiODKTDsKR52gdwE1f1AcWQc5wti/5ABimrR0AAuvadPuX/3rfNhRkjIXEMiF2VtPQ3c5qWnU1+AhLqZrZycCsIhXRVdyWfCOXQmIcEcY4sg36X56CWSiGkgHlmw9CGjAqoXNhkgx54kJIY+fB2PYe9H/8yd7BzX0a+tpfA5LY/bGvl6kFYUWDwfqVg58+Fh7MyIGy6HuQRs5M+HxKLcLR6bYWqIYWURCifKP4IcmfIAiCIAgiyXhPng6FXwNARlEelKywGGITjg7aw0hTDfchMRi7M2VqQGThSOOtaPLvRbizmpnulav5a2qAftaMI1Opmtz17BRCxKNazsouxwAAIABJREFUhCPKNyLaMWr2Q1vzuOjckWQol96ZcNHIDFOckKdeBseNT0Oe+S3AGeF31hBD9ft6mVoQxhjk824G2rvVsYoqOG54IimiEQBkyA4UmDqjcXDUeRsjbttoyTcqIOGoy5DjiCAIgiAIIsnYg7FLheXc4QOF5XRwHMUWjqIHY7fppyMer963FQXOUWAsUme1rnfuUVvrAUf4OktMgmwuo5J7pxvQmYRHo2Bswg5vOAHtjYcBS3C6/I3bIFVOTdKsRFhGNuRZ10KafCH0DW/A2PIuoKv2DWUFrHx84ieYJKSKiXD88yrA2wKWX9rxDr1M/6xCNJrcRDXeRgx0Fdu2o2Ds+CHHEUEQBEEQRJLxHI3cUS2I1XHkPlgNbhhIZawd1XKEjmqRHUecG2jTIz8xVo0WNPv3I3LOUddRdbF0wVzewHUVcJBwFC9ui+Moh0rVzni4uwHqn35uKwOT5y2DPPbcJM0qOiwrD8q8ZXDc+DSkCQsBS0YXGzwRLJIrqQ/DnFkpIRoBnc85sgtHORG3I6JDwhFBEARBEESSsXZUcw0ShaOMwlw4C8JfdHWfH97qyM6cVMF92OI4EoQjq+MoIAapRgs4dETjlG8LOI9fONLdbhg5YqC2kIuhqoAjr8vHJUTcFkcJlaqd2fA2D7Q3Hwaaxc87adrlkKctTtKsOgfLLYKy6HtwLFsBafQcwJEJ9CuHMm95sqd2RmPNOaqNKhxRxlG8UKkaQRAEQRBEkmm1laqV2LbJHTYA9V/uDS23HDiJ7IF2S36q4D5scRyFStU4opWq+SxlauykG7w0G5ADzzr9RhNa1NPIc5q36nrGkRaxo5op30jXwajjTtx4NHIcEQG45oe25nHwukPCuDRuPuS5NyRnUt2Atbec54Ye6P7Gut7Rkeg5rI6jWl8jDG7YujdSqVr8kOOIIAiCIAgiydgzjiILR2ZSPefIVqo2NOg48iPQDS2IjGBXtDZNFI6k7bWQ1h0Xxk75vgLn5ps1tf2YnUerrgEvtgZjm4Si1K4CTBuspWrkODoz4YYO7d3/Aj/2lTDOKqdBXvi9tBRfmCSn5bz7GrmOLGTJ4UYSqqGhoU10FxncQLNffMBAwlHXIeGIIAiCIAgiyVgzjrLL7cJRTmX6CEe6X0Xr8VPCmKsiKBxFKlML3IC1NhwW1hRNLUD21uOAEW7X3aafhluzlrN1rVxNjeg4MglHnNxGPYE9HJscR2canHPo768E3/eFMM4GjIZyyY/AZHqvEd2HMYb+WQXCmDXnqEX1giP8N8SlZIqf90SnIOGIIAiCIAgiyXTUVQ2I4Dg6mLrCUeuxOiG8O2tAEZTMYH1Z5GBsAGjT6oU1mVxH/3NLIW04IYzXe0+Bc24a6Vq5mlZTA8QqVUMGiPjgnNsdRwo5js409PWvwdj+N3GwXzmUK+4Fc9D7jIifjnKOKN+oZyDhiCAIgiAIIokYmg7vSbFEK3uQPbsonUrVWqKWqQHRgrF1Q4Nu+T6foWlwFGUi64tjwrhXb0Gr4GbpmuPI31GpmkQCR7y06Sp0HhYPHZKCDNkRYw+ir6Fv/QuML/5XHMwpguOqB8Ayc5MzKaLP0VFntUbKN+oRSDgiCIIgCIJIIq0n6wV3TmZJgcmdEyZ32EBhOZWFI/chsaNabigYG7CLPIEv8Z5D2wE5nBmiaBrkdldRXn8HpM3iMU/5zDcHXROONE89kBEWihgYZHOYqkQ3tfHi1qxuIypTO5Mw9qyD/v5z4mBmDhxX/TtYbuqG+hPpR0fCkT0YOwdE1yHhiCAIgiAIIolY840iBWMDQE5FKZgU/urmPVkPrdUXcdtk47F0VAvnGwHRHEctB7YIoxlaOEA7Z0YZ5DV7hPWtmtfkOuq8cMQ5h6qLpQsOZgm6lQtAxIdHteYbkYvrTME4sh3aX1YAplwZKE4ol98HVlSetHkRfZOizDxB+PdoPqFMljqq9QwkHBEEQRAEQSQRz9FaYTmacCQ5FLgqxOwjq7MnVWg5LM4rXKqmQeyAxgAEBAVvs1iOlqGqoZ/lTCAnOwvsK1Fkqw+5jjrfWU1vbISRLwajOqwlVEq/Th2LiI7VcZRDjqMzAqPmALQ1jwG6qXMik6BceiekgaOTNzGizyIxCSWZothvzjmijKOegYQjgiAIgiCIJNJqCcbOHhRZOALsOUfN+09E2TK5uC0ZR+FSNaszKAuABP+Ro9DzxTVmxxEAFH1rFpS3RNeRW22FT2trX+pcQLYWqaOaSTjihg4mZ1t3I7qIxxqMnUTHETd0GCd2gzemptDaV+CNJ6G9+TBgcZvJ37gNUuW0JM2KOBOwl6s1hn62Oo4KSDjqFiQcEQRBEARBJJHOlqoBQG5legRkW51QOSHhKHKZmmfdOhjlecIas+MIAJSMU8gbMhVsrxgkHs466ly5mlZt76jmNAdj6zoCTigiHtxaapSqcUOHtuZxaK/dB3XVbdB3fpyUefR1uKcB6p9+DrQ2CePy3KWQx81PzqSIMwa7cBT4O2FwA81+8e9OHglH3YKEI4IgCIJIYY64a/HOkS+wpX6fpf040VfwWBxHrvLOO45SUTgyVA2tx04JY+ESu8jB2O6NXwClpi/znMNpcRxBb0XxP50P50diaV+L6kGb7o9w7MioNTXgJdaOaqZSNb1ThyE6wG11HCWpVM3Y/jfwg5tCy/oHvwVv61qYOhEb3uaB9sbDQJPoNJSmLYY8/fIkzYo4k4jmOGpRveCmrC2Xkil20CQ6DQlHBEEQBJGiNPs9eG3/B9h6ej/ePfoP7Gg4lOwpEb2ATTgaXBplSyDHIhy5D6aecOQ5Vid0icsq6wclK6N9ye44Umtq0NYmXgOnrkNiMpA3Xhhnvv0ou2g52CHR1RDIOupCqVqRpVTNfCPB5U4dh4hNKoRjc3cD9E9/Lw62uWF8+eeEz6WvwjU/tDWPg9cdEsalsedCnntDciZFnHGUZIkZR6fbmuHXNco36kFIOCIIgiCIFGV30zFoPGx/2H76QBJnQ/QWXSpVSwPHka1MbUisjmoutK7/HNxSpuZUVSBrEFA4Sdy8eSeyxo1FbrUYiNTkd8PnE69jNLTqCBlHwhNoZ6eOQwDc2wLj8FZwd4NtnTUcOxmOI/2jVYDfLijqX74N7m1J+Hz6GtzQob37X+DHvhLG2dApkBd9H4zRrSaRGDJkBwozcoWxOl8jGqmjWo9B72aCIAiCSFHqTOGOAHDccwo6N6JsTaQjul+Ft1rM7MkeWBR1+0jCUaqVMLoPi+Uq4Y5qOgCvZetseNatBx9syTfSNMA1FHBVArLJqWL4AM9+lC24AaxWdLQ06A3Qm+o7nJ+/6RTgCpemMQ7IzOQyYtQ2vjPwxmqov78T2p8egvrSHTCq9wnrk+04Mg5thrHns8gr/a3QN76Z0Pn0NTjn0N9/DnzfF8I4GzAKyqV3gslUDkQkFnu5WoMtGDvfmZPIKfUp4hKOGGMFjLHVjLFdjLGdjLFZjLF+jLG/Mcb2tv+/sH1bxhh7ijG2jzG2jTE2tWdeAkEQBEH0Tep8onCkcR01raejbE2kI96TpwGT8JPVvxByRnTHS0a/PDjywk9MdZ/fJjwlG2tHtZwhwWBsq2iUAa2hGb6vv4ZRLj4pzlBVwDUUjMlA3jhxt6avIGdkoDhrsjjsb8bp9/4QU0jjug7VkoWkMAmMmcKwJXoi3RGcG9D+9gzQ0p5l5WuB9s6T4P7Av7Fu6PDqbaHtGRiylYxIh+qd+Wlt0N5fKQ4q4vvK2PIOuMfulCI6h/75H2Fs/6s42K8cyuX3gTmSk2dFnNlYhaPaiMIRfb53l3gdRysA/IVzPgbAJAA7AdwD4O+c85EA/t6+DAAXARjZ/t+tAH4V57kJgiAIos/COUedr8k2fszTuXIcIj3wHBWDnmPlGwEAYwy5w1O7XM19uLMd1Vxo/fyLgHBWYXEc6QaQVR5YyLcIRy27wQ0VRYPPgWKEnUIcgLu8DZ6PPoo6N62+HryfeFPrYJZMI1ksgyPsGNv+aitPQlM19I+eBwB4LB3VspUMSAksW9K/eN0S1MygXHk/kG3KQdH80De8kbA59RW42gZt7a9hfP5HcUVOERxXPQCWlRt5R4LoZUotOUcBxxFlHPUU3f4EZ4zlA5gH4LcAwDn3c84bAVwO4IX2zV4AcEX7z5cDeJEH+BxAAWNsAAiCIAiCsNHo90A1NNv4URKO+hTWfKPs8uIO90n1nCOb4yhUqmbtZJUNz/r14DlO8PywmMM4h9NRBhbMHcoeAiim8gLDD7j3gTEJxdkjhCO2jcpD3Uu/hXYqcslax/lGACRRxCJEeHMt9E9eirjO2LEWxv4NcGvJK1Pj9cdgbHxLGJMmXwipfDzks68Wxo1t74G3iB0AiegYdYegvnI3jO1/E1dk5AREo9yOP78IorewO44a0dgmCkcFJBx1m3ik/0oAdQBWMcY2M8aeY4y5APTnnAe/wVQDCH5bGATgqGn/Y+1jBEEQBEFYsJapBTnqrku5TBui+9g6qpXHdhwBQG5lqgtHnXMcGT4Z3q3bwC1lak5NA3NVhpYZkyKWqwFAfuYIKCbHEJcZ1HkDULfiqYjvE62mBigWRQyHpYSJscSVVKUbnHNoa38DWPKLzGh/ewbuFsvvdYKCsTnn0P7+G8AsursKIZ/zTwAAacIiwCxu6Br0L1YnZG7pDOcc+pZ3of3hHuD0MXGl4oRyxb1gRYOTMzmCaCdHyRJKYjWuo1kV/+7kkXDUbeIRjhQAUwH8inM+BYHHSPeYN+CBv9hd+nbLGLuVMbaRMbaxro6eqhIEQRBnJtZg7CBevQ2n26gbUF+hKx3VguQOGygstxw80aNzigdD021iWLirmug48n19GNA0W0e1QL7REPHA+ePF5ZY94LofEstDUaZYnqAvqkTrru1oefcvtvmp1dU2x5FTDt9oBL66OkBExvj6A/DDW4QxqeoCQDKV+3mb0WzJvnElyHFkfP0B+PGvhTFl/k1gGYGbRaY4IM+8Rtznq/fBG0WxkwjDvS3Q1jwO/YPnAF0VVxYMgHLtw5AGjknO5AjCBGMMpRbXkRmXkml3mBKdJh7h6BiAY5zzYJT+agSEpJpgCVr7/4PF+8cBmKXo8vYxAc75s5zz6Zzz6SUlHX95IgiCIIi+SG0UxxFAOUd9Cc9xq+OoM8KR6Dhyp5DjyHOsDlwPd/7LLC2AkpWBwHNEMRy75cMNgR+Gi8JPhqaH842CZJUDDlP2ENcA9x4AWSjIyIdszs/JckD/RiXqf/s7qCfFa6PVdFCqZnAADIQd7m4IZRgFYeXjIS+4GfLMa4VxT4ulXDEBjiPubYb+8Yvi/IZOARs5SxiTxs4HCsrCA4YO3ZrXQwAAjKM7oL78b+AHNtjWSePmw7HkPyH1H56EmRFEZKzlamYo3yg+ui0ccc6rARxljI1uHzofwNcA1gBY1j62DECwyHgNgKXt3dVmAmgylbQRBEEQBGEimuMIoJyjvkRrtxxHqVuq5jlszTcK3qD7AIQFJc4VtH76j8DCMDGMOkMqCOcbtcMYi1KuJkFiLvTLsLiOLhgOg6uofeJJcF0PjWu1teDWUjVz23AjcQHO6QTnHNr7zwJtJteY4oSy6HtgTII040owk+vE4xSvcSIcR/onLwE+kxtTdkI572axYx4AJiuQZ35LGDN2fgxefxREAG7o0Nb9AdrqBwG3pWujMwvyRf8K5YJ/AXMmLruKIDoDCUe9R7x/Hf8FwO8ZY9sATAbwKIDHACxijO0FsLB9GQDeAXAAwD4AKwF8P85zEwRBEESfRDP0mOVox9y1UdcR6UVXu6oBgKuiFEwKf4VrPVEPzdsWY4/E0RI130gsUzNadPC2NnAAeon4Zd6ZESUrxVqu5t4HrvsAuFCYmS927cp1Qj9/KNq+3ommN8NByf7TdUCeKcOIAwozi1RUphYJY8868P3/EMbkc/4JrCAgYjJJhnLh7UB7G3aPU8yJ6u1wbOPYVzC+el+c38xrwMzOIhPS6Nlg/cyuNg59/Wu9OMP0gTfVQvvjAzC+WA1r4ggrGwnHkl9CHjM3OZMjiA6ILRzlRF1HdExcwhHnfEt7WVkV5/wKznkD57yec34+53wk53wh5/x0+7acc34b53w453wi53xjz7wEgiDipXHnYdR8toMCdwkiRTjlawI3fWF3KZmQTOUzDX433Ko30q5EGqH7VXhrGsIDjCF7YFGH+8lOh82ZZA2kTha2jmqhfCMxoNR/qN1pVZgJ7gwLN5JhwGHplBYicwDgNN0UcB1o3gXABZlJ6JchOpf0i0eAOyScfvEl+A8dBldVaJIY6uwAszhSEhPinE5wb3Mg38YEKxsJacol4lh+f8jnfQcA4HGK17E3S9W4rkL7+7PiXPqVQ5p2WdR9mCSHArODGHvXw6g90CtzTBf03Z9B/f2PwE/utqxhkGZcCeXah6OKcQSRCvTLyBUaJpghx1F8kB+XIM5w9q76C96Y8B28M/eH+GjJI8meDkEQsHdUG5DdD2XZ/YQxyjlKf1qPi23As8r6QXJ0LrgzJ0XL1dyHRQErN4rjyLt5DwCAjRWFMqemg2Vb8o3aCZSrWVxHzV8BCGQWFWbkg5nziQoyYZw7BNA01D7xBNQTJ8CLROeLYs0zkujGwor+4e8Ab3N4QFagfOM2MMl+cyaNOw9sxNk24SjrdO8FuBub1tg6fcnnfxdMju0eYyPOBiutFMb0da/2+PzSAa76oP3tGejvPAG0iSIvsgugXPUAlDnfBpMpWJhIbSQmoSSrIOI6Eo7ig4QjgjjD2fbYK0C70+jgqx/YygwIgkg8dd4mYbkkswDlLtFhctRNwlG6052OakFyK1NUOLKVqkV2HLXtD2wnTxCFowxk2PKNBPInWE54AFwLCBiKJKMwQ+zQpl06Alxm8O8/gFNPPwNYg7GZ5auwlBv93Gcgxv4NMHZ9IozJZ18TtfU6Ywzy+d+FJ0MUjjLffw7c3/MuSd5YDf3z1cKYNH4BpPJxUfYIwxizuY74wU0wTu7p0TmmOkbtQai/vwvGjr/b1rHKqXDc8ASkIZOSMDOC6B7RytUKSDiKCxKOCOIMxt/kRvNesblh41eHkjMZgiBCWDuqlWQVYLBFOCLHUfpjbVvfmY5qQWwB2ft7z9HRFSKXqnFYhSP1SH3gh6GWYGxZdNZZYZmlQIb5OnGg+RCCndCKMgtE11FxNozZAZHD99XX4CVW4cgawi1m85zJcJ8H2t9/I4yxkkpI06+IuV+bMwu6yY3k0DQ4G04GnEs9OT/OoX3wHKD7w4NZeZDnLu30MdjQqWADRgtj+mev9NAMUxvOOfTNf4b26j1Ag+XzQ1Ygn3sjlMvvA8vOj3wAgkhRoglHeSQcxQX5DQniDOb0Vnstf+POIxh8ycwkzIYgiCDWjmqlmQVwOcQn+DXeBrTpKjI6KMdIZ4yTe6B//AJ4cy0ABrD2/8w/Bx0jrD0FikkBDYFJ4e2E7duXXQWQp10OaeDoyCdPALZg7HiEo4PJdxwZmm4Xw4b0B+AHoIW386nQ/j977x0dx3XffX/vzGzBFvReCIIAK1jFJlKiGiW5SLJkWc225NiJ4/I4eZ2TJ4nz2kkeO3GS87i9cWIndmInlmXZptWt3gslNpFiAxsIEpXoZYHtuzNz3z8Wi507swvsAltQ7uccHXHutIvFYnfmO9/f9zccCX/XB2NbLMtmPlF+MzD0Vmx54ixQvBqAD5IgodDixFgwVlql3LESwv4uEIo4HdXMuoNz4SiKsv9hwKvN4BIg3vq/ZixX8sqss8geiuRKqWfegNqwFcLK9FxjqK0HQDuOM2Pins+A5CXvGiOEQLzmk5HuYZPQ7tNQu09DqNuQlnnOR6hvHPIrPwZtP2ZcWVQN6aN/DkFXxsfhLBTK45Sq2SUrTNO5WTkzwl89DmcJM3qizTA2fr4rBzPhcDhRfHIQHs2Nl0AEFFvzJ8N/8zE6eUNMQdHrG0GDc3EGlaqX3of8wg8AOTTzxpOkGu8vd52C6Q9+COKYOZA6E+hL1WwpCUfVzPJ8KFXz9Q6DysrUsrWsECZ7HoAxZrtwzyhAAbHEAr+FFT4teQmCsbUU6IQjbweougFEiLiaSqyFGAu6EX1H0CoH1J01EA9dAS3ROY4kvVCkF5KWJmrnSUPpkrD9LgjlK2bcVx/cHxWOAEB+7ScwVa0GcSTufJQMNOg1OJhIbTOEdTekfCyhbgNI3QbQ7tNTY8qB34Lct14XnL44ULtOQX7pX1lRcBJh/V6IN/whiImHxHMWLuVWo3DE843mDi9V43CWMKMnLhnGxi9052AmHA4nit5tVGqJiEYAjOVqHtaxslhQzrwB+dnvpCQazYqQP6ctuH1X5pBxFCccO9edMQ1lagnyjULdowAA85YyUCF2KSqqgCTG3CJUkeP+TMRSAlh1gmkgFr5tEkwoMFcwq5U7V4ESGEvVDDfI80M4Ui8fhXL0mUm3XXahIT/k1/6DHSyuhbjz3qT298ps5zp7OBhbCLghv/rjOb9XlQO/AXyaz0pBgrT3C7MWegxZR73nQTtPzGWK8w6qyJDf/RXkJ/7eKBqZbRA/+ueQbvlfXDTiLHjMognFury7ArMjR7NZPHDhiMNZwoyc5I4jDme+oe+opu0Oog/IXmw5R5RSKEeehPLKjwGqZuWc6pk3QUdyI5gbw7HLk97XUpIPkzMmgij+IPwDRgdBNjEGY8fvqBbujuQbic26jmrIm7rxV06+hPBPPofwz78Etf+i8WQFuu5qbjavr9RaDmiyjmhdPtQd1UCR5qaYRm4wphZVgvlwaayceBHyM/8MZf8vEX74z6Dowqkzfv73HgUmtO9NEumiJiUnqnl1jiNHMVt+SDuOQz350qznp/a3QT3B7i9svwukOH43vmQQqleDNGxlxpT3fpNzMTZdUFc/5N/9DdT3n4Lem0mqVsP04Pchrr4mN5PjcDJAha5cjQdjzx1eqsbhLFGUUBiulg7DeHBkAoHhcVhLeRgih5MLDMKRxnJd62CFoyu+YShUnXIkLWQoVaG89T9QT7zAriACxBv+EMKKbQBopAskpXH/TROMs/9WIb/y41gYLFUhv/soTHf+dTZ/XABxhKPa0qT3JYTAuaIKoydjzlH35T7YKqcPl84kBuGofnrHEa3XBWNLkZ+fTgxCefPnEfEw5IP88r/B9Jkfsm6S/GZgQFNK5e4BKmLCm1lUkG9agYlw7PWRP8V2ZJMo2GPS3F8WU/8EG84sB6G8+C+gfa0Qr/vMjC3m54racxbqiReZMeGq2yBUrUr6GJ4w6zhy1KwDqVkLeuXc1Jjyzi8jJWIlqYk9VFWgvP4TMOJHQSXEHXendJx4iLsegKzJ/KGDl0AvHQFp2jnnY+cSdbgL8r5vACGfbg2BsPMTEK++D0QTZs7hLAaWOytxzhV7GF5jT/77lROfhX+lyeFwZsX4uS6oYTn+Ou464nByxqBf7ziK3VwXmR2wSzHHRFhVMODPrcskHVAlDOXFfzGKRqIJ0u1/AXHzR0Dyy0Dyy0EKKkAKK0EKq0CKqkGKa0CKa0FK6iCULoNQWg+hbDmE8gYI5SsgVDRCqGyCULkSQtUqCNVrIF3zafb8l9+HqrmpzQZyIITAUOx3TQQBturULmwdunI1z+XcdlYzlqolchyNQiqxIOxkS2Is5oiIoJ59i3WcjV5hRAcAIOZCIK8mNhDygzWHBFCSpws3LtEFY+vNJPOgo5py5Ik4N/iAeuIFyI/9H1DPaMbOTeUg5Ff/nR0sqIS4+1MpHcejC8d2mmyQPvT/AGbN66+EIL/0Q1AlnNKx1RMvgg62M2PS3j8GMWRVpY5QsQKkiQ3uVg78BlRVEuyxMFDe/JnxPWUvhnTP/4G0+5NcNOIsSjYUNWBzSROKzA7sKFuDxvzqmXfiTAsXjjicJcpInGDsKFw44nByA6UUw4FxZkwb8kgIiZNztLDL1WjID/npf4J64T12hcUG6e6/hZCBp/2kaSeIzkGh7H8kq2UpvivDzHJeVTEEKbUbuPx5FpDt7oznOJIR6aoWgcoKwr0uWBudCEqsw8dqKok4z868aTi2euZ1wxhTrkZVIMxmYllFCxym+oTzNel/38QWf8MsQScGpy3hon0XEH70L6H2nMnI+ZWDvwNc7HtIuuXLIKbURBl9qZrdlAdSUA7xxs8z43TwcuScSULdI5FsIw3C6msg1G9OaX7TIe1+AEyJ40g31NYDaTt+tqHuEdCes8wYWbEdpoe+v6i7xnE4oiDiI3U78KV1H8PemqsgLAJndq7hryCHs0SJF4wdxXWeB2RzOLnAFfIipMacgFbRBKeJvZnVl6st5Jwj6huPuCi6TrEr7EWQ7v0HCLXN8XecI4QQiNc+yM6l7wLopSMZOV88fPq29Sl0VItiCMhuz61w5O2M5zjSuY36XICiwtpUgJBOOLIIRaDdZ4A4gdBq60HQoM41kb+OXQ649TNCqTWxqCDpbiQIyW0osHJgH6BonMD2YiBf977wuSA//s1IcHYahU61vw3qsd8zY8LGWyHUrU+wR2I8+nDsSZeksPZ6CCt3sed9/ymoOmEjEfJbPwe0ZXAWG8TrP5fy/KaDlNRBWLOHGVMO/nbBuo7UiwegLesjFY2QPvY1kLz8xDtxOBxOHLhwxOEsUUbjBGNHGb/AHUccTi6Il2+k7xJUa2cDlLu9QwsywJW6+hHe93XQQZ2IXVQN0/3/BKFseUbPL9Q2G8Jw5fcezdoNorebFUdS6agWJV5ntVyhKgo8XezPFHEcsWJPeDLfSFxfBmje2xIsEAUL1DNvxD+BHIR64V1miJjyAZsmeDno1e3kRZ5UBrsUP0fHJOgzjXLXUU0d6oB67m1mTLzmkzB96rsgekcNVaHs/yXk575nFNNmAVXCxkB6ZynEax+a1fG8+owjU6REjRAC8eZDsaTdAAAgAElEQVQvRgSx2Nkhv/yvM/4c6uWjoG2HmTHxmk+D2ItmNcfpEHfdB2hFRVd/pHxyAaJ3cgpr9sy68xyHw1nacOGIw1mCUEqndRyNc8cRh5MThgz5RoWGbSryCpkbXp8cwFhI77SY36iD7Qjv+wbgYkubSEUTTPd9G6Qg+e5ic0G89kH2BnH0SmLhIs0Yg7FTF470GUe5FI58vSOgckx0s5QWwOTIQ7xgbKnUAtnBhjxbpFLQgBfqxUMJzxH3d1OgccQYxIfIciLXkckQNJ27jCPlvUfBOENK6iCsvR4kzwnprq9D2HmvYR/adgjh33wN6vDcHvYoR54EHWGPIe39Iogl9dI9RVXgV4JTywQENk3+ELE6IX3oT9idJoagvPXzhMek4QDkN3/GjJHKlRA23JLy/JKBFFZBaL6JGVMO/Q5UTi2PKddQVz8o05GQQFi1O2fz4XA4CxsuHHE4SxBP5wBCLs/UsmRn7fme9n7IgZB+Nw6Hk2Gm66gWRSACamxsiPJCyjlSu09DfuxvAR/7s5L6zZDu+SaILXsdHYXSZRDW3cCMKQf3geocE5nAqytVs81GOKqvYFw7vivDOfvs1ndUc04TjJ23womgSSccicURR5Gimb+Nff/T/otQhzvZE+evxVQmjUE4ipzbZqqETaqEHpOoF4py4zhSu1tA2z9gxsRrH5wKLSaCCGn3A5Du+jpg0bWUHuuF/Nv/F4o+IyzZcw91QD3yBDMmrLsBQsNVszqeV1emZpMshmwRoX4ThC23s/M4+xbU1oNxj6kcegyY0Py9EAHi3i9mNNRZ3HkPIGocae5hqC2vZex8mUCfzURq1oI4SnI0Gw6Hs9DhwhGHswQZ1QVjl1y1UtM2GaCqiomLPdmeFoez5NF3VCuPIxwBxpyj7gWSc6S2HoT81LeBEBueK6y5DtKdfw1izkuwZ+YQdz0AiBrBwDsG9YPnMn5evXBkr0vdZSWaTYYSN72Aky2MHdWi3yl6x9EIbFtqDcHYFrHI4CgSN30YRBfgq7awIdlEsgP2hshC0K8r2/QDiLigSuK4jkwmfaZR9oUjSimUd3/FjJGatYYySgAQGrbC9KnvgJQ1sCvCASgv/ADyW/8DqsTvlhr33KoC5ZV/B7TlmbZCiNd/NpUfgcGjC8aOlqnpEa/9NEhJHTMmv/4TQ9c4dbgT6gfPMmPCltsglOtegzRD8ssgbLiVGVMOPw4aDibYY/6hL+0UVl+To5lwOJzFABeOOJwliL5MrXhTEwrWLGPGeLkah5NdZFXBaJAtOSvNi+++0XdWWwjCkXLyJcjPf58N/wUgXHUHxA//KYihbCg7EGcJhC0fZcaUo0+D+icyel5DqdosMo6A+ZNzpBesHPWViIg2rJAQ7hmDdVmewXFkdgdBB7QPNUjE+bJ+L7Odeu5tY8lQtLsaVdnwZABR4cou1cAu1cTmFxYgzAfhqO2QrpwIEK99KGEODSmshPTAP0JYd6NhnXr8OciPfxPUM5bUudVjvzdkjEl7vwBidSY5eyN6x1E0GFsPkcwQP/JngDZnKuCB/PKPQCezlihVobz2U1bYcpRA3HX/rOeXCuKOuwFJ857wuabtejefoCPdoFp3HhEMweQcDoeTClw44nCWIPpg7OLNjShYzT75Gz/PA7I5nGwyEpgA1WSc5JtssIrxb2SrbaUgmpbRY0G3oQX2fIFSCvnAb6G88V/QZrgAgLjnIUjXfxYkx21yxe0fByyO2EDID+Xw4xk9p8FxNItSNQBwNsxP4cheXwG9aCQPTkB0CIBZQVjnOJLOvs8sk/qNEddH0072dxPwQNV3v8tfE8uqSpBzRAhBreNmlOftQFnedlS5gyBC7H1HqQBAH5adWaiqQH7v18wYadoJoXr1tPsRyQLx1q9A3PtFtpwKAO09h/CjfwG158z05x69AuXgPmZMWLkr8nrPgWQdRwAglC2HeM0n2Xl1nYR64kUAEXcZ7bvArJdu/KOsOROJvQjCZp2o/P5ToKH5+VmrRV+6SJZtyGoZMIfDWXxw4YjDWYKMHI8jHK3VOY4ucMcRh5NNBgOsS6A8TjB2FLMoodLGdhPq8Q5nZF5zgaoKlNd/CvXwY+wKIkD80J9C3HZXbiamg1gdEHd+ghlTT74MOj6QYI+5IfuDCA6Px84vCsirKp5mj8Q4V1Qzy57LvXOa22zRd1RzLjd2VAt1j8Kxo8FQpmYSnMDZ/cyY0BxxGhHJDGHtdcw69YyuXE3MA+yNkYUEOUcAIBATSqwbUWLZCBF6Z1L23UZqy+vAmOb3RQRIuz+V1L6EEIgbb4V03z8CTjbzDD4X5Me/CeXY7+N2XKSqAvmVHwOKxrlldUK88fOz+TEYPEk6jqIIV90BUtvMjCn7fwW1uwXKfl0J34rtcxa2UkXcdhegFaoCbqjHn8/qHFKFUgq1VddNbfW1OZoNh8NZLHDhiMNZYgRHJ+DVXOATSURR83IUGkrVuOOIw8kmQ/5xZjleMLaWWkO52mCCLXMDlUOQn/se1NOvsiskM6Q7/xqiLpQ61wibPszegKsylAO/yci5DMHY1aUQxNkF/epL1SbmiePIsbwS8YKx7ZuqjPlGfgDa0kCLA0Lj9qlFfbka7TwFOqF7v0e7qwUSC0dTyB5A93oTkt2OajQcgHLod8yY0HwTSEltSscRKptg+vR3Qeo36U6gQnnnYcjPf9/gkFFPvmRw8og3/CGIffrPnGTQOx+ncxwBk8HfH/pTQNvBTQlBfuJbQDDWxAOSBdKNfzTn+aUKyXMagryVY8+ABjwJ9sg9dKidFSQFCUJjdgU3Doez+ODCEYezxBg9yeYZFK6rh2gxo2CNvlStG1RVszk1DmdJM6jvqDaN4wgA6uxsmHLPPMo5ogEv5Cf/HlRfUmR1QLrnWxDiBP/mGiKZIe5my2bU8/uhDl5O+7l8ho5qpQm2nJn5kHGkKgrzQAKIdHyjMpsTFeoegalIMeQbmfpZ0UlYuwdEky0jlC0HKW/UbEGhnnmLnYRzFUCkaR1HU4TH2ewaAEB2hSP1+POAV+MyFM0Qd903q2ORvHxId30Dws57DOvoxYMI/+ZroCORhhd0fADKu4+y+zdshbBmz6zOrcejy5iyzyAcAZEgavHGP2YHKXv9Ie66HyR/duWcc0XcegdbLhn0QTn2TE7mkgyqvkxt+RYQqz3+xhwOh5MkXDjicJYYhmDszZGLcWt5EcyFsQsj2ReA98r8K33hcBYrQ4HkOqpFqbWzYkO/bwyhFDoqZQrqGYX82N+AXjnHrnCWwnTfP0KoWpWbiSWBsGYPSGk9M6Yvl0kHxmDs1DuqRdELR57LfXHLkzKJv28Uajj23rOU5MPktEENsB2yiCCCwI+QznFk7mDLp4XmmwznENazY8rZN6ZClAGAiBbAuRII6YUjPwDdQ5C4wlH2StWofwLK+08zY8JVt82pVToRREi7PwnpY3/NuncAYPQKwr/5GpQL70F+9T8AWdMZzGyDdPMXE4Zxp4pX1jmOZihViyKs2ZOw6xcprYew5bY5z222EIsd4rY7mTH1+AugvvEEe+QOSinvpsbhcDICF444nCXGiC4Yu2RTE4BIXoLRdcTL1TicbOCXg0yorEAEFFvzp93HbspDkSXW/YiCoteXW7GXjl5BeN/XQYfZzw5SUgfT/f+UchlOtiGCCPHaB5kx2nUSaufJtJ7HIBzVzN5xZCktgOSIuTpkXwCBweS6aqWL+GVqFIKFFWysTZUAYOyoNhpzJpGyBgjlKwznEFbvAbRh8RNDoF2n2Y3ymwFVBQ3F76w2RXgcMOVOOFKOPMEKXBYHxG0fT8uxhcbtMH3quyBly9kV4QCUF34A2s2+ZuJ1fzAnwUqPPhw7GccRMJnZdNMXAMNcCMSbvwQiZje4XI+w5aOANlw6HIDy/pO5m1ACaF8r4NZ8D0hmCCu25W5CHA5n0cCFIw5niTEaJxg7SoEh52h+BWSPBCZwavQy3IYnyhzOwkbvNiqx5ENMotNYnSHnKHflamp/G8K/+xtggp0DqV4D6b5vgzjTd3OaScjyLSB165kx5d1HGHfLXDF0VJuD44gQkvNyNYNwVF8BNeQGMcXew8q4H9ZaGxRCIGvzhVQVpglNgHUctxEAEKsdwsqrmTG1hQ3JhnMlIJhmLlfLYakanRg0tHQXd34iraVEpLAS0v3/BGGGHDFSt8GQHzUXKKXw6sKxk3UcAZGQeulDfwpoOkYKG2+dFy5FYrJC3H43M6aefBnUM5KjGcXH4DZasS1rXeg4HM7ihgtHHM4SQg6E4DrHOgGKNzdN/btgtc5xdG7+OI76fCP42fnn8XzXIfx360tcPOIsKgb9+nyj5Nom64WjHk9uhCMa8EB+5p/ZgGNEuiBJn/g7EKsjwZ7zD0IIxGsfYsboYLshN2QueHvYPCB73dyyW3IuHHUa841C7S3MWLh/AqI4ZnQbuTwg0dI6UZo2a0cvcqiXDoP63VPLRDABztUzC0chV85K1ZQD+wBtSamzNBLMnmaIyQLx1j+BuPeLQDy3jmSBdMuX01aiBgABJQxFI7CaBQlm0TTNHkaEZRsinxlNV0O4+j6IOQjEToSw8VbAoel+qIShHH4idxPSQVUFausBZox3U+NwOOmCC0cczhLCdbYDVFamlh31FbAUxUpd9I4j14X5IxwdGDgDFZGbC58cwNHhCzPsweEsHIYCbFbGTPlGUWodrOBwxTcMNY3OmGRR9j8C+FjxS1i/F9IdfwkiZTd0OB0IlU0QVu1mxpT3fg0qhxPskRq+Hrak0F47R+GoYZ45jpZXIjzQrtvKBCgeQ0c1syvWnUpo3AGS50QiSG0zUFAZG1BkqOfeZjfKbzYIR5TGcxzpBY3MC0fqUIdhvuKu+5kg8HRCCIG48VZI932b7RgIQLz2QZCCirSeT59vZE/BbaRFWLYRpjv+EtKu+0GE2XUbzAREMkPceS8zpra8Djo+kKMZsdArZ9nPYXMeyPItOZsPh8NZXHDhiMNZQiQKxo5SOE9L1UJKGJcn2Buh06PtOblB5nAywaCfzaSZqaNalCKzEzbNzVlYlTGgcy9lGvXKOagtrzFjwuaPQrz5y/Pqpi9VxGs+BWjnPzEI9dTLaTm2tzvDjqP2bDuOWOHIvqwMUNzMWDSHyeA4Gottl6hMLQohBKIuJFs98zobBu5oBEI6gY/q/iZyVKqmvPcogNhcSUkdhLXXZ/y8QuVKmD71ncjrW1QN4er7IGxOv8tptvlGCwmh+UYgX1NaqspQDj2Wuwlp0LsihcadGRMlORzO0oMLRxzOEsKQb7SpiVl2rqgCkWI3Sv6+EYTGPcg1bRNXIFOFGfPKAVyayH7baQ4n3VBKMTxLxxEhxFiu5h1MsHX6oUoYyms/YQeLqiHu+UxaS2ByASmsgrDhFmZMOfw4aDBOe/cUkH0BBEc15VWSCGtF0ZyO6WysZpaz7Thyd7COCyv1wlTJOodEUwgAEjuOHMUgyzbOeC5h3Y2AJv+LDneBDsQeihBBAixs2TVIGNHOalQJAAgzYcuUEgCZDV9Wu1tA2z9gxsRrH8yauEpsBZBu/QrMn/23iJMniQy1VNELR45FKBwR0QRx1/3MmHrubdDRKzmaUQSqyFAvHmTGeDc1DoeTTrhwxOEsIUZ1HdWKt7DCkWCSkL+yhhkbv9CT8XnNxHlXfOfT6dHLWZ4Jh5N+xkNehFRNK3PBBKfJNs0eLLU5zDlSj/0edJT9jJBu/hKIoQxoYSJefR9g0pTbBNxQjj4zp2Pqg7Ft1SUQxLmJB7nMOKKqCm+nrlSn8yJMdcXsmLsbFIkdR8K6G5MSUYijGPryG0NIdn4zaDjWcp4QAqpOClRhY74RIWZoA5nTDaUUyru/Ys9ZsxakYWvGzpkL9MHYsy1Vm+8Ia/YAxZprJapCObgvdxMCQLtOAQHNgz6rIykhlsPhcJKFC0ccTgahoTFQXc1/rqCqitGTrNBSsrnJsJ2xs1puc45CShiXJnrjrrs43gOfrG+7zEmWi+M9+NGZp/GvLU/i6Y73cHq0nb+eOUDfUa0srzAlt44+56jbO8SW7mQI6uqDcuhxZkxovglCbXPGz50tiK0A4tY7mTH1g2dBPbNvd+/tTl9HtSiO+gpA857xXRmGHAjN+bjJ4OsbgRrWCJ/FDoTaWiA6YqIBVSgQGIMsCFCF2KUnCcuQPJHvSLH5xqTPKepDsi+8ywhFsDcAQd1nWXAycymU/TI12nYItP8iMyZe+9CCd+Xp8YZ1HdUWoeMIAIggQrxa5zpqPQB1OHfXS4YytZW7GFcdh8PhzBUuHHE4GYL2vwJc/DfgwvdBJ87lejpwt/cj7I4FhpoLHbAvM96wFOo7q+VYOLo00WsoU4uigqJltCO7E1okKFTFC91H4A774JUDOOfqxHNdB/HDlifxcOvLeLf/NHp9I1kRIJY6ho5qSZapRanMK4JJiN0geOUAxkKZLTGllEJ+/T8BRSNO5OVD3POZjJ43Fwhb7wBsmi53cgjKodm7C9KdbwQAosXMBmxTanQBZYiJi2yJTmlTMUSH7vJSjSyH4nVUQyT0mhSyrqnpIA1bAZvm7yTkY8p0CBEB6Fx7ocl5xs03ylwODFUVyO/9mhkjTTshVK/O2DlzhUf3oMyxSB1HACCs2gVSWq8ZoVAO/S4nc6FyCOqlw8wYL1PjcDjphgtHHE4GoOEJYCT6Ja4CV34PKue2ffzoCV2Z2ubGuE87jZ3VchuQfd7FCld5Ivtk+NToZS5uzIKRwERCd1GvbwT7+0/j4daX8a9nnsKznQdxdqwTfjk7Doalht5xVJ5kMHYUgQiosZUwY5kuV1PP74+URmgQr/uDaTtiLVSIOS9SsqZBbXndUKKXLF59R7WauQtHQLxytfhOzXTT+8pRZrm8xgyzvkwtFPmsSZRvNFMoth4iSoZQaUO5mkknRBEfqBqOW6qWSeFIbXkdGNP8LogAafenMna+XOJdAuHYUQgRDFlH9OJBqEMdWZ8L7TgOhDSvva0QpGZd1ufB4XAWN1w44nAygb8X2s4pUIPA8P6cTQcARmYIxo4yn0rVQoqMNl2Z2kfqdoBosiiGAi70+2dfNrJUGfCPJrWdTw6gZawdz3S+hx+2PIFHLr6KAwNnMOAf44JdmhjSBWOXWQsSbJkYQ86RN3PCEQ24obz9P8wYqduQle5QuUJYfzOgdcRQ1eAiSZZMOI4AwNmQm5yjrucOsfMw+Yz5Rr4RAAnyjcx5EFZenfJ59d3V6JWzoFqBxlzL7mC2Ap62rDqOaDhgcKEIzTeBlNQm2GNh49E9jHBIi1c4AgDSuAOkvIEZy4XrSL3wLrMsrNq9oDtacjic+QkXjjicTBDoN46Nvg8ayp3AoQ/GLtmSQDhazV7Qutt6mfyKbHLJzZapOU02rCqoxYp89gbp1Ogl/a6cGRjQiW3VthJU5k3f2YmCosc7hLf7TuK/L7yIH599Gi90HcYFVzeCSnjafTnxkVUFI4EJZqwsRccRANQ62LLT7gwKR8r+XwF+zZxFE6S9X1h0eS1aiChBuubTzBhtOwy190LKx9KHY6cj4wgAHDkIyHZ39MPV0j61bC8QQdwuo3A0WY4Zz3EkrLoGxJR6SRMprgWpXsOMKWfeiK0nDnYHcx4wfiYiHJmyk3GkHn8e8Go+a0UzxF33Jd5hgaPvqmafxe91IUEIMWQd0bbDUAez17iDhvxQL7OuP2H1tVk7P4fDWTpw4YjDyQTxhCOqAoNvZn8uk4yeYMWV4s2NcbczFziQVxUre1HDctZbO0fRl6mtKawDIQSbitm5nx3rgKzGz0HixEcvHO0sX4vPrf4I/rT547itbifWFNTBIkzfGcsd9uPk6CU82bEf/9LyBH7d9joOD57DcGCcu5GSZCQwAapxJ+abbLCKqbsfqm0ljBNvNJi4FHEuqD1noba8xoyJOz4BUlSdYI/FA1l5NUjlSmZM2f9Iyu91n76rWm3pnOcGxClVa8/853a3zm1Uvy0yB3MtWzqJoC/SUU0vHI25IeiCrlNBv6965k3Qqe8CE6gac10QQQCCXUBwNCuOI+qfgPL+0+x8r7oNxFGSYI+FjawqCGgyzwgIbFJmQ8fnA2TFNpAK9ppEOZg915F6+SigLSPPLwOpWpW183M4nKUDF444nEzgjyMcAcB4C6g/O7kTWgJDLviuxHI1BLPJUJKmpWANG5DtykG5WliVcWmCDV1dUxiZc1N+NZN1FFDCaB2fXd7IUoRSikGdcFQx6TZymPKwsaQRH2/Yg69u+AQ+3bQXV5evmzGwWaUqOj0DeKP3OP7r/PP46fnn0O7OjeC4kBiM01FtNlhE09TvMEq6c46oEob8+k/ZweIaCNvuSut55iuEEIh7HmLGaO85UN3T/pnIRFc1IF7GUeb//nqeZ4Wj4mIKYjNDKo25fSilQDiAsCiCajqqCYEQxLwygxiXCsLKXYDW1eJzgbZ/EFsm+ewOZgugBrLSVU058gQQ0mQbWhwQt3087eeZL3h1QrVNskAgi/82gxBizDq6/D7U/rYEe6QXQze1Vdcsavcnh8PJHYv/E53DyTJU9gLyROIN+l/Nuhtj9CTrNipsrodoTuwmKVyd+5yjSxO9CKvaMrU81NgiT+ZFQcT64uXM9qdGs2cNX+iMh7wIaErLzIKEQrPDsJ1IBCxzVODG6s34/JqP4ivr7sSHa7djZX4NTDPkJ4wF3Xi64z2E1dyUOS4UDMHYKXZU06LPOUp3uZp69BlAFwgt7f0iiDS9M20xIdQ2gzRcxYwp7/5K43KZnrDHj5Ar1vFOMEnIK5/971yLQTi61JvR75qw24e+N09MLVvzCETPWJxg7CBAadwyNan5pjnd5BJznqEsR9GEZBNiZ3ewTnZay7DjiI4PQj35EjMm7vwEiNWeYA8jI4FxXBzvSaub9lDnEP7zUCvODrhm3jhF9MHYjkUcjK2HLL/K6EbMQtYRDXgiwdgaeDc1DoeTKbhwxOGkG32ZmqB7kunrjAR0ZhF9MHZJgmDsKHrH0XgOOqvpy9RWFyxjbjA2Fq9g1re7+zAR8mZlbgsdfZlaRV5RUjdv+WY7tpSuxD0rrsefrb8HDzTeiO1lq1Fsid9JK6CE0OHOTkvwhcqQPz2OIwCoc2ROOKJjvVAOP86MCc03QahtTts5FgritQ8CmrJAOtoD9WxyZcj6fCNbTWmkhCoNWMsKIdlj7hvZG0BgKP0CQZTe1z6AGooJ0DXNEcebId8oGBHKDMHYLk9aAtX15Wq0/RioJ/oZpxNqLDaAkDhiZ3rFT+XgbwFFI5o7SyFs+nDS+18c78F/nX8Bj7e/g4dbX4ZC1TnP6UjXMP7q+Q/wqw/a8dVn3kf7qGfmnVJAH4xtlxZ3vpGWuK6j9mNQ+1ozel710hFA+3CmqBqkrCHxDhwOhzMHuHDE4aQbfZla/lrAzoocGHgNNA0XgsmidxwVJwjGjlKwNreOo7Bq7Ka2tpCdU3lekSHM+fRoOzgzE084ShVJENHgrMLNNVvxxbV34Etr78AtNVsNjhl9uSGHRe84mqkkcDr0jqMB3yhCytwdX5RSyK//J6ANQM/Lh3jdZ+Z87IWIUFoPYR0reCgH94GGgzPum6mOakDk5jWb5Wrdzx5klivqIg4Tc50x3wgAQjrHkUUqBrHP3W1FKleClGgedlAV6rm3Jhds7MYWGyDqRSIz0nk5rA51QD33DjMm7rofxOBySrA/VfHqlWNT2WeDARcupqEU+4nTse/xoKziN8fT+325lB1HAEDqN4NUrWbGMu06MnZT42VqHA4nc3DhiMNJN3rHkbUKqLiZHQsOAa6TWZvS6AnWcZQoGDuKPv9o/Hx3VsvrLk/0MSVODlMeauzGANmNupDsU6OXeShzEqRDONJTZHFiW9lq3FC9mRm/NJHZcpmFjF8Owq252RJAUJLAvZUMDlMeijQlhyoo+ibboM8F9fw7oN2nmTHx+s+CWGc/14WOuOsBVoDwjEY6aM2Ar2eYWU5XvlGUbAlHVFXR/UIs38hsAUyTIqipVvd5Eoy8x/WOI2v5prTMhRBicB0pZ96Y/NzROY7MNsCkzzNKb5ma8t6jgCbwnpTUpeSsOufqwrjOPdvhTpCbmCSBsIJjPexnwRtt/XD5Qwn2SB2942jJCUfxXEcdx2fVeTEZqG8ctEv3uczL1DgcTgbhwhGHk270wlFeJUheJVCwkR0ffAtUTd9FWyJkfxDj59lSs+KNKxJsHcFeU8qUPIRcHgQGx6bZI72cM5Sp1cV9irauqB6iJnzTFfJktA35YsEoHBUn2DJ1ljnKIZFY/tFE2IehwHjajr+Y0LuNSqwFEGfIjpqJWgcrRHR7BxNsmRzU74by9i+YMVK3AcKa6+Z03IUOyS+DsPmjzJhy9ClQv3va/fSOI1tNejqqRXE2ZEc4Gn7/AgKDsfdveb0NUbHEFMdxFK+jmrVmR9rmI6y9HhA0xx/rBb1yDpEStJgwRAQBsOkCs9MoHKndLWw4NyKljSTJv2tKKQ4NnDWMt89RODp+ZRQhhXU5hxQVz51LX1MJveNoKZWqRSHLNoLUrGXGlIP7MnIu9eLBSLfe6LlL61nnHYfD4aQZLhxxOGmEKkEgpHvCb6mI/L/8RkBzQw3ZDYwczvicxlraQdXYxYVzRRXMBcYgZC1EEFCwWpdzdD47OUeRMjW2vElfphYlT7JgVUEtM8ZDsqfHJwfgDsc6/QhEQKlVfyM1e0yChOXOCmaMl6vFZ9CQb1Qw52PW6crVeuYopCr7fwn4NWH/ognS3i/wcggA4o6PAxaNoyXoi3TSmoZMdVSL4lhRzSx7MiQcdenK1KpXTX6GSCJMlbr3ccgXKVPTvGfEMCBJyQdFzwTJywdp3M6MqWeiIdm6cjVDeVx6hCNKKZR3f8XOq2YtSMPWpI9x2d1n6LQIRB6KuIKzzxUzOOUAACAASURBVCQ60Bn/c+CZlm4oanocoZ6wLuNoiTmOgASuo66TUK+cS/u5DN3UdCHxHA6Hk264cMThpJOALgjYXAIiRi5KibkAKNnJrh9+L9KFLYOMHteXqU2fbxRFX67mylLOUbu+TE3KM2S3aNGHZJ93dSKozWLhMAzoxYo0uFz0NObXMMv6vCpOBL0Tay4d1aLo/1aueIehzjJPTe05A/XMG8yYuPMekKLqBHssLYjVCXHH3cyYevJF0InELi/vFZ1wVJtmx5G+VK09M8JRz/OxMjVJAqxqRNQw1RSCiLFLSxoKAKpqdBtJOldSGhB15Wpq60HQoBeGcjW7XqDVl67NDtp2CLT/Ijunax9KSWQ9GMdtFGW2riNKKQ50xBeOBjyBhKJSqnhkXcbREnQcAYBQtwFE1zQg3a4j6h6ZdNRpzrtqd1rPweFwOHq4cMThpBNDvlElu1x6LSBqnsKpIWCIDdFMN6MndMHYyQpHOXIcnRvXlakVxi9Ti7LcWQmnKfZEOawqho5snBgD/lFmOR35Rnoa81lh4Yp3GH4582WZCw1DR7U0CEfFFifyxNiNcEiVDc6mZKByGPLrP9UdvBbCtjvnOsVFhbD5I4BDI4IoMuRXfwK1/RhowFi2lmnHUTYyjjxdA0zDhaIyCWRSnDTrO6qF4ucbWazsPNMBWbYRcGqEODk46cpghSNj2djcHUdUkSG/92v2PE07IVSvTrCHkR7v0LSlpR3u2f0uL494MOQNJFz/1On0fF8aStWWoOMoisF11H0aas+ZtB1fvXgATI5W5UqQwsrEO3A4aWDMF8SRrmH8+ng7/uHVU/jOmy24ODwx846cRYM08yYcDidpDPlG7MUxEa2gZXuA/ldig6PHQIt3gFjS/wQWAEZOso6jkhmCsaMYA7IzL8bIqoK2cbasaU2CMrUoAhGwobgBBwZiF2WnRi9jU0lyP+dSY8CX/mBsPQVmO8qsBVOOGgqKdncv1hUtT/u55oKsKnCFPCix5Ge99IpSauyolpeGDlOEoM5RhlZNF6Zu7xAqbanlWKlHnwZG2b9Fae8XQQwdqZY2RLJA3P0AlFd+PDVGu05C7ppsflBUDaFqFUjlKpDq1fD16IWj9HVVAwDH8spISdhkIL23ZwhKMATRkr4cn57n2RLrynWlACI3D/HyjQBjvpFFTF+uWhQiiBDW3QD18ONTY2rL6xA3bp9mLyAdwpF65g1gTOOsJAKk3Z9K6Rj6bKMCkx3j4ZgjucMzAJWqEEhqz3z1jqJlhXZ0uWLHPdozgs4xD+qLpi9hnw5KKbz6cOwl6jgCAKG2GaRuA9NUQDnwW5B7/z4t3zXGMjUeis1JH4pKcWXch4vDE2gbdqNtxI22YTdGfMbOoe92DOGXD1yDwrz0NhngzE+4cMThpJOZHEcAULQNGDkChKM3jSow+AZQd2/ap6MqCsZOsZk/yZeq6R1HmReOLrv7ENKUqdklK2rjdFPTs6F4BSMc9XiHMBKYQEkas3sWC5noqBaPxvwaphSrbWJ+CUdjQQ8ebn0ZfiWIGlspPt20N+0le9MxHvIy73WLYEK+yTbNHslTZ2eFox7vELaXJe98oGO9hqweYf3NEGrXpWV+iw1h7fVQjz0LOhLnM3KsF+pYL3D2LQDAXV+TMNJtx3CHgtHeSHf4dCJZzbDVlMYEKkrh6RxAwar0heZ2PcfmGxUWApiMTTM4jhIKR5n53BGbb2KEIzrQBnVkDMK0z2XmVqpGwwFD23Wh+SaQktoEexgZ8rtwUZcF95FlO/B0x3sIKBG3ZkAJod8/hmpbag+Z9GVq92+ux4vne9HSHxOun27pxlf3rNXvmjQBJQRFUxJrFiSYl7jILO56ALJGOKJXzoJ2t4As2zCn41JXv64kkvAyNc6s8YdlXB7xoG3YjYvDE7g04salEQ8CspLU/i5/CM+e7cFDW6dvusNZHHDhiMNJE1SVgaDOZh5HOCKCBFpxE9DzZGxw4hyorxvElt6OGO5LvZA1FnVLSX7SXXzyV9YyT649nQOQfQFItsw9RdSXmK0urEvq6WqxxYk6exnTUe306GVDa/ilTliVMRpky2fKMyQcNeVX49Bg7An65Ym+WT0tzxRv952EX4k8PbviG8bpsXZsLklOVE0H8dxG6XI96XOOejxDoJQmdXxKKeTX/xPQ5oTl5UPc82Ba5rYYIYII8eYvQn7iW8AMJZkmC0FlkwmVTZGbavm/Pg8UVERcSVFnUtlyEHH2l2fOFVWMs8l9uS9twlHY60f/G8enlk1mQPDFShXiOY5UQhA2CEdzd9fFgxRUGJwe6um3INywCkCi7LvZPymnShjKW/8NeDWCvGiGuOu+lI5zaJDNq6myFWO5oxL1jgpcGI+ViXe4+1MSjlz+EM4OsJ81Vy8rg1USGeHopQu9+OOdK2Ezz+59p3cbLeUytShCzRqQ+k2gnSenxpSD+0Dq1s/ps15tPcAsk5q1II7MONY5iwdKKUZ8IbRNuoguDrtxacSNbpcXc43Hf6qlCw9sXg6TOD+u7ziZgwtHHE66CA4xrVFhygeREjxOzm8GrIeAgMbaPvAa6PLPTntBIQ8NYew3+wCqovCTD8BUPn0+xkicYOxkL1gkqxnOhkomI2O8tQclSTqWUkVWFVwcZ1sDrymYvkxNy8biRlY4GmvHdVUb541QMR8Y9LtANZcIRWYHLBl6KlxjL4VVNE89LfcrQfT6RqYNOs8WIUVG2wT7Xjvn6sqqcGToqJaGfKMoFbZiSESETCNPDD2yH66QF0WWmUtR1HNvMzfdACBe/zkQqzNt81uMCNVrYPqjn0DtOA7a1xr5b7iT/U5IxPgA1PEB4Pz+yLJoBqloBKlaFROUHMmXdjlXVGHgnVNTy+nMOep97QMowZgAU7mmBMBkto1AYK7VO478ho5qJsEJgWTOjSJsuBmKVjg69zbo9ZtASHqFIzo+APn5H4AOsN+zwlW3pXQjPx7y4uxYBzO2q7wZhBA0OCsZ4ajd3YfdFc1IlsNdw8xN4cpSJ8ocVtzQWIkfv3cBo/7I57M3JOOV1l7ctT7571wtnjAPxo6HuOsByBrhiPaeA+06BVK/adbHVC+8yyzzMjVOPLwhGWcHXGjpd+HswDhahyYw5p9b1qRZFNBQ7EBjiRNvtvXDP+lKGvYG8dalftyyijfOWOxw4YjDSRfJlKlNQggBrbwZ6PhlbNDXDbgvAPlr4u5DFQX93/oHhNrbAQDBtkuo+eH/ByIkFkb0wdipij4Fa5exwtH5rqSOQakKteUN0JFuCM03QihbPuM+7XHK1OocyYsMawrr8MqVo1Md2TxhP9rd/Yag5qVMtsrUgEj2VIOzCudcnVNjlyZ654VwdMndi7DK2rA73QPwyQHYsnTDo3cclach3yiKSATU2EvR6Yl1eezxDs4oHFH/BJS3f8GMkWWbIKzZk7a5LWaIrQDiuhuAdTcAAGjIDzpwCbSvFWpfK+T20xBp4pDiKZRQ5Aaz9xyishNp3A7p1q8kJeA5GzIXkN39LFumVrW2BJgUYaXyfBBzrNyTymFACSNoZt0nmcg30iI07oBicQDR9vUBD6jLAxL3404CkHqJqtp2GPIrP5oqxZvCVgBx28dTOtaRwfNQNfJOsSUfqwoiZW7Lnezvssc7jJAiw5ykI01fprZ7eeTz1yQKuH1dLX55LFbK/uTpLtzZPH0zikTohSPuOIogVK0CWb4FtCPm0lMO7gNZtnFWrzMd6Y4I0lGIAGHlrjTMlLOQoZSiZ9yHln7X1H8do545OYkKrCasLM1HU6kz8l+JE8sK7ZAmXUUWScBTLTFR+7GTnbh5ZVXW8yI52YULRxxOujAIR9N3jSH25aCOlYBHU6s+8DqocyUIMV7I+g4dnhKNACB0+TL8HxyHbdvWhOcYPal3HKUWGF2wehkThJpsZzX15EtQ3vz55L9fhHjrn0Bce920++jL1FYVJFemFsUsmrC2cBlOjcYuhE+NXubCkQaDcJRiYHKqNOVXG4Sj66tm/6Q1XZwf6zSMUVC0jvdkzXWkzX8CgDKrvkX43Ki1l+mEoyFsKJ4+g0DZ/wig7QQmmiHt/QK/EJwlxJwHUrceqFsPEUDLN3+Bth89itJ6EaXLJdTtLIfN4gHUmbMk6KX3If/ubyHd/bczulkMndXa0yMcUVVFzwtsMLZN8E3dnJgS5BsFdNlhmco3ikIkM4S110E98cLUGO1sBYri/W2n5jaichjK/l8yx57CWQrpjq+BWO3GdQnwyQGcHGW/p68uXzv1N1dkcaDQ7IArFBHBVKqi2zuY1PearKg40j3MjO2qjwn3H2uuw6MftEOZLEfvGPPiRO8YttSk/r3Ag7ETI+66H7JGOKJ9F0A7T4As35LysRRdKDZZtgHElt7vDs78xx+WcW5wHGf6x3Gm34UzAy6MBxI5KqeHAKgpsGFlqRNNk0LRylInSmyWab/779lYzwhH54cm0NLvwoaqzH6+c3ILF444nHTh112cT+M4mqJiL+Bpw1Rb1dAIMHYcKN7GbEYphevxJwy7Tzz33PTCkc5xlGwwdpRCfUD2heQCstVTmq5xqgLlpR8CPhfErR+Lu32kTI0NBl07Qze1eGwsXsEIRxfHe+CTg7BJcws/XSwM+EeZ5Uw6jgBgRT57AzvgH4M75IPTnOZE4BQIKWG0TfTGXZetcjVZVTASYFvYprNUDTDmHHV7hhJsGUHtbol0htIgXn0Pb/GcRrzdQ/CMqvCMqug4Hoa0906s+eMPgw5eBu27AHWyxI3Jy9FAR7oR3vcNmO7+O5CixMKBXjjypMlxNHysFf7+2GeIrTgP1BVbNi/T5edFg7F1DwAyLRwBgLB+LyPuqBdbIMb9/kteOKKu/khp2uAlwzqyYhukD/1JyiWdx4ZaGfej05SHZl0TgQZnJY6PxMSldndfUsLRqf4xeEMxF29hnhlrymMiQ7nDij0ryvHWpZjA/OTprlkJR9xxlBihciVIw1bQ9mNTY8rBfSD1m1MS5SmlUFv13dSuTds8OfMTSin63H609LtwZtJNdHnEMyX4poJFErCi2DklDjWV5mNFiQM2U+pyQF2hHbvry5iujY+d6uTC0SKHC0ccThqgVAWCA+xg3sw3XMRaDlq4GXDFnkZh8G3Qgg0gYkzsCJxuQbC11bC/7+gxhPv6YKoyupt8/aPMRb5oNaNgdWoBqQVrWPEmGccR9bpAR4zbKe88DOodg7jnIRDdjUS7ux9BNfa0xCZZUipTi1JrL0ORxYmxyQBohao4O9aBbSl0lFqsqFTFkJ91uWRaOLJJVlTbStDrG5kau+TuzWqWkJ62iStT2T96slWuNhKcYLKm8k02WKX0trKtsZeCgEydZyQ4kfBno3IY8us/ZcZIcS2EBEIvZ3Z4r7DuD3ttGYjJAlKzFqhZCxGRmwS4h6dEJLXzBDCqyeOaGEJ43zcg3f23EMrjO8gMjqPLfUmHo09Hz3OHmOXlexqAUMfUsrV5ObtDKCIchczsezsrwlHZcpDyximRh46OJNgyuYcK6sWDkF/596mfKXYiEeKehyBsuT3l1zekhHF0mP1e3162BpLOobVcJxx1uHXu5gQc7NC7jUoh6Ob48fXLGOHo3fZBDHoCKHek9hlodBxx4UiLuOt+yBrhiPZfBG3/AGRF4gd/euhQOzCmeeghSBAad6Zzmpx5QFBWcGFoYkokOtPvmsoiS5WGYgeaKwrQXFmI5opC1BXaIQrpcxDfu6meEY7euTyAfrcflU7+979Y4amxHE46CI0CGuEDYh4gJdkKvvwGgGg0XMULjLA5Eq7HH0dcKMXE83Es8wBGT7D296INDRCk1LIcDMLRhW5QdfqwV7WnJfG6Y7+H8tK/gSoyM27oppZimVoUQgg26spxtA6kpcxIYIIRTOySFY4sPBVuyq9hli8lcPtki3OuxK65aLlaphnSB2OnMd8oikU0oUJ33B7vcNxt1fefYm9IAIg3fwlkibfTTjfebrbrpr3OKI4TQkDyyyCuvgbSDZ+D6dPfBWnczm7kn4D82N9B7Y7/WWstL2K6X4bdPgRHJuJumwpdz7HfSyU1rHPQ3KBr1hD0QyEEslUrHBFYhOyU1gjrb4ot+PyggWCcraYXbKkchvzmzyA/9z2jaJRfBum+b0O86o5ZiXInRi5NNQ8AAKtoiiuq1zsqQRA7/lBg3ODwicfBTtZlqC1Ti7K5uggNxbHsM4VS/P5MciXpWoyOI16qpkWoaDT8HSsH90WE4iRR9WVqy7ekVBbJmf883dKFu37xFv7kqSP4j4Ot2N8+mLRoZDOJ2FZbgs9ua8T3bt+K5//oJjz8wDX4qxvX47a1tVhe7EiraAQAV9UUM58fKo24FjmLFy4ccTjpIE4wdrIXksTkBEp04YYjB0HDEddM8HI7/Mc+SLi/+5VXoQaMgauGMrVNqbs8rKUFsJTEBDAlEIKna3CaPQDadXra9er5dyA/88+gociFphKvm9osytSirC9qYC6yB/xjGPDFL/1YSmQzGFuLvqSi3d0POYlMl0wQVMIG4aopj82LmU5YShf6YOx0l6lF0Zer9cQpV6OjV6C8z5bBCutvhlCzNiNzWqpQSuHtZl9/e930XTGBSF6PdPtfQlh3I7si5If81LehXjpi3IcQOPSuo0tzE2y9V4Ywqu3SSQhMHlaIlIp1YkHQh4C+TE0ojJvhlwmE1XsAMSYM0ZF4rqPEwhF19UPe9w2oJ140rCON22H69PcgVK2a1dwUVcGRofPM2FWlq+J2ucyTzKjU5dG1z+A66nZ50eXyTi1LAsH2ulLDdoQQ3K3rpPbs2R6ElCS6AWrgjqOZEa++n1mmg5dALx9Nal9KKe+mtsh55Nhl/OCdc0x56XTUFdrw4dXV+N/Xr8P/3L8bz//RXvzgY9vwhzuasGNZKZyWzD/4IYTg3o31zNhzZ3vgCyf3M3AWHrxUjcNJB4Z8o+mDsQ2U7gbGjgHK5BNNNQwMvQ1U347xJ9ibOs/VGyGPjKLgYg8IANXrheett5H/4Q8x2801GDtKwZplGHwv9mR7/FwnnMsTl+Hpn4KL1z4I5YNnAV+sTIp2noD8+Dch3fV1tMsepkwtT7RgmWPmG6pE5JttaHBW4rI79js5NXoJt9i2TbPX4idXwlFFXhEcprypJ9JhVUa3dxANzhT/RtLAxfEeKJr26AUBH649/Aradt46NdY50QvXz78Em2gCMecBZhtgzgMx2wBLXuzfk+uIOQ+w2IzjUuKLtkG94yjNwdhRah3lTClMt5cVfSmlkRI1rQPQVgBxz0MZmc9SJjzhheyJuTJEq5kR5aeDCCLEW78C5DmhHvt9bIUShvzsdyHe8mWIzTcx+zhXVMHVEmum4L7ch7KdsxcDu3VlalW7VkIZign+Ymk+iBRzT1BVAcJBBBX2AUo2ytSiEKsdwsqroZ5/JzKnkVHQ6iqcG6Tom4jMlRAfCPqhfc5DQIC+81BOvgwiy4BQP/UoghABYvONEBq3g/T7AeIHAUAIUO7Iw/Iie1IPjc6MdcIdjjmYJCJi+zQl1Q3OSvRpSn473P3YUNyQcHu922hTdRHs5viX/LesrsJPDrVO3bCO+UN4O8XW2nrHkYM7jgwI5Q0gTTtB22IB88rBfSArts34nqF9rYBbI9RKZggrlvY1zWLi4aOX8PMjbQnXWyURa8vz0VxZiPWVhVhXUYjCvPSWt8+WW1ZV4aeHWqfCuT0hGS+d78XdG2b/AJgzf+HCEYeTDvSOoyTyjbQQ0QJafj3Qp3myOXYcYdoEzzv7p4Za9q7HmZuaAQBr3jmHTS+fABAJyXZ+6Fbm4mOuwdhRClbXscLR+W7UfiR+XT2dGATGNa+FIEHY/FEIK3ch/OQ/MOvoQBvC+76B83s+wRxjdeHsytS0bCxpZISjlrEO3Fi9xZAdsZTIlXBECEGjsxonR2Pvx7bx3pwIR3o30eqBLpT43Cj1jGPYERFvKBHQZrdjY18H08o2pRhKIkTap3/4qyAmY4aK3nFUnqHfRa2ddRj0+8cQVmWYhMhXv3r2LdCeM8w24vWfA7E6wEkvereRrbYspfImQgik6/4ASl4BlHcfia2gKpRXfgz43RC33Tk17Gww5hzNhW5dmVrdthrgYkw4su/WdUsMRoSEXARjaxHW72WEo38/oGDfSa2bpnPyvziQ64F4+u8FABeOx1kB3La2Bn91Q/O0v1tKKQ4NnmXGNpU0Tput1uCsxIGB2N9qh7t/2tyqQ52Ju6npsZkkfHh1NZ7QlJg8ebo7aeFIVhWm5I6AII83pIiLuOt+yBrhiA61g146AtI0fVaRwW20YlvkoQVnwfOL99vw3++z1+tWScS1DeVorizA+soiNJY4IAnzs0jIIom4a30dHj4ai4V4/FQn7lpfZ8hU4yx85ue7kMNZQFBK45aqpUzRVYBZa0enUNueASYzhRRRwPnr1k2tPX/dWgzVR24MQ+0dCJyJXYiGvX6Mt2rKvwhB8cbpW3EnQp9z5DqfuJRH7zYiVasi4a+FlTDd/48gujBXZbwfrbqb+bmUqUVZmV8Dq6ZEIaCE0DZxZZo9FjeUUqNwZMveDVyTrlztUg5+F76+VrS72OyOVYORv5FVQ2ypZGt57dxORlXQtsNQDvzGsMovh+DWPJ0XQFBiSa0TU7I4TTYUmrX5A+qUa4H6J6C88zCzPanfxLv0ZAhvj75MLfXwfwAQt98F8ZYvAzpBRtn/S8j7H5nKTDEGZM++VE32BdD3OiuU5DtZKTVv80p2p2hHNTvrqsq2cERqm4GCyPfxyX6qE43Sz/PnruDC0PR5UhfHezASjG1DQLCjbM20+1TbSmHSPPjwyH4MB8bjbusNyTjRy3bQnE44AiIh2VrODLhm/DmmzqcrU7NL1jk//FmsCKX1EFay0QSRrKPE70uqKlBbD7DH4Z/Ti4L/PmIUjfJMIr53x1b83S0b8YkN9Vhdlj9vRaModzYvg6TJT+oZ9+FQ5/SdXDkLk/n9TuRwFgLhCUDR2LQFE2AuSbx9AggRgYq9zJi5NAhrQ+TGz1VVCEUXbv3B7dugTir6E889NzU+duoyoAldzF9ZA5Njdk+nCtewndjGLyQOztTnGwl1G6b+TeyFkO79e5BlG6fGuorKEdSU9OSJFtTPoUwtiiSIhpbGSzkkeyLsY54ImwUJRebMiBXxWO6sZG4kxkIeQzv6TEE9I5Bf/jdceOsnUDQXX4U+D8o944BkxiqPl9mnq7AMPtPcbeDq8eehDrPCqN5tVGLNh5hBJ5w+56h7MudIeedhIOCOrRDNkG76wpw7b3HiY8g3qp2dcAQA4vqbId3+F4DImsbVo09Dee0/QFXFKBy1z95x1Pv6B1ACsc8PR3051O52ZhtLo+5hSdAHNawglM+G92ZdOCIE4vq9UEDwr2PTizPp4tmziQP2KaU4qHMbrSuqR6FlepefJIios7PfjYlyjt7vHoasxr7/6wptqCucPkR5WZEd22rZ65ankgy55cHYqSFcfR+gyWGkw52gFw8n3J5eOQv4NN8b5jyQ5VsyN0FOxqGU4udHLuIXR+OIRrdvxcYF1tK+1G7B3ib2O+exUwmcnJwFDS9V43Dmit5tZKmY/c2Xcw2QVwv4YxeeRbfVoe9H5+BaZXTiuKqLcHl7I5qOtMH73gHIwyOQSkvSVqYGAAVr2eC78QSOI0qp0XFUt55dNudBuuvrUF7+EdQL7+KCztmxEqa0PancWLwCxzT5Lpcn+uAO+eA026bZa3GidxuV5xVlVSAwiyYss5ejwxP7W7k00YsSa5KdB2cBDfmhHH0G6rFnADmECxt3M+tXD/VAXH8zxN0PoMpehLLzz2No8gk+FQS03/Hn2GQpjDgnQn7QUOT/CPlBg9F/+yIh75PraNAHeMcAZTKzi6pQ3vwZyD3fmnq9DR3VMhSMHaXOUYaWsdhNfrd3CGrXaahn32K2E6++F6RwFk5JTlIYOqrNQTgCAKFpJ6SP/w3k3//fyHtxErXlddCAB86Gu5nt51Kqps83qr91E5SeWIcnYjJBLLYA0DhPgj7IniBUMeakIBBhErInWEcR1t2A3x86hXawN2N7GiQIKAId6gAdHwCdvJmPSi7UZAWpaAS1TIouNLIu6uqKbucLyTjdH/u7fq21D1/ZvRq2OJlC3d5B9PrYkO6ry9cZtotHg7OKKcHu8PRjR7lRDEumm1o87t6wDEd7YnN77WIfvrx7FQqs04vo8RxHnMQIpcsgrN7NdElTDu0DWbkTJM71j76bmtC4E0SaH/k2nNShlOJnR9rwyDH2YabNJOK7t2/FhgUmGkW5Z1M9Xm6NOVuP9Yzi0ogbjSXZ/8znZA4uHHE4cyWguyDPm312CyEEtPIWoP1/psasyxywbSiCZ8d6AMZuVKdv2Yi6012w+EOYePFFFD/0IEZOsCF7JZtmF4wNAI7lFRDMJqihyM1wYNCF4OgELMW6m/6xXsCrscdLFpAqXfkCACKaIH7kq1BshbhkZ8WLlcdegqKYIG7+yKznG6XSVoxyayEGJx0eFBQtY+3YVdE852MvNAZ8bNlCtvKNtDTmVxuEo3g3PXOFqgrUs29FysS8EcEsIJnQWVTBbLdu+wOQqmM3bGsKl2GoP+aYOy+7saVe1wY9CdS2w5Cf/U5sPj1noLYegDjZAcfQUS0vs8KR3nF0xTuE0LHHGbsxKVkGYevHMjqPpY6vh82cmatwBEQcndI934L81LcBf8zBR9sOw+ZzQ7IA8mQHem/3EJRQGKI5tU47lFL0PM8KR5WriyFrTDWWNWtABF1nz5AfQRvrkLGIRXFvjDPNmGDHL0w7mKCyW1YAf3PNMsjP/AR0pMOwD1m5C9ItD4FYZm53Lqsq7nvkHQx7Iy+2X1bw2sU+fKy5zrDtwQHWbdSUX43yJD8DGpyssNvlGYSiKoxjUaXUkG+0cE87VgAAIABJREFUe3ly77Vd9WWodFrR7478LkOKihfOXcEntyQO4QbiBWPz7J2ZEK++D2rrQWCyRI2OdENtPTj1PRGFKjLUi2y+GO+mtnChlOK/Dl/Erz5gHZt2s4Tv3b4VzZWZvR7IJKvL8rGpuggne2MPKh8/1Ymv3bh+mr04Cw1eqsbhzJV05BtpILY6hD1sl6Xi25ZhuCS+UyZks6Dl5khJmPull0HDYYye1DuOZi8cCaKI/FWsMyheuZrazZapkZq1IHFaCwORzjQ9Wz6EgKYcyBoOos41COXNn0F+79dTT3XnwsYSNlPp5OjltBx3oTGgc7nkQjhqKmBzjrq8gwgq4QRbzw618yTkR/8Syqv/PiUaAcDF0mqomjK1Eks+KqrYDlP6bK1O9wB8uifpyUAad4DUs0HByju/iDiTgCkhM0qmHUcllnzkibGg2pAqY1jW3ugRiDd/EUTkz5EyibdH5ziaZcaRHqGiEab7vg04da3We8/i1q8WwuKYFOcphadzIOXjjxy/CF9vzIUi2a0wh9lsnbyrNgIITi1TSoGQH6EK9vM322VqUf7jYCu8NCau2GgIn297DOGHvwk61MFuLEoQb/w8pNv+d1KiEQBIgoDb1tYwY/HK1QZ8Y4xjCEjebQQApdYCps19WJVxxceKROcHxzHmj5UV2s0SNlYm97qLAsGdOrHr6TPdUNTpvzO9XDhKGVJca8gpUg7ti3Qj1EC7TgEBT2zA6mDK/TkLB0opfnrIKBo5zBK+f8fCFo2i3LuRrVB4tbUPY75ggq05CxEuHHE4cyXNwhGVZQz/9gyoomltXGTBSMidcJ9LO5owVlkIxeWC+539kYwjDXMpVQPi5BydjyccsWVq/z977x3YxnFnj7/ZRScKCfYqUqIoUr1bstwkd0uyHZfYiZPYzqUn59z5fneXS3Lfcsk3lyuJS5pjn+M0J+5FbrItyepWL1QhKYpd7A297u78/gBJ7OwCFEBSfd8/0g5mFyCwZebNe+8jzzdKhAY3e4zK/i7wI6SOtO/1sayOyWBOVjmbrRP2otM/MM4elyd6gxdeceQ02pElC4GWqITWJBkd6YIOnkH0rZ9AeOPfQAfUvvpTheXMdnVmmcqql2NyINcUJ2wpKE65k2eVJAMhBLrVXwE4GRHjG4K49zVQSlVWtVTVBhMFIUStOnLESQZuwa3gis5P9suVDFXGUenks9xGQZzF0D/wE8DJEvxZ+cAt37YiIyt2rvsmYFfreIdVGxXdvAThE2wlPstSRRn5SBA0KiLiYO8zF4I4OtY9jA8b2GDwh8UDyI4OA1EFMewogO6Bfwe/8Pa0rbxra0og36Oh36MKl1ZmG5Vk5KI0jUw/QgjKbaxyUplzpLSpLS/Nho5Pfai/tqYEBln/bk8Qe9vHD7nVrGoTA7/ifjbkfqhTFYKtsqnNXKmR/JcgKKV45tNT+MvhRKTRUszOv/RJIwBYVZ6HAlucOI6IEjaMk/mm4dKDRhxp0DAJUCEQC8ceBeEA4+QmBL7t2xE61Q3v3vgKdS81MuXALbwEmz4+eKAch8PrloACcL3+FhNkas7PgqUw/bBuORyzxq+sRqkEepZ8IzlEKqmIo1mD7Iq8dHwzhHf+EzQ68dUKi86EmXZ2JfhKC8kOCGF4ooGxbQ4EOSbHOHucO6irq0280hMA0IAbwpbnEP3T34O2HFR34A0ILb8HbZkscVKTpHKfUnVU50otHFYJklUEbsl6pk069A5c/U2ISMJYm5HTw64/95lbpVYlcTRyP8hwgl/1+XP+/lc6KKWqqmqWkpwkvScGYsuG/rM/Bilg7cH2PB63fMcGez43oZyjjndZm0zp9TUQB2UKJLsBehtLTCPkB6gdYYklSY28E+cTgiThie11TFu5NIS7xROqvlzV1dA/9F/g8idWfbTAZsbyMvY3fVc2YRoOe1GvuJ+kozYaRbmNtcIriaPdrRPLNxpFptmANZXs4tebx5MXxAA0q9pEQbKKwFVfx7SJe14ZWzCjQgRSExuardnULj1QSvGr3Q3465FWpt1q1OHndy5FTf6FGY+dC/AcwX3z2XHUm8fbERHPbTVLDecPGnGkQcNkoMw3MuaBTKJCEqUUrtfeAAAMb+qCFI4NIHqokennNEqYk+lj2vor8tAxrwxCWwus9vja52TVRgDgOEtlNdrfxlZoMlpA8pLnIrR5e5kqXybegIobvwmY2BA92nwAwuv/FzSUXG11NijtanWuNkREIUnvyw99imDsHJMDunNYxWs8zFCQeE2erglZB6kQgbj/LURf+A6koxvHciLk4Kqvg/6Rp9FcfRWojHbNMTmS5gpNlV0NAPir7gOsMsJWEtF78C2mT64587yElKsUR5k5oAB0a76Ssh1Hw8QRcfkg+OPnEW82qjPipgDEbIPu3v8NUsZaJS2ZHG75thViR12SPRMj0DWAwYOnmLacnAgshRyyZvMoWKVHwVUExMISB/AMADlzERGVxNH5VRy9faIDpwfZZ8djwk7w8mUYXg9+zdfA3/E4iHFyJO6ds1nF18enuhCIxp41e/vqmPtQrsmhItJTgTLnqCcwhKAQe5b2+0JoHIj/vQTAijSJIyAWki3H3vYBdLj8SXoDPk1xNGHwK+5jVUfDXZDqdwAAaOthJvgelkyQ4vTJRg0XDpRS/HJXA145yiqhbUYdnrhzGarzLh/SaBR3VBfDrI+PMYcCEXxyemrU5RouPDTiSIOGyWCKbWrB/QcQbYs9YCSfAPe22PF7KDsQyzRKKLBIyDezVq4jty9C1KBD8bR4tpBzEsHYo3BUswNJdx37EKTKfKOSOeMSaMqV1ypHCfRF1dA/8P8AOzvQpd0NiL78Q1DvxCxm022FTC5ERBJQ756YkuRShLKi2oWwqY2iNCMXBpmFyycEVZ9vPFBKITbsRPQPj0Hc+adYNTMFSFENdJ/7D+hu/y6IPRd1w+y5mkxtBEydXQ0AiN4E/rqHmbZ+hWUw9zwpv/JFCToxfq/wGc3wzroaXOVV5+X9r3SoKqqV5qZEGFJKMRQ6jnbvh3CFG1J6L2IwQ3fXv4CrYqsIGjM4THcegNR2JOXP3fHuHmQ4OUxfZsDKBy2459+csPZtgHOODhlFPHRmAlJaApIRJx+pKADeIUSzKiEhnmHGEQN05PxVtBwKhPH8XrZIxE0zC7GwSraQklkA3YM/Ab/g1ikhcFdOy4XTEs/tC0RFbGnsgS8aVCldV+TNntB7WvVm1T2qbaTowB6FpWx2vgOZ5vSrb1XnOVCjmNC+NY7qSMs4mjhIZiG42TcwbeLeV2NFHhp2Mu1c1dWTWpjUcH5BKcXTO+tVZentRj2euHMZZuWeu6qyFxJWo16V+fbK0dYrMl/0coRGHGnQMBkEp5Y4cr32OrMdFSoAnRXdCsVRpiGmsJiXFQGRrWIGHRbUXT8beYU8dCPc0WSCsUfhmMUqjrzN3RDDccVQOvlGEpVUk/FRpcdoVgfJUUzuh84g+tL3IQ2kT/hwhMNcJ6t+qh28cuxqKuLIcuGIIx3Ho1yxYn7a05nSvlJXA4SXvw/x/ScAT4LMDUcBdOv+EbrP/ghcQWxy6I+G0OZjJ+3jEUfA1NnVgJGBvuxa6M9gJ2PnuqIaEBu80k9+hwIPS1p1L7rlnL+3hhj8E6yo5o22oDe4B36hA92BHfBFx7cMjYLo9OBv/zuE89iqgDwvQXjr3yEqclNGQSkFdfdCPLEFwsZfoHDwz7j7B3asfNCC6csMMGckUvZVsQ2eAVDoENGzE9xYRbVzr64bxW/3nIIvEleWWvQ8vnV1FfibvwX+1r8Ff8t3oH/ov8HlTcyalgg6nsPaalZ19M7JMzjQ3wBRpop06DNQkzVNuXvKUN5DR7PiJmtTk0OpOvqgvhPBqFqpSylNoDjSiKN0wC+/F5ATQq4eSLUfQmo+wPRThmlruHhBKcVTO+vx+jF2/OAw6fHkXUtRdZmSRqO4d940JvOtccCLo92pLxJquHihEUcaNEwGSsWReeLEUaiuHiFF6GjmZ+5DKOdaDEG+akjHiCOrnqLSzg7mGq6phj/XhsLSmLJjKqxqeqsZFtlkh4oSvE1dI/8XQM+wn5uMQxy1+foQFOO5RSZej3JrPPCTWJ3Q3f9jtSTbNwjhlR9C6qxP+/PPd7KTgw5/H4bDE7e/XUo4m+KIUgqawOp1rjBjAjlHYt12CK/8ELT7lPpFYwb46x+B/uEnwc1cwUxOG9wdjD0kz5SJ7LOofKbSrhYPyo5NCgYy2MFi3jmuqAbESrPT5v0odrPkxRkhmGQPDROBIEl4Yf9p/HDjYbx+rA0BGWmRSHGUClzhRmZ7KKTO5kkGwvHg13wVxz5SnLuSAPH9JyAe3Ri79l09EI9vhrDxaUSf/waiv/sWxI9+BaluK0yms1Q95HlwlYqFCXc/iLVCnW/EnT/C+niPCx/Us/eVR5dVIifDBKI3gp99A/g5q0EMU09wrJ3NrrTX9bmxpZWtcro8rxo8mfjwuyJBzlFYEHHwDEsOX10+ceLohhn5jFrJFxHw8Sl1RlZIjECSPT8MnA4GLbw5LZDMAnCzVzNt4rbfA0J8cQ72XJBCBUmr4aKERCme2FGHNxKRRncuQ2XO5U0aAUCxw4JVFWze66tH1YVLNFx60IgjDRomCCpGgMgg22icOHHkeu01Ztu8cCGMlZXoNbArmDY9hYUSkJHLt8oRhYmPD9wkHY8jdyxCUZkOOosJ9pnsQHaiUFZWc41UVqN9zWx1GosDJJvtK4fSpjbTUQpeIb8mpgzo7vlXEKWVJuyH8Pr/hdS0L63Pnm2yoziDDS69EkKyo5KAwRBb2SdPRhyJ9TsQ/eVDiP7mEUin9yp3PydQEkddgcFxiRk63AVx0zPqHCOOB7doLfSP/gr84vUgvF61b51LYVNLYZV/Ku1qAECyS8AtWguREAxb2AyvnHNMHNGQH8In/wMAKHGz96oz/vErJWlIDy8easEL+5uwvbkPT+2ox71/3IZf7qpHlyegrqhWcvYCChIVEBBY8sMvnEFU8iXZQw1zgRMntlMceEtp6aQQtzyH6HNfRfSFb0P8+NeQ6rYBKdiBqUgRGpLgbhIQLFwOYoyTCzQaAfwuwDEXYZElMc5XMLYoUTyxna1eVuG04t554ysNpwpFdguWlbLFKJp74kSKmTdiQfbkVMClGXkM8eSK+LCzrQshIW5Hzc0wYUa2LdHuKcGo47FOYTd583i7ym6iDMbO0GxqE0JMdSQj3BQVZbmqVedVsadhYpAoxRPb61TWzkyzAU/dtQwzciZ+TV5quH8+O97a2dKHLrc6XkDDpQWNONKgYaIIK9RGhmwQPv08AQCItHcgsIeduDvuuxcA0B1gB+CZBgnZniEURbIACug5YE4muzLcVVOC4QXFKFuSD46fGk+8o5p9CLhHKqsp84240rlJBzgSldDgYh+o1ZmJSSaiM0C39h/AzVdYasQIhPd+lrZtTak6OjbUwqyUXo7oD7oYxU2mwQrTyDkqDbRB/PAXgBCOEXLvPwGpryXZoaYMNr1FpXpq8iSu9kRFAcIHT8Y+owyk8irov/QkdDd8GcSceCDmiwbRrrCpJTvXlJhKuxoA8Cs+i6HsYkhc/JFrEwSYEpBdUwlx158Bf0xxVugeBJFN+gZCbgSEiVcs1BBHRJRUlgR/RMArR9vw+Rd34DlzBnpmlIxdialUVAsI3aAQVe2ucALVXRIQQmCbXoiGHRHsetEPSVRkTPjPbh0QoxQ9p6PoGpwGb3gWOrdFMXBIgLdFgvlqhbLU0w9wRsBWhbDIHvt8BWO/faKDCYgGgL+7tiatkvSTxXpFSHZXvwGjnM7S3Crouckpcgy8TrUQsqWJJbdXTsuZNNFw15xScLJDNA36UNvNKsn8CtLfqgVjTwjEkQdu7pqkr2vV1C5+SJTiZ9tO4u0T7Bg3a4Q0mj4JIvdSxMKiLFTK/mYK4I3jV06+6OUKjTjSoGGimMJ8I9cbbzDbhhkzYF4Yq4xzxs9mwDj1ImyhEGwDx1Dgjg3iSjJEOI3sJKP2oZVY8Hd5oJ1vg/ZsAh34FNRVC+prAg31gEZ9aVmUklVWk9oVwdjj2NTaFTY1I6dHhTX59xazW3wN/MoH2RdEAdL+NxLvlAQ1mdOglymbvNEAWr29aR3jUkNvkB3kjxI2lEoQN/0W9dSJf9Tfge/rbsMZyQLh/Z+DRs69hUltV0uccyTueRm0l7V58Dd+Hfr1/wSSNX5FIqWyLd+cBacxNYn4VNrVgFhg8eDi25m2HHc/aPP+CR/zbJA66yDVfjS2bRQF5FJ2ItmpqY6mBDuae+EKRhK+JlHgZFYmPvj7z2PDPz+MxqvmwJBCxlGyPCNX5FRa923b9JitqfVQFNte8IPiLAsJvAGkZC5O7ubw8a+8eOWHbmz+jR/cys/Ce7QNGHlrzmqEvlCxUOLqAxxzAMJPSUW1sBhF7VAzGt1nUgpWdQUj+J99rL3vxsoCLCo+P2qnUVxTngenzOYliBx6BozQczosyZkau5HcrkYpcLSTJcsmY1MbRb7NjFXlrDpOab9RKo60YOyJg19+L5DI5pdVBJKbvEqthgsPiVL899YTeOckS+A6R0ijCqf1An2yCwdCCO5fwC44v3vyDPyRK6eq8eUIjTjSoGGimKJ8I2FgAL5PtjJtmfffO7Za2BNgV26LEAQ3MojODASQ4/GAEGB+VgSQqUtcOiPqZhQBrqPA4G6g92Og8y2g7UWg6Vng1M+Bkz8Grf9v0NO/Ae14FTSYWPkBJKisVt8OKkRAu9hqP+MFY6ttaiUqm5oShBDwK+4Hf8OXmXapYVdaldaMvB6zHOzfUDvUlKT35YFeRRWvUeJIqv0IQvcp/Fh/Iw5zJdjHl+GHulshDndD3PI/5/xzVdpZC0SLt1ul/pLOnIS0702mjVReBW7ezSm9h/Jcq8lMPYx2qu1qADCQmc9s5/g8ELa+AHoOVD9UiELY9AzbmFmIkhzWIqPZ1aYGbylWmLkkQo+h0nzs/OJafLfLh9/tO42hQOLfnlKalDgSJB/8wtlzwUZhq4gTDF11AtrCqwCjbBLDG0BK54Ff+SB09/8I+m/9EZ6ZX8Th14fQ1yxCEgCdxYScmTmQPHHbq3X1HKaKOA0HgJAPcMxHRHKDIn4988QMHZeeEkWiEl5p3or32vfgtZbt2N5Te9Z9nvn0FHzh+KTErOfxratnpfW+UwEdz+G2apbY7ugzYlF2Jcw6Y5K90oM8INsX4OEJxp/9Bp7D4uLsRLulDWVI9vaWXgz44yR6q49dfNGCsScOYssBN/cmVbtmU7u4IVGK//zkBN6tUyzyWgx48q5lKL8CSaNR3DizEFlmttLk+3WpFUTRcHFCI440aJgolMTRBBVH7rfeBoT4YFdXWIiMq2OllL1RL3yy3AICinIaAvTxQXi2z4csnw+ZRopyK8vk75Kc8NKzrDCLASDcD3jqgPaXQaXEgajKjCN3fQekrgZAlK2023IARz4SQaISGtxKm1rquRPcwtsBucqEShAPv5fy/gCwIJu1q51yn0FQSKwUuByQKBib+oYg7nwRDSQXPSSuwDnDZWI7VwGpbivEk1vP6ecqtDhh5uMTqJAYxRl/nASkYT+ED5+GnAhFhhO6m76Z0gDaGwmgQ0GKpHOuJeo/Wbtaf4hVYOT63YCnD6KCHJsKSAfeAoZYokt30zdQqqjGpPyONKSP1iEfjnax19lv71uJ762eixnZiScM7qiI3x9owv1/3IafbD6GU/1sDllE8iAqJQ/vd4Ubkr6mxKjiaBR9p6PQP/pL8Hc8PkYU6e/7P+BX3A+uZDaITo+Odz5l9im8aTGi9XXscW9dqPij+gF9JmApVdnUTBNQGx0bamGIzT19dXBH/En7n+hx4f16dkLyyNIZyLVeGOvU7GJ2eO3y6pHLp2aVTQUF5qwx23HfMGt5XVzshEk/NRb1xcVOlGVmjG2LEsWGE7F7izcawInhVqb/NFvi57+G1MAvvwdQWJh5zaZ20UKUKP7jk+Oqe0+2xYin71p+RZNGQIzEvnsue997/VgbROnsClINFye00gcaNEwAVBKBMJufAlNh4s7jQPT64Nn4IdOWec9nQEZyiVo8bAUxhwGwT1sIwhFQLwCvH0TwIy/qhRAUUJMJdAZ0iEqxyXUUHD4Rs3GnTvFZk0HwAP5WwDZT9ZK5MBt6mwVRbyzcLuoNIFq3j2GfudJ5SSf2Hb5+JlPFyOlRYUudbCOEA794PcTNvx1rk45tAr3qfhCjJaVjlGbkIdNghSsSC5gVqYSTrtYpsw9cTJCohL4EVjXho18DkQAO8NWqfV7mF+IGqRnilufAFcwEcU5NsLoSHOEww16I47JJR5OnE2XWmC1C3PI/sbwUGXS3fidpnpES9W6W5Cm0OJFlTG8AV51Zhh09cRvmqF3NMsEMj/6Qm9nO8cW2pQNvgc6+ASRz4lZXOehQJ8R9bNA+N2cNuNK5KImwwZTdgSFEJWHSmStXMpR5FguKsjAr145ZuXbcXl2EffUd+MWTG9A+r1IlRYpKFBsburCxoQsLirJw//xpWFWeB79CbcQTM0QatwR5o20QpFBKKh4lceRt6QYx28adjHa8t4fZLlu3AsHao/HPk22FoVxRndDdD2QuAyEkQb5RelYxQRKZaw+I3c92957A7aXLVf1FKVbFSI5pWRmqcNbzBUopTgdPI9shYNAdX23/pHEA8wsmbyEDYvfQadZ8NLg70D/MWganwqY2CkIIPjOvFE/tiI9FNpzswBeXTMfB/lOMUjTTYEWlfXwLsYbxQazZ4Fd9HuL2PwAAuJrrxy02okENiVL0eIJoGfahdciH1iE/Wod96PQEwIHApOdh0vEj/3Iw6ka2x9ri/zfqOJj1PIw6HmZd7F95nz8dasaHDawCNCfDiKfuWoZSGeF6JeOuOaX488FmREfIoi5PELvb+nFtxdmLRGi4+KCNFjVomAjCfWyVJ50dRJcaeSGH5/33QYPxCQGfmQnrjfGAxA6/MqeFBzcy+SA2ALY1ACwgAHS7jiLYvQE1y0tROxQfSJ6kdsxx6TFjejEg+ADBD4j+kX8T5Nn4mhMSR4QQOKpLMbA/vtotttYyxBEpnZv0b1UqNmY6iqE7i01NCW729RB3/xUIjqzQRwKQTmwGv3h9SvsTQjDPWcFMSmoHmy9L4mgo7IVA42o1i84IS/txiI0xNcFBrkS1z2kuB4dIMZZEOyG89zPoPvdTEN3EAt/Phhn2IoY4Ou3pwuqiRRDrd0Cq38705RavBzdtQcrHVp5r6djURpFjciDH5MDACOEzaldbmF2Z9rGCQgTeaJy04SQJzsCIokSMQtj6O+jv/n7ax1WCUilmURNlykOLA/x1XwIA2A0WOAwZY8oNiUroDgyNEXYa0kMoKqomDXfNiU/yCCGYGYnixufehCcnE3XXLcbpaxYgYlCHoh/tGsbRrmEU2MxYU+XH8nICsyE20M42zYcrXIeINKpMkuCJnIbTlPx+OwolceRrTm5HBoBAzxAG9rELFsW3L0f/P7w4tm29fhazQECDXiASBDLnA8Ckg7EPDZxirpdR1A42YWXebGQqSOB3TnaoVFvnOxBbjhZvD3qDwyjNNzDE0YenuvD1FVVTpgaqsBXg2MAZDHvZofzKaVNHHAHAbbOK8eyeRgSjsefJUCCCLae7cDLC5kktz6sGRzQjw2TBL7kTpGw+EA6AFKsXeDTEIEoUPd4gWoZ8aB0liYb9aBv2ISwkz4HzhBOr6qcCuRlGPKmRRgycFiNuqirEB/XxZ+VrR1s14ugShXaH16BhIpiCfCMpHIZnwztMm/3O9eCMMQtPRPSiL8hK80utyvyA1rH/DR1uwalHdiC7oQd2PfvQ/NjAQcy9EaT0PpCKh0EqvwVS/Y/A7B8AxXezh/QnL1MvzznSGQFdgJXncmWJ840kKuHUJGxqoyA6I7j5tzJt4uH3YgqwFDFPUV2tJzikUuZcDlDZ1IwOiFufBwD4YEAdSfzQflkXI2joQNvYque5QIWtCATxyedAyA3XYBvEzc8y/UjONPCrHkr5uJ6IH51+NvtqIucaANRMkV1tQGFTc/IG8LKwX9pyEFLzgQkdWw7p+BbQTrYUOX/Dl0FMcaVWaQY7qdRyjiaOLad74JMFfTpMelw3nbXq+Dtiak/7gAtXvbEFj320E49dU41ie+IsmB5vEH85yOGf3y7CXw9koterg1VfCoeRzepxhRtSCoy2lrPPJn9HP8RI8onTmffZ6p45S2dBF/JC8satc9Y1c9idXH2ApRTEEFMWTYY4CokR7Oo9kfA1CRS7Fa+5ghE8t5clMNZUFmBJydRk/EwEe/pi12C+MwKDLv4s9oUFbG3qSbZb2ii3FaLfpQdk99EKpxX5tqnNGcow6HDbLFZJ9JcjpxES4+eRmTeqKpdqmDi43PKYdVQj4iBKFGfcfuxo6cOfDzbjRx/X4m9e2Y1bn9uEz724A9//4DCe3dOIj05141S/Z1zS6FwiN8OEp+7WSKNEUKo/D3cNo3HAk6S3hosZ2h1Jg4aJYAryjXybt0B0xSeUxGyG/Y47xrY9kdNwRdhLtDRDOSDsBRBbmR06chpSQETjg9swS1Fm2aU3YmfTO1CCEB6wVUE+8ES4HzSa+IbumBVfTc+r0IHIM2iyikCsiQfrHb5+pmyvgdMxVWHSAb/wdjYDwNMPqXFP8h0UcBgymGBR4PIMyVYSR7mDXcBImPghrhhSkgHpIa4Ep0is1LN0dCOkxk8T9psszDoDShQlpRsPvAbI7VS8HvztfweiS71svTIUu8iSDYdhYgO5qaqu1qfMN3IUgRSxK8nC1t+BTiJvi/qHIe74I9NGyheBq2ItSSUZLGHY4UvRxqpBhbdOsOfaHTXFMChULv4OlphzFmTjvvnT8OfPX4uf3L4Ii5M5IP9TAAAgAElEQVRU/AoLHLY22vC/3y3A/9rYhI7BPMjv02FpGCHx7KSfzmKCuTB+X6aSBH978t+84132ei9dtwLB2ngwtb7UCWNF/LqllMZspY6Y2kiigkwZFUM6xNHevjqExOTXwbGhZrjCvrHtZ/ecglceiK27MIHYo+j0D6BtJDCa44DiPDYAXVl1aTLIMlrhdrNK59mF5yac+jNz2Xthy2AYHn9cObU4Z6ZmedUwJWgf9uPPB5vxbzKC6PMv7sQPPjiMZ/c24uPGbjQOeBERLwxBlAh5VhOevnsZShwaaZQIlTl2VXXLV4+2XaBPo2EymPRdnhDCAzgAoJNSuo4QUgHgJQDZAA4C+CKlNEIIMQL4I4AlAAYBPEApbZ3s+2vQcEGgIo7SI0GoKML1BhuKa7/tVvC2mASfUoqewCmExPgkhCdArjnRJdsKYDaGjpwGAAjDEfTe8xGKP7wXndG4TP5gOIhZ/iMozGBDTQlvAjUXA0HZgNbXDGQpwk/BKo7yK9nPMm41NbfSplaStk1t7PNaHOBqrod0fNNYm3RwA7iqq1OuPDLfOR2t3vhveHyoFasLF561wtulhF5FNb7c5kNj/09kU5PjZX4B/lXYDAAQPv419HkzQBxTLyueYS9mApqbeUBuSOOv/SK4nPTUQlNhUxvFVNnV+hWKtjxzFvjVX4Hwl3+KW17dvZAOvA1+xf0T+qzi1heAsEyhqDNCt+Zrqmui1Moqjjr9A5CopFlM0kRDnxv1fSxBctdsdRaJ/wxL7lhKYqQLzxFcU5GHayry0DTgxau1bdjU2K2aDFEQfNo2gE/bBnDtjBLcNb8TNlOsjyvcALPu7NelbXohgt2DY9ve5m7YK9X5ZUIogq6PDjJtJetWIPTOy2Pb1hsU1hm/CxAlwDEbABARXZCH2us5GziSGvHrjwaxv5+1ya3Kn4uTw60YHsmlG1Ud3VF2FU72uvCeokLPw8tmIO8CBWIDcbXRKK6qsKKlK66IPdbjQsuQb0rKcwuShN5hHeTfd57z3BR7KHdasbjYiUOd8Uqd7T0mzJ3hB0+4y9LureH8IiJK+P3+0/jr4VaIKagpx4PdqEe504oKpxXlWRkod1oxLSsDOo5DSBARioqxfwURYUFEMCoiLEhMe9I+stfCgoTp2VY8ft1sFCZRkWqI4f7503BYdv/Y3NiNr6+sQrZlaipNajg/mIrlge8CqAMwWp7nPwA8QSl9iRDyDIC/AfCbkX+HKaWVhJAHR/o9MAXvr0HDeQWl0qQVR/7duyF0y7ImdDo47o5bxoJiH/pCAQDxG2qeWQeeEABWAPEVV6AXUrQEw8dbx1oiXQGsDpThFX03xJFV6ohEsK3nKG4vtcJhUEx8rdNZ4sifmDiSV1ZLlTiSqIQG1+RtanLwi9czxBHtPQ3aVQ9SXJPS/lWOEph4/ZjUPiiGcdrThVmZl0cIJaVUpTjK88S2KYADPPv93ze/DK/VxgmXHVwFumBDEbxAOADhgyegu/9HIPzUrihX2ouwtfvI2HZ7Vi6iHAe9JIFMWwhu4R3j7K2GK+xDV2CQaZvsuVajCMmuc7WnTRypFEemTHCOYnDzb4F0dONYu7jvjVgYapokndR8ENKpXUwbf/WDCY+TbbTDxBvGVB1hKYqBkBt55vQrX13J2KBQjiwvzUaRQ51zpySOMkrVv8mMHBu+t2Yuvr5iJn5/6ANsPmWAJ6QmsXc0AYc6CnDPQjeunu6HJ9KMfMuKsxIztumF6Nt1fGzbmyTnqGfrEQiBuKLOUpQN5/zpaP9p3B6mIo7cfYBtFggfmzRNxqa2q/cEohKby7YirwZZRivebY+rSmuHmrE8dzae2F4n17yiLPPCBWIDwGDIjVNu9ry4rWIOGlrPMBOmd0524LFrUntWjYfjPS6EojKSTichqhscZ4/J4TNzyxjiqLPfiFnTAliYPx0Z+gtH1mm49HGy14WfbjmO1uHklRMTwWEaIYiyrJjmzBghiqzIMhtSXkjUcH6wclouiu1mdHpi2apRieLt4x348vL0cyM1XDhMaomREFICYC2A/xnZJgDWABgt6fIHAKOz4btGtjHy+o1Eu6o1XIqIDAHykvW8GdDbk/dXgFIK16uvM23W1TdAlxO3E7jDjSqbWqHFAMAGYDEAdoIS9dVDkuVWWIqyMW3pasxp7GX6tXh1qBveDp+iag+simwCX3PC/AxbZTEIz8FgJnAWsxMbUjpH1R8AzvgHVDa16RO0qY29V3YJSMVipk08uCHl/fWcDjWZ5Uzb5WRX80aDCIpxi4ROFJAZjJGNncSBXsTl1Aaew9euqsJ02Qq4RDi8qps/tk27T8VCyacYOSYH7Pr4Kp3A69CRmQeY7dDd8p20B35KZVtJRi7shvRD6+WYrF2NUoqBIFtRLc+cCQDgr/4cYJbdO8QIhG0vpPX5aCQIYYsiFypvBrhFaxP2J4SgRJFzdGSwCa6wL6XMHA2ALxzFplMs+SIPxZZjNONoFBmlyYOLLcYAbpszgH+/swtfXjmIaQnUI/4Ijz/tc+Jnm/Nwxk3hiSTPpBuFrUJRWa25K2G/jndYm1rJ2hWItrRA8scmc8bqQugLMsdep5IEeAbHQrGBiRNHw2EfDg+eZtpW5c+FgddjTlY5sozxnC4Kit8eOISGBIHY+gsUiA0Ae/rYym755ixU2Apw52xW4flhQxfCQuq5fMnwaStLSuZmRTEYdsMXTVD0YgqwqiIXTkucpJQkgjN9RizP1QKcNUwMYUHEM5+ewrfe2DsuaZRpNmBhURbunluKv7u2Bk/dtQxvP3ID3vnyGvzi7uV4/PrZuHfeNCwuzobTYtRIo4sQPEdwr4LYf/tEx5TcCzWcP0z2CfskgH8CMKqtzgbgopSOGs7PABjVQxcD6ACAkdfdI/01aLi0kEBtlM5DKnjkKCJNLEmRec89Y/+XqABPtBmusJI4MgGYC4AHUMG8ZnD4YJ8Z9w87F1aCEIKrM+chwyuvTkNQO6RHh3czgoJsQmMuBjiZXFQMqP9OALxBD9uMIuRX6kBkpaVJbjmIOTF5psycqZxANbVE4BffyWzTpv2gw4knRIkwP5sly5o83RgKXx5hfb3BIWY71+ceu9kfyL+OeW1+YRZMeh6fX8SeUx/yNRhGfBVZOvAWpNbDU/o5CSGY7mFJlebsAuhu/iaINX0FTN0we65NVm0ExO1qoxi1q6UKTzSAsIxoNnJ62PUxMouYrOCv+QLTnzbtS+t7Fne/NJZdFTsoB/7mb4CMc40pA7IPDpzCb+o24Ncn38bbbbtweKARAyG3RiQlwceN3QjKBrs5GUasTFIC3X+GDWrPKElOHI0S+joeuKo8gB/foceTdy5FRabaAnG634gffVCAZ/ecRig6/sDbNoMNNva2qBVHlFJ0vMdmxZWtX4lgbVxtZ71BoZLxDgLECFhnjDWFRPbekypxtKOnlint7jBkjCn7OMJhVX68glwkSrCljq26tnpGPpaWXrghpScSYKpEAsDKvNkghODa6flwmOKEizcsYFtzLyaLT9tY4igvK0Y0tninLoBbDh3HoaqYtVJ291kZUk+DhlRxoseFv3nlU/zlcAskxaMmJ8OIx66pxtN3LcOGR1djw6Or8fTdy/H4dbNxz7wyLCp2IkuzOF1yuKO6GBmGuHJ9OBjB5sZzc7/ScG4wYeKIELIOQB+l9OBZO6d33K8RQg4QQg7092vVXjRchAhOzqbmfo1VG1lWXAVDWXy12hdtgyhFMKxUHJlrgLGJfC7kqiPCESz4wTVj284FsYG847rrsPhDtgrNYJhHZ4Ciw/fh2OowITyQUc5+0CTV1RzVZSqbGilNXBaaUqqyqdU4Jj+ZH31PkisnOyjEw++lvH+h2YlcBSHwfP0H2N5di4h47sq1ng+obGreEauUMQMHM9jV4eVlscyVNZUFyJdlg0TA4U3zMqavsPFpUB977MlAatqH8sb9TFtLYQXI9GVJ9kiO4bAXPQrCrHqKrIfK6mpKMnQ8KCv25ZgdDNHMzVkNUjCT6SN88jyocPZzUOo5DenI+0wbt3gduLzxqxuVWhNb4TzRAE4Ot2Hjmf14rv49PH3iDbzRsgP7++vRExhiJvZXKiiNyevlWFdTAh2nHk5RShFQWdXOThyNwop8lLz6K/yXsANflDphVKhpJErw7gkdvvTSdhWJIIdtulJxpCaOho81M6HZvMmAwjWL4sQRz8F6nSLHxt0POObFnh8jUCmOuLMTR33BYZxQkC7XFcxnFhjmZE2Dc4SgONVuQVSIfxemCxyIDQD7++uZ6yPTYB2zPht4DrcqqpJNNiS7yx1gFBoEFDmZsXtGqzexFXGy6A+6YHH0g5D4LN8doNjXPjDOXho0sAgLIn69uwHffnMv2l1qldEd1cX4w4OrcN/8aVhY7ESm2ZDgKBouRVgMOqytYfP1Xq1t1RapLiFMRnG0CsCdhJBWxMKw1wB4CkAmIWR0VlkCYDS5sBNAKQCMvO5ALCSbAaX0WUrpUkrp0tzc5AMsDRouGJRKHHPqtqtw42kEjxxh2jLvu4/ZdkeOIigSRKT45FLPccg2ySfBBEA5s9/0h+aOqY6yF42s1BqNqJ6+AAUKW8XxYT3CYhjt3o2ISiN5SQnsaongmFWKghTzjc74++ET4rJ5A6dDhX1yNrVREELALVnPtEkntoAGvUn2UO8/3zmDaROoiF29x/Hb+ndxbCixXe9SgIo48sXIC3HVF3C4x8e8tmxklV7Hc/jsgnLmtQ38bASIbFUv6IGw8SlQafLSYuofhvDxb1A23A+dGD+eh+fGwqjTgTIUuzQjDzb95Gxqo1Aql1rTsKv1K/KN8kyZzDYhHPjVXwFT2dDVDemQugqiHFQUIG76TTxcGwDseeBXnj06sMiSjZn28QPSASAghNHg7sCmzkN44dRGPHnsdbzc9Ak+7T2BM/5+iFNwHlxqON7jQvNQ/BriCcG62Ym/y/CAG2IobjfT2ywwOBKHIos0goDAPlvIwXrkPrYKeV+6Gn/z1VL8qsCFldPU46IebwT//N4h/K8Pj6Dfpz4vVcRRU7fq3tbxLqs2KrppMXijHqETsYUH88Iy8LIMJypGAd8QY1MTaQQClU8ECQw8e74nwtbuo8x2rikTs7NYS8Oo6sjl5dHRyyoNvrR0+pSXoE8HQSGsstmtyJvNBM6vV5wjR7uG0TbM3ovTgZIozLIL0Otiv2mrt/ecPLv29dfDaKAozGEtlG8qiFQNGpLhWPcwvvzKbrx0pFWlMsrNMOI/1y7G99bMhc2YeiVVDZcW7plXBplhAU2DPhzuGkq+g4aLChMmjiil/0IpLaGUlgN4EMAWSulDAD4BMDoTfhjA2yP/3zCyjZHXt9BLdVam4YoFpRQIKVbz0lAcuV5n1UamOXNgqokrQASpFb7oEIYVNrUCc3aCqkd5kKuOOJ4bUx05F8bD5hxrb8ei9w+ByKr1BEUOpzx6CNSPdu8HEKQQkKEgjgLtoJJa9ZBdlQNHQXwlmFKAFM9O+PcqlRkz7MVTWrKXq1oFWGUlPoUIpNoPU95/SW4VKhLkLfmiQbzbvge/P/UhzvguPeVjb4B9COf5XCBFNTiZvRghhcVGnm20bnYx7LIBmy9K8UEFS0TQjmOQ9rMVAdMFpRTCR78Cgh7oJRGlLvY7bvKmbjkchfJcq8maGmUbMDm7mrKiWq5JPZHmCirBzbuZaRP3vgbqSX7uSYffBe1vZdp0N34NJIWQWkII7q24Fp+fcSNW5c9FWUYe+BSqqoWlKJq93djafRR/avwYPz/2Gl48vQnbu2vR6u1BRBTOeoxLHW+fYCfJV5fnJq3ilayiWsK+0U7Iq2MZOAfM2SLIiMqIM+pQVhXFT1bPwr/cmI9Ms/q73trUiy/+dSdeq22DKJuVmQuc4E3xVfuox4/IMEuwd7zL5huVrluJcONp0GCM+LeuVtjUPAOAIYd5/inVRgbOAY6Mb0vu8PWhycNe79cXLkhY5a86swynWu2Qk6xZGRweUBDe5xsHBk4hKsV/jwydCfOcrPV3WpYVC4pY9dVkVEe7FcRRgTP+/j4hOCHyfTz4osExK15ZAUtO7mnrR5c7kGAvDRpiCEVF/HJXPb7z5j50uNTnyrqamMpoRQJiXMPlhSK7BddUsKrnV4+2XaBPoyFdnIsUwX8G8Dgh5DRiGUbPj7Q/DyB7pP1xAN87B++tQcO5heABRFnwJNEDBmfy/jJEu7vh37Wbacu8717ZlheeSEyNpAzGLrAkeo/EqqOshUXMCrM+Px8F02tQtbuB6XvarYM/ShCR3OjwfQhJbwP0skktFYGA+maelRdmtl39PIhRreyglKLerbCpTUHmjByE14FfyIYAi0c+SMnmAwA84fDZ6dfj9tLlsOjUk7+e4BD+dPpjvNW6E+5IetU+LhSCQgTuaHxgRiQJOYEAdDd9A/s7WEJpWWk2Y5sy63W4Zx77G73mckAomc+0iZ++DOkMW3Y6HUhHPgCV5fhUDLJkbJM7PeJoMORhVFYEBLMcU1shb6J2NVVFNXNiBQa/6vOASaZGEcIQtv8hYV/q6oH46ctMG1d9HbjyRSl9JiBGHk2z5eO6wvl4aOZNeHze/fhC5U24vnABptsKYUiB4BWoiHZfH3b1Hsdfm7bgiWOv4g+nPsSOnmMIX+J2z0RwBSPY2sRm0yQLxQYAf8fZK6qN9Y2yJII5ZINpDktqm2oKETz6CW6tmosfrRvCmiovYxsCgEBUxNM76/GN1/egvi9GHhBCxrWrBfuG0b+3nnm9ZO1VCNXWxvY36pCxglVnwtUPZM5n7h/pBmNTSlVqo5KMXFTaixL2/6C+C4Ne9tlYOc0FrzBx5c5k0Rd04dNe1g6+PLc6YY6fUnU00ZDsQFTAkU72Xr6ohM0YnOqcowP9DWNWvEyrAKc1vhBFAbx1QlMdaUiM2hGV0StH26BUC+RZTfjvdUvwT6vnwqqpjK4Y3D+/nNne3dqPM+5LY4x9pWNKiCNK6VZK6bqR/zdTSpdTSisppfdTSsMj7aGR7cqR189eCkSDhosNqnyjfJAUVuoBwPXGm4AUH2zpp02DednSka0ogONwRWLhzOqKaslCP/MQ8cjyUngOS39yE4gib8O+fh3mfHIcJm+c9JJAcGw49qAOif3o9G8GtbKrpInsahbCTpy6TgQhJRj8dvoHmOouek6H6VNkU5ODm3czIFdZBFyQGnakvj/hsDC7Et+oWY8VebMTKi/qXO34bd072NZ9NOX8Ixr0TImlK130drATseyAF4ald4Jkl2B/B5tFsaxUrYC4Z14ZjLr4d9DvD2NblaL6F5UgfPBkyrZAOaSBdog7/si0zdCx9p0Ofz9CgrqiVDIoSZwyax6s+qm1rkzEriZKIoZCbOB6IsURABCzDfyqh5g22vgppDb296SUQtj8LCD/fkxW8Nc/cpa/YHzoOB6l1jxcnT8HD8xYjb+fdx8eqboNNxYtRpWjBGb+7EGkEii6AoPY2XMMG9p2XbJWz2TY2NCFiEy5WWg3jxvIrFQcJQvGppSq8o2MQ94xtZEchtIIiCAhP2MGHljiwvdv6U1Yfa2h34NvvL4HT+2ogz8ijEscnXl/b0w6OoLsxTORUZyL4LFYvpFlxQxwspwRGg0DATfgYC3K4TSDsU97unDGz35HNxQuSFhswh2K4Nk9jUxbvjOMnMwodvWcUPU/HxAkERvadkGU2UVNvAELcxKXl75+ej5sxjgh6w5FsaOlL2Hf8XCwYxBRmaKs2G7Gwnz29231TR1xFBGjODwY/+4JAW6tYUnQ9+rOnDWkXcOVhVBUxNM76/C3b+7DmQSKtPWzS/CHB1eN5SxquHIwvzATs3LjY0oK4PXa1LMjNVw4XLi6pRo0XIqYYL6RMDwM38ebmLbM++4ZGSBTACcREt0IixFQCpVVrTCh4ggACLo+YgfexbeUAmAf0uaFC2DJzcf8jWy+Uk9Qh95g7L38Qie6zYrw2wTEEelllUvdDRH4WtWDVGXmTKW9aEptamOfx5QBbu6NTJt08J20J61GXo/VRQvx1ep1CdUqIpWwu/cEnql7B7WDyfOPqBhF9N3/RvSZRxF9/huQmvYn7HcuQMUouk+y51leJAJ++b0YCoTROBAnegiApSXqSW+m2YA7qtnwwr+e7AN369+yHX2DED78RVrfMxWiED94EpCTbwYLsm/6psoK1pJGwKvyXJtqZRswMbvaYNgDSbbGatNbYNYlD/rk5t4Ikq/I3dr6fCxPZgRS/XbQdpZM4q97GMTiwFSCIxwKLU4sz6vGvRXX4btz78FXq9fitpJlmJ01DbazEHOnPV1oTKP63MUOiVJsUKgq7pxdAm6cipoqxVES4igsDkGgMpUgdLBlJl59N5RkIdSwA5mGWBh0mTOK793ciweXDMOiZ1UuEgVeP9aOL/xlJ5rnzWRW++XEkTLfqHTtCtBoFKETMVWhqpqaux/ImA6iZ1Uu6SiOJCphWzf7PJphL0oa3P7cnka4Q/HrgOcoaipi39mJ4VYMhs5/Rcyt3UfQr7CE3VKyFCY+8TVu1PG4bRZ7b31nAkodpU1txbRc1aJMu69vyjLIjg41ISS7B5l4A74wfy5ja/aGBWxqPDeh3BouPRzpGsKjL+/Ca7XtKpVRgc2En69fin+8YQ5TYUvDlQNCCO6fz+bYvV/XCW/48lMqX27QiCMNGtKBkjhKMd/Is+Ed0Gj8hqjLzYX1utGy6G0AhuAOxyb1foFAoPHJiJHTI8uQvNxt00vHMHwiPpAkHBk5ZhyEENjXrUP50VZkKwadx4YMYyGFHvQjqJcNesN9oNE42UDdvYAnvkIqChQDLQLc9ezEnVKKBvfUl0ZPBn7RWkCmFKKD7aBtR8bZIzmyjFbcU3EtHqq8Eflm9cTHL4TwXsce/P7URnT41KvF4qevgDaO5IX4hiBs+CmErb9L2T43GUgH3kafwiFRUDIfRGfAgQ62FsGsXHvSaiUPLCwHL5sQtw77sZeUgFt6N9OPthyElEYlO3H3X0AH2HOTX/NVEHseZijsKac9qdnVBkJuJoCagIxVM5pqpGtXU1ZUk1fxSwTC8SNB2TIMdY59xzTogbjt9+w+pXPBzV497nGnAoQQ5JgcWJQzE3dNW4Vvz74b36y5E+vKVmCBc0bCktybOg8x2S+XMg6dGWJWzXUcURGsSvjPsPeHZBXVlGoji5QJQ2Hyc0VfFIKR2GDiY8QvxwGrq3x46h4HVs9QP5MGA2H8ubgQH3/zXnizY8cdJY7EcASdHx1g+peuX4lwYyNoOAzOboJlMTvAh7uPCcUehZo4Sm7jPjHcpiJdbihckLBvfZ9blQc0b5oEszG20EFBsav3eNL3Ohdo9nRjfz+7iDInqxxzssrH3U9pVzvcNYyOBJWlkkGiVBWMfXV5LnJMDkZlGZUEdAYmX+1MohL297F/55KcKtiMRqydzZ7/bx5vv+xUhhrSQzAq4KkddXjsrf3o9ARVr989pxS/f2DVuEpNDVcGVlcWwGmJj0GDgoj36zrH2UPDxQCNONKgIR1MIBhbCgTgeY8tme34zN0gOh1ihQVbQCmFOxLLaVDb1JwJpfujGDrUhKM/UlqzeqBUHdluXAPOZMbi9w5BXs7CJ3Bo8sRXfQYcCtmwvyX+t3QcY17qbxUgCoCrvgOUUngiAbR4e7Cz9zi8jE2NVxEDUwniyAdXeRXTJh4cvyrV2VBmzccjVbfijtKrkJEw/2gYfz69CW+27oQrHPvtpI7jCYOjpcPvQXj5B6Cuqc2dkIMOd0Hc+xr6rKwVqqAwFly+X0EcJbKpjaLIbsHqynym7a+HW8Bf/TlV6Xhxx58g9bAVhRJBajsK6eAGpo2bdQ34mhiBOsPGnh/N3q6UJiFKtdE0W37CvKqpQLp2NeXEOC9JvpEcXGGVSkEn7nkV1DcIcfsfgKBMWcHrobvx6+PeH84VCCHINFoxzzkdd5RdhW/UrMejVbeByIKL3VE/Pu2deBbWxQRlKPYNM/KRZRnfvhc4w07ck2Uc+QT22OYAS7aFWwZBo/E2ndOCSPseZBrZEvScrgn/55Z5+K91i1FoVyvCOufMwJs/+DJqb74KrpbYs6xnWy0EX/xebS5wInvxTARrY/f6jGuqQHSyYgihABCJAPZq5tiCFIRI49cCAQ8Dl3jBQ5BE7OipZdrmZJUjLwFRL1GKJ3fUMaqFEocFf7OMff+Tw23nTXUUEEJ4t50NE3foM3BLydIke8RR7rRiXgF7H3g3jZDsxn4PhgJxa6JZz2NBUWyMUG5lxyNTkXNU7+qAOxontnjCYUlO7Blw95wyeS1INA54cazHBQ1XJg53DuHRl3fj9WPqBZUCmxlP3LkUj18/GxZNZaQBgJ7n8Jm57Jjq9WNtECQpyR4aLgZoxJEGDSmCCgEgKh+YcoDx7BUgPBs/hOSPD7w4mw22W28BEAQQm1T5hSBEGpOVqyqqJc03AsIuH3ytPWh9rQ6uk8oKTKyyg7NYYLtxDZydQ5h+sIl5rcGtR2hkXuI3ACG9zCbhi/elHcchcBz6M+xoyC3Gp6XVcP34emydH8DPjr2KX518Cy81bcHOHpZgmupqaonALbmT2abtRyEpqk6lfUzCYUH2DHy9Zj2uzp+TMP+o3tWOZ+vfxSft+xD46FeASpg98nn6mhB98f+D2LBzUp8p4bFHcm8EKmLQwk7W8sxZkChV5xuVjb/i97lFbN5VbbcLx/u80N3xOCAPQ5cECO8/ARpOXlWHBr0QPvwl22jLAb/ma2ObJdZcGLn4eRcQwugKsGRXIqiqqZ1DZVu6drV+ZTB2knwjJfhVDwHGjHhDNATh7Z9COrmV7bfisyBZ546QTRcFFieW5lYxbXv6TmI4nH4W1sWEAX8IOxVZNHeOE4o9Cn+HQnGUoKqaKIURFNh+didL+ujoEKLt7P1dnwRNttMAACAASURBVBeETVcGgjipI0g++IUuXFWWiz88sApfXDwdOo4lFUWDHgfvuh7PXbsER7uGE9rUCMchOBKMbb2eJWjg7gPss0E4Vq2oVhtlJs3/OzJ4mik2wBEO1xaoFUxAzL5wspclYL97bTXmZZerrsXzoTqilOKDjn3wC3KSjGD9tJVJLWpKKFVH79d3MtlZ40FpU1tWmg3DSBZWuW1qiSNKKfb21TFt85wVyBhRNhXazVhZzo6BfrWrganop+HyRyAq4OfbT+K7b+9HVwKV0WfmluL3D16NJQms8RqubNw1p3Ts/gUAPd4QdrVcepWMryRoxJEGDalCZVPLBTkLGUKjUbjfeptps69bC86kB3ACQIytccsmVokUR8kwfDRG6lCJ4kgKqiP7ulgFsnkf1UIfjK9aCpTghCs+6O3PsCJAOXRIJhxxd2NT50G80rQVz2ZZ8dR1d+OPy2/Gu3NXoPm2hQjdUQlfnmFcS8q5tKmNgiusAiliJznSocmpjkZh5PW4vnABvla9LiExIVIJe4ZO4/m5y3CssBxjU4AMxW8XCUJ8/wkIH/8mFjA7RZDqtoF2HMNAhh1UFozuMGTArDOgadCLoSC7Sj03f3wSY2aOHcsVcvK/HG4BceRBd/O32M7uHoibn0moEKKUQtj0DOCXB+cS6G57DMQUJ0d4wqFCkdOhLNOtRH/QxZSd5kBQNcXV1JRIx67Wr7SqpaA4AgBiccSqrMlA+9i8MZJdpiJLLwZcWzCfUeiJVMKmzoMX8BNNHu/VdUKUndvTsjKwoPAsFcMkCf4UFEd+oRNystkAC4wZcWKWChFwwV7oo62QQvFrmMswgA4cg93AEryucMxWZNLz+OqKmfjdZ6/G/ASfdSgnC3/71j78PgKEzXHlVMm6WL5RuK4eulwbzHNZkgPufsCRyKamDMZO/NwKi1EVwbMouxJZRquqrycUwW/3nGLarq3Iw1VlueAIh2vy5zKvxVRHU1uGXomjQ00qsnhl/uyk2UyJsLqyAFZFSLaSmEyGT1vZSdVKWfnyCgVx1BMYQjCNIgNKtPv60BNkf9fluewzVplTUtfnTktBpeHSxqHOQTzy0m68dVyd1VVkN+Opu5bh76+bDYteUxlpUCPTbMAtVey479Xa1gvzYTSkBI040qAhVaiIo7MHY/u2boM4GFdNEKMBjvXrADQCiJFFIhXhHZGCU5pORTVg8EjcItT6ah38nUrVB6s6MpSWwrxwIUyBMOZuZlVBHX4dDvQbsL3HiFcGnXhamI4XxRJsjGZif38DmrxdcJsssZIqaaDIko0qR8nZO04BeMVEWqrfCeobStI7fWQarbi7/Bp8ofImFJjVE6OA0YSPqpfgz0tvROfyu6D/0hMgCgsdAEjHN0H46z+DDk6+hLE890ZpUxvNaFLa1BYXO6FLULFJCaXqaFdrP1qHfOBmrgQ3/1bmNalhF6Tjm1XHkE5+AnqaVTVwyz4DrmSOqq/Szng24khpUyu3FcCiO3v1r8kgVbtaUIjAE5UHHhNkG+2qfsnAzbsZJLciyasE/M3fBOEvvsF4LGR+EdMWC8q+NLMLRImq8nXumlN6VntgaMANKRLPNTM4MqC3WVT9VPlGouI3dfcDoIAoAMPs9aBz+pFpmM60eaNtEKT4+VjutOIXdy/D91bPhSmgPk9Pzp+JN/71K2heUgPOqEfRTYsRamgAjURgvYElCWjAA8AAZJSrjpNqMPb+/noEhDhprud0WJWvvhd4QhH819aTTCC2Ucfhb6+Jf6bqzDKV6mhnz7lTHQ2GPCoStNCSjWsK5iXZIzGMOh63VrH3undOnv1ZMOAPo76fteOtkBFHVr2ZyVGjoGibRHW1vf2s2mimvRjZipy2JSXZuLaCJc1+u/cUhgJTtzCi4eJEbfcw/uGdg+jxqlVG984rwwsPXI1FxckXPjVoAID7FORzbbcL9X3ndgFAw8ShEUcaNKSKNIOxqSTB9drrTJvt5lvAO/wA4llJ3ogfdGTF2RslEGXB2GbeCLtePdkYxZCMOKISxdARpeqnF8lUR5V7G2HvZRURZwI6DIV5RKWJZaYYOT2KLNmYl1WBGwoX4P7p1+OLM29OaPE6FyDTlwIO2e8iCRCPfDDl71NqzcMjVbdibdkKWBPYE/ptmXgpQ4cNPUdB7ng8FnismOTTwQ5E//JPEI9vnlSgqLj9D0AoRkImI472tbPKh1TL3y4udqI6lyU7XjrSCgDgr38EJId94Itbn4c0ECdzqKsH4ifPM31I3gzwKz+b8P2UOUc9wSH4oupBKRBTMtW5WGL0XNrURpGqXW1AYVPLNtmh43hVv2QgHA9+zVcSvsYtvA1cYVXC1y4GzM0qR0kGa2HZ1HkQwhRVeTqf2Nvejz5fnHAx6jjVpD8RlBXVLAkqqlFKVcSRzZ7BdhqOP3fIcCekoMwipedh8HfDwMmvUQmeCJs5RgjBHTXF+OoH21G5h10wAICQPQPbHl2Prd97GP0CRWgk30hdTa0PcMxLSJqlQhwFhJDK+rQsd9aY9QkAoqKEV4+24XMv7sC25l6m7xcWT0eBLd6XEKIibU662hgV4lRBpBLead+NqOwc1nM63Dlt5YSeb+sUdrWDZ4Zwxj1+SPZehV2xOs+ObEXOltKu1jpBu9pAyK0i7q/Kq0nY97FrqmGW5WD5wgJ+vbshYV8Nlw+e3dOosiUW2814+u5l+O61NTBrKiMNKWB6tk1V4fe12rYkvTVcaGjEkQYNqSKYXjB2YO8+RM/IJpQcB8dnb0VMbRSHS5b1kHYw9hE2q4g3lwKQE00UStWRZfky6HJzwUkUi9+ZgIWEUtiDfuQ298Py4nHY/99OOL/yHh70V+Pv592Hh6tuxbppK7Eyfw4q7cXgzhNpBIxMthevZdqk2o9Ao8kDjCf8XoRgnqMMXz5xCCta66AT1ZPik642bOupBb/wduge/HcgU6FSEyIQP/41xI1Pg0YSEyTjQWo/xuTe9NnUxFEwKuBYNzupWz5OMLYchBB8fjGrevnoVBf6fCEQnQG6tY8DcoWPEIH43s9Ao2FQSYSw8SlA/t3rjNDd/l0QPnGp8Qy9SaWwS6Y66gu5MCSzeHKEw8xzbFMbhVJ1lMiu1jfBfCM5uKJqcLNvYBut2bEMpIsYhBDcUrKUCcp2RXzY03fpBWUrLRg3VhbCZkp8/sqhyjdKUFEtJA4wgdIceIZEoUEfIM8OoxLIMPt5eLsfDqParpaIjM4vzsG1f/4Atz/5Vzh61PlhTblOPPzyLrzUMgyuPBeG8vh9glIKuAeATHXlM0ppSsTR7t4TiMgszSbeMEZGUEqxo7kXX3ppF36xqx7eMLsIUmw348GF5apjVjtKVdfWuVAd7ew5hu4Aq169uXgJnGmoCOWYkW3DnHxlSPb4qrzd49jURlFhY58xE805UhJ8hZZsFRk8inybGY8ur2TaPjrVjUOdZ8+o03BponXIh1rFuOL++dPwwgOrsLBIUxlpSA9Ky+vm0z1oHry0sxEvV2jEkQYNKYCKESCiGASZ8hN3RmwQrFIb3XID9NndQDwBBxFRRFCITwyG07CpiZEoXCdZUsi5YAaAckVPVnVEeB72tXcAAPJb+jDtcAsSgScUWToBNcSLa/hhrG08ii/u24THtr+Nr+7ZiAXv9cD+s72wvN4Aw6EeRE52X5DqTkpwc9YA8ryMsA/SiS3n5L3EPa9A392AVS0n8ejejzCrV203ODx4GiExAi5vOvQP/Re46utUfaT67Yi++I+QFDk244EKYQibn4kfA8CASnHkxJGuYURlq4JFdjOKHclVbEpcW5HP9BckilePxs474iwBv+ar7OcaOgNx6+8g7n0NtJvNJ+GvfwTEOX4J88oU7WpKsqbCVgCzLrVw2slCqWyK2dVYa4Yy3yiVimqJwF/7JWD0O9OboLvl2yAGdcWsiw355qz/n73zDG/jOtP2PTPoAAH2TooU1TtVLVuy3OQqtxQ7m8R24sSb4vWm7iabuuvd7CbZ9E1248RO8qW5xr13WbJk9d6pwt5JACQ6MPP9AEngDMAqqiW4r4uXrcHMAAQxg3Oe87zPy+J8sQPf5vaDQx0ILwRavQG26Nx6N48hFBtIzTcqS51090dEp5pNMov3UHc7KfS2owUSiw2SLOFUI5Ak0oXUXoKx1IDRrOq4qFBc18jN3/0dtc9vRI7oOrhFVR7OqeGbC27iQFtSYHN/L5gKkMyponNY9aCSKCmTMWKQROeUJ+xjZ5e4aHJx0VwsiokjnV4+98w2vv7ybpo9qSH7xVkW/vP6xZgNqY69uOtIzDo65K5Puf5Oh8b+Dja1HxC2zXCVsyB36jBHjI2b5oquo5cONxMZJiQ7HFPZris5vjiNcFRhLxQcUO5wP73jvOb6IwEO9J4Stq0onD3i9/sH5ldSkyfmVP1o/aExh35nuLB4VldaubA0h/tWzcJiHLurNkOGQVZMyaciOzHOjKkaX31xF72ZktfzjoxwlCHDWAjpBvCmXCRl+CwV37ubCB0+nNggQd4nVgKi88UTFie63rB4zpGCsd0H61GTBv32ikIseS6gkNFcR1nXXI1kij/38ie3UPv8DhZFXawqmsLKwiBXlwVYVxFgTWmI68xdrJK7me1vo9DnwajGwOzAUCG2gvYcHj4k+GwiGS3IC64WtsV2Po82yWUyatMB1K1PDv3bGfJzY0jjo9OuEtrBR9Qoe7vjgpBksqJc+48oV98rOnUA3K1EH/kXYrtfHFPpWmzLX8CdWE12W7OIKIlBm1Uxk2W0ppapjdFtNIgiSykr/c8ebKRvIHtEmXs58uw1wuPq/tdR33tc2CZNXYY8f+2oz6fPOTrZ10pM97fTNI2DvfoyNXHF6kySvlxNHEh36splCnTZIGNFsrkwfui/MNz6DYwf/SHylFTHx/nKpcULhMypqBbjjZad5/AVjY/nDzYJPRJnFDiZVTg2h8lYHEc+fZmaNSG2aKoaLw0DcM0DafDa1qBLvNeabCEcRtFpMhiSnYxjamIfJRpj0cubuOW/fkfx0dR7d33MymefjPKj9VH6Q9pAmVrqZ0/VorT41gvbzEpOisiwsW0fMS0hImQZbVRaKvnOG/v4+8c3s7tFdC8A2IwKf3/RdP7wd6uozk0Nzx5kpquCQp3raLI6rAVjYZ6r3yxscxisXFex4rQXSi6vKcaR1Jq8NxDm3VPpQ7J3N/cQiCbug3k2M9MLUj+LJsVAmV28x5/qa03ZbyR2dB0V/lbZJjszR8kpNCgyX7x0jrCtwe3jkWEWpjJcuISiMV45Ii7o3DTn7ORYZvjrRJYk7lxSI2xr6wvw9Zd3E4peeCXuf81khKMMGcZCYOz5RmowSPeDDwrbCr70fmSbWIqkaaV4wokBnaqBOywKBsUjCEfJ+UYAubWDVnGJ0VxHitOJfU3c+SKrGjM2H2XRM1tYVbySCnsWNoMWz8CWJHoc8QG7lJdo8y5VzMU1S3RdeI6cftDzZKEsug6SO9552tGOb5u082tBH9GXf0ZyNyTsORjWfpYKRyFL88X8mR1dR1EHBuKSJKHMvQLjh7+HlKfL5IlFib31ENHn/xstOPwqsdrVgLr9aWFb51zRyVRkjU/e9MHYy8aYb5TMtTNLybUmRM5AJMYzBxJ/b+WKeyClLXzSe2PLxrD2M2OaaBVbc4WuXGE1SqNPdE+0B3pxhxPvjyLJTHeN7GSabEYqV9M0LbWj2gRK1QaRzHbkqlqk7JHLY883LAYTl5UsErYd9TRxYpTQ8/OBSEzlhUP6UOzyMYsF/ibxM6vvqBZVgwRiOnEpOc+uryceiC2boeQ6yL8k8Zi3ayCoOkG2ol+EOIGqRYRtWVNTGzq4Onq49meP8HfuXpxmsQRPA54+oHLHwxHePq6hOUVhQNM02vwbU9xNLrNYttQV9LCvJyEgRGPg7Srlzkc28cqRFvQyuSzFnV0Pf2Q1H108Na3TKJl0WUeH3A2T4jp6tWk7noiYPbRuykWTEsJvMSpcpesopA9iH2Rzvb5MLR95mM/i6ZSrhWORFGfYsoJZYyo5n1+Sw7rZ4n349ztO0JLGRZbhwuXt4+1CKanLYmRNzYX13ZTh/OPqGSUpAuT+Njfff+vAaeWAZphcMsJRhgxjYRwd1dyPP0GsM+HysC6biuMyvRvCSSCWRURN1PD2RQxCy2eH0UrWCMHY3btE4ShvYbJaP7rryLVOzAIK7NxFpLmFfIs40XPbbERkGTlJOJIr5qUKR+eJ4whAcuQiz1olbIvteHZSzq1pGrE3HoA+0cljuOY+JGt8BXhR3rSUcgF9yZWUV4Hhw99N68LR6rYQ+dOXUVtSXQOaphJ7/ZeQ7MKx59BVJZZrFNlyaOsL0OBOTHoUWWLxBLqcmA1KSueLJ/bWD60ESSYrhuu/CMNkFxmuvhfJNjbHjSRJKa6jOq+Y/aEPxZ6aVYIlTUj5mWSkcjVvxE9ITeqoJRtwmXShx3+F+CItnPQ+zam+5whG49fHgtyplOpKbl+9AIKyN57soCeQaGVuMypcOX30TpqD+Jr04diiYOvTlamZZRPGZLF7sEwtbzmSYoX8VWBKeh/bTwnHOywyBikhuKpE8IbF0ldnTfpQbwn4wJUL+eOHV7E6jTul2w/fPlDD1149QntSB6We0H48uiBuh7GCbJMYory+dQ8aGpoGje1mNu7K5eUDvYSiqWVMKyrz+e3tl/ClNXPIsY1dnJnhKk9xHW08TdfRgd5TKSVbywpmpggzp4N+orStsZsWryi0aJrGphThKH3eEKQGZNf3tw8tXIzGnp4TBGOJz71FMbEgt2aEI0Q+tXIGrqQMsHBM5ccbDmUmfn9FPHtAXCS8dmYZpjF0ac2QYSQkSeLzq2ezpFwco752rJXf7xh7jEOGM0vmSs+QYSwEdYNpa/rVlUhrq5BtZChyUvTVG3Ud7I3APDwhMdg6EBUnVyXW4fONAHr2iMfnLkpe5R3ddWSeNg3zLLHdsvf5F3CaajDKCZEISaLb4UByWmHAVi9XzMc1o5zkX6zvZBvRYJjzBXnxjcK/tdYjqLrMnYmgHlqPevTdlOdKLiGyGy3M0ZVObe9MFYEkgxnDVZ9Guf6LoM+t8XYSfewbxLY9jZY06Ff3vY7WKp7LcNknaA+LDqUiaw7bGkVxa25RNnbTxDqd3DyvAltSfkFPIMzLSXZ1ubAa5dK7Uo6TF12HXL14XM+lF46OexPXX7ybmihSzs45e2Vqg4xUrtaRxm10PuR/nUmiaoDG/lcJxroIRNup73+JUMw9FJSdTG+oj22dh4c50/nB07rJ0dUzS7GNo0uQvqua3nHUHxXP7zAlhH4tEob+HpBNkHsRAJJsgNJ1iQP8HrS+RFizJEm4jEn3bVLL1azFuSiWVIHVWpRD/rKZOCWVT2x9jh9cGaMsjc67qb6TOx9+l0d3n8ITaqQjsFV43CS7KLVfLnzWm31dHPU00eU28O4eF/uPOwiEU6+F6lwHP1i3hP9et2TEsrThSOc6OnwariNP2McrjaJLtcCSneKgO12m5TuZXSi+2S/oQrLre320ehOCnVGWWFIx/Pig2JojCOnBWJg2XbB3OlRNTbkuF+dPx6SM/XPvspj47MViGfuWhq6UDnkZLkxO9vSzr028pvRZXRkyTBSDInP/1YuozBYX2h7aWsdbdRML+s8wuWSEowwZRkFTYxDS5Q4MU6rW/asHIRq38EomA0XfuhXZkmyzl4C5qJqCNyLW/nsjortopHwjTdNSOqrlLtKvCo7BdXSj6Drqe/U1wqfqybOIWRYeu52oLCPlOsCWDbnlGGwWHFOSAsI1De+x9Db7c4FcUIVUKf4ep+s60txtxN78tbBNKqhK2+VqaYE4eD7V3z7sJEaZeQnGj/wAqVD3N9RUYhv/QPSp76D5PWj9vcQ2/kF8/uolMG0F7QExI6TImsPWBrFMbfkIk43RyDIbuUkXDPzI7lNCO1554bXIMxMlNVLhVJTVd4z7uaqzSoTSiJ6Ql96BDmqtgR48SZ0IDZLCNOfZLVMbZLhytc7g5ARjD6JpGt3+UErr4/OJ7uBeNBLlC6oWorH/VaJqkBJbHrV5YvnSu+378YbPzxKW+t5+djWLE+2xhmJDPJ/I36wLxy5POEQ0TU1xHDmS3aWD2Ua5K5AMSe3n7VMguzaxX/spwcmRbRHF50CsQ+h2JskyjurU767y61cgyTLBQ4ewLSxn+Uwrv7vdyB1LZAy6UWIgGuMXm45w75N7qO9JCAqyZKLcsRZFSggWmqbxVN0uth/KYttBF33+VAEix2riy2vm8NBtK1k+gTLaZGa4yimyit3cNrbtG/d5VE3lufpNgmtQkWRunnIxBnnyw39v1LmOXjjcRDQpVFpfplZbljuiiClLMlUOsXnHWMrVjrgbhXurIskpZddj4dqZpSwoEf8O/7PxMP5wdJgjMlwo6N1GtaU5VGT/9btpM5w9sixGvntDbUrp9Hfe2Mehds8wR2U4W2SEowwZRiPUCck2b0MWkiH1i9K/dRv+rYkV2LxPXYa5Sj9Rnwrk0B+pR9US7hxFstAdEt06I+Ub9Z9qI+JNDPBMLjuOKv2EYHTXkf2SS1CyE5NaLRSi7d/ux+HLF7riaANZR3J+FnLl/KEVZdcscTLlOXz+5BwBKEt0rqO6LWieia18arFoant5xYThus8jGVJLtIptuSnti7d3De94krKLMdz+HeTadSmPafW7ifzxS0Rf+rHYnttowXDFPfhiQaGrl0FScBrt7NC1Q142zmBsPR9cMAWDnOQm8PjZcDLxfkqShHLd5+M/V/w9htv+A2kCWSBmxUiF7r2rGyj1O6QLxa5xlmIepkTuTDNcudpk5huFojG+8sJObv3d29z2h3fY0pDaLetcE1UD9IYOpWyPqF6a+l9D1aJcWrJQcEFE1BhvnqdB2fqcmfnF2dTkZQ2zdyqBDrfQuMCU7cDoSIg6wVgXMS1xvcqSjFVJlJnhbo+7jfIuSj150VWgDNybQ76EyASYFCM2g7gAoXcdpcs5qli3Mv669u7DsSbuQjUbJO5ZYeDBa9zMz00VLBt6jfzXq0U8uiObYESizH45ZiXxOXcHwtz/xnYefzdGZ2+qy8mkyNyxeCp//shqbppbgUE+/eFoWteRp5GOQGrw9ki813EoJVft8tJaCk5TAB6OK6YXi25Of1goTUvJN6oavkxtkCpdOd2p/pGFI03T2NIpXsPzcqqxG8ffwVGSJL60Zg5K0ndFpy/Eb7bVjXBUhvOdYCRNKPY4BPUMGcZKucvOf1y7SBhvhmMq//LSTqFcOsPZJyMcZcgwGin5RqkrtlokQtevE04U8/QinNcu0O1VAMS/ZN0hcQBlN0xNmWyO5DhKdRtNG6YUZmTXkWQ0knPnR4UjYp1dtP/bd8g1iEGovTYbsXwXUsXcoW2umedvzhGANGWRGECtqcR2vTChc6VvL38XUt7wA6dlOtfR/p6TBKLDtxeVDEYMl30cw01fBYuuXMPXi9YktoRWLv4QkrOAdr84MSq0ZnO0s59+XYDljDRdeMZDgcPC1TPEMrI/7zwpuB4kSUaZtRpl4TVIxokHyE5LKlfzBWV+9W499z21lTfqxEm93vVzNhmuXE3vODqdCeefdp7kvYHOeJ2+IP/8/E4e3HLsvHIfDbqNDraa+fn6fH79bh4nu+NiQSDWTqvvHayKictKRAfgIXcDp8YR3Hs2CEVjvHRYLBUaj9sIRu+o1q/rpuYw2Ibu35q/Ly4O5y4X3EaDSAYrlFyT2NBRH+/ANkC2SbxveMJ1aFoiTyqrWrx+ZZOR0rVLAAgeOYDtItH1WE0rP7txLl9eM0foAAagaRJvHs3i/pcq2d0Uv9bDMZWHd53kw3/awBtHe9BI/V66anoJf/zwKu65aPqES2eHY7qzLI3raOxZR63+bja07hW2VWeVTMh5M1ZsRgNrdffVQfGyLxhhX6t4Pxkp32iQal3OUZOvi3BseMdPo6+DVl052/LCWcPsPTrVuQ5uX1glbPvL3gbqurzpD8hw3vPW8Tb6w+KYYvXUohGOyJBh4iwqy+XLa+YK23r8Yf7lxV34Ixn34rkiIxxlyDAaeuHImrpi637qaaItiRyWnLtW6fawArMAiajqxxcVJ79htQg1qbeMy2QXWrrr6d4ldj3JXThceOXoriPnNdfgXCe6XMJ1dYT/53kUEq9Bk2XcednIxQlbfYrj6DzqrAbxlU9Z5zpS97+BFvQNc0R61OZDqFv/ImyTpi5FXnDNMEfEmeEqx5lUghLVYuzpOT7CEXHkmmUYP/pDpNLhB+5S4VTkRdcDDFOmJpbKLC3PE1aAJ8qHFlUJ/z7c6WVn8+j5GeOlxlmKpkFDm5l3d2dzpEVjT2sv7x400e+Pr87Hy9TSB/6eLfTC1cHeerqD4uRooo6jRrePP+vaWWvEOxV98bntdPuHFyHPFlE1QJvvMH/elsNP3y5kX4uV7Q02vvdqIX/eno0/LOGNnKAzuIOFeTUUW0VB/NWm7ULr77NBLByh+dXtnHx8PTGd0/OtujahY5DTbGRNzfgmR/4mXZmaPt9IJxwJ3dTc7WgY0ruNhl7UXHAM3PMjIehNfPdkmexCmWdMC9IXSQj6esdRyeWLMDqsqH4/hnyQk8oDtHAQokZkWzk3za3gJ7c6WVaZeu/s9ql87aVdfOWFndzx8Eb+b/NRYYI5yMxCB798/wq+tXYBxVnjd7KMBUmSWK1zHR3xNKbcI9MRjkV5pn6T8F1sVcysq7zojGeU6cvVtjZ00eoNsLWxS2iaUZVjp9Q5fNOMQbLNDkFEVDWVRl/HsPtv6RDdRtOcZYIoPhHuWjqV4qzEGCKmafxw/UHUTFD2BcmzB8Rx6/WzMqHYGc4s188u4+9qq4Rtdd19/Ptre8+rxbO/JTJXfIYMo6EPxtY5jqKdnbgfeTTxh3A9TwAAIABJREFU8IIKbLX6sN4ZQHxl1RM+TnKrcrOSQ3dIHGSX2MYbjD1S15PRs47y/v6TWJeKAbaBje9h2yWWOvXa7cRIbMs+jzurDSLPXB3PZRokEkTd99qYj9dCvniJWvLk1paNYe1nR51MyJLMYt1K9Y7Oo2PqcCNl5WP44P3Iy98P+lV7SUa56jNIA3kbqcJRLtsaJ7dMbZCqXAerqsWJsF7cmAy0qJk9R7I5cMJBTE38/pomcawxPumc5irDdI7K1AbRl6vV97cLE88soxWrYfwd3zRN46cbDhGOpf+s7Gru4ROPbUrJ4jnbvNe4i39/OY/1daLTRUNi/bEsvv1CCVtP2egK7MYbrksJyu4OedMGx58JevadYMsX/pdHy2/n1Wu/wtu3388Ll/wjkb6EkP6MLsPjulllo7aD16N3HNnKEtdeVPUTjInCkmOgHEhTVfB0IuWtQDIMLw5IkgQl14M04NbpbEQbcJPIkoxL5zpKLlervm2NEJA953PvAyB48OBQmdoQnk7IXoAkSXjDJ4nJe/jkJT3ct6aTPHuqMLS5vlMIcR7Eao6xrtbMr95/MXOKzky5VzLTnGUUTyDr6I2WHUNZaoPcULli6O9zJplR4GRWkiNUA1441MSmU+MvUxtE7zo6maZjHkBX0DNUCjzIisLZafcdD1ajgc+tFs9zoN3D8wfPnyzEDGPjeHcfB9pF59u6OZlQ7Axnnk9dNIPVujHnu6c6eeC90292k2H8ZISjDBlGQNM0COoycXTCUfdDv0ELJVb+cz9+qe4s2UBild0TFt1CLtN02nQT/xLryC3T05WqDc9wrqPEAF9SFIq+8k+YqsX9zC+8gxJOBISqskxvJFFmly7jKLls4nxAMhhRFl0nbIvtfgEtFhnmCJHYm78Grzh4N1zzD2NuL78wrwaDlJh4eiN+jnmaRzgigSQrGC75MIb3fVMQv+Tl70Mumjr0b71wZJcdHOoQB3nLTiMYW8/f6VxH2xq7Odo5eSUIb9a18bFHN9Hak37C3tZtxutTUkSbc4G+XE3PRN1G60+0s1Un/lkM4rXV4w/zhWe38YcdJ876Kn40pvLglkN840UvHX3Di3feoMJDm/P46VsF7G7fTLYpzILcqcI+G9v20R85M7kFod4+Dv3vMzy77DM8s/AeDv70L4S6EgGb3TuPsenTP0bTNI51eTmgC9+cSMegkTqq9etCsS2KGYM8IAD1daOFY5C/ctTnkEw5UHhZ/B+xCHQlzpttEktSfdEmImq866KtNJ91m3/Ogq99hLUv/hfl1y4HIHj0ANZa3fXk6QDXfIKxHlp864c2zysNcv8Nvdy+qAxlBPHcoKjMmuLj0loPn6hdfNY6C6bLOjrqaUop6RUedzeyu1v8Xq3Nm8Z019mbHOsn4i8ebmaLzjk6ljK1Qar1OUfDlIVu7RA7qZXYclMy5ibKJVWFKQsND7x3lN7zwC2ZYezoQ7EXl+VmQrEznBVkSeIbV81ner6YM/jI7lMZEfockBGOMmQYiXAPqEmlDIoFjIlJYmDPHnwbNg7927asGssMfVlDYpIUjHYTiiU7BCScpmm0+sUJ4kjB2MFuj7CiLRsNZM8ZrR15OtfRKWEP2Waj+NvfQslNPLfFoeLSuYh6TGGisfgKvaUwB1N2YnU76g/iazr/wnvlBddAsuujvwf16KZRj4sdegf18AbxXLXrkKtqhzkiFZvBzNycKmHbtnE6LOQpCzHe9ROUqz6D4eavoaz80NBjwVgYd7h/6N8SEg3dMZJdvNW5Dgocw5c+jpf5JTksKBEFkUd2n77ryBsMc/9re/jXV/fgDY0s7B1vtFNzjsvUBhkpZ2ki+Ub+cJT/2ShO5mYUBrl/XSszi4LCdlWDX285xlde2IknKJZdjYTmb0ZreQGt4RG0hkfRGh6L/zQ+PvDzBFrjX+I/TU+iNT018PM09Yef5bOPvsLvdzSgaqIYUGrXKHOkDi0OtVu4/8Ui/nfzZpbmVWJJcoqF1Shvtuwa82sf9XdTVZpf287bH/4PHi39IO/9w8/o3jH86uSJh9/k6K9fSCnFWFqeN6HJka9ZLxwlJuEp+Ua6MrWYPH1Et5FA3kWJhYzuZrRI/O9vMZixKGK+mDuU+P1zF9aw5D/uHhKNAGSzBykpoFoL+kApIGaw0NT/mtAxDyRqXFdx78Xz+PUHL0ppJy+hMaU4wJrFbqrLgiwuqCHPcnr5auMl7joSv0c3tqd3HfVHArzYuFXYlmvO4orSxWfs9aXjquklWJPcbV2+kHAfdJgNzCse+/2k0lGElORW7Qx6UgTa/kiA/b3ivXtFwexJFfk+t2oWlqTfqy8U5f82Z9wCFwqBSJRXj4putUwodoazidVo4L+uX0yeTfxe++E7B8+56/pvjYxwlCHDSKQJxh4KMY1G6XrgV4nHJMi95wrdCfKAxKBa7zayG8rQNGNKJspIwpHebZQ9twrFNFq5zuiuIwBDQQHF3/omktmMpIDJKeHadwI5lghXVWUZd/+W+Fkl6bzvrAYgWbOQ54p/G3XHc0Kosx7N007szV8J26T8KSirPjLu51+qC8lu9HWMuPqdDsmShTL/KuSpS4RBfYcuVD3P4mRX85lzGw3y4dpq4d9v1rXR4p14e/WtDV187NFNvH4sdVU8JyvC7GoxW6Wtx8jxrvFlVZ0pRnI+TcRx9JttdXT6krtuaXx4aS8uq8rnL+vkhrmelMjhLQ1dfOKxzexvczMSmqaidayHk7+B3h3QdxT6jkDf4fiP99DAz0HwHoj/ePaDZx+aex9PHWzjk28bOOxOHT5cXdHDQ7Xb+W3te3ysuhujLlMrqko8t9/BvU/upgDx83Og9xQN/cNnsIyFvhMt7PzWb3l86kd49ZqvcPKRt4iNIkAOsuGfH+CVQ6JwNBG3EaRxHJXHhSNNU/FFRbfhoHCkRUKo3d0YZtww5ueRJBlKbwCkeCltZ0LkzzaLq7Pu8FG0YUpkVZ8PyzxdKaunAy17Ac2+N4moYvlWse1ibMa4YDUt38n/vm8FX1ozh0WlOSydYmPVIjdzpvoxGTUMksKqonlj/p0mi3RZR0c9TbTpAqA1TeP5hs0EYknXGxI3TbkEkzK5wd2jYTMZuGpGaobiIBdV5o+r+5zVYEoZS5zUuY52dB0VMsayTXZmZk+uKFCUZeXjy8Ry+pePtLA7M+G7IHirrh1fUmZZjtWUUjqUIcOZptBh4b+ur8VsSMrwUzW+8fIuGt3nx1jwb4GMcJQhw0ik5BslBnXe518gUp8YqNtXzcBUpi9ZSbiNNE0dyDdK4DJPpz3Qi5aUiZJrzhLaVuvp2S12ZBs53yiZ0V1HAObp0yj8py9jzpGRZAk5EiWnTbTL98ROomrxCZnrAsg5AlAWryM5K0jrPInWmL7bjqbGiL70UwgnCWuKCeW6zyNNIK+m0JrNFIfoRNveNTm5Lu0BcfBdaMlOCcZePkn5RslcNKWA6tzk8FV4dPepcZ8nEInyw/UH+fLzO+jyieULRlnikpkGVszzMqU4iMshigAPbT0/2juPVK5WOE7H0fGuPv6yV7yGrp7VR4lrIMNGhpsWePnSFWFcFlEw7ugPct/TW3lsz6m0oqgW6YP6P0LnepJz1sZCV8jIP++byU/qqgmpYgmh0xLjH1e18y81ddgUFbOs8fHKOn67ZA9LClOHGR19Cr/a0M3h4zmEwolr8tWm7WPK/0om6g9S94fXeOnKL/HEtDvY8x9/xNcwjAAlSZRevZQ1f/46N+18AIM94cI7Nn86wSSbXq7NxKqqiU2OhuuqFoh2oGoJV5giyQlnkLuDYKsNSSf4jIZkLYPcAedQbxtaKH7Pchodgtskqvbji7akOwXBY/uwzNQJFt4eOkw+/Lpjsk2zyDGLuTWKLHHz3Aq+t24RFZVtOGyJv+HSghlkmcbooJpkapylKd1J9VlH27uOpIgpl5YsGLGr6ZlEH5KdzHjK1AbR5xydSso5Csei7OwSF7OWFcwSwtUniw8umCJ8X0DcLRAZJsMtw/nDswdTc9+MmVDsDOeAWYUuvn6luCDQF4ry1Rd34h2H4zrDxMlc+Rn+5omqAdyhY7T6NtId3EssaWBPINVxBBDt7aXnT39ObJcl8j61VnfmQiAxUPJFm4lpCSFCxkiWcUpKC9xxB2MP21FNz9hcRwD2lRfhujRxc3YeqkdOyi6KSSq9oXgpTWpA9vnnOAKQskuQapYJ22I7n027r7rlL2itorCjXHoncv7EM3X0rqMDvafwR4PD7D129M4lOZpFe3/ivCZFZmFpjv6w00aWpJSsoxcPN+MOjP3Le19rL3c/uiklkBhgWl4Wv/rgSm5bOAVJAkmC6ZXiZ3VLQxf7Wsfn3DpTpCtXk5DIM4+9REfVNH74zkGhi1KePcr187xkGcVy1OlF7fzk1rKU0pWYqvHzd4/wjZd305fkttH66uD4A+A7NebXM8jbnbl8fPt8tvamimCLyv1867o2Lnd2IuvEqgpbgB/O2sw3FnTjSlMpebJd5p1d2dS3mtE06Ay62aGbyKZD0zQ63jvIu5/6EY+U3saGu75L21u7h90/a2oJtf/2MT548k9c8/L3mPqhK8hbNI2LH/hi/HzAkdWLhGPWzS7HMIHJkRqL4W8RS48HHUf90dRuaoPuQbWjBalo9bifD4hnHRmdgAYdpwBQZAWnSSyzSw7JFtC5oDSfB3dWGb1hsVzSaiim2DZ8/tJ7HQcJJmXHWRQjFxXOHXb/M026rKNj3uYh11FHwM1bLeLnpsJeOCnB0BNlZoEzJcsDQJZgReX4FwBShaP2IVF5b89xgrHE/dqimFLyxyYLgyLzpTVzhG31vT4e3XPqjDxfhsmhrsvLQV3u20jiZoYMZ5rLaor5+xXThW2Nbj/ffGUP0YwQfcbJCEcZ/ubQNI1grIeuwG5OeZ/lmOdPtPrX4w4fpiOwleOex+gNHURVY6mlatb4IKznd/8PzZ8oy8m6vhZDTnLtrQS6UozkjAkAp2kqsmSgNaDLNxolGLtbV6qWN2Iwtp6xuY4AjOYkkavdTY5PtIL2BPeiatEUx5H7yPnpOAJQltwk/Fs7uROtWyxPUVsOE9vyuLBNql6CvPDa03ruac5SspMmcjFNTQljnQj6YOzWHrE8aGFpzri7Qo2VK6eXUJDk2ghFVZ7cN/rfPxxTeWDzUe57eivNui5MsgR3LJ7KAx+4iJq8LGqcCSdEvitCjlN0HT14nriO0pWr5VmcGOSxv/cvH25JKTW7fXEvWSYnZfYrcBjF51ClnfzkplpuX1iVcq4NJzv45OObOdzRi9b2OjT8GWK6UkLFDqU3QsVtUPFBKP/AwM/7ofx99BfewnfqL+PbB6fjjYruJotB5a4V3Xx6VTfZZpls56XxTl85Yi6MJMHanDr+uHQvV04LIemcTtGYzMGTDjbvc+HpV9jQuhffMEHZ/rYe9v33ozw1725euPg+jv76BSLe9BZ1xWqm5s61XPvmD3n/0d+z6Jt34KgUXX81H76SGffcQGdVCT3liccktAlPjoLtvWjRRGmvOTcLgy1+jQyXb6T5vfRtbMG6UOw4N1YkxQwlAw0AvF1ogXhpmT4kuy9ST1TVi9UaxgrxbxvwdtBuFj8rBslOuf1KJCn957kv7E/pjndR4ZwJdRScTGqySlMWYza07SOqxni2/l2hTMssG7lxysoz4rgZK5IkcdOc1FKxecXZOC3jfy/LbPkY5UTJXX80QFfQg6qpbOsUhcHF+dPPaJfKBSU5XD+rTNj2/7YfP60S5wxnlnS5b2Wuc+MgzJBhkI8sruaamWLG5a7mHn70zsERIygynD4Z4SjD3wSqFqU/0kibfxPHvY9y0vskncHtBGKpJQ0xLUibfxMnvU/QZ0xqrC0ZwZRH8NBh+l9/g8Rmhby79J3UikkWaGJqiP6IOKF2meKKearjaHjhKBoM4zlUL2zLGbPjCOKClj5IO9V1pAX6oPNU4t/hKPa6TqQk11FUC+AOHb0gMo4GkUpnIRWLKxWxnc8N/b8W8sdL1JLLZWwuDFffe9phobIksyR/hrBtpy5fYrxE1RhdQXE18Gi7ODE8E2VqgxgVmdsXiZ+nJ/c1EIiktuoe5HhXH596YjN/2nVSCPAGKHPZ+PmtK7jnoulDVninyU7hQE6QJMH0CnGSsau5hx1Novh6LkhXrlYwQrc1PZ5gmP/bLE68F5QFWFgepMi2EklSKLKuIPlrO6r58EYOcO8lM/nOdbU4zGImS6s3wL1/2cIz++pIGUvZq6HmU0g5tUjOWUjO2UiuOQM/c9ndX8rdL/Xy6qlUEWd6QZhvXtfGxVP9SBLk2Raj5C5Dyl2KVLoOKv8uLkol4VQCfL18D1+/spOK7FRXmqffwKa9LnYdN/Jyg+gCaXxxC6/f8k0eq7id7V/5FZ5Dw4uTBSvncPEDX+RDrY9z6e++Sslli4TQZz0rfnIvp24SnT7lB06iHJvYfWy4jmoR1adrjAD2gRBsrbOVSKgSyTjxSbuUNROyBpwy7fGwY6vBgklOPqeakrMX87VjLEo4XMKxME1GhPJpCYVyx1oM8vBt6d9t309USwhmDoM1xWV5LkiXdVTnbebJUxvo1N07r61Yhst07jtFXTWjRAiThomVqUHceVbpEEsuT/a1ccTThDucEF2VNN9PZ4JPr5yB05z4TIaiKj/dcDgz2TsP8UeivHpULFWdaO5bhgyTiSRJ/NNlc1OatDx/qJnH9tQPc1SGySAjHGX4qyWi+nGHjtDY/xpH3X+ksf8VekMHh9oSj0ZY66M5N5eGvDwCRiNYCkHV6Pq/Xwr7ZX9kDbIt+VKKl4S5Dzew4e7v89bt97P34f+HRmJQbcCB1VBEMBqmN9SXdKRE0QiOI/f+k2hJVkxHdQnmbMew+6enCEieAGiAeKPVmsTsn2jISPhgL9l+cdLeHdyDo6oQ2ZiYsAZauwl7xvYen20kSUJefKOwTT20Hs0Xd3nE3noQvKKYaLj6H5BsYxcARmJBbg3GJAdKXyTAEffEhbauoIckaRO7YmNvqzgZOhPB2Mmsm1NOVpJg4Q1FeP5gc8p+MVXjTztPcM8Tmznenfr5uGVeBb+5bWXarkGXliwY+v9FpbksKRevkYe21p0XEw+966jUNnbR7lfvHcMTTLipjIrK7Yt7cRin4DDGB+smxUWuWSz36AruIaL6WF1dyIMfXMnMAtFlEtEkfnSsmn8/XIM/KgMSFF6OVvR+QsebiHaJeVihaIxfvHuEzz+zTSh5BDDIEh9dauWLV7ST74jfzxTJkpJ5I2VNh2mfBoco0iqaxhqtgW+sbeWDtb2YDXrRVKK+1cov33LzlwNHUVWV9+77H15f9zUan90k3PuSsRblMO/Lt3Hrgd+w7t3/YeY9N2Byjk0A8EsSx2aJ7tCZ63fy9m33T+g+pu8qOZhvpHcbWRUzBllBU2N4XjmIbdkl436uFEquRdMM4POg9ffGmxfoMpM8oaPCtaJ6Ek5YVVNp9rYQ05XoldgvxWoY/rPcHfSmuCcvKZ4nOF3OJVOzSijVuY6Oe8UJ8dycKuboul+eK+wmA9fPTjhzTIrMZTXFIxwxMlUO8diTfa1s6TgobJuXU43DOLwwOFlkW0185mJRoNpc38mGk6cXjJ9h8nnzWBv+SGLcmmudeO5bhgyTjUmR+Y9raylxivet/910hHdPndn7SX8owjsn2vnB2wf+5rKVzo9v9QwZJoF4CVo3/ZEG+iMNBGNdox80gFnJw6Lk4Q0fFwQegIDZTH1BAVlRM7b1LxI+nhggSxYj2bfUIgbNluFv9fHi6s8R6o53S5vzsauwkFgxPPW9zWx76FG4dhZ8KSlLKGyg/2ADjilFaSc+Ew/GTmYw6+hQ0rY24k6k+A1YbRADRI21l9H3zi7y+vtx2+1oA+6bqObDq50ka1rpkAtAMsj0nTxF3qJy4tq0g/NJo5anX0TMWQDegQleLEJsz8tIuWWoh9aL+y66Hrl68loyWwwm5uVMZVd3YtV/e+cR5uToXWBjQ1+mFgu6CEYTX2IFdnNKIOlkYzMauHVeJb/fcWJo22N7TnHrvIqhjJhmj5//fGMf+9J0/Cqwm/nK5fNYPkJ+x3RXOZ+efSPucD8V9kJqs/rY0bRl6PH9bW62NnaxonJiq/KTxYrC2Rz3ttDi76bElktt/tjKSA+0uXnuoFgScP1cLwUOKLJeJGzPt9TiCR8jpsWDxDWidAZ2UGq/lFKnjZ/fsoRfvP4mT58UHXJvdORztM/BV3N85P/5VQL7fwTRKCgKuR+7C9ett3C8u49/f30fJ3tSBZPqXAdfu2ImGF8gWb7JsyxAllKdMpLBjlb5oXjntrZXQYu70EyxGFN6u1k7A5ZUBnhsZzY7G8XSh1BY5qfrT/La23XMeeQt0sVFSwaFihtXMv1j11B+7XJBvB4PLx9pIZxkfXN0eyg7dJI+TWPjJ3/A5Y99e1xuQ73jyFYW/0z6UsrU4vd3zd2F9512Ku6sndDrT0YyZkHJWmh7Ke46cuSQbcqiMylAP6T2Eox1YjUUAhqKKwgY0TSNNn8XQd33X555AS7TyN8z61v3CA6lbJODhXkT+W46Mwy6jh498Xbax11GO1eXT6xM8EzxmZUzUDWN+l4f759feVrlQfqco5N9bcLfC2B54awJn3+8XDerjBcPNQvfBz/dcJilFXnYJngdZ5h89KHY188um1DuW4YMZ4psq4nvXb+Yzzy5Zajznwbc/+pefvG+5UzLH3vG5EjEVI3DHR62NXaxrbGbg+2eoSzKJeV5XD5t4sL+hUbmDp3hgkbVovgizQNiUSNRbWy18hIKdmMpDmMlDmMFRjk+uS6wLqEzsGPAzi8OrPoMQfrm+1E+MhflmaNI/RHy//FWJGPyfgqaVsmmT//7kGhkrnbgvEic0HY+epJQt49+g7iiH375AM/8a9zRZMrJwlFVhKOqGEdlEY6qIlpe3yHsnzeuMrVkiohnGw2WoQy6juKDR7VRFI4MM5eTXXUZcvAxXMEgbmtC4e/2b+WSh65FliPYy51Yix1IcicwOIHKBhYAZyZnZ7xIsoJSu47Y+t8ObVP3vASq6GaQ8ipRVt8x6c+/tGCGIBw1+7to8XenrIiPBb1w1OU2AgnhaFlF/mmX2I2F98+v5JHdpwgPOELa+4O8WdfG2hklPHuwif999wiBaCzluLXTS/j86tlkWUYv0ckxZ5Ez4J6YW5zNyikFbK5PTNIf2lLH8rP0+w6HUTZw14xr8EWC2AzmMb2WqKryo3fE1f+irAhrZ/WRZ6nFpIiyiSKbybcsoT2waWibJ3yUHPMcLDEJU9Nf+EJlGwssufz30akEYonrrjFg4Qv9Cnd2BlkVHSgnjMXoeui3PN7s52FjAVFd/aAE3L6oik8sn4YnsovuJFdUOreRcKwkQe5SNPsUaHpqKDPOGolQ6naj5eTwqVXd7Gvx8adtOfT6xSHJQUnhyNfvZuErm5n3xjaUaIzsuVVM//i11Hz0KqyFpxf6rmlaSobHjHf3DIV81/9lA4d+/jRz7rt1zOdM5zjStBi+iOhwsQ/kG/neOYRlTi2yJU16+ETIXUqsYSMKfWjuDgzZhTiMNvojie9Gd+jIgHDkRrbHr73ekAdPuE84ld1QToF1ZEGlqb+TIx5xgrmmZCHKOcwJSkf1gOuoxS+WtUpI3Dhl5YidTM8FZoPCFy+dM/qOYyDf4sJhtNI/kB2mF42mOUuH7Qp5JpAliS+umcMnH9s8NPnq9AX53bbjfPbic1/emAGOdno53OEd+rdE3F2cIcP5RlWug3+7eiFfeWHn0P0kEI3x1Rd38cAHLiLPZh7lDOlp7wuwrbGbrY1dbG/qpj+UPoJhW2NXRjjKkOF8JtzbSU/LZkJFGgGlJ8UhNBwGyTYkFNmNZchS6sffKNsptV9KrmUeHZ2P4dNlhmCQiV03jdillRhfa8C2Sv9FWs7xP6yn8bnNQ1sKbqsS9vBu7iBUH88WiMwRXRbGAwmXVLi3j57ePnp2DR/+mzuuYOxkhnMdFaP5PchFDqhZjORwIGU5kKbEsMg9wKXkq1HcnkRpW4QwltpsXObhLMxuoBmYeEeyyUaedyWx9x6F0MBkKqhzWChGlOu/gHQGgl3zLS6qs4qFFtDbO49w05SLx30uvXB0qku8Fs50mdogOTYz188q4+mk7mh/3HmC1461sqUh1fnnNBv50po5p/Vle/fyaYJwdLjTy8ZTnayuPvdWertx7CLAU/saOdYlTtg/vLQXm9FBnmVh2mNyzLPoDR0krCZW7Nv73qCyqxmJGFgcXFkVY3phI9/eWcKJvsTAKawYeXDRVdRfuozPVPTS1ezjh00FHFHy0YdOFTksfO3K+dSW5RJTg/QGDwiPD+c20iOZC9Cq74aOt6A7fm/MCgYp8HrpdLmYXxrk/hvaeHBbNnvr7WhaQnCLmYzsvPFSjl80n7VzKpi7YiZTi7OxmiY2fNE0FfyN0H+Cne1hGpJMcIoksby/j1DS/tu+/EsKLppNwbKxOTJShKPyAvzRdlSSBTcFi2JCCwfpfngv+Z/9woR+l3RIkoQy/Xa0E79G6qhHc+aTbXIKwpE3fIIi20UQbkA2gy/ip13XpMEkuyizX440ggCkaRpvtuwStpXYctMGxZ9r4q6jBTx64i1h+8qiOVQ4zv0940wiSRJVjmL2955M+/i56CJXk5fFbQun8PDuU0PbHt9TzzUzSqlJ01Uuw9nlWV2n02UVeZQ6M6HYGc5Pllfm84+rZ/HjdxJzmo7+IF97cRc/u2XZmBrEBCJRdjf3snXAVdTgTt94Q8+2xm40TTuni5Znk4xwlOGCItjdwinvU2ilY1OQLUoBDmMFWcZKzEremC9ss2ahorsDn8lEh8tFSB9aajcRuWUaJ/vrKbDk4jQ5kCQjvhYbWz7/i8R+EhScIfHgAAAgAElEQVTdKdbz973Ri2RQ0KIxInNF4chwUJx0jMbEStUGSec62oVkA8N1V+v2TdxAjbKBbFMW7qTV6a5g78B7MNz7Ww+UAGeuY8t4kExW5HlrUXc8k/ZxZfWdyPlnbvKztGCmIBwdcjdwRWntuDImNE2jI0k4CoUlGnsTDjYJWHqWhCOADy2q4tmDjUPaw6leH6d6U794V04p4J8um0u+fWKrQIPMLHBy6dRC3jmRqGV/aMsxLqkqQL5AvsC7fEEe2iqGFS+f4mNWcYhC6+okcbubuLAbBmJIUowim5PG/oTqEaCP/inTcZoSpYlTgF/O0PjJhhgvHhYdda95XRxpdtHlA3+aasZrZpTyudWzcAwE2XaH9uvEj5HdRnok2QDFa9EcNdD8DET7yPX5iBgMuO12TAaNjy9z81ROhL3HHfR6xXuFpyCHJzr7eeL5HSiSxLT8LOaXZLOgJIf5JTkjripqkT7oPw79dfH/qnFp6Nkj04DENXLp1ELW/e4rPLvk00T64kKLGony9u3/zk07HxhTnpyvUcxWsFcUpO2mJkkSgT0nUINgWza5ZVKSo5SQpxiz0g69rThySzFIylB4tUoEb7gOl9JNOBal2dcuHC9jpNyxFkUe+Ro96mmi2S8Kw1eU1p63A+jqrGKmOcuo88Yz2MrtBazSBWf/tVKdlV44KrbmUmE/N8LZXctqeKOujY6BLLWYpvHDdw7y81uXXzD38L9G/OEorx1rFbbdmKbTX4YM5xO3zqukodfHX5I6+x7q8PCfb+zn21cvSLmnqJpGXVcf2xq72NrYzf7WXiL6ji0jUJVjZ1lFPssq8tCIj7n/FsgIRxkuKNqOPo82a/jBrIQBu7GMrAFnkUGe4ArJQEmFPRymqrOTXtVMh8UK+eL5ImqUFn8HPSE3BZZ5bP77nxF2J9wrrktLMRYnXq+EwuU/+A7S9xW6m9t4sCeRqSPFNCrLp+ALmfHVt6OO0JkKwDm9bKhjz8RI5zoaG3mWHEE4CqsR+iK+oUlrsCuAJd8OQ2koUeLi0UQdUpOPUnsD6q7nQRVdOlJVLfKi687oc9dklZJjzhoKRlc1lV1dx1idFAI9Gr3hPsJq4jPi8Yqi08xCJ64JtG+eKKUuG5fVFPNmXVvax61GhfsumcUNs8smbWJ597JpbDjRMVR4caKnn7ePt3PFBWIb/sW7R4TwUYtR5QO1buyGMrKMU4AYcAxoTTnWYTRiN1jxRRNdzzoC3TiMdmGAZDFKfPUKAwtKYvx4Q4xkt3VDauQUTpPGZ7NbubKqGPOAaJTebTR/TG4jPZJjKlrNp6DleaS+wxR5PEQUBZ/FgtWgsawkiNWi0txp5vApG5FoquMlpmkc6fRypNPLE3vjg8Qyl40FJdnML85hQYmLclMP0qBYFEz9TPaEDbzTJZa63TS3HGd5Pqse+jJv3Xb/0Pb+U21s/MR/c8UT/zrqZ9ffJAop9opCOiObhW2OgTK17t9vw7a4Ftk2+Sv5piUfIbL9uxiVRsguwmXOojuY+IO7QwdxZmXT5G1L6exY6rgcs5IaUp9MTFN5q1XsfjfNWUalo2jyfolJRpIkbq1axd6eE2iaxqK8mvOupO5MUZWV/p64onD2ORP6bEYDn1s1i6+/nPgc7W9z8+Kh5kxZ1Dnk9WOtBJJDsW0mLqk6t/mBGTKMhXsvmUmTxy843d863saUbXbuXj6Nbn+I7Y3dbG2Il5/1BsYebO00G1lSnsfyyjyWludRlHXmmwmcj2SEowwXDOHeDgJVKilBy51+lD0duIoWkb/qZhR5ElwtSRMNCTDuaML0dAOxq6cSu2kG2MXnCMbCNPp2Yv+oGetxJ4Ej8drwWd9dS3LmTJZxCopkAgW82RIkdWcucuRy3UvfB0BTVfyt3fTXd9B/qm3gp53++jZ8DR2Y85ws/9FnJ2HAV0Rc0BlbNpSmAbEIxnAQp2TEqyUcCK0nT3Lg3nc5urEXTYU7Aw8gK6eSjm4CyhA7up07pKw85BmXoB5+J7HR6ox3UTvDA2lJkliaP4PXmhOZVbu661hZNBeDPLYsqHa/WKbm7bORnMu1rGLsHb0mi7+rrU4rHC0oyeFrV8yj9DQCXtMxNS+LK6cX8/qxxHP+Zmsda6YWocjn9/rP9sZu3tC9Vzcv8OCyahTZViJJfuAAyW4/PUW2fE54E26WiBqlN+Qmz5Ka/XP9bIWZhRLfeiVKYxrBCGBFpcRXLjeSb68Ceoi530LJnkVPqCmN22ji+SuSwYZW8UFw70Zqe5nS3l4a8vMJGY3UOKPU9xsoLwxRmBPmaIONxnYzo63nNXv8NHv8vHQ4niWUbYww39XHfCcscNmZ7vBjkBPXx4tthcS0xHdJhTVAbfRVtNj7qPrAGmbfewuHfvH00OMNT23k4E+fZO7n3z/sa1BjMfwtonBkLLEQDotvuN1gJdLSSfhEN66bPjrq+zURZJuDkLsaY14zdDeTnVcqCEeBWC8N/QFCqjhwLrAuI8s4uttyd3ddSkfQy0sXTd4vcIYwyAqL86ePvuNfGQ6jlQJLNp1JnwGXyc6s7HPrJFlVXcjFVQVsOpVwW/9y81FWVReSbT2/cqf+VnhW16jhhtnlmVDsDBcEBlnm22sX8NkntwiO999tP85bx9uoT+OCHw5FlphblM2yijyWVeQxs8B13o8rzwYZ4SjDBUPb4RdgVuIjq0RjWP5vG5Et7UiAjz2or+2j4PP/iCHvNEt0dCvU4RY/UkTF8EIdTqkU7qqiN+RJOSznmjKyryqh408nCG9SUWZoQgcilzkxYG3VhXQWJ4UjS7KMvawAe1kBRRfPPb3fZUQkYC7xCWoATVXQ2pugvx+trx/NF0C5+C4kxQ5YkCQTWtuz4NlLnqLgLSyEAZFFLbSRf7GT3iO9tDfH6Dsu45phhqHEEA04CUx8wtnyxk7aN+6n8qaV5NWe/uBfWfF+1ONbIRIE2YDhmvuQ7COvtE8W83Onsr51z5BryBcNctjdwLzc6lGOjJOcb6Rp0NIjPr78HAhHMwucrK4uHGqtbJQlPrliOrctrDpjX7gfWzqNN+vahkrkGtw+Xj/WyjUzS0c9NqL6CMV6sRmK02aenSnCMZUfbxADsStywlw2rZ9c83zMSgDYA6RvPz+IWTGRY3bSG0qEmHb6e7H1KhjNWcgOJ7LJSjyYXqEmT+HXH4D/frudN+oSx5gNcO/FCjfPlQXRVMmGmHqAnmADyeRO0G2UjCRJkFOLZqtEaX6K8u426gsKiCoKC3LDbOqwYDJqzKvxUVMeoFyZibvPyN5W95iyB9wRIxu6ctnQlQuARY4xx9kfF5NyIjzXVibsf1NpB5KvDU48hFZ5O8t+8Ck63jtI945Eu/pt//wAhSvnULAifYleoLUHLZb4m5nzXQQN7clrB9gMFhRZofeZPSDL2FYsH8/bNi5sl32Qvme+RdZSBWNuCTaDBX80Uc4aiIrNGbKkcvLMo7seQ7EIG9vEBgoL86ae1YDlDONnmrNUEI6WFcxCPseOK0mS+Nyq2exo6iYUjV873lCEX24+ylevmHdOX9vfIkc6PBztFEOxb8y4vzJcQDjMRr57w2I+9cR7eJKaeYxFNCpzWuPlZ5X5LC7LxT7BLMW/ZjLvSIYLgqjPg68i3jZ4kFy/j+zrS2mt9xJpi5drBHbupOne+8i/714cl1wy8ScMiMJRqDnuyJGMRgrvuhLZ5iHX7KIj0E1fRLwZSYpM0Z3T4A50q/RW7IbEZKXNL870S2y5E3+9p4UDWAFoqAfeIPb6E0OPSBXzkZQy3e414NmLORYjKxikL6nDWuzmmUzf1kowEMJ9qAnXjKnA4aSD24EKSNtge3jUWIxtX/olB3/2JAD7vv8I1739ozEH1g6HlFuO8cPfR23Yg1Q6C7lw6mmdbzyYFSMLcqeyvStpYtp5hLk5VWNyPCULR16fgi+UcFPYjApzi0afxHmONbHnO39EkmUWffMOsqpLxvlbpPL1K+fzyO5T+MJRbpxTTlXu6Lkwp0Nljp1rZpYOOU0AfrutjiunFY+4ShqIdlHf9xwaMQySlQrHtVgMZycT6uFdJ2l0J1x+EhofWdqLSbGSb7UhXjNxwk09eJ/dimtlLoYsJV5iqcbI01Q8hYWocvx31SQNtzOPEvuqtM9tM8G31lazvLKF5w82UZxl5a6lJVRku9FCDUgW8bPXE/KgJglYiqSQa84hXn56+kMIyZyHN3YN9b/9T6Z+XKaxMJ9CK5TaorQMdFmzmlX8ynH+Yd6VWIMq7u4O9rd0srfXzD5PFkf6bYJ7KB1BVWGn28VOtytuskzCJKlcWzTgeAh3w4mHkMvfx+WPfpNnl3yasCd+j9eiMd66/X5u3vkA5tzUNr/pOqr1hUXRzW60oUWi9L1xCOuCBSjOyWkXnA7F6SQSmU3M24TS2UB2XpEgHCVj7AtRWnHVmO4973UcxB9NRIgbZYXVxWMvs81wbrioaA71/e20+nuYmV1x3jivSpxWPra0hgfeS+S9vXi4metmlbGw9PQ6J2YYH3q30fLKfIr/RktyMly4lDpt/Od1tXz+mW0j5hbZjApLyvMGXEX5lE2yK/6vkYz3MMMFQdv+54TyMFlVyfb5UKwGiu+ZiSE3kSOk9vXR8Z/fpePHP0H1j60EKxlNDUNYLDcIt8bPk/Px25BtcaeRSTFS7ijGUVeA9700oda68bfLNE3oUNOqF46sZy/IOD0SWuNeYYtckWbFz5FwxOT1id2gtKnZsKiIubVmvDv3A8WAXXeCOmDsAXQRX4A33/+vQ6IRQCwQ4t1P/mDUHKixIOWWoSy6/qyKRoMsKRBbD7cFelLCZocjWTjqcovuj8XleaNay9VojDdu/ibHf/8adb97hRcuvg9vXfMYX/nw2EwG7l4+jftWzTrjotEgdy2tERxNLd4ALx9pGeEI6Oh/e6gjY1QLUN/3HP5IapbQZNPi8fOHHSeEbaun+ajOD1NozUORUv/+fW8dou/R18i9xIrBFIp3A4yEIBZF0RTy5SnC/u7wEYKxnpTzDCJJEtfNKuMX71vBN9cuoDKnAEmajmy5nMABCOyNl7/F1Bg9QdFZmWtxIUvHgU3AEaAv5fzjIdLn5433/Svbf3CUt+/YTu7xDtA05uVEUKTEfSIQC/P6kcdQm54mO7ifVbntfLamgf9bfIAXL9nBjxcc4u6qRpbluLEqY+u0OcjlJQGcxqRj1BA0PIwj6wSrfvNPwr6+hg42fPz7aFrqPczXqBOOqgvxR8RrymG04dt0DC0QwX7xynG9zonguvF99LzYDL1tODQlrcNE0SBnjzIm111fxM/WDlHYXF4we1zB/hnODRbFxF0zruHLC27j1qpV51W+020Lq6jKEccKP3rnINHYyK7LDJOHLxzl9aPid+BNczOh2BkuTOaX5PCVy+cJUzEJmF3o4s4lU/n5rct5/u4r+M51tdwyrzIjGo2R8+dbI0OGYYgG/fQXiZOXHJ8PZWDgbsgyUvLZeSgOcdDb//obNP3DfQQPiKGuoxIUO8xEuoJoIRVDQQGudWKZlRoxsf7qBzl4w+scueMdAknlH3pcpkQwdF/ET39SqK1BUsi3nlubv6ZpqI37hW1SRWrHGcngAEs8ANUSjeIIBITHY7fMxGCSMO96nZjHC+g7v7kRwp1GwN/SxUtrvkDjs5tSHuvdd5J93390TOc5X8k1Z1HjFEuqtnceGfW4/kgAX5JzoNsjZkEsH0M3teZXtuE5nHBDBNp7eWXtP6W4JsZLMNpFY/8r1Pc9jz+aPih7sil12rhhluiM+93244SHmXQEot34NTF3RiVKQ/9L9IXr0x4zGWiaxk82HBJel8Mc45YFHqyKBadJ/DuqwQidP3sVqf0YudeWIhl0X9nmIqSae8jNvgqjnOxc0ejwv5dW3BgZGevcy5EMS2n6wmO07DqkcxvJ5JgH71MxoAXYPvDTQtyFNHY0VeWdj30P94FTAHTs9PDa2nfIaghhM2jMcEWE/Q9oWTxCOZ0WGx6rdegn6DAztTTCzfPhG1dZ+NOHcvjRTcXcc1Eul1TbyLGOnBt20/I14ErjmOl4k8qlHcz7wq3C5sbnNnPgR4+n7K7vqJa9uhAtSfwySApm2YT3pX0gSdhWnnnhyFBQANnzCRxx8//ZO+/oOOqzCz8z24tWq96rLffeGxgwGAMOvXcIgRDCRwo9JEBCKEkI4ZCEFgKEklBDMLiAMdjghnuVLTdZve9K28vMfH+srd3RSrJsy4Vkn3N07J22M1tm53fnvvcVm6pI1seLuVm1bZhy+tbZ7ev6LZ3d2SBSenci2rknOHL6mqN3PNFpRH42U319ta/Nzbubj935OIGaxRX1+MLR73a6xcDUouNf8p4gQX8xe3Auf7l4MrdOLuOR2aP5+ObTefHSKdwyuYxROSmJ7K4jIPGKJTjpad46HyU5pjOZLJPiUZeHaW0iefdOQbCoB17hxibq7n+Qttf/gRJSD0J6xN99mVr6T25C0KhziTY9sZJQe2S+Y34t22cvwe4djkYwqpYzatJVZTBd3UaZJvsJv/untFaDN0ag0xkRsrqKPgewRN056W63apYyKBVlWDpayU/jbx9HCSUBXe3meziU66ht8x4+mXonret39bjMxt+8gXNHVY/zvwtM7OI62uGspiPYu1Mu1m0UlsDRoRZN+5JvtOvVhXHT3PsbWTT7XvzNPaQnH4KO4D4qXfNwh6rxhhuodn+OJAcOvWI/cN34UnQxrqMmt59PutjuD+L0rOl2uoJMjedznIFDi3dHwjf7mlhVpXYUXTLGicUgk21OV5UJBStbqH/gHaxlEpaR3ZRrpEyA0u8jGNIRBA1Zpsmq2Z5wXVwb+L5iHDaM9HsfxJ2jdrKlGno6T7mIuI9WECmzcxAJ9un9O77pt29R9e9vVNPy584kb/QdpJDFQFsYi1Yt/lWFjbzrz2a3NZX6lBTqU1JoOPhnhQZtM83yNiyWNUwo2cT1U3byxAWVPDa3jhsntzJjgJuspOhvwellAQZliZB3AWTPJs4q2rGN8bfryD9bLY6sfeBvNK1U35TwdhFd7TPVv0dWnRmpuQP/1hqMw4aiTY28r4qi4A75qPO0sMNZxbdNO1hcu45/7/ua1ysW8bcd81lWvxlZOTL3hf2SS2n5cD9yayMpihYh5hiztElIS/ZiHH7oLL1mn5PNbV3cctkjMWj6oSFFgv95xuSmcs4Q9Y2UV9fspr7D18MaCfoLRVH4eLv69+K8oXloxcQwMcF3mxHZdq4dX8oZA7OPa6fh/1YSGUcJTmrkcIj25AYgKsTYvV60ggFsJdARDZjVGAMUPnIWdX/ZTKgy5gdQlnG++x7edevJvPvn6At7t94q7hrV0CFY68E0Zgym0TZinTL+ZomNjy5QrTvhidvIyZtKpjKeVv9mOoJ70AgGci0zVct1DcbOMZ/oMjVQurqN8ocjaHo4RVhLoTXSYtoYCmEJhPEYosuGLxyEfnsL/m3baXzid2T89GY0SbFdwDxAA9B9pk7tojV8efmvCbnUAkrqmIF4a1s6hQ05GGLFrU9zzlfPIHxHL3CKrdmkGWy0Hgg4VlDY0LqLmTmje1wnVjhqa9chK9FPbJ7NdMjuZf5mJ9XzVnY7r31HFYvm3MecL57GYO9bqZmiKLT6N9LsX6eaLisBnMEdpBl7Ppb+IivJxPnDC/hgS1RIfGPdXs4bmodBG73DLskB2qWaXpt01Xu/RlL8pBpG9VuHPW8ozLPfqEt8Bmb4mVriJcVgw6iNiuMdCzfT/u4Ksm8Zija5y+daNEDe+Qg2tZBh1RVi1ubiDUdL9Jp8q7Hq8lUlsn3FZW8Gf8z33xXE84v/oJk5lKSzRqBN7VqCChEXUv2BP4iEcpu6/BkBM1Ufr2PDw6+p1k4dM5DpL/8cQRDIsp9H0LWAyRkNrGgy4Jeix+AKiSxtMDI5I0Ca8dBiiiBARpJERpKXqaWRc4o7IBKWwG6Wqez4mBzLTGxpU1AMGVDzAUhRR58QaOSMPxTyhd9B7dLIjQUlLPHVlY9x/voXMKZFXFixbj1RJ0BuJLg+JINPEgjJenavraL1zFGEJ4zCv2sxrpCHjpDvkKJQs9+JgtLreaEn9IUF6AePxfl5JanJKRQVDMId8mLSGrE01dDoSEI0HbrU7Kv6jSgxYmCqIYnRaT3cXEiQ4Ai4fepgvtnXhCsQcS8GwjKPfr6JZy+YqDqPJ+hfdjR1sKslWnYsCjB3aCIUO0GCBGq+myOtBP8zNG9diJIe495RFFLdbkibDHkXRYKaYxCVJvLumYHt/Llx2wru2UPtXT+hfd4nvZZwSI1qt0GwwU/6ndchCGqX0PLbPlLdUM+ZNY7Bt0WeVyPoyTRNYGDyFZTYLsSgUTsGTp5g7ChytbpLTrf5RgcxF0JMHkZ6h7o1vDIsA3lQ5Ji8q1dT/f2fEazpWsayF4jPItnx4jw+n/tgnGiUf95kzl32Jyb/6Q7V9MZvtrLzxU963teTHEEQmNDFdbShZTchueeyn97yjSYWHtpttOetL3rNh2rbsJvF3/sFYW/3QbqxyEqYOu9XcaJR57b821CO0ClxuFw7vhRDTDlXqzfAR1vVd1Edvk3E6GxoJYkspzMywo+hybeGJt+3R1Du1T2vr91Dkzv6eoqCwtUTnGhFkQxj5LsiewM0PvUp7s+3kvuTMfGikSELBtwaJxpB5HPU1XUUlNtxBLbHLXsoJDlAW0AtJGsW7Eba34bjH8upuvFlGh77mGCVp+vL1nVLgBtoBqqIuJI2ASvJO9vLxeW3c+a8K5j87GxGPXAqsxfeg9YsATKCIJKfNJsMUyozswPY9epzRVAWWN5ooMp9ZINJq0HGbo58LmVC1HoW0+RbG3FTlt4ChgzV8iJ+znxuCIMvi4rdnuomvr7hKRQ5sp0mvR/3D8bQ/ssZ+D64kAVNFj6pNjG/xsyX9SY+rfbxTW465acNZ5dVotrThDPo6bOTaEXjNiqcR+Yis196Ke3LGgnuqcdYt5f0oISlpR7/ht3oS+JLkruy39XI7g51bthpOWNOuFM2wX8XdpOeH04dpJq2vbGdp5du77dzcYJ4urqNJhdmkJUIxU6QIEEXEr/4CU5aZFnGaahUTUv2etGhg9RJCKIGCi4Dk/quiODZTdrcHLIfexRNmlqQUYJBWl94kYaHHyHcFp+zE6qrRaNXD5YNI09Bl60ux2rb0kbVR+Wdj7VWEzP+dnef3AmKosQHY59gx5EiSyg16rKL7vKNOueJuoh4dABTKIRZUbtTwhdGL/5kj4eGh15FCcUO/oJA9GJFkWXW3PMiK2//k6qlNcDQOy5k1r9/g85qouTK08k/Vz1AXnv/y0edzXMiGZFSgkGMCkA+KUC5o+dsh8aYz0+Ls2u+Ue/CkaIo7Ho14pQrmFvG5dX/xzVt9zP20fNUyzUt38qSSx5BCgS72wwAYdlLlWs+HcE9PS+jeOkI7u1xfn+SZjZw8YhC1bQ31+/Fe0AkUxQZZ0D9ObcHBFKSTyPP4UDoMjBpC2yh3rvsqIWvfW1u3t2kfj9nDXaRZw+RYUpDI2oI7G6k5qf/wjhgINnfH4Co6VJaaymGkhsQ9D13GTJq07Dr1SJks38DknxoATCWtsA2ZCX6vouCgfScGRHrDoAk4125m5rbX6T5T6tRQnmAofuN9YDGoCV5cBoF55Ux7M5JjP/tqZgyq4FvgaXACkRhK0VJZeRbizm3II0ia5cMKATWtxrY3ZGJTTeQZH1ZzN+guD+7fnDnn0Ub7zxt9W+kxvMZstYCJTdDkvq1FASFqb8axNRfliFqI69FzfzVrPnzO7y/dxl77x6J+/bx+C4ajCM/FXdYRFL6x7F2kHlVK2n195yl1xPGIYMxjhhJy/uVyG3NULMD2upoX9aAcVTvwpGiKCyp26Calm/JYFBywpGQoP85b2g+k7vcAFm4s473N3+3y9JPVtyBEF/sUkc0nD888d1OkCBBPAnhKMFJS2v5EuTsmDseikKa2w1pkxA0EReSIOqh8Kq4u8M4N2LKaSX/z89hmT49btu+deupuePHeJarQ5fb331FFUAbdoWxXzkXUIdzr/qx2uEy6enbsRZl9em42oMefFI090Uvakk1HF57+v5Gaa6EQExulNGKkFHU4/JApFwthnSf2hGgjMpCLrV3Pg43u2j/z/ouG6kCgoS9fr68/Ndsffpd9WxBYNIzP2LKc3ciHrCpC4LA1L/ehdYa/WyEXF5W3vHsd/aOpF6jjSv5WNO8s9vjCUghHMGIkOn1i3j8UceFRhQYm9e7e611XQWOLfswZVk47V8XY8mzobdrGf3QWMY/fq5q2dpFa1h67ePI4XhnmD/cyj7Xx/gkdSCwRjBg0apzKloDW47be3PV2BJMuuhr0u4P8eGBAYcrsJeQED0WQVGwm8cipE4gKXU2+a1tiLJaJGoP7qLG8zmycmQd/BRF4Y/LtiPFtIS1m8LMHdGBUWPArk+i/T/raXt7Jzn334xtWAuC0kWssw2Hwqs7z3u9kWEaj0hUhJSVAM3+DXHLKYrSrSgoKcE4t1GaYQSp519M1kO/QDCq98G9eDm1P32OcPNAYCSQCViJlKkdDQHAiUZoJtMkUpRk4qoBqczIii+f3Opws6rZQbpRS67FRK7FSq4lmVxLKrmWTHItOeRa8smxFJFjKSXHMogC6ziyTOPoWrPoDlVT6foPQbxQcDlknBr3fIMvz+Xsv43GkKHHc/1IlkwMsquj+zytvmLU6Mg02hlgy2Vs2kBm5oxmbuFUziucghizj0E5zAf7lhGQ+pjZF4P9sksIVHtoeGEHzi/raXxtF96dboxDew+3Lnfup8Gnvtlxeu6YfivjTJAgFlEQ+NVZo8jvUnL91xU7WVvd2sNaCY6Uz3fV44/5jc+wGOKEuwQJEiSARMZRgpMURVFwKNUPw4QAACAASURBVOVEMjEiJPl86BUNpE5RLStoTShF18C+VyEUI/C0rkTUWsh84D7cS76k5fkXUGI6gMkdLhoffwLrmbNIv+1W/Fu3IbftBWIEEX0GolFtz6/9fB+NX0fvfOXOnsCgW9QD7t7oegGeZUrttkXy8UTpUqYm5I84dC6KdQA0Lu58aOqoRnZmIOZGB8iBy8ZhfnYFij/ieHC++y1Js0egsR18XyVCnh0snPUiLd+q81+0ZiOnvvUgRRfEC3/WwiwmPHELq+58rnNa9byVVL63lJLLT+vDEZ98jE8fFBGLDtQ/NvmdVHuaKbRmqpZr6qVMbUS2HYu+99P6wVDskfdNQ2uOri8IMOr+cQgaWHvf/M7p+z/4muW3Ph1x1B3IkXIF91Pn+QoZ9eBVLyZTYJ0NwJ6OaNepgNSKN1yPRacWlI4FdpOey0YV8Y+Ylvf/3LiPC0cU4PCuVekESb4gTdtk0ie6MKSMwSJqKWyYR3VaKpImKny4Q9VUuRZQYJ2NRjw8Z82CdTvZVKcu5bxivBOjTiEDGy1/+RrzpHPIni5B4+fxG0ibgpJ+Bs5tVQQdbsJeP2FvgLDHT9jrR/IGIo9jpuuni1jPjwosbd6trL34Dbw7HIS9gQPrRL6T1pIchtx+PoNvPQ+9zUKbP95tlGKMBCdbpkwm93dP0vDob5BaowO44L5Kan96N1m/egjjoIMhywoQAnydf60btiAF2rENSMGY0V1GUu8IgsApOUmkGbV8UuVEitEiK9p9vLm7lktLUrHpDy1aCQKkGsGoKaXGU40Uc8xBuYPKjo/JtcwkKfM0FGMW1H4EcvTzHhqbhevT4bi0hxbzNIKCTatBv7Mes8ND1imnk5KWhU1nxqa3YNOZ0fcSMB2UQnxeGy0FbQ108GnVKi4qnnFY4o1p7Fj0A0oJ7NlLoDpyo8A4fBiisedjCMsSS+s3qaYNTi4g35LRwxoJEhw9SQYdT5w7lh9+sBpPMCLaS4rCw59t5KVLpyZaZ/cTiqLw8bauodj5iVDsBAkSdEtCOEpwUuLYtRIpT11fneZ2R0rUtPF114LOhlJ0bUQ8kmKycRoXg8ZM0qwzMI4YTvPTf8S/TZ354V78Bf4tWwEF22T1YEZbOoJIx6Ao6x74ovP/OpulM8y1r8QHY58E+UZVXfKNCg+deYEhE7QWCEcGIIIcJMlhw5MbFe/EkUlkvvgU7lc+wLNsGbIngPOd1aT94LTOZbSGJvSSutOUKTuVM+f9lvTx6qyDWIbcfj57/7mEphXR0qNV//ccObPGdobVfpewG6yUJedR0R51Lqxt3hknHPWWb3SoMrWwL8Defy7BlGNl8A/Hd7vMyHvGIWgV1vw8Gvy++7VF6G0WJv7xdhzBrTT5vo1bz6LNJc8yq1NYseqKcIei5VltgS3HRTgCuGJ0MR9uqcJ9YMDhCoR5e8MOThmsLjlteXMX6x/4EI3JwIBrz2TYnRdhL7yIovp/U52aQkgb/Yn0SY3sd31CQdIcdOKhRQ85GKTu0495vsVCrLl3RI6Psfk+rD4N0vow6bc+AG1fQeOquG2ErTMof7WBHS9cF9fqvTeEN0RGjz4PY1FEPBI0Aqk3ZtF8dXy3OPe+etbe+yKbH3+LIXd9D+udOpW4lmYYgUaIlogZBgwg75mnaXj0NwT3REsUJYeD+vsfIOPnP8M6fTqRjegP/CWz792v+OrKZzuX19kMFF4wlukv34FGrxaYIm6jnhmWYiJZr+GDfQ484ahDrNEX5rWKFi4tSSHX0rfuKWYdlNhyqHE34ZdibiwQosazmHRpLOlJ4yKla1Xv4At28KWUxmYludsrqKRaB2NLIJBsxqSVMWsUzFotmd+20frKEnRFRRR8P14M743x6YOo97ay1VHZOW1nezWrmsqZmjWs5xW7IAgC9isup+nxJ6PHP3Vqr+usb9mFMxh1o4oIRxTQnSDB4VKUYuVXZ47i/vnrO+MkXYEwDy7YwPMXT8Z8iJskCQ7N9sZ29rRGfxdFAeYOS5SpJUiQoHsSknKCk5JWr7qkyeL3Y5SAtJ4vcgVDGhRdDWKXAUPdPJSOneiyssh54nFSb7getOoLjnBjI+HGJgx5Xe5iJamXq3y/nNb10Vrwyc/8CGuBemB/KOLzjU6scKRIIZTactW0XoOxDyAIQiRINoZ0uwb3erUw1mqoIO2eO8h5/LfoCgpo/2QToYaouCRoNUx/5gwGjdSj00PKyBLmrvpzr6IRgCCKTH/pZ4j6qHjib3Ky5u4XDrnvJysT0tWZKhXtNbTHDNogKhzJCrS2dwnGLug9K6vqo+UEnW5G3T8NrbHni+4RPx3PlOfUmUflz3/EtuWvdisapRiGUmCdo3LjpBnU4qM7VE1Acva6f/1FklHHFWOKVdM+2FyLOxD9yTP6A5T/IeJyk3wBKl7+lI9G3cLCi17GuX8Yha1ODCG1oyogO6jsmEdAUpeuxqIoCp7V39Ly58f4l9VCeyj6nFpR4crxDkQZMvRnYjv3Qmj6BFpXddmGyI5PNLw9+Hes+8UrhyUaASgBmapHNqqmpZydR/Jp2T2uE3S6aXVvRhaixxzrNopFm5ZG7u+exDxV7f5UAkGaHn+Stn+8gWflKrxr1uLdsIGmfy9i009+T1KyiCVJwGwVsKSYGPPza1FcBqR2O7KnANk/FCU8CUU5BZhEpPRtIJBHpAQuFUgGLOSabVxflkVml8+xJyzz1u5Wtjv63sJbJ2opSsomWR9fMtzi30CN5zMkvZ3NKWfxklQSEY26biMQwv7Ecq431FKaLJFjlrDrFfQaSNKbcX8eKf+zTOtdqOkOQRCYUzCJLJM632pp/Sb2uep7WKt7rNOnk3rTjeiLi7Gdew7Jc8/rcVl/OMjyRnXZ4pj0gaQZbYf1nAkSHClTizP4wZQy1bR9bW5++8UW5O9oafrJxLzt6hLbKUUZZFoP7aJMkCDB/yYJuT7BSUd75UbCReofrjSXC1LHI2h7v9MvmHJRCq+A/W+DcrBmW4Ga91GKrkGwFGO//DJM48fR9PunCVXHWHQF0OfGCEfJmQiaaK6JLMmsf3hp5+P8cycz8MazD+vYFEWJ66iWfaKDsRt2QzjmDr8lBVLy+raydQC0R91KRmMrtU9vZ/Bbp3RO84Rr2NX+T6wDCkn+0z1ICzbQ9uZKsu6eE93MjEEUn1lAZl4jaTefiyWvb/X19mHFjH7wajY88nrntN2vf0bp1bPIO2tC347hJKLQmkmm0U6TPyKwKCisa6ngjNyxncscFI7aXVrCMS3Kk406BmX0PqDb9dpCzHlJDL51XJc52UATEHVvDL1jLKJBZMWt89Cm6Bn02gy0I7peqAtkmaaSaox3PZi0WRg1GfilaGh5m38rOZYZve5jf3HZqCLe37yfdn9ECPGFBT4rT+LiMRHRJ/h1Hb7W+JyYhi83Mv/LjZScX8rUXxdRn5mCzxAVxMKKm/2ueRRY52DSqj+n4TYHLX/5M4bBelqvmcm8D9S5SOcM7yAjSSLTNAWDLhOq3gbPPvU2/DJf3LmJ+lVHJ7K1fVxNx8ombFOjwnbRY2PZPHMhnTVegtDZTU6TpCXndrVw2fFxC85RVaSNGRi3fdFoJOvBB2h77R+0f/CBap7znXfjlh81VgRiz+tBWh64t+cD0GoRtFoEjSbyf0FACYVQwmGUcBikyPl9hl7L6sunUTs0es4KK/Cf/U72f7Gd8XVtaEw6RLMB4cC/okmHJtmMcVQBghixV4mCSI45A6PGQKNP7YCs89SwsPpDWgIKXTORAEYIHZxuaUW6yYY1x0iTUe2KNbbLtG2PlDxbpk/r+Zh7QSdqubj4FF6tWIhfipTVKSh8VLmcmwbNwW6Iz37qCfull2C/9JJDLreyaXvnc0Ekj29GVh/cqAkS9CPXjC1hd4uLJbujN+2+3tfEa2v2cPOk+HNTgr7hCoT4YrdaeL5gWHzTgAQJEiQ4SEI4SnDS0dy2ApKjriFTIIA5LENa3y64BUsJSv7FUP0+HDQ4KxJUvYNSfD2CKSdSbvHsM7S99jodH88DQJtmQDQcyMYQBMhUh0PvfXMr7eWRAYXebmXaiz897HDQtoCLQExOhlGjI0Xf9wv+/kZx1CFvXqSaJhaM7PtxWUpUD4VAA0I9eLY4sIyMvTuu4A7txx3ajzwBmraE0O+oJWVIdLCXdvOpBO59B+err+JdtpT022/HOHTIIXdh5P1Xse/9ZTi3RgfgK257hgu3/A2d5bvVTlYQBCZkDGZ+9erOaZtadzMjayR6jRZJlmj2R4SPrmVqEwvSEHt539z7G6lbvJ4pfz4bjSH21G8ABgM5wGYiLdQjDL5lNGKqBndxGGOJ2o0hCnryLGdg1XVvaxcEgTTjCGo9X3ZOaw/uIsM0Hq147N8Xs17L1WNLeH5lRee0LyusnDnYRYo+RPlj23pZG/Z9vBdPVQtnvDCKlvx03KboPkuKnyrXp+Rbz+osv/N+u4aWl58n467T0Q/L494PwsTKbJnWEGcP7UAv2rGLBVD5OvjVnWx8LUE+v30LbTvUJXUAGpOB9PFlaC0mNGYDWrMBrckY+ddiRGuO/D8yL/J/wSrhJVpOZh5q59z6p0g1DkNj0hN0uCn/80eU//nfpN1YgDYlRiBzBim/cxFbXZ+Qd/ZERt53JdkzR6vODYIoknbzjejycmj5y/OdYk6/cEAgOpSnQBcMM/2tZWw+azQ7ZqoFzI0Dc2n1hpj0wWq03QS86wpSSblqCtaZkfOMIAikGpMxavTUeBoJSBI723Xs7tB2Zo/FkkaQ2ZpmisQD7qYCEwGtlpAY8xohIC3ZBYA2Jwd9cXHfX4Mu2A1WLiiazjt7o98pvxTkw8qvua7sLHRi/13StQc9rGlW585NyRyGRZdwIyQ4vgiCwP2nj6Da6WFXSzQ64LW1exiQlsTMAX1rTHIsaPcH+bS8Fq0oMHdYPmbdd2dY9VlFHYGYUt9Mq5FJiVDsBAkS9MJ35wyX4H8CT/1OQiXqUrM0txtSxiHo+t55TLANRck5D+pjup/JAdj/NkrJTQiGVESDgfTbbsU8cQLNzzyLISbUGXs2gj56gSwFJTY8uqzz8eRnf4wl7/DDQbsGY2eb0o5rZxolHESp3Y68dx1y5XpwNsQtI/Ql3+jgsrokFEMmBA6W0igUzylg131rGfzOaWiT4gNfRStk3zmMBvy0u2qx65Ow6a0Yh+dhnjoQ78rdBPfspe7ue7CedSZpN96Axm5HCbki4cHeqkib7MwzEDQGNHodM17+OZ9Mu7PTPeGubGDDr15j0tO3H9HrdCIZllLEkroNnXf6/VKIbY5KxqYPpMXfgXygNXxznHDU+wXf7tcXYclPYtD3x3aZU0ykatkOjAE2ARGnjDvkRT7DgrFLO3r/Xhe2huFYz4sXjRRFIVxXjxIKYS0sRidaCckRIURBwhEoJ8PU1fF0bLhoRCHvbKykzRd5LYOSyMJyG9ebK2jbGhmAmPMzmPDkD9j54jwav1ZnfTVt7ODzmzdy1kuj0BRl0G6OOhJlQlS7F5Kjn0HoH1/i/moxOb+9BMPALD7cIrGzWS00XDXBgU4DWboRCJWvQ0jtKGqv9PL5D7fgrvWrpieV5jDkRxdQdtMcDCmH332xziPSHtwVfR5tOenmEYiiBmN6MmMfuYFhd1/EHvd7ECOO1D+/A8kVEblrF62hdtEa0icNYdR9V1F4wbTOoHQA29lno8vKpvHxJ5A96tLK44GgwOjPNmFr7mDthRORtdFw7OpRRXhSrcx4cxkml/q1DVW30fS7+Tje+ZbUa6ZimR4piTHrTMhksKS+HW84/vysEQSmZ41kcpINTc37B78uAHgs6vfIrDXi/iySq2eZNu2oz/elthxm5oxWBVY3+hwsrF7D3MIp/fZ7sqx+M1LM996qMzEp89BCfoIExwKjTsPj54zlB++vwumLuuAe/2IL+XYzA9KOf2fa7Y1OfrlwE82eyHllfnktf/jeeNItJ7+4GgnFVpepzR2ah0Y8ftejCRIk+O6heeSRR070PvTISy+99Mitt956oncjwXGkpuIDpJTogMQQDJLp8iAUXNqnVtSxCKYcEDTqUhAlBK4KsA1D0ETurutycrCdew6GTD8arRMEEQqGImiiuurOF9ez9+1I1kPB+dMY//j3j+gCfXPbHupiwrGH2gspTuo5d6Q/UDqakXd8g7T6PaQvXkLetgSlYRf4410NIKA97WYEw2F0PAq2gS96ASIYLGx7ZgtNb+wh1OTHNjoHoYcGKGE5jDvkxeFvJySHsAzLxzN/K8IBASi4dy+uRZ+htUvopS/BVxsRAH110FEO5gIEXRKW/AyCDhfNq6NZTS3f7iDvnEl9Lns7WdAIIgEpSI0nWuLlDLoZl1bGXlc9uzpqCIYEyistxJbN/OzUoT2GhSqyzDc3/57RD04mY1JsGaIRGBKzHQOQBjTT5ndQ522Kc1p0LG+i/JIvqfzHctLGDyK5LCIehVtb6ViwkNa/PI/jjTfpmD8fyenEPGkinnBt5/oByUGKYdihu/b1A1qNiCS3sK42mndT7dBT/N4K/FsjJX+nvvEAJZfOpOymORReMB05GKa9fD+KFBk0+5qD1H7TxrAZVjRmjapsDRRcoUpCm8vJu+0sDAOzaPUq/HJhmGCMwWV8oZc5w1xYlQxS9i5FRJ2/07y5g0U/2IyvKTogypszkcnP3MHkZ+8ga/oItKbD6+Z2EKM2A0dgBwfLEBXCKCgqp5hT2o6XaPdIyRVm1y3LUQJqwdBb28K+d79i3ztfoTEbsA8vRjzQfU6XnY1l5qmIRiMaezJBSYtjbyt+n0LArxD0K8h6E+aSfDRJSYhmM4LJiGgwIOh0oNFEz6l9zS7RahEMBkSTCdFiIc0dJqvBRW1xOpIuKh75bGaqxw2k0JSGPacAQ+kANGmphOsjZRpyuxfP1xV4Vu3GX5zDQneA5Y1eQnL8OT7LJDElw0+eRcBmGYmQPCoiZocjQmRzahqhmNWszRLBDzcDkPaDW9CmH31pcr4lgya/k9ZAR+e0Jr8Ts9ZIruXot9/odbCodo1q2pl548m1fLfOpQn+u7AadAzLsvNZRV3nKSIsK6yuauGsQTkYdYfupthfzNtezcOLNuEKRlVjhy/I0r1NTClKJ9nYt3D+E8XWBidvbYheG2sEgV/MGnnIrqwJEiT47+PRRx+tf+SRR17qy7KJM0SCkwZfaxWBIvWFeprbjZAyBkF3hF2y0qeD5IHWaOkPIecB59ENnWKUaDSiSxXAA6TmIOhiSja8ITY99g0AhtQkpr1w+CVqB4kPxu7/fCNFCqPU70Tetx5l33qU1qo+rytOugjBdphOKmupKtw3pSgy2Ay3Bqj/6w6kjRpID2M53Ub6xUVorPEuJBkFZ9CF0wSaZ8+GTyvQfFON4ApiGWHGnFkOUhehIdgG+/6OkjUbUicw7rGbqfrPctz7GyOvgyyz/JY/cP7aFxC/Q/ZxgHHpZaxqKu8UbVr87ex3N9J4wLEWCcWOfgZLU6293uVsWLoJFB9lN3XthlRM1x4JimKm0SfhCLTQFV0zVFy7jHBHxImy9PJHOO33V6Gp241v4yaQ1UKDa8FCsmedipitQyayjqT4aQ/uJsVwfNwLkwt28e6WZBzeyGcgLAvMVwYwlb0UX34ahd+LlsCmjRnIjFfuYcJTt1Lx8qeUP/8x3ppmHBUeFt20ibP/NgpNiUxzcsz5SBQIXTMcl1GPKMn8+RsZd1T/waCVuWysE0IKaXWb0ejUokj1slaW3r2dsE9GZ7NQduPZDPnR+SQP6p+sCZ1oIc04ihZ/tOFAW2AbKYYh6DXJSEqQtoA6ADkrfSKnvzmazU/9i6blW7tukvad1Sy/5Q9sePg1hv/kEgbfOhddkhldZiap119Hy9qdLD/lLqRAtCzXnJ/B+Wv+iinr0M0AFElCkaRIuZokoYTCgIKg00Vyj7TaztyjrhQCAwJu3t+3lBZ/NMTca9ax4PQivlc4jcH2yGvbsWAhLS+8COEwsiiwOTeNbS4/4WD8OcqokRmVGiLHJCEIkbD3yo5/k2+djb74Rqj7BKljK15B/f7KiyOlgpr0dAxl/ZPHIggCcwun8nrFIpV4tLh2HVmmFPKth++GjeXL+g2qx+nGZEamlvSwdIIEx4/RuSn85JShPL002h23weXj4c828fTc8Wg1x/aGRCAs8aevy/m0vLbb+Q0uHz/6cDW/O288Q7NO3u6uXUOxpxZnkJEIxU6QIMEhSDiOEpw01G5/j3BadCCgD4XI6nAdcBsdWSZKpPPXAAg6IdAYnSF5IneJk0cgCBoURYmUQQlyxG0kRu9cbXtmNVUfRdpYz/jbPWRO7Xv741hkRWZx7TpVJ5DTc8di1Bz9nSnF40DetRL52w+QlryIvPkzlLod4Ou5+xMAohahYASaMeegPf37aAYdXptoALRJ0LqSg2UuGq3Enk+bCLZH7sR5qpvx7GzFuaiOhpcr8O9xYcxLQpfd/UWKYtSgjMpEmjMA3dQcbGNT0StyN5G0RJ7TvRsCzYipg7EPK2XPW4s75/qbnGgMerJPHdXt2r5QmJdW7eK55TuobHMzLj8VrXjim00aNHqa/e2qga9fCtIe9NAR8lJZZ6TDExXDZg/K7TWbYMPDrzHw+lLSJ+TGTDURyTaKvrKSHKDG8zkdof1x28g0pZGXmUPW9EKcS3ZSUChSNkyDsns74fqGHl0i4cZmzGdMxydFO4MFpQ5SDEOPeZlmINRGq7QZvVZhS130HNKWl8nQHZWc997D6Kzx5xat2UjWKSMZ9uMLSRlZiq+hldbNtVR92cKQU2xYTOA2GiNZaEC1Q8eHW3Q8s1RDeZcGaBePdjI8J0Caq4NkOaiaV/FhPV/fX45tYCFjH7mRU16/j8Lzp2FM698Bh0mbQXtgV6d4Bwph2YNNP4A2/xbc4WiTgIPZVfbBxQy6+RxyZ43D3+yko6Imbrshl5e6z9ex44V5hFw+UkYUE3b7WHjmPQQd0SwSjVHP2Yt+1+lOOxSCKCJoNAg6HeJBR5HJ1OlOEmLdSd0er57hKcU0+Rw4glFnpawolDur0Aoi+ZYMjGVlmMaOYX/dHpZdPoX9Y0tUZW4Q+XZMyrBwbr4FrehROfAkJUh7YCcGrQW9fToes4hLjjpKdWiR//gNSDJJZ87CMrH/Avu1oobipCy2tO3rLClTgD0ddQxPKUaviRe/+sLejnq+6dJJ7byCKaQZT95BcIL/LYZkJtPmDbKzOSqaNrh8uINhphQdnWjaG40uH/d8so6V++NvqsQSCMt8saueIZk28pJ7sFufQFz+EE9+uRUp5jf7x9MHk28/DKd5ggQJ/mtIOI4SfOcIdDTjK5SA6EV7qtuNYB+FoE/pecU+IAgCSt73QPKBO5r1gbcaqt9HKbwcwh6QvJBRiKCNXnAH2/1s+d1KAIouPoWSK08/4v1o9XcQkqP1K2atAZvuyC4qFFlCadwTdRU17Tn0SgexpiIWj0MsHY9QMBJBf3RBxYKoQzEXqkoCc6emsLMqvh227AmjbbQzYsxtkBSmPVCBM7gLSemmdbZWxJ9npwbQShLJXi/JXi/yLifG0i55Bh3l4Ksn99RLGHDdWex54/POWRt/8wZFl5yCfUihapVdLR08+tlmqpyRTJb9Dg+uQIhfnTXquOZO9cTE9MHscEbdYrs7atEKGhQlPhh7UmHPzrVgu5uWdZuZ8fdbuswpJtZtFJTaqXZ/RlBWi40iIrmWTJL0kYvK7FMLOWvRNTT86t/IbnVmTHf4N20mY99FtOUKHBQXg7ITT7gGq+7YdnBxuCNOwWklHhZut9F6QGyTtRqq7r0Gc3bv7hdRp6XkspmUXDaT1g272PuHv9Pw0g4K7xiCOaDjY2c2K/dZqHF2L/7m24OcPsiNLhwmzavO/tn0UhVttfmc/dmtZJ825ph+5kRBS6ZpAnXeaFdIV2g/rmAVbQF1rlOqYQQaMeq4zJoxkqwZI3Fs3ceW37/D3re/6CzjO0jQ6Wbz42+x7Y/vYc5Nw1vTrJo//aWfkT5+0DE4sp4xavRcVjqTJXUbWNO8UzXvq/pNtPg7OD13DMvM7Wy6anK320itauG0NgODrp2NoK3CotdT427AL0W7UMrI1Hi+Id1YSVhQfx/09T7CB0pZjrSbWm+kGZOZWziVDyu/7pzmCfv5sPJrrhkwC414eKU7iqLwZZ3abVRozWSALbeHNRIkODH834wh7Gtzs7ne0Tntgy1VDExP4ryhfROoD4e11a08+vmmzi6dBzFpNdxz+nDWVrcyf0fUheQLS9z36XoenDWSM8ty+n1/joZFFXUEY87h2UnGQ2YkJkiQIAEkhKMEJwmNu+ZDafQiVxsOY/P5Ia9/WncLggal4FLY/2ZEMDqIexfUzgPbUNBoIU3dhn7rH1cTaPNhSE9m6l/vOqrBXXdlaoe7PcVRh7Tm38h714Kv49ArAAgiQu5gxOJxCCXjENKL+n+Qai1VCUcFp2Ww8526uMXKbpzD1Bd+gkYfET4yzZPIME3AHVqLM7gbd8jb7ebDGg2tSUm0JiUhtIpYVjeRM9qGxhgzMAo5Yd+rTP3NDGoXrsHfHAkfloMhVtz6NOd89QyCKKIoCu9vruKFlTsJyWqXzBe7GyhOtXLDhAFH+4ocNXmWdLJNKTT4ohfGYUXC49PgD0aPW68RGZXTs7i6919fMvLuyYjaqEikKCYEIdqm3ROqo8bzBbISUK2rxUzqFhOGAWGI+WoYB+eQ88Rl1D/0PnJ7VPTT5hdgmzMb76pV+LdGu5a5Xn8f269m0xHa2zmt1b/lmApHkhykXW4EUUCrgbkj2nl9dVRgW6loqHZ6KOjDXVY5GERZ8xVJHeWszSzmuRWlbLLnIys9f4+MOplrJznQiJDlaO+U6BRZoXqtnYE//y3WwuPXDcimH0hbmdvDvwAAIABJREFUYDt+KSrq1HoWoxAdQIiCnlTD8G7XTxlRwqmv38+4X9/E1j++x65XFhD2qoUSyR/EtVfd3nn4Ty9lwLVn9eOR9B1REDkzbzxpBhuf1axFjnELbXXsY5ujsttuaTpfkFGLNjJg7R4EBeq27iHrgXvRpRVTlLSHBu9e2oMu1Tot/nhHlrLkQJma3Y5x6NB+ProIg+0FTMsazorG6Pet1tPC4rr1nJ0/8bC2tdVRSZNfHdp+Ru7Yk0JIT5AgFp1G5Ddnj+bW91fRGHMD449Lt1OUYmVEtr1fnkdRFN7esI+XV++iy+UCBXYzj80ZS0mqlVkDs0kx6VW5QWFZ4defb8bhDXLZ6CJOBiKh2NWqaecNzU+EYidIkKBPnPiajAT/84R87Xhy1I6TNLcbMXk4gqH/MoAEUQeFV4EhUz2jfTM0LID0AlUgtr/Zw7ZnItlIU/9yF6bMo3M+1ftaVY+zTYfO+jiIIoWQVr9P6I2fIW9bcmjRyGRDHHoamnN/hu6Hr6K7/DE0ky5GzCg+NoMAS6nqYfZ4G4JW/TzjHruZ6a/c3SkaHUQQRJL04yiwFlKWXESmKRW92HOZhTIsA/dFw9kTNuNqDXWZKaN1LeN7/z4dvS36XjZ+s5WdL36C0xfk/vkbeG75jjjR6CCvfLubpXsau513PBEEgQkZg+Omd3UbjclNwaDt2VlQv3g5A65Xd8oThGIOnv4dgR1UuRfEiUaa+iDinR/R/tTfqfvZm/h3qAUBQ2kGuU9dgWQ1U703xNqvfXz7RQf6yTNIveEG1bL+bdsw71e7crzhOvxh9XeiP2l3r0aOuRieVuDC3hYV4SRF4bU1h3bqBSorWfqL3/DcnnZ+cuZN/HX8OWxILuhRNDLqZGYMcPPA7EZK0oJY/H6sgchrK8sCcs7FFN1013EVjSDyecoyT1FNixWNAFINw1Vuo+6wFmUx5dkfc1nl24z51fUYUnvuZpR75jgmPHXiy83HppdxxYDTMXYp3+pONBocMHHeXz5j4JqIaAQQKC+n9q6f4d9WjyhMIMc8iyxTXty6sQgKhBdHHK7mKZMRNMcuuPeU7JGUdGmysL5lF5vb9vawRjwhOcyymE5tAMPsRcckhy9Bgv4gxWzgt+eMxRBzUyQkKzy0YANNfXDDHgpPMMwvF23kxVXxotGMkkxevGQKJalWIHJ+vW3qIH48Pf43+7nlO3hpVUUkEuEEs6XBSaUj6n7VCALnDe39XJYgQYIEB0kIRwlOOI075oMpOsjXSBI2rxfST+n35xI0Rii6BnRd7kYpfkhV2/E3P7mCsDtI8eWnUXLZzKN+7iMNxpZrdxB+826kFf8EKdTjckLWQMQpl6O96kl0t72Cds6daAZPRzBaj2q/+4QxGzTRsjutSSRjRGRAqTHomPn2Lxj94DU9ilaKLKA4PWhFLWnGFEptBRRac0n2BxB6uNaSB6VRO7yQKq8Oqct2LcltXPTJNDJG2zqn/eflBdz49jes3N/cdVPouwRq/vaLLVQ099HRdQwZai/CrFVnQcWXqfVsMXdsq6Tg/FzEmOOTgnogIlq0+jfR4P0GugygxeXVaB5chOCICLqyO0D9Q+/j26J2VegLUsl+8kqaghY8bgXX3noWzb4XsnIxjR+nWtb7ykeYNOrBbdcyqf5CURQcMS3oAbTlLYz+zzeqaYt31VPZ1l13QWjo8PLyW/O57r3VPFwygy+LRuLVdZ/LJQoKI7K93DKthd9fWMd1kxxk28IIikJW+4HSP9GIWHoD2vSR3W7jeGDWZmHTlXY7T0RHqmFEn7dlTE9m7CM3cNn+fzLpmR9hKVAL8taSHGb+8yHEXkTN40lxUjbXl51NiqF7oSvNYOPqAbO4ePJFlD76GNos9fFIDgd1D/yC9k/mA2mkGudQZJ2BRujeuK1r9COEIsLcsShTi0UURM4vmk6yXu2eW1S9hoYuvzs9sa65go4Yx6coiMzM6RqmnyDBycWgDBv3n64+b7X5gjy0YAOBsNTDWodmv8PNbe+vYtledWidAPxgchmPzRmD1RB/g+vy0cX88syRaLs4eN5cv4+nvtxGuEsDieNNV7fRtOKMXhtrJEiQIEEsCeEowQlFCnpxpaut8akeDxrbUATjsQk5FHRJUHwtaA9cZGv1kF2KEBOK7GtwseP5dRgz7Uz98/8d9XNKskRTTMkRQI65d8eR4vcQXvwC4Xd/gdIWXwaBwYw4aBqa2T9Gd+sr6K5+Cu3UKxCzy45Lq/NYBEGIlKvFMOWpuYy890q+t/YFSq88o8d1lZAb9v8Dar9FCXg7t2fRmchJGcpA+3XkmE/BpMmMX1kU8A7MYLc9nVadUSV/mFIEznltNENuKmDtBafy6c0X0BZQC29WvZZHZo/m93PHq6za/rDEgws20OpVu3CON1pRw9i0aCcmSYbWDvXFam/ZBNWfLKH0anXpkUY/EBAISi6afGvj1tG8ux3t8+s7B70HUfxhnO9XEG5Ri0xJJXbOXXo9yYMjQqhz+34+P+cBki66RLVcYOdOrNXqPK324B5Csjr7pz/w+HYS1MTsv6Kw94mtjAsFKE6JDq4V4O9rdnc+9obCLNxRy10frOSKN5bxRruGBkvPJQ8lqQI/nGLmvdObeHboNmbktKPXRl+fVLcbvSSBzgYlNyFYCnvc1vEi0zQRgXgxJ9U44pBuo+7QWUwMv+sSLt39Bqe8dh+FF81gwLVncu7SZ/o95PtoSTPauKHsbIqsUbeXVtAwM2c03x98DkVJkemGAaXkPfsnTOPGqjcgSbQ+/wLNz/wJORDErBtCie0SjJr4z4jyZcTtI1osmEYee7HQrDVwSfGpaIXoextWJD7c9zXecO/uC284oCp1AxifXobdcBxuOiRIcJTMKsvh2nHqrn87mjv4/Vfbjsjls3RPI7e+v6oz//AgNoOO388dz3XjSxF7cW6fNSiXJ88dh6mLaD5/Ry0PLdyIP3TkgtbR0OEP8lUXN/UFw49tzmCCBAn+u0hkHCU4oTSWL4DC6EBYlGXsHg+U9r/bKBZBb0ApPRvC+xBM8RfHGx/7GskfZuZbP8WYfvSDn2Z/e2fnG4AknQmrrvtQakVRkCtWIH31d/A64xcw2dDMvBFx8AxV97cTjqUU2qPdeFIHQNpZP+h1FcVbC9XvQvhAXkhjJRRGu9YJRgEtXuyGwdgNg/GHW2nyfYsnrG6Fq5h0NJtScXoDZPvcWA6UBjWETLx22vcod8W/x8Oz7PzqrFHk2CLvw09PGcofYlr8Nrn9PLRgA3+6YGKvpWDHmnHpZaxs3IaMgqNDiyxHL1gzLAaVEBKLHApjKwup3EZBp4LeHhHgWnwbUDmNAmG0L6xHs0ZdkqbLz8c66wySTpuJNjMTkIFtQLSzjCXfxjlLr2fRWW/h2NJEy9qdLPvpq4ydOQHf2qg45Xv5E3SPTiUkH3RzKTgC28k0HV4Wy6FweNfG5uyjb3RRv7iF81Y+TGpqCr9ctLFz3ld7Gvlkew2b6x0s3dOI7+Bd6h4GBslGOLNMZM4QkUHpKQjCaBRpMlT/i8LW/TTZbHiMRix+P+kuV6Q0tuhqBJ2t2+0db3SaJFKNI2j1R8uSDtdt1B2iTsvA62cz8PrZR7uLxxSTVs+VA05nZ3sNrqCXQcn53QokmqQksh95GMebb+F89z3VPPcXSwhW7ifroQfRZWZSlHQhDd7ltB9wuQkOP8KXkawT85TJCLoj63B2uGSZUzinYBLzqlZ2TmsPefhP5QquGHAaYg83FFY0biUgR0V1g6hjWtbRfR4SJDiefH9SGXta3SpH8WcV9ZSl27hiTHGfthGWZf62ejdvx+QUHaQsPYnfzBlDrq1vDU0mFabzpwsmcu+n61SB2isqm/n5vLU8ee44kozH57xwkIU7u4Zim5hQkChFTZAgQd9JCEcJThiyFKIjuRGIZp+keDxorGUIxuyeVzwiFMAFNBMZ8HoRdIAufsAgub3sfGkjpVfPouii/gnn7muZmtLRRHjJyyj71nc7Xxx+BppTrkcw9ZwrcsLo4jjCV4ci+SPlgd2gODZC/aegxNx9c7Wi+DwIplgxZA8wHhAwatMosM7BHaqi0bc6RoCIEDIbqDYbsPp8bK3Q8uzOIrxSF9FHUbh6eD63nDoMbYzL7PzhBexrc/PBlmgns22N7fzhq+08OGvECQuItepMDEkpZLtjPy1dundNLEjvcb8avl5F0YVlqmka02AOuo3aAxUqz6n2tc2dopFoS8I6cyZJs85AP3Bgl+cQgeHADiB699KUaeGcL69j7U/nYU0NIYhu9m6Tie0nE9q1G2vdmTiyo++bI1BOunEMotA/F9GBUAtu0UekqCBC498qGHrnRWRMHMKpikJZehK7WqLhxr/7als3W4qiFRSmFovMGaJhSqGATiMAdmAUoEHQaFAKr0JT/R457bvhYHmauRAKr+zxO3CiSDOOxhWs7Oygl2Eaf0Ruo+8qoiAy1H5o95eg0ZB6w/UYBg6k6Zk/ofiiWXzBPXuovesnZN57L+axY8gxn0qyroz6159D+HoPgvdAN7Vpx7ZMrSsjUkuo87ayrqWic1qlu4Fl9Zs5LXdM3PLOgJt1LeqyzmlZwzFr/3c+Dwm++2hEgV+eOZIffrBa5RR6fuVOSlKtvZZ0Azh9QR79fBPrauJLO+cMzuXnM4cd9g2koVnJ/PXiyfx83loaXFHX35YGJz/+6Fv+MHc8Gdbj89sQCcVWO9e/Nyy/V+dUggQJEnQlUaqW4ITRvOMzlOToQFiQZVI8Hsg4tZ+eQQYcQAWwElgHVAHdd+4CUGSZff9YiTEtmcnP3tFP+wH13t6DsRVZQlr3MaHXf9K9aJSSi/bSR9HOvuPkFI0g4qgwxJYXKqpOa51TFQmlfgHUfawWjQAsJaDr6j5xAdGcAUEQSNIXUWq7hEzTJETUgoM/JPDnTXk8sb00TjRK1we4Y8+XDH/9E5VodJA7pg9mYpc7cIsq6vjnxsqeDvu4MDE9ErjZNd+o677GIuprEGLK7zw1PjSGSI5Xs3OV6uwvNLgRVzdgmT6NrF8+RNE/Xif9h7dhKCvrQZgSgaGAus2wIdXEtL9fzKj/G8HImwsYc4cVr1H9eQ29tAiNEB2UykoQZ6CC/sLR/rXKLaTzBaj/2MW439wERD4/t0wq62l1FSWuZu4cFeLDG/X89hwdp5SIB0SjFA6KRgcRRB0UXA5pUyMuo7QpUHTtSScaAWgEPUVJc8k2z6DAOoeUHjqpJYhgmT6NvD8+jS5fHSIrd7ho+NXDON//AADN3g7Ej7YitEYEJsFoxDQ2Xqw51szKG0eBRV3qvbJpOzucVXHLLq3fhBzjhrXpzN2G8idIcLJjNeh44tyxWPXRe+KyAo98volqZ88l0Tua2vnBeyvjRCOtKPCzU4fywBkjjth1XGC38NeLJ1Oaqr5Jua/NzY8+XM1+R/cZe/1JUJL5pLxGJahpRIFzE6HYCRIkOEwSjqMEJwRZlnAaKoHoANLu9aI1lyCYcntarQ9IQBsRV1ELEO7TWo7N9cjOFhq+2MW63+9g5j8f6dd8jgZfz44juXEP0uLnUZriRRZELeLEi9BMuhhBq4+ff7JhKYVATPi0ey/Yom2olbAbqt8Hb/wAhrSpkDXrQD5T84G/g+wFMohVO0RBQ5pxFMn6gTT51tIerGB/m46/rUijyRXvXpme1sZ9g/eRPNVC7YoG6r9YTs6s6apltKLII7NH88MPVlHtjAqML66soNBuYUZJNzlLx4FcSzojbINZ4I2WhwnQo83c11JL9gx1166wL5eDbqMOpZJYR452/j4KX3wBXWbfjk9RZPBUQsceMLoRUqPrCRotStEIqNqOOdNJzlV5tL9RAeHI4DRUsQ9L4+l0ZEbzo9oCW0kxDD3qbC5JDtJOM7GfE/f8KiY/dxc6a7Q0dEpROsOyktne2B63DbvfzbSanZzSsZ/Jv78EbUrXz1IKMBK6yQkSRC1knwWcmPbzh4NWNJFiGHKid+M7g76wgLxn/kjT03/Eu2p1dIYs0/bqawQqdqFJVpcjmidOQDQcf+eORhC5sHgGr+5ciDscdUl9WrWKdGMy6cbIb1u9t5Xtzv2qdU/NGY32ZCqBTpDgMCiwW3h49iju+3R9Zyc0dyDMgws28MIlU7Do1cOeT8treGZZuaqECyDdYuDXZ49hRHbPGXd9Jd1i5LmLJvHA/A1sro9mXTa6/fz439/y1HnjGJZ19M8TiyQrbKxrY/GuepbubcQdUF8LzyjOJM2ccBUmSJDg8EgIRwlOCG27lyJnxPxoKQqpHg8UXtLzSj0SAlqBFhSlFUE4dNcKOSRRv6SS/R/tpOo/Ffgaond9Blx3FkUXTO9l7cPcOzlMs0+dVZRtTkUJ+pBW/At543xQ4vdZyBuKdtYPEdLy+21fjjnWUmiLGVS5o+2gFV8tVL0H4S7dygQt5J2PkBybqVFKRPg7mMHjB2qB+CBHrWgm23wKS3am8sq3NUiy0GW+wmXjnJxd6EDnEiEIedNS8bUtItyShjZdPXhOMuh48txx3PbBqv9n777j46rO/I9/zp2qGbVR75KLbLk3uQDGgLEdeoAECGHzS7LZEFI2m4QESLLZ9LIhkIRN2zQSkiyQQEIPYNNs3HuXbNmSJVm9jqSRpt37+2Nkja5GcpdkzPN+vfRCOvfemTPySMx89ZznDLzYMoBvr9nDL25dzKTU8an4sgWyGNxXqCQjiSTn8GFioHUPcWnRSpeOg20klUR2BmyofhWSo98j1dBN2tSVpwyNDCPcHxYdBG8ZhKPBmhHuQ6VHl/4ozYJRMANqD5IyDYybCvH+PRqMhn7zBuqrcwe2gw/qXXQFj5FoNzc4PVMtNa+iJw4KF8NhfNsTyfu3RabzlFL8x+JJfPbZrfiVBXs4yPyGoyytLWN6Sy3Oibnk/OwuNOfQJqYjh0bi4qe5XGR+9St0/PVvtP/5LzCo8W7P+vUx54/1MrXB4m1x3DJhKX+peG2goiigh3i6ci0fnvIeHJqN14/vNF2T4UxmpqdoHGYrxPmzuCCdTyyZwi83RitZj7X38O01e/jetfPQlCIQ1vnpuoM8fyB245E5OR6+sWrOeQ1WEhw2HrpxAd9avYd1ldEK6s6+IJ97dhvfvmYOiwvObUMYwzAoa/Ky5nA9r1c0nHRzj5tmvINeVwohLhgSHIkxZxgGrUYZg3sbJfX2YnPmo1ynu8ODH2hBDzagLF5OFCqcbLl2sCfA8ZePcOyZcmpfrCDQYd5pJqmkgPzrlzCvf0nL+dLU24E+qAlxst2N89gegm/8FrpaYi9wuLFc/iG0mVeP+e5o56I3GOKQN4ny2mzKu1yUd7nxhqwU7ttASXKYEm0v0+L9ZDkH/TvZkiD/DlTc0J5WLiCHSFh0QhWQBUOWprX5/Hz/9X1srm5hcBUNQHZikI9f1kpuchA/dqodaST6fKR7vcSlWDEa/oqhL4P0ZabvdX6ym2+tmsuXXthOuP/NYW8wzJdf2smv37+E5Lixr/7aUmNe7jjSMjXD8JI01bw8qrPcSvI0DX9PMz0JrQyuyHFs9pL0oWuHvy09HFlu6D0AXeUQ7h32PJqOYehhVGY0+FGahpE3DY7tI3UJKG8fnWsiPZTCBytxtV1GT0q0z1Bb395zCo50XadTq2bw7xX2tjL/W8MvOU198k88tG4jx+M9FHS1EBeKNDCNv+Yq0j91Kcoy9LFKaCQiz2vPB+7AMXkSTT/8EXrP8EtglM2Gq3TBGM/OLM+dzsrcBbxSu3VgrM3fxQvHNjEndSLVPeatxpfnzhu3Xm5CnE8fmFtERUsXqw9HN3vYUNXM77dUcOP0PP7rld0cbIqtOL1tdiGfvGQKVsv5f+3lsFr45nvm8OO15sCqLxTmgZd28uWrZrJq6plX3B9r72bN4XrWHG7geOfIrRhOWD45i9I8aYothDhzEhyJMdd+bDN6xqA3d4ZBSlcX5N90GleH8bdvx57UjdIU2in66fa1+qh5/jDVz5RzfPVRwr2RChJ7kpucVaVkLJlO+pJppC8qwZEyOrseDe1vlNnRQmjzY8Oeq029DMsVH0W5PaMyl/PFHwpzpLWL8mYvZU2dlDV5Odbe3V8abm4629HQxe4GgEkAJNmClCT0UJJiZdrkxUwzPAz/aIuABiLLDyGy7LB64HYAtlS38L3X9tLWG4i5etVUJzfOrsduNZdoe10uupxOUru7SenpQTWvBV81Ru4tKFu0mqg0P5V/X1rCT9YdHBhr6OrlP1/eyY9vWohtFF5YjqSixcvm6mbT2EjNPgPt+3AMaqHVsr2ejEuvAaDu4D9g0qB5N/aQfcVdKEs0DDH0EPQc7a8sKgf95Ft5A6A5IJiC0ZuIiotWlClNwyiYDlV7SFmVR7CxD9/eSKm+/uhGuDdaZdYbbqI31ESc9eyWA1at/ivhRebfK46uqTjTY5cAdL3+Ot2vv0E8MLU98sZCOZ2kf/Ye3MsSUGpoGCChkTBzlZaS+9Mf0/jd7xGorIo5Hjd/Pprr9HZgGk3zUidT72tlT1u0+vOwt5aqbvPuiRMSspiQkD30ciHekZRS3HfVDKo7eihvjv4/6bHtR/n7vuqYpVtOq4X7rprBiuLR/RmwahpfvGI6KS47f9wW/ZkM6wbfeW0v7b2B09oFrrGrl9crGlhzuN600cNIkuPsXDUpkxXF2czMSpaAWAhxViQ4EmPKMAxafDtgUFPshL4+HI4ccBWe4uoQfa1v40w1GFpZMlhPrZfqZ8s59vdyGtZVY4QNkmcUMemDq0i/ZDoZS6aRVFKAGqY58mgYuqNaZl1F7EmJGViXfxxtwvwxmdOZCIV1Ktu7KW/ycrCpk/JmL0dbuwjpxqkvHkZn0MbmtmQ2twEVkd2sshKclGQkMS0jiZKMJKamJ+Ky24mEUIN7P9UCuQTDdn6z+TBPDNO0Ot5h5f4rZ3LFpEyC4fk09m6hK2juH2VoGi2JiXS4XGR4vST0VEHFryL9aZLnDLyounVWAVVt3Tyzv2bg2j31HTz01gHuv2rGqL/4CoR1/rT9CH/eUUl40PfbbbcyPWO4HlxdOFLM5em1Lzcz96spdB3eQ9+QH7HE44k4r54UCYu6j0TCoq5y0EcucR+gOSFxaqSHlXtipL8PAHVA+cBpymLFKJgJlbtJv2MC9W1+Asd96DuP4OgsxZ8UDaZa+/aSF3/1qe97CF9DG8GM40D0e2Jv6aXwxltjzg0er6PlF78yjdnycsn65lewZTUAEhqJ02PLzibnRw/S/Mj/0PPWWtMx96WXjNOszJRSrMorpam3w9RrL6ibl2FelTNvrKcmxKhyWC1899p53P23jaY/Lg0NjXIT4/jOtfPGbBm6UoqPLSomOc7OI+vKGPxK6ucbymnz+bnnkikxry86+wK8eaSRNYfr2V3XzqnE2Swsm5jJyuJs5uelDLshiBBCnAkJjsSY8tbtIZxtXuaT2tUFuded4k14kGD35v7QKFZHWQvVz5Rz7B/ldB3tIn3xdLKuuorZX55G2qIS7InuYa8bC/Vdjaavs7oGBUlKQ5t/A5ZL7kDZxn/3pbBuUNPRMxAQlTd1crilK6Zx5PnW0NVHQ1cfbx6JfK8UUOhxU5KRyLRMRUl6mElpCrtFp6ajnG+v9lHW7I25ndnZHr62YhaZCZFGyDZLAnnxV+ML1tPg24hfN4d4IauVupQU4vx+Mjs7cdY9Bx07MbKvQzkjzaU/u7SE6o4edhyPXvtS2XEmpsZz+5yi0fmGAAcaO/jvN/ZT2Ra768qyiRnDltLroQq0Qb/VmzcfJ2V2KUYoROPRl2BRtEpJtfSRtfh2jOPPRgIjPbZqK4YlDhJK+sOiCahhm+jmEKkOOxK9L5sdo2gmWuVuMj9STN3/HCDsDRL8zXr4YnQ5T1ewikC4C7vlzF7A737wFyR/1dwM3GOfHvM7xQgGafzhg6Zt1ZXdTuZX75PQSJwVzekk40tfxDtlCq2P/gFCIeyTJxN/xfnaHfTc2TQrt064nEfLX6Y3HBsKz/JMIDPuwq5yFeJsZMQ7+fY1c/mPZ7cO+8euSwvT+eqKWSQ4TlG+PgreN6sQT5yD76zZY5rb47uqaO8NcN+VMwjoOusrm1hzuIEtNS2mPyANx6YplhSms3JKNpcUpp/1bnBCCDEcZRhnVzUwFkpLS41t27aN9zTEeVSx71cEc6PvbN19feT7nDDxYycJjgKE/duwOMwveL0VbRz+wx46DvqIy8gl45LIsrPE4rwLogzXCPnp3fwUP3Vhar70mbXP4giHUJmTsKz4JFrGuTUEPle+QIin9hxja20rh5q99AaHNgQ+ffnJLqamJzLV2EFJfDsp9iCHu92Ued2UdSdT3h1Pb+jsQiibBpNSFcc6DHqD5mOagg+XTuJDCyaO+Fc1w9DpCByiuWcLYTVMSGIYZHd0kNTbCyhIXQTpV6IsDrx9AT7x1CaOe6OBg6bgB9fNZ0nhuTW0HMofCvP7LRU8ubuK4V4jXjUpi/uvmoHLPjT39wLbTSNv3vk8y/70IG0vP03z4g6wRr83qS3ZpOuHIHSKMneLCxJPhEVFKHU6L0QNIsFRjXm0txuq9uCv6qT+V2XoIYO+/34PWm40NPU4ZpDlOv1qjZoXNtIRfgXrsmizT5s/xMSMu9GGPBdaf/s7Ov/xjGks/fOfImFFCjC0N0QKMBMJjcTpCnu9BKprcE6dgrKN/RvRU6nqauCJI29gDKpxsCiNe6bdSKJ9/P64IsRoe/5ADQ++eWDgawV8dOEk/l/pJLRxfr24vbaVr/xzZ8xrrwkp8dR7e+kLnfw1maZgfm4qK4qzuXxixriEYEKIdy6l1HbDMEpP51ypOBJjprvpkCk0Akjt7oasVScJevzooe0xoVHzluO0bHUw+4EHTNtsXyj0o9sIvfE7GpQO868YGPf0dOGvx2pyAAAgAElEQVTQrFiWfght7rUjVGyMnaOtXXztlV2mredPV1ZCHCUZiUxNT6QkI4kp6YkDL1iMuhpoj4QGeXF+ripKhLxV6JqL6o4eypo6OdjYSVlTJxWnuewtqENZc+x5GfFOvrZiNnNyTv4Xc6U0PI4SEm0TaOjYTGe4DDUoSEEp6j0eghYLqd3dqNbN0HkAI2sVCYnT+f518/nk3zfTE4iUuesGfHP1Hn5562KKUuJP99t2Unvq2/nB6/uoHabBZYrLzheWTWfZxMxhrgTzkj5o2lCDu6CEcEszbcF9YI32ntK6wqRxbOTQyOIeEhadaYm7ItKLKgBEK+5UXDxGwXQcxj7Sbp9A8/8dRf1jP3wmWnXU6T9EunM+Fu3UO9oEu3xsue9nzHxrGYPjyBRrQUxo5Nu2LSY0Slh1BfFXS2gkzg9LYiJxM2eM9zRGVJSQxZU5c3ijbtfA2KL0EgmNxEXvxun5dPlDPLq1gjSXg89ePo1LzvMffc7WgrxUHrl5Efe9sJ32QUvqhqs2HmxaRhIrirO5anIWae7ztwOcEEKMRCqOxJg5suc3BPKjAVGc309htxUm3T1CcNSHoe9AaebQqGFdNfWvh5j39Y+N8ozPnNHZROjN32Mcjexisy2/mLcmzx44Pr27h5vm3IJKGL6x8Vh69VAdP3rzwCn/mgWQ5nYMBESRsCjppDuLGSEfHH8G/M2QPCdm17LBAmGdihYvZf09lMqaOqlu7+F0fjNdMTGN+66cTYLzzP/CVvHcyzT3bSZ5ZW7MsaSeHrI6O6OdtNwTIPs6tjTq3P/iDlMlUG5iHL96/xKSnGe/05ovGOLXmw7zj73Vwz7ua0ty+MylJSd5nJ3ADtPIyyv/wuJHvobv+T/i/WiWqdooy6uT3N1gvglrfGQZWtJ0cBWcpx39dGAvYF4iaHS2QO1B2l+ppf2tRnofXIElPRoAZ8QtJNU555S3vunf/wfLhCriPzptYEzTdSYn3YXFGn0zHGpro/Yzn0XvjO6iYy8pJPeHd6EsQ5t/S2gkLl6GYbCj5TD72ivJd2dwZc4ctHfQ7p1CnAvDiNTbjXeV0XBqO3v44vPbqfOOsHspUJDsZuWUbK4uziIvSQJfIcS5k4ojccHp7aghMOT9eWp3N2TcNEJo5MMwdsWERsdfPUrlU61c9r9fGr3JngUjFETf/izhLU9DKPoXo4YEcxVM9pTLxz00CoR1fr6+jH/sqxn2eJLTZgqISjISSXOfWf8lZXVB4QdP61y7RWN6ZjLTM6O7X/UEQpQ3R3ZrO1GZ1NjdN+ga+OxSCzdO96FUB3DmfzmcdON7qLxpLc1Prmfyz5egHNGgoNPtJmSxkNPejsUwIlvSH/kVi1Iv5VOXFPOzDYcHzj3u7eW/Xt7FQzeWntUWvttqWvnhm/tp6Ip9sZgR7+RLV05nccGpHp+52qhhXTUhXxy2xiq6J/hNoZE1qJM0NDRKnAZ57ztPYdFgGpEQZheRpXQRKikNIzwZz3sg0NhHYPVR+GC0UqOtbz8pjpknXRbXtHE/B3/xLEsrb2Zwq9MkPckUGhm6TvNDPzaFRnGlE8j62q0SGol3HaUUC9KnsCB9ynhPRYgxp5Q6ydYq4ysvyc3Pb13Ml57fTkVrtBo4I97J1cVZrCjOZnJqwgXRikEI8e4kwZEYEw3Va2BQtZEjGMRtJEYqHGL0REKjIX1oqp87RPmvK1n+929fUP/j1I/tJvTGb6G9LuZY45DgKMedOlbTGlZjVy9ff3U3Bxo7Y45dV5LLh0snkZXgHPfvr9tuZX5uKvNzo9+vNl87ZU27aekJsahAIytBEWnCvA/IBiZzJr/SlFJc+svP8+y8uzlwy+uU/N8yLMnRcu8ep5PqtDTyWlux6ToYYWhZx/vj9lI5eQEvVkTLyHfWtfOTdQe594rYhswj6fYH+cWGcl44eHzY4++dkc89l0zBHdPLaKgOwLzDys6vv8WUu66n6W+PoX9jselYWnen+YWzuwhybxmF0OgECzCbSEVUdEmYSsnGCAVI/0CY4B8q8PZORYuLPNaQ4cMbqCTJMXnYWwwHgqy/+2GKPlJIKGFQqGkYeBKWms7tfOppenf1L83RFJ4PLCH5g0tQamhtl4RGQgghxk+qy8HPbl3Ec/tr6fYHWZifxqzs5AuyQkoI8e4jwZEYdf6eZvpygkSqDyJSu7pQGcPtpNYF7EYpc/fjo0/sZ9/D+7j2tYex2C+Mxn9GVyvhtx5FP7xx2OP+3BI6XNHeNwo1rjvXbK1p4Vur99DZZ/7e2i0an7t8GjdMzxvhygtDisvDpUWLgYNElmYNVk8kPJnO4C3ZT8Wdl87KF77HP5ffy973rGbGU1diy4/+m/ltNo6lpZHf1oYjFKlrUaEOPp/9OrVt89jdFn0uPneglgmp8bxvVmHM/Qy1oaqJh946QHNP7A5HOYlx3HfVDFNodnLmaqP6N6po2dLIvOuO0XmleYmaLRTqb/7dz5kF+XegtNH+X4ENmEMkPIo+ZpVRiBYKknV7mJ6NtRjLiwaONbZtJTFr0rBB3N4fPknH/ipm/OlaBi+0dIdtOJzR0sa+sjLa/vRnALREJxlfvA7XgiJipRF57khoJIQQYvy4bFY+MLdovKchhBAxZGG7GHUNR14Gi/nNa3zYBYnTh5zZCewEzMHG4Ud3s/2rG1nx7HexJbhGfb6nYoRDhLc9Q/CPnx0+NHImYFn5SVqu+bRpOM2ZhG3U36DH0g2Dx7Yd4YvPb48JjbIT4/j5rYsv+NAoKg6YB0yEmILzPiLBRCVw+ju3pS+exlVPfg3/0R72rFxN7x5zP56Q1cqx1DR67NEeRjbN4Fsle8hymoOfn71dztaalhHvq7MvwLdX7+GBl3bGhEYKuH1OIY/ecekZhEbtRCqOonZ+Yy3FN82mZ/dG9CvNIVZqV1f0u2bzQMEHUZaxaqrpJBIeDfkZyJ6EtSCHHI+BMah5VNjZQ9Xbr8XcSkdZNbu/82dSSj2EC8whYUpctC+S3tND0w8fBF3HUZxJ7k//ZYTQaCJSaSSEEEIIIcTIJDgSoyro9+LLMvduSe3uRssY2iy5HdgNmBs1H/z5Vrbe9yYrX/w+ruzxXeYFoNfsI/Tnewmv+xMEh/ZHUWizVmH76P9gmbmChl7z8qFsV8rYTbRfV1+QL7+0k99uqYhpunxJYTq/ff8lTE1PHPN5nRsFFAILgOGCxCqGLos6lfwbLuGSX36OYHMfe69fg3eNefmYbtGo8aTQ6Ygui0q2hfj+zHLiLNHnbNgw+Pqru6np6Im5jzePNPD/Hl/P6sP1MccKkiO9DT5zWQlxttMNFw2GVhvVramkaX01mZYGQjdNGbnayOKGortQtvOzG9zpcxNZthadl1IKcqeSMDGDuEav6ez6ug0cfvTlga8NXWfDJx5GDwQp/sYsGFSNZA+DyxUJjgzDoPlnPyfU2ETCdbPJefAObBlDn+c2YC6R55IsAxBCCCGEEGIkslRNjKqGw/+E3EGNecNhEgM2SJo56KxWIn1qzFUiex/cwM6vv817Vj9IckkB48noaSe89jH0srXDHlcZk7BcfTdaVrQnS73PXLmS7Rrb4Ku82cvXXt4V03RZU/CxRcXcNX/CO3zdfAJQChwBhvYJ6gK2AsVE+h+d+nFO/fj1+I63sOtbj3Hgg+uY9OAC0j9cHD3BolGfmkJfbRsZWh8KmOju5b+mVfCVfVMw+u+j2x/igZd28Ktbl5DgtNHq8/OTtQd562hjzH1alOLOeUV8uHQSDuuZVrx0MHTJ3s5vvMXkBSnodKNfYf6ZSe3ujsxQs0PhB1H2sQ8yI5KAGRjG3oHcR2kaRv50Mmr3Uz3oTM+qHLYt+RW9je3Muv8DHPrtSzSu24sj1YZ1YZYpZvZYi9C0yO+arldX49u8kfR7ryFh+dDKRoBEIlVGsoWxEEIIIYQQpyLBkRg1oVAv3antDH6apXR3o2UsH1Rt1AQcgCH1MDu+/ha7v/M2y5/6BpmXzWS8GHoYfdc/CW98EgLDVLA43Fgu+yDarJUoLfrGP2zo1PlaTaeOZcXRCwdq+cm6gwTC5jAuyWnj6yvnUJo//tVb54cFmAKkAmXA4IbqOlAOtAAlgD3m6qHmfv3/4TvewqHfvcSRL2zDX91D3tfmms5pz0uh90AzBckKjQCXpnbwiYk1/OpoNKip6fDxX6/u4j1TcvjZ+nK8/uDQu2JSajwPLJ91lhVfsdVGx185Qtfu45Rc4SZ040ywRZ+PtlCIJJ8PlCXS0ygu+yzu83xKQ6lpRPpVRSiLlbicEpztVfQNqrrKvqeE7ff+lq4jdVT+7S0Apn19FuFBQZumGyR6Ik2xA9U1dD73JLkPfxB70XA7GOYBk5CCWyGEEEIIIU6PBEdi1DQdegWyo08xSzhMUp+Cgtn9Iw1E3uybQ6MtX1zN/oc3s/iRf6fwFvMOSWNJrysj/PpvMJqrhj2uzViOZem/oFzmPiveQA/PHFtPVzAaNGlKI92ZPPQmzjt/KMyP1x7kpbLYnbpmZCbxjVVzyEyIG/V5jL1UYCHRoGiwVmALkfBouCAhSinFJb/8HL6GVmpf3EztTw7SV+Nj0s8Wo+zRoKJvejoV+5opSszDbq3lA3n1VPXE8XJj+sA522vb2F7bFnMfVk3x4dJJfHDeBGyWsw0v2omtNlrLlJl28NjRrxrS2+hEtVHuLaj4CWd5n+dbFpGg78jAiGZzkBKfTp0/uswz/Y4iar63h0O/ewkAZVUk3lxoigiTSMGqOdEDAbyv/JncB29Dcw2tJrIQeQ5kjNLjEUIIIYQQ4uJ01n9yVUrlK6XeUEodUErtV0r9R/94ilJqtVLqcP9/Pf3jSin1iFKqQim1Ryk1/3w9CHHh0cMhvG7z9vSenh4sGcv6K3PqiFQbmEOjDZ96if0Pb2bWfR9g+mduHrP5Dmb4Ogm9+nNCT3512NBIpRVivf27WFd9OiY0Otx5nN+V/5PjPebwIteVilUb3ea7xzt9fOrvm4cNjW6dVcAjNy+6SEOjE+xElh9NJbbRcRDYSyRYCnMymtXClU98jbRFJQC0PH2MA+9/A70rYDpPn5lOJR30WOegCudy7w3pzMw6+ZK4kgw7v71tKh8uLTqH0Ci22qjmxcNYjjeQnGJBf19JbG8jnw+yr0UlDbdsazwVAPmmkcQ4D9ZB/2vS4qxk/mt0yeDkuycScDtN13gSLgN0+g48T9rHlwwTGrmI9MSS0EgIIYQQQogzdS61+iHgXsMwpgNLgE8rpaYDDwCvGYZRDLzW/zXAtUQajhQDdwO/PIf7Fhe4lgNrMBKjS4OUrpPsMyB5LlBD5A18lB7WWfeR5yj/1Q4m3nU1C773sbGdMJFlaeHdrxD8w2fR978ee4I9DssVH8V614NouSWmQ2FD57XjO3iq8i36wuaAId4Wx6q80tGcOuurmvj43zZyuKXLNO60Wvjaill87vJp5xBUvJMoIIdI76PhloDVEel95B3mWJTNHcfK579LYnFka/eu9c3sWbWacL256bVRkEhDVjsBlwOHw853rrGSOUy/absFPnmJhV/cajAx9SjwNpEgq4GhuwieWlvM/Pd9by0Tp9pROW5Clw/T2yh9GSpl4Rnez1iZhBGKVoIppUiJ85jOyPpYMcoZCQOz75lqOuYOO3DYPYS963DNjV0OahgZREIj93mfuRBCCCGEEO8GZ71UzTCMeqC+//MupdRBIBd4L3Bl/2l/BN4E7u8ff8wwDAPYpJRKVkpl99+OuIgYhkGH9RCDd7xK6u3FmrEUpdUytFpCD4Z5665nqHrqINlXz2fp776E0sYu5DBCAYxjuwlv/htG45Fhz9FKLsdy+YdR8Z6YYx3+bp49tj6mpxHAxIRsbiy8BJfVGXPsfAjpOr/fUsGfd1TGHMtPdvGda+YxIWWsd866ELiAecCx/o/BlW29RHZdK+JkO2o505NZ9c8f8PKKL5Ay20Pe9ZPJTsqn1dJtCgeDeoiqruPku7NIccXx/eusfPofIXr786DZ2Yr7r7KSnzz4fnQiS+pa+u/fA6QTWUp3sl5MsdVG1c8dItnXij3dju3zC+jTovdjDYVIckyF9CtOcpvjTaGsMwjWr8aWHXnsyY5EWnrb0fsb5tvSnaS9vxCtupNAjjkQ9LimYehbsCSa+3kZoTBGeCKaYwKya5oQQgghhBBn77z0OFJKFRF5l7YZyBwUBjUAmf2f5xIpNTmhtn9MgqOLTMfeNwnnm7dJT/YBuakMfdMb6gvxxm1PUftiBSlzJrH86W9gsdtGfY6RsGgX+qGN6Ee3QqB32PNUSh6W5R9Hyx++QfehjhperNlEX9hcNaJQXJE9hyUZ0yLbjY+Cdp+fb67ew47jsX10rpyUyf1XzcRtfze3MdOACUAKkWWRg/+NTwQwbcA0YOgSvl6glYSJLby/4hOoQTlmvJFEbXcjPaFoDyvd0KnuriPLmsqklGJ+c1OQ1/YcpjC+myumONHiUxk5EDL659FGpBIviUiIlA4MDRxbiewYF3Xkp+sozLOR8rGp1GeZ+2ilheJR2deP2nPw/NGweJbiP/wCjuIMLEoj2ZFAmz/ax6noPxfi7uiib9BjsaHhdnpRmvnxhZq7CPdMwFE0ccwegRBCCCGEEBerc35XqZSKB54GPmcYhnfwGxTDMAyllDHixcPf3t1ElrJRUDC+W7CLM2cYBh3GLiJbpUe4+/pwFl2KUjWmc4M9AV67+W/Uv1aJuyCDlS9+D3vi6C0nMUJ+jKpd6Ic3oh/dNmJYBIDNiWXJbWjzbkBZYn9MwnqY1+t2sa2lPOZYgs3FzYWXkRefHnPsfNlb3843Xt1Nc4/fNG5Rik9eOoXbZhe+A8KCsZJEZOnaESJL1QbrJLJ0rZhISNPa/xENhdSQ4jdNaeTHZ9Hga6YjEA1xDKA+1Er7s4cpWPVpPrJiDnTshqbXoaECw5UICamQmIayn6wCrbP/o4LIz9GJECmOmGqjZ8rI0NtJe99EeubmYAz6N7fqkJRx26AdDC9smtNN3yEnytmKPT+VFGeSKTiypGv408zVRilxHrQhoZFv5zGC1QkkvXfWmMxbCCGEEEKIi905BUdKKRuR0OgvhmH8vX+48cQSNKVUNpH91gGOY+6Cmtc/ZmIYxq+BXwOUlpaeUegkxl/X3rfpyzcvjfI4M1H2btNYoLOP1Tc8SdP6GuzJ8ax66fu4ck6+49XZMEJ+jMqd6Ic3oB/dDsG+U16jii/BesVHUQnDb1nf7u/mmaq3aeiNrfSZnJjD9QWX4LIObc57fhiGwdN7q/n5hnLCuvnHI9Xl4Bur5jAnJ3Y5nbASaZqdSmQnv8EVYuH+sdMT7A5Qt6aS2pcq0GY6Sf5Qkel435UpVL79MwoXfRybZx5G4nRoWQ+tG8HnhcZKDGc8JPaHSA7X8HcERKqLuoCjRIIt8/O38Q8bmPy+ApyLM6lzm0PXNNdiNMvoPA9HS8KKa6i7/3Nkfe1abKkJJNri8QajvzsGB2MKRZIjwXR9++Ob6DvgI+ub3xyzOQshhBBCCHGxO+vgSEXKGX4HHDQM4+FBh54DPgz8oP+/zw4a/4xS6glgMdAp/Y0uPl59C6joTmN2NOKTzQFMX6uPV695nNbt9VgcNq5+9tskTy86b3Mwgn6Mqh2RZWiVpxcW4UpGK16CVrIMLWfqiKeVdVTzUvVm/Lp5aZqG4qqcuSxMLxnVSp//3XSY/9sZ289obo6Hr6+aQ2rMblLCLA1YRCQoiu1JNTInkMbRJ3az7iO/Rg9Ed2abUO0n8/5i005mwUWpVO7/HYUT78CRUwCZyzE886HxNfDuh77uyEfTMQx7HCSmQVI6ynmyijvz87jm2YNMWGQn+apsGuPjzdVGykWy80LbQe3UNIeDhBU3UP/Vv5L70AdIcSaZgqPBkhwJWFSkYXa4u4+mH/2TwOF2cv/nkTHtkSaEEEIIIcTF7lwqji4DPgTsVUrt6h/7CpHA6K9KqY8R6Up7e/+xl4DriKy/8AEfPYf7Fhcg37436ck3VwCkuFJNQUpvYzevrPwL7fuaQSmW/enLZF0++5zv2wj6MSp3RCqLKnecWVg05VJUTglKG7qFe1RID/Na3Q52tByOOZZkc/PeosvIdZ//iqnB9jd08PgwodGdc4v4+JJirPJm+TTZgVlElq1VAPow5ygiS9xS+z9cgGLCHZNpXHecsl8+N3Bm5cM7UH4rGV8pAmf0ORSe4aGy8m8kPa+InzIf1/x5WPLfh+FbBA2vQm9/wWWgF1pqoKUGw+YATyF48lFW8+58gxm6gVZ2kNT3FhLUNDqGVhvFzUepkZ/PF7LE96yi46mnqP/aP8j579twWZ34QrE/zymOSEDtr2ik8XvPE2r0kvWtb2JNkYo7IYQQQgghzqdz2VXtbUbequbqYc43gE+f7f2JC5th6PSwFV2LvmnTlEaSPbpsrafGy8sr/oz3cGSJ1+Iff4qi95/9bk9GsA/96HaMwxsjYVHIf+qL3J5IWFR8ySnDohPa/F6eqVpPY297zLEpSXlcl7+EOOvJdsI6dyFd5+G1B0x7g7lsFr68fBZXTMoc8ToxEkWkN7+HSPVRJ2AjGhSlMNyvR6UUix/5DL6GNqr/8fbA+NGfb0GzuEn/QhpGQvQ6Y0ISHblhup57HstPf4JzUjGuhaW4FizDntMLza9D0Bu9g6Afmg5B0yGMxGLImoey9QEdDN4ZrnXtIbKvjVTytQ2tNtLiSbYXn/N3aLwomw3PHXfQ8rOf0/Cd5/F8ZUVMcOSyxuGw2PG+vIfWX72BEQyTdOstuBbMH6dZCyGEEEIIcfFSkTznwlRaWmps27ZtvKchToP/wAvUptUQsEV3REt1JpMRF3lz29fi4/lFv6e7qgOAmffezsIHP3Fat22EgtDditHVgtHVAl0t6E2VGFU7IDRyVcYAd8qgyqKpZ9Qs+EB7Ff+s2UJAD5nGNaWxPGcepWlTxqQJ9VN7jvHI2+Y+PA/fWEpp/vB9mMSZCgEWTnfb9lCvn1dW3UfT+n2m8elfvhnPR+2EU2MDSVXfjfX3u9EOtgBg8XhwLVpA4qWp2Fw1KCMYcw0o8MyHjKUoay96oIWqx1+mcE4Pmk0R1DSOZmaagqMs12V4HNNO94FfkIxQiJq77yHU2Ij76ml0f2wGgUHLQ3Mc6fj/dyPda/YD4CieTM6DP0TZRn9HRiGEEEIIIS4GSqnthmGUns657+a9usV5YoR8+NU+AjbzUi2PI9rraPPnXx0IjSbeuZzS//545FpDh55OjO4W8LZgdLdgeFswupqhKxIW4es480nFp6AVX4JWfCkqZ8oZ7ywV1EO8dnwHO1srYo4l2+O5uegysl1jE9q09Pj57WbzErkVxVkSGp1XZ/ar0BrnYMWz3+bFpf9BZ1n1wPiB7z9DafrHcV/vx59mDoKM7HiCX70MbV0N1sf3EW5vp+uVNXS9ApZkB2m3TydukhVzDmlA+3bo3IeRfjnVq+vJm9qJ1h+QxFQbKTfJ9iln/OgvNMpqxXPnB2j+yU/pee0gronJBK7IAMBtOPA+8CzBI82Rc+PiyLjvSxIaCSGEEEIIMUqk4kics9DBv1CX3E6PM7rFeKI9nlx3ZAlVwxtH2P3pv+H2aKTPyWbi9bPA147hbYbuNhhSzXPW4lMjYdGUS1DZZx4WndDa5+WZqrdp6osNrEqS8rm2YDFOy+guTRvsm6/u5rWKhoGv3XYrf7pzKWluaYQ93rqPNfLiZf+Or87caPvyPz1A2vsKaezeiG4ZppKoO4D1iQNobx1DDfoVbM9xkXJjPnGTEmOvGSKkaRy5CKuNTjDCYWrv+RTBujoArCWZWKZk0ffSPlQo2pw8/Yv3knDVleM0SyGEEEIIId6ZzqTiSIIjcU4MXy3+mseozMgwjRcl5BJndaL7A4T+/Dh4u0ZnAglp/T2LLkVlF591WHTCvrZKXq7dSnBImGVRGlfnzmd+avGYLE07YVtNK1943vwz8B9LS3jf7MIxm4M4ubY9R3hp2ecJensGxjSblZUvfo/M5TNp6t1KR6Bs2GtVeSvWR3ej1Zp/Plwzkkm5Ph9bmnPY6wAaExNpj4/2ELMqN5OTbn/HNsUeTtcbb9L8o4dGPB5/9XIyvvD5MZyREEIIIYQQFwdZqibGhGHo6FXP0JZo3tEpzuIkzhp5w6tv3HQeQiMFbg8qIRWVkAYJaaiEtEhVUdbkcw6LILI07dXabexpOxpzzGOP5+aipWS5Us75fs5EIKzz43UHTGPFaQm8d2b+mM5DnFzK7Eks//s3WX3tA+jBSOCoB0O8/r5vcM0bD5E+bSFuLZ+m8BaCqtN0rTE1leB3r8Ty0hEsz5Sj/JFKGt/+DnxlnSRemkHyihwsceZf1SFNoz3OZRpLi5t7UYVGAPHLLqfjyb8SrKmJOWbLySHtk/eMw6yEEEIIIYR4d5HgSJy99h3oqgOvy7yrV4oz0ttIr29A37X31LfjcJsDoYHPU1EJ6RDvQVlGr39JIBzi/46sod7XFnNsWnIh1+YvwjGK9z+Sx3dWUtPhG/haAfdeMR2rdu5BmTi/cpbP4/I/3M9bd313YCzY5eP50k8OfK2siqx7ppJ33yws7kG/ei0a4RuL8S/IIfiL3SQcacJiURA28K5rpHt7K56VOSQsyUBZItVuDX4bWKLPA6tyk3QR9DYaSlkseO66k6Yf/NB8wGol4/4vocXFjc/EhBBCCCGEeBeR4EicFSPUg1G/ho5415CtwK0k2NwY4TDh1W9A/1JIlVOCSs6G/qohU0BkH783f4Zh8FLNppjQyKosrMxbwJyUSWO6NO2Euk4ff9purn66cXoe0zOTx3wu4vRMvHM5vroWtn7pf4c9boQM6n9WRusz1Uz471I81+Sajltz3Fi/cylNL9TQ8eBOEvUAqekWnIRofbYa74Ym4qYlE2zOhfsAAB3bSURBVDKg6yOlpv3f0uLmoF1k1UYnuC+7DPuEIgKVVQNjqR/9CI7Jk8dtTkIIIYQQQrybSHAkzk7ja0CADrfHNJziSEIpRXjrDozWVnQdrCs+gXXOqvGZ5ylsajrIwY5q01iKI5Fbii4jI84zwlWjyzAMfrLuIIGwPjCWHGfn7iXF4zIfcfpm3ns7vuMt7P/J0yOeE6j1UX7XWjzX51H0gwU4csxLzlJuyCfpyixqvruHit8dxuWElHQLKS1hEup6Cd41E80RDYki1UZTR+0xjTelaWTc9yXq/+vrhFtaSbr1FhLfe9N4T0sIIYQQQoh3DQmOxBkzfDXQsQtvXBwhS/QNrEKRbE/AaGsnvGUbfT1gu/l+rDMXjeNsR3bUW8eb9btMY+nOZD5UvHJclqadsK6yiU3VLaaxT14yhUTn2O3kJs7ewh/dA0pR/usXCPn8WOxWlM2KZrOaPtcrFMf+dR/p/1ZA8i2ZA8vQACzxNoq+v4DcT86m+2/d6PXgt1kJpdhIvD4OiG5qcDFXG51gLyig8A+PYgSDKNv4/WwKIYQQQgjxbiTBkTgjhqFD/T8xgHa3uSl2siMBi2YhuOZ1WqsCWK79AokXaGjU5u/i2WPrTWNOi533TVg2rqGRLxjip+vMO3DNzvZwzdSccZqROFNK01j00CdZ+KN7TnuZY1+ohXrfevrCzaZxW4ENz70ePI7ppMeV0tK7gzb/voHjF3u10VASGgkhhBBCCDH2JDgSZ6ZtG/Q10Gu302c3V8CkOJII797L0eeP4Z92G7NXLhunSZ5cIBzk6cq19IWDA2MKxc1Fl+FxxJ/kytH3x61HaO7pG/jaoim+sGzauPRZEufmTP7NnNY0ihJupN1fRnPvVnSCpuPt/gN0BaoIG37TeKrz4q82EkIIIYQQQowvCY7EaTNC3dD0BhBbbRRvc2HzBdj2n6/THprOygf+ZTymeEqGYfBC9SZa+szbol+ZM4cJCdnjNKuIo61d/HXPMdPY7bMLmZiaME4zEmNJKY0U53QS7EU0+TbhDZqbo4cMn+lrq3KR7Hj3VBsJIYQQQgghxofs6y1OX+NroPsJWCx0Oc07oaU4ktj5wCsc3eVg2WNfRl2gW8ZvbDpAeWeNaWxaciGL06eN04wiDMPg4bUHCevR3jUZ8U4+vHDSOM5KjAeb5iI3fjn58ddg00YODVOdc6XaSAghhBBCCDHqLsx39+KCY/RUQ8duADoSEhm8F7hDs9P2z2Ps+XU5Vz7+nzjTL8wt4yu8x3mrfrdpLMOZzPUFi8d9KdjL5XXsqW83jX12aQkumxQFvlvF2/KYmPg+Up1zGfqrOlJtNGV8JiaEEEIIIYR4V5HgSJySEQ5iVPwFgLBSdLjM24cn6HGs/8TLzP/2v5K5dNZ4TPGU2vxenqvaYBo70Qzbpo1vOOPtC/CLDeWmsSUFaVw+IWOcZiQuFJqykhFXysTEW3BZI0spFRrZ7svRlISKQgghhBBCiNEn7zzESRm9XYQ3PIQlK5IxelMy0AdtBW5RGuUPbCZ1wUxm3XfHeE3zpPzhIE9XrsOvm5th31K0lORxboYN8OtNh+nsi87NbtH43OXSEFtEOSweChOuJxDuwqI5sCj7qS8SQgghhBBCiPNAgiMxIr3lGKGXfoh1dhoAhsVGm9MBRmjgHGudwfGXannvjv+9IPsaRZphb4xphr08Zy5FCVnjNKuoA40dPH+g1jT2oQUTyUlyjXCFeDezW6RRuhBCCCGEEGJsXXjv9MUFQa/YTOiJr2DJtqFskQa83ZkFBAeFRgB7734j0tcoLWk8pnlK6xv3cajTHMzM8BSxML1knGYUFdJ1HnrrwKD6LchPdnHnvAnjNichhBBCCCGEEGIwqTgSJoaho296ivCmJ1EeN1pOSuRAvId2KzAoNwru62bie28k87KZ4zLXUznceZx1DXtNY5lxHq7NXxSzDMwfCnOktYushDhSXI4xmd+z+2o43NJlGvv85dOxWyTPFUIIIYQQQghxYZDgSAwwAr2EXvkfjIrNoMBSkhs5oFnoyyzE19tkOr/jKS+X//iz4zDTU2vt8/L8MXMz7DiLY6AZtmEYVLZ1s6Wmla01LeyuaycQ1rFoittmF/KR0km47KP349HS4+e3WypMY1dPzqI0P3XU7lMIIYQQQgghhDhTEhwJjGAfevnbhLc/B23HAdAK0lAJcZETMgppD/earvEf7GbRf/3HBdnXKNIMe21MM+wV2UvYdszL1ppKtta00Nzjj7k2rBs8sauK1Yfq+dSlU1hRnD0qTap/saGMnkC0fMtls/Dpy6ae9/sRQgghhBBCCCHOhQRH72J6cxX6nlfRy9ZCYFAw5LCiTepvHB2XQCg5E6+32nRtauICnKkXXl8jwzB4/tgGWv1edAM6u6w0d9gI+pL554Z9pn5CJ9Pq8/PtNXt57kAtn7t8GpNSz19T4u21raw53GAa+7fFxaS5neftPoQQQgghhBBCiPNBgqN3mRPVRfreNRgNh4c9xzIlB2W1gFKQU0xHwIsxKHIxuhV5My47rfur6/RR3uzFabWQ4rKT4nLgibNjHaU+Ps8f3clrh1to6YintdNGKHzifgKnvNamKYK6OVraXdfOv/11IzfPzOdfF00mwWE7p/kFwjoPrz1gGitOS+DmmfnndLtCCCGEEEIIIcRokODoXWLE6qIhlCceLdsT+SItH8Phor2z2XROZtoilDp58LO3vp3Hd1WxvrJp2CqfJKeNFJcjEibFOQY+T3VFP09xOUh02tBOslSsNxhiV107W6pbWH+sgQZvAIg/6dxOcFg15uaksCg/jUUFqWTEO/nT9qM8sauK0KAAKWwYPL23mtcqGrhnyRSuKck56ZxO5omdldR0+Aa+VsC9V0zHegEu+RNCCCGEEEIIISQ4uoidTnXRAFcy2syrsKS1QagDHC5Iy8cb6CZshAdOU1hJdg7fi0c3DNZXNvH4rir2NXSc9O46+4J09gWpbDv5tCyaIiXO3h8mRYMmu1VjV10be+raY6qETmZSajyL8tNYmJ/GrOxkHFaL6fjdS6ZwbUkuj7xdxubqFtOxjt4AP3hjH88fqOFzl09jasaZLdWr8/p4bPtR09gN0/OYnpl8RrcjhBBCCCGEEEKMFQmOLkKnW10EClU4B8uslaiJpdC+BRr7g42cyaAUbX5zAJTsmIpF2U1j/lCYV8rreHJ3lama5nwI6wbNPf5hG1mfjiSnjYX5qSzsD4vS3I5TXpOf7OaH189nfVUzj7xdRkOX+Xu4v7GTu5/axE0z8vm3xZNJctpHuKUowzD46boyAmHdNLdPLCk+8wclhBBCCCGEEEKMEQmOLhJnXl20HMvMFaikzMj1fY3Q/FbkuCcb5UqiJ9iLP2zuDZTimDHwubcvwDP7avj73mraekfuITQpNR5PnIM2n5+23gCdvYHTblJ9ppQySE4IsaQgnVtKSpiSnnhWy8qUUiydkMHC/FT+b2clf9lRaQp9DODZ/TW8UdHAx5cUc8O0PCzayPfzdmUTG4+Zl/x96tKpJJ5G6CSEEEIIIYQQQowXCY7e4U6/uoj+6qJVqImlKEvkn97QA9C8Flo2ATpY7ZBZBECbv9N0fbytELslkXpvL3/dXcVLB4/TGwozkkX5qdw5bwLzc1NMW9qHwjodfQFafQHafX7afAHafH5aT3ze64+ETL6Aacv6kcQ5wqQlB0lPDpCSFGJ+xgSuz19kus+z5bBa+OjCybxnag4/X1/Ousom03GvP8hDbx3ghf7d12ZkxS478wVD/PTtMtPY7Oxk3jM155znJ4QQQgghhBBCjCYJjt6BjHAIvWwt+p5XT6+6aMZyLDOvRiVnmW/HWw4NL0NwUECUMxllsRIIB+kO9pjOb+uayKMbd/NGRSNhY/iaIYumWDE5mw/MLWJS2vBb2FstGmlu52ltP+8PhQeCpRNhUqvPT3cgiM0W4Lh+EJdT50RGlO1K4Zq8heclNBosJ9HFd6+dx+bqZn66rozaTvOSvPJmL5/8+2auK8nlE0uK8biiS+L+uO0ITd19A19blOILy6afdYNtIYQQQgghhBBirEhw9A5i6GH0g28R3vQ38Dad9Nxo76KFA9VFA7cT6IwERl3l5osS01AJqQC091cbGQYcaHCypiyFAw0VI95fnM3CjdPzuG12IZkJcWfx6IbnsFrITowjO9F8m829HTx2+FXcenT5mMvq5NaiZVg1y9CbOW8WF6Tzhw+k8tddVTy2/Sh9QyquXio7ztqjjfzrosncPDOfmg4ff919zHTObXMKmZg6fKgmhBBCCCGEEEJcSCQ4egcw9DB6+XrCm/4KHfUD4yEU1sHdgk5SXQRgGGFo3QRNa8EImg9a7RiZE1FA2NBp6fWy5ZiL1WUJ1HaM3Icn1eXg/bMLuGlGPgkO27k+1NPSGwrwdOVaAnp0GZuG4taipSTaXaN+/3aLxr8smMjKKdn8YsMh3jjSYDreHQjxyNtlvHCgFptFIzxo17d0t5OPLJw06nMUQgghhBBCCCHOBwmOLmCGoWMc3kR445MYbbUABNB40TKdpx1zaAi7sRPGbQRwK4Vb2YivsJBQvZ9452HiXU4S3U4SE1zEW33E92wnXm8lwaYRb7USbwlj1QyMpCyM1EIsdju+gMGTe3w8sy+Ldt/IT49Cj5s7505gxZRs7BZtTL4fIT3MEW8dm5oO0B7oNh1bmVdKfnzGmMzjhMyEOL75njncVJvHT9cdpKrdvLTvaFt3zDWfXVqCyyY/dkIIIYQQQggh3hnkHewFyDAMjKNbI4FRcxUQqS56RZvKn23zaSYe+ldIBbAQUHG0R06KfPgGPhlyy9n9H1FOq0GCQxHvUCQ4ghxtNegOjFxhNCfHw51zi1hSmD4mPXoMw6C6u4n97VWUdVTj14Mx58xJmcS81MmjPpeRLMhL5fe3X8rTe6t5dGsFvuDwDcMXF6SxbOLYhltCCCGEEEIIIcS5kODoAmIYBkbVTsIbn8BoPAJAGMXr2mQesy6gXiWe9/vsCyn6QtDcM3yzawAFLJuYyZ3zipieGbtr2Gho6m1nf3sV+9uP0RUcGoBF5bhSWZVXet6bYZ8pq0XjjrlFXF2czf9uPMQrh+pMx+0Wjc9fPm3c5ymEEEIIIYQQQpwJCY4uAIZhYNTsJbzhCYz6SMNqHVinTeCPllKqNc+4zMtm0blyso2Pll5CXpJ71O+vM9DDgfZj7G+vpLmv85Tn57nTuaVo6ag2wz5TaW4HX10xixtn5PHjtQc40hpZrvbvl5WQkzT6/ZeEEEIIIYQQQojzSYKjcabXHohUGNXuB8AANmkF/MFSyhEtbdhrrJrBFcXd3DChD5szRLdh4Ato9AYVvoAW+Qhq9J74PKAGvu4NavQGLQPjw9UZue1hrizu5qop3cxOfy9O6+iFRr2hAOWd1exrq6Km5+Q7xQG4rA6mJRcyw1NEjiv1gq3gmZ3t4be3XUp5cycJDhv5yaMfvAkhhBBCCCGEEOebBEfjRK8/RHjD4xjVe4BIYLRd5fIH60LKtOH74GjKYOmkHq6b4WWiZyqZcUsARV+4jZ7unfT4K+i1WTBOM0zRDdADNvy1iooGxY54B4YGSe4QWAy2tLmo9O0h2RGPxx5PsiMBjz2eJHs8dsvZP3VCepgK73H2t1dxxFtH2NBPer5Ns1CclMcMTxETErKxqLFpxn2uLJoas6V9QgghhBBCCCHEaJDgaIzpjUcJvflHqNs3MLZHZfGodSF7texhr1HKYEmRjxtmdpIeb5DlWkqyYwoARqAdZ/0rOLsrSAV0pfDZ7fQ4HPQ4nQSsI/8TawoMW5DyJDvVVgseotvb6yi6gjpdwXroir3WbXVGA6UToZIj8rnb6oypBNINfaDJdXlHzbBNrk2PGcWEhCxmeIqYkpSH3WI76flCCCGEEEIIIYQ4/yQ4GiPh+gqC//w1WueRgbGDKp0/WBeyXcsb8bqFBT3cMMtLVmIIq3KRF7+COGsGhh6C1g3Q/DYY0cBHMwzi/X7iE/MgNYcQQbqDvfQEffSEetEHVff0BBVbmh10Bs+8gqcn1EdPqI/jPS0xx2yaJRIm2RNIdkSWaJV1VNMV7D3l7Wa7UpnpKWJacgFuW9wZz0sIIYQQQgghhBDnjwRHo8gwDAI7NxBa9xdseiMn4pkKlcofLaVstBSOeO3cPB83zvSS54lU5sRZM8lzX41FxWF07oemNyHQGnuhMwEK5qBskYofGzY8DhseRyKhviAH/rIL18IpNKTrbGzqJWSYK4M0ZWBVENDPvndQUA/T3Nd5Wg2uATyOBGZ4ipjhKSTFcf53jhNCCCGEEEIIIcTZkeBoFPme+gXWmtew9S/bOqaS+aNlAWstk0a8ZkZ2LzfN8lKUGhgYS3ZMI9O5CNVVDs3rwN8ce6HSIHcmJCYxXIujhrXV7PnhDhY+dC+7E9rY2HgAMJ8Yb9VZlO4nPc5DtutavIE+2gNddAS66fB30+7vpiPQTWegB2PYttqnz2V1Mj25gBmeIrIv4CbXQgghhBBCCCHEu5kER6PIcdm1hB9/jTqVwGPWBbyuTUYfobHz1IwQN81uZXJ6YNCoRlbcJST7w1D3G/DHLgsDIDELcqei/n97dxsjV3Xfcfz7n5l9mt21vX7Yxdm1vca4DkQCImwEBQK1QiCE1mmFUKtWokkU3iRRqraKSF4UtVUqRZXatFXeVClqlJKkiDQJahMVQxGpSgMYnGADDUmA9QNrb2yvHxbbuzszpy/m4u56BvDaM56O/f1Io7n33HvnnivrL9/9zblncrWTTE9PnuDZzz3OzOF+rv36/Xz/4HbGJvbXnn/xKj44vIF8lCgWhojI01PoYag4ULNvOVU4OnOcwzPHqoFSFiwdnplicvoYM5VSzTEAHbkCv3JqkutLyLXJJNeSJEmSJF2sDI6a6GD/Sr7acRtb86vfITDq5o737WXD0PF5I4UK0cNwWkvP7idg5m0Co3wnrL6GKBaA2tDo1W+9yDN/uJXLP303A5+5la+PPclUaf48Q0Gw+T3vZ9OKDWc86icfOQa6+hjo6oP++dtSSpwoT58anXR4eorp8ixDxQHWLxo5p19jkyRJkiRJ55d/xTfRc3sO8e+F0brb1i/v566rE2uWv1zzaFkPfbzn4EE6Tj5We2AEdBZh2eWwpI+Ics0uU2NH+O9P/YCJp/bxgQc/z/g1i/nGLx6vebysr9DDR0dvYFXf4NleYp3uBcVCN8VCN8O9yxv2uZIkSZIk6fwzOGqiD21YyT89/yp7jhw/1TY60Mvvbxrl0sGdnCjvqTlm8ckyQ4deqU6kXeiE7t7qq+ut9+KckUHzQ6NKucJLf/sM2//kSfrWjnDbM3/HD/Nj/PSN7TXnWdM3xJY1v+ovl0mSJEmSpLdlcNREhVyOj21ax58/toPhxUU+tmkdN67tZvz4Y5woH52/c4Kh2Ur18a/RIejuJRbwWNfB7fv4r3v/jYPPjTN69y1s+MoneXjiWSanjtXse/3gFXxg5ZXOMSRJkiRJkt6RwVGTbb5sJbkIbr50iBPlMXZPPUqF+ZNH5yPPSP8QxcLCR/+Ujs+y/f4nefHLT0MKNn7pXvj4Jh7c8ySlNH9EUleug19fcz3rF4+c0zVJkiRJkqSLg8FRU82Qz+1j82VTHDi5jQMna3/NrDvfxUjfJXTkzvyfYmrXESZfmODA8+P87IGf8OauI3Qt7efGb36Bl94L23f/qOaYoZ4BfnP0puqIJkmSJEmSpDNgcNRE5emT0PkKb7w5wdTs8Zrtizv7uaS4/G0fGZs5Os3kjgkmd04w+cJEdXnHBDNHpuftt/SqdWx8+D4eLb3CvoOHaj7nqqXruHXkmgWFU5IkSZIkSSYJTTR1+BC/7NzD7GmPpgEM9SxnoGsREUFKiZkDR5n40TgTT49XA6IXJpgaO/KOnx/5HJfdcxuDf3EXD+3fxsnyzLzthchz28hGrly2rqHXJUmSJEmSLg4GR01U6nqN2XT6fEY5hnO9FKeOkfaP8/oPxnj+L1/k6Gu1I5IAupYtonfVIL2rB+nL3ntXraB39RD964fZVtrFt994qua4JZ19/NboTQwVB5pybZIkSZIk6cJncNREAx3reGTXbk6mIB/QSYW+2Rleqkxy4rU3ObzzGFHuo/uTN9PdW6TY30fPkn56B5bQu3yAvsEBuotFOnIFCpGnI1egI5enkMszUy7xvV1P8fqxfTXnXb9omDtXX093obMFVy1JkiRJki4UBkdNlOtdzeSJYDqXz1ryQEd1ce0iWLuyzlEnqq/Zcdi7sPMFwc0rr+K6wcuJiLPvuCRJkiRJEgZHTVfOd0CqNP08vYVutqy5gTX9Q00/lyRJkiRJujgYHDVRSonSeQiNVvWuYMvoDfR3FJt+LkmSJEmSdPEwOGqyT2y4g1KlxGylzGwqUaqUma2UT7WVUpnZSonZymnbUjlbL516n9tWThUKkWfTig3ctPJK8pFr9aVKkiRJkqQLjMFRE0UEgz1LmvLZlVQhCOcykiRJkiRJTWNw1KZyjjCSJEmSJElNZvogSZIkSZKkugyOJEmSJEmSVJfBkSRJkiRJkuoyOJIkSZIkSVJdBkeSJEmSJEmqy+BIkiRJkiRJdRkcSZIkSZIkqS6DI0mSJEmSJNVlcCRJkiRJkqS6DI4kSZIkSZJUl8GRJEmSJEmS6jI4kiRJkiRJUl0GR5IkSZIkSarL4EiSJEmSJEl1GRxJkiRJkiSpLoMjSZIkSZIk1RUppVb34W1FxC+BsVb3o0GWAwda3QnpAmAtSY1jPUmNYS1JjWM9SY3xbrW0JqW04kw+6P91cHQhiYhtKaWNre6H1O6sJalxrCepMawlqXGsJ6kxGllLPqomSZIkSZKkugyOJEmSJEmSVJfB0fnz963ugHSBsJakxrGepMawlqTGsZ6kxmhYLTnHkSRJkiRJkupyxJEkSZIkSZLqMjg6DyLi9oj4aUT8PCLua3V/pHYREQ9ExERE7JzTtjQitkbEz7L3gVb2UWoHEbEqIp6IiJci4sWI+GzWbj1JCxQR3RHxTET8JKunP83a10bE09n93j9HRGer+yq1g4jIR8T2iPjXbN1akhYoIl6PiB0R8eOI2Ja1New+z+CoySIiD3wF+DBwBfA7EXFFa3sltY1/BG4/re0+4PGU0nrg8Wxd0jsrAX+UUroCuA74VPZ/kfUkLdw0sDmldBVwNXB7RFwHfAn465TSZcAk8IkW9lFqJ58FXp6zbi1JZ+fXUkpXp5Q2ZusNu88zOGq+a4Gfp5ReTSnNAN8CtrS4T1JbSCn9EDh0WvMW4GvZ8teAj57XTkltKKU0nlJ6Pls+RvUGfRjrSVqwVDWVrXZkrwRsBh7O2q0n6QxExAjwEeCr2XpgLUmN0rD7PIOj5hsGds9Z35O1STo7Qyml8Wx5HzDUys5I7SYiRoH3A09jPUlnJXu05sfABLAV+AVwOKVUynbxfk86M18GPgdUsvVlWEvS2UjAoxHxXETcm7U17D6vcK69k6RWSSmliPCnIaUzFBF9wLeBP0gpHa1+sVtlPUlnLqVUBq6OiCXAd4D3trhLUtuJiDuBiZTScxFxS6v7I7W5G1NKeyNiENgaEf8zd+O53uc54qj59gKr5qyPZG2Szs7+iFgJkL1PtLg/UluIiA6qodGDKaV/yZqtJ+kcpJQOA08A1wNLIuKtL2W935Pe3Q3Ab0TE61Sn89gM/A3WkrRgKaW92fsE1S80rqWB93kGR833LLA++3WATuC3gUda3CepnT0C3JMt3wN8r4V9kdpCNmfEPwAvp5T+as4m60laoIhYkY00IiJ6gFupzhv2BHBXtpv1JL2LlNLnU0ojKaVRqn8j/UdK6XexlqQFiYjeiOh/axn4ELCTBt7nRUqOSm+2iLiD6vO7eeCBlNIXW9wlqS1ExDeBW4DlwH7gfuC7wEPAamAMuDuldPoE2pLmiIgbgf8EdvB/80h8geo8R9aTtAARcSXVSUbzVL+EfSil9GcRcSnVURNLge3A76WUplvXU6l9ZI+q/XFK6U5rSVqYrGa+k60WgG+klL4YEcto0H2ewZEkSZIkSZLq8lE1SZIkSZIk1WVwJEmSJEmSpLoMjiRJkiRJklSXwZEkSZIkSZLqMjiSJEmSJElSXQZHkiRJkiRJqsvgSJIkSZIkSXUZHEmSJEmSJKmu/wU0dWDng9UYqQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "ppo_updates = 0\n",
    "save_interval = 5\n",
    "#Plotting Flags\n",
    "indvplots=0\n",
    "rewplots=1\n",
    "stdplots=1\n",
    "which_plts = [indvplots,rewplots,stdplots]\n",
    "\n",
    "test_avg_rewards = []\n",
    "test_stds = []\n",
    "\n",
    "max_frames = 100000\n",
    "compensator_test = compensator(num_inputs, num_outputs, ppo_baseline)\n",
    "\n",
    "while compensator_test.ppo_compensator.frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    #collect data\n",
    "    log_probs, values, states, actions, rewards, masks, next_value = compensator_test.collect_data(envs)\n",
    "    \n",
    "    #compute gae\n",
    "    returns = compensator_test.ppo_compensator.compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    #update policy\n",
    "    compensator_test.ppo_compensator.ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, values)\n",
    "    \n",
    "    #plot\n",
    "    avg_rew = []\n",
    "    std = []\n",
    "    \n",
    "    #Environment testing and data logging\n",
    "    #***************************************************************************************\n",
    "    for env in tests.envs:\n",
    "        env_rewards = ([tests.test_env(env, compensator_test) for _ in range(test_itrs)])\n",
    "        avg_rew.append(np.mean(env_rewards))\n",
    "        std.append(np.std(env_rewards))\n",
    "\n",
    "    test_avg_rewards.append(avg_rew)\n",
    "    test_stds.append(std)\n",
    "\n",
    "    if avg_rew[training_env_index] > threshold_reward and EARLY_STOPPING: #avg_rew[0] is testing on the non edited environment\n",
    "        tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, 1)\n",
    "        early_stop = True\n",
    "    else:\n",
    "        if ppo_updates and ppo_updates % save_interval == 0:\n",
    "            ppo_baseline.save_weights(baseline_dir + env_name + '_weights' + str(ppo_updates/save_interval))\n",
    "            tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, 1, str(ppo_updates/save_interval))\n",
    "        else:\n",
    "            tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, 0)\n",
    "            \n",
    "    ppo_updates = ppo_updates + 1 #Loop counter\n",
    "    #***************************************************************************************\n",
    "\n",
    "ppo_baseline.save_weights(baseline_dir + env_name + '_compensator_endweights')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train compensator with action appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "state = envs.reset()\n",
    "early_stop = False\n",
    "ppo_updates = 0\n",
    "save_interval = 5\n",
    "#Plotting Flags\n",
    "indvplots=0\n",
    "rewplots=1\n",
    "stdplots=1\n",
    "which_plts = [indvplots,rewplots,stdplots]\n",
    "\n",
    "test_avg_rewards = []\n",
    "test_stds = []\n",
    "\n",
    "compensator_test = compensator(num_inputs, num_outputs, ppo_baseline, action_appended = True)\n",
    "\n",
    "while compensator_test.ppo_compensator.frame_idx < max_frames and not early_stop:\n",
    "\n",
    "    #collect data\n",
    "    log_probs, values, states, actions, rewards, masks, next_value = compensator_test.collect_data(envs)\n",
    "    \n",
    "    #compute gae\n",
    "    returns = compensator_test.ppo_compensator.compute_gae(next_value, rewards, masks, values)\n",
    "    \n",
    "    #update policy\n",
    "    compensator_test.ppo_compensator.ppo_update(ppo_epochs, mini_batch_size, states, actions, log_probs, returns, values)\n",
    "    \n",
    "    #plot\n",
    "    avg_rew = []\n",
    "    std = []\n",
    "    \n",
    "    #Environment testing and data logging\n",
    "    #***************************************************************************************\n",
    "    for env in tests.envs:\n",
    "        env_rewards = ([tests.test_env(env, compensator_test) for _ in range(test_itrs)])\n",
    "        avg_rew.append(np.mean(env_rewards))\n",
    "        std.append(np.std(env_rewards))\n",
    "\n",
    "    test_avg_rewards.append(avg_rew)\n",
    "    test_stds.append(std)\n",
    "\n",
    "    if avg_rew[training_env_index] > threshold_reward and EARLY_STOPPING: #avg_rew[0] is testing on the non edited environment\n",
    "        tests.plot(ppo_baseline.frame_idx, test_avg_rewards, test_stds, which_plts, 1)\n",
    "        early_stop = True\n",
    "    else:\n",
    "        if ppo_updates and ppo_updates % save_interval == 0:\n",
    "            ppo_baseline.save_weights(baseline_dir + env_name + '_weights' + str(ppo_updates/save_interval))\n",
    "            tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, 1, str(ppo_updates/save_interval))\n",
    "        else:\n",
    "            tests.plot(compensator_test.ppo_compensator.frame_idx, test_avg_rewards, test_stds, which_plts, 0)\n",
    "            \n",
    "    ppo_updates = ppo_updates + 1 #Loop counter\n",
    "    #***************************************************************************************\n",
    "\n",
    "ppo_baseline.save_weights(baseline_dir + env_name + '_compensatorAction_endweights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
